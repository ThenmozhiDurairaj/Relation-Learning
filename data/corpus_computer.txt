Computer is connected to a network which allows us to easily share files and data.
Computer graphics are graphics that are created using computers and the representation of image data by a computer specifically with help from specialized graphic hardware and software.
A quantum computer is built by NSA that could crack encryption.
Computer hardware is the collection of physical elements that constitutes a computer. 
Computer hardware refers to the physical parts or components of a computer such as the monitor, mouse, keyboard, computer data storage, hard drive disk (HDD), system unit (graphic cards, sound cards, memory, motherboard and chips), etc. all of which are physical objects that can be touched.
In contrast, software is instructions that can be stored and run by hardware.
Computer Hardware is the physical part of a computer, as distinguished from the computer software that executes or runs on the hardware. 
The hardware of a computer is infrequently changed, while software and data are modified frequently.
It is important that you understand what is your computer made up of and that includes the hardware components. 
Networking hardware constitutes all computers, peripherals, interface cards and other equipment needed to perform data-processing and communications within the network. 
The computer is one wonderful technology and in today’s’ world hardly any of us can do without it.
Software means computer instructions or data. 
Anything that can be stored electronically is software, in contrast to storage devices and display devices which are called hardware.
The system software of a computer is a program that runs on top of hardware components in a computer. 
Without the system software, it is extremely difficult to make use of the computer. 
One of the basic functions of the system software is to save information from the computer memory to an external device and to recognize text from external devices, such as the keyboard, onto the monitor. 
Computer software, or simply software, also known as computer programs, is the non-tangible component of computers. 
Computer software contrasts with computer hardware, which is the physical component of computers. Computer hardware and software require each other and neither can be realistically used without the other.
Algorithms represent a set of instructions that are used by computers. 
Shor's algorithm, named after mathematician Peter Shor, is a quantum algorithm that runs on a quantum computer for integer factorization formulated in 1994. 
Informally it solves the following problem: Given an integer N, find its prime factors.
An algorithm is a method of solving problems both big and small. 
Though computers execute algorithms constantly, humans can also solve problems with algorithms.
How a computer executes algorithms
A computer is a machine which executes algorithms.
Can your computer run that game? Does your computer meet or exceed the system requirements?
I was reading about block ciphers and most articles state they are being used in symmetric key cryptography.
Block ciphers are used in public key cryptography, though typically as auxiliary building blocks rather than as the heart of the public key scheme by themselves.
A cryptographic system that uses two keys -- a public key known to everyone and a private or secret key known only to the recipient of the message.
Public-key cryptography, also known as asymmetric cryptography which requires public key.
Open source commonly refers to software that uses an open development process and is licsened to include the source code.
Generically, open source refers to a program in which the source code is available to the general public for use and/or modification from its original design.
Shareware is proprietary software that is provided to users on a limited basis and only for a certain limited trial basis and pursuant to a license which restricts any commercial benefit, use or exploitation of the software.
Shareware is software that you can use on a trial basis before paying for it. Unlike freeware, shareware often has limited functionality or may only be used for a limited time before requiring payment and registration.
Software becomes an abandonware and it is no longer profitable, obsolete, or because the owning company has gone out of business with out anyone purchasing the rights to the software.
Antivirus and Internet Security software for home or business. The world's fastest antivirus updates.
Antivirus and AV software are computer software used to prevent, detect and remove malicious computer viruses.
An ARM Company Keil Software makes C compilers, macro assemblers, real-time kernels, debuggers, & simulators.
The most downloaded Video Editing softwares includes VideoPad Video Editors and Reallusion CrazyTalk Pro.
PICAXE Programming Editors are the legacy softwares for programming, testing and simulating BASIC programs for PICAXE.
A device driver is software that allows your computer to communicate with hardware components or devices.
A device driver is software, provided by a hardware maker, that tells the computer's operating system and software how to work with that hardware.
A general phrase used to describe any software that provides security for a computer or network.
Sometimes abbreviated as WP, a word-processor is a software capable of creating, storing, and printing documents.
In mathematics and computer science, an algorithm is a self-contained step-by-step set of operations to be performed. 
Algorithms exist that perform calculation, data processing, and automated reasoning.
An algorithm is an effective method that can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function.
Starting from an initial state and initial input the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state. 
The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.
Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). 
Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. 
Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements. 
Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are often used as a way to define or document algorithms.
In computer systems, an algorithm is basically an instance of logic written in software by software developers to be effective for the intended "target" computer(s) to produce output from given input (perhaps null). 
An optimal algorithm, even running in old hardware, would produce faster results than a non optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why the algorithms, like computer hardware, are considered technology.
For a given function multiple algorithms may exist. 
This is true, even without expanding the available instruction set available to the programmer.
Rogers observes that "It is important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. 
The same function may have several different algorithms".
A computer is a restricted type of machine, a "discrete deterministic mechanical device" that blindly follows its instructions.
Melzak's and Lambek's primitive models reduced this notion to four elements  discrete, distinguishable locations, discrete, indistinguishable counters an agent, and a list of instructions that are effective relative to the capability of the agent.
In computer systems, an algorithm is basically an instance of logic written in software by software developers to be effective for the intended "target" computer(s) to produce output from given input (perhaps null). 
An optimal algorithm, even running in old hardware, would produce faster results than a non optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why the algorithms, like computer hardware, are considered technology.
It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm.
Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(n), using the big O notation with n as the length of the list. 
At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. 
Therefore it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.
Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. 
For example, a binary search algorithm usually outperforms a brute force sequential search when used for table lookups on sorted lists.
When a problem shows optimal substructures meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems â€” and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. 
For example, Floydâ€“Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. 
Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. 
The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls.
When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. 
By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.
A greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. 
Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. 
For some problems they can find the optimal solution while for others they stop at local optima, that is at solutions that cannot be improved by the algorithm but are not optimum. 
The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. 
Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.
In optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. 
These algorithms work by getting closer and closer to the optimal solution as they progress. 
In principle, if run for an infinite amount of time, they will find the optimal solution. 
Their merit is that they can find a solution very close to the optimal solution in a relatively short time. 
Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic.
When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.
When searching for optimal solutions to a linear function bound to linear equality and inequality constrains, the constrains of the problem can be used directly in producing the optimal solutions. 
There are algorithms that can solve any problem in this category, such as the popular simplex algorithm.
Problems that can be solved with linear programming include the maximum flow problem for directed graphs. 
If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. 
A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e. the solutions satisfy these restrictions anyway.
In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.
A divide and conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances are small enough to solve easily.
Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments.
The first step towards an understanding of why the study and knowledge of algorithms are so important is to define exactly what we mean by an algorithm. 
According to the popular algorithms textbook Introduction to Algorithms "an algorithm is any well-defined computational procedure that takes some value, or set of values, as input and produces some value, or set of values as output."
In other words, algorithms are like road maps for accomplishing a given, well-defined task. 
So, a chunk of code that calculates the terms of the Fibonacci sequence is an implementation of a particular algorithm. 
Even a simple function for adding two numbers is an algorithm in a sense, albeit a simple one. 
Some algorithms, like those that compute the Fibonacci sequences, are intuitive and may be innately embedded into our logical thinking and problem solving skills.
However, for most of us, complex algorithms are best studied so we can use them as building blocks for more efficient logical problem solving in the future. 
In fact, you may be surprised to learn just how many complex algorithms people use every day when they check their e-mail or listen to music on their computers. 
This article will introduce some basic ideas related to the analysis of algorithms, and then put these into practice with a few examples illustrating why it is important to know about algorithms. 
One of the most important aspects of an algorithm is how fast it is. 
It is often easy to come up with an algorithm to solve a problem, but if the algorithm is too slow, itâ€™s back to the drawing board. 
Since the exact speed of an algorithm depends on where the algorithm is run, as well as the exact details of its implementation, computer scientists typically talk about the runtime relative to the size of the input. 
For example, if the input consists of N integers, an algorithm might have a runtime proportional to N2, represented as O(N2). This means that if you were to run an implementation of the algorithm on your computer with an input of size N, it would take C*N2 seconds, where C is some constant that doesnâ€™t change with the size of the input. 
However, the execution time of many complex algorithms can vary due to factors other than the size of the input. 
For example, a sorting algorithm may run much faster when given a set of integers that are already sorted than it would when given the same set of integers in a random order. 
As a result, you often hear people talk about the worst-case runtime, or the average-case runtime. The worst-case runtime is how long it would take for the algorithm to run if it were given the most insidious of all possible inputs. 
The average-case runtime is the average of how long it would take the algorithm to run if it were given all possible inputs. Of the two, the worst-case is often easier to reason about, and therefore is more frequently used as a benchmark for a given algorithm. 
The process of determining the worst-case and average-case runtimes for a given algorithm can be tricky, since it is usually impossible to run an algorithm on all possible inputs.
Sorting provides a good example of an algorithm that is very frequently used by computer scientists. 
The simplest way to sort a group of items is to start by removing the smallest item from the group, and put it first. 
Then remove the next smallest, and put it next and so on. Unfortunately, this algorithm is O(N2), meaning that the amount of time it takes is proportional to the number of items squared. 
If you had to sort a billion things, this algorithm would take around 1018 operations. 
To put this in perspective, a desktop PC can do a little bit over 109 operations per second, and would take years to finish sorting a billion things this way. 
Luckily, there are a number of better algorithms (quicksort, heapsort and mergesort, for example) that have been devised over the years, many of which have a runtime of O(N * Log(N)). 
This brings the number of operations required to sort a billion items down to a reasonable number that even a cheap desktop could perform. Instead of a billion squared operations (1018) these algorithms require only about 10 billion operations (1010), a factor of 100 million faster. 
Sometimes, however, even the most advanced algorithm, with the most advanced heuristics, on the fastest computers is too slow. 
In this case, sacrifices must be made that relate to the correctness of the result. 
Rather than trying to get the shortest path, a programmer might be satisfied to find a path that is at most 10% longer than the shortest path. 
In fact, there are quite a few important problems for which the best-known algorithm that produces an optimal answer is insufficiently slow for most purposes. 
The most famous group of these problems is called NP, which stands for non-deterministic polynomial (donâ€™t worry about what that means). 
When a problem is said to be NP-complete or NP-hard, it mean no one knows a good way to solve them optimally. 
Furthermore, if someone did figure out an efficient algorithm for one NP-complete problem, that algorithm would be applicable to all NP-complete problems. 
Algorithms for finding the shortest path from one point to another have been researched for years. 
Applications abound, but lets keep things simple by saying we want to find the shortest path from point A to point B in a city with just a few streets and intersections. 
There are quite a few different algorithms that have been developed to solve such problems, all with different benefits and drawbacks. 
Before we delve into them though, lets consider how long a naive algorithm â€“ one that tries every conceivable option â€“ would take to run. 
If the algorithm considered every possible path from A to B (that didnâ€™t go in circles), it would not finish in our lifetimes, even if A and B were both in a small town. 
The runtime of this algorithm is exponential in the size of the input, meaning that it is O(CN) for some C. 
Even for small values of C, CN becomes astronomical when N gets even moderately large. 
One of the fastest algorithms for solving this problem has a runtime of O(E*V*Log(V)), where E is the number of road segments, and V is the number of intersections. 
To put this in perspective, the algorithm would take about 2 seconds to find the shortest path in a city with 10,000 intersections, and 20,000 road segments (there are usually about 2 road segments per intersection). 
The algorithm, known as Djikstraâ€™s Algorithm, is fairly complex, and requires the use of a data structure known as a priority queue. 
In some applications, however, even this runtime is too slow (consider finding the shortest path from New York City to San Francisco â€“ there are millions of intersections in the US), and programmers try to do better by using what are known as heuristics. 
A heuristic is an approximation of something that is relevant to the problem, and is often computed by an algorithm of its own. In the shortest path problem, for example, it is useful to know approximately how far a point is from the destination. 
Knowing this allows for the development of faster algorithms (such as A*, an algorithm that can sometimes run significantly faster than Djikstraâ€™s algorithm) and so programmers come up with heuristics to approximate this value. 
Doing so does not always improve the runtime of the algorithm in the worst case, but it does make the algorithm faster in most real-world applications. 
As a computer scientist, it is important to understand all of these types of algorithms so that one can use them properly. 
If you are working on an important piece of software, you will likely need to be able to estimate how fast it is going to run. 
Such an estimate will be less accurate without an understanding of runtime analysis. 
Furthermore, you need to understand the details of the algorithms involved so that youâ€™ll be able to predict if there are special cases in which the software wonâ€™t work quickly, or if it will produce unacceptable results. 
Another class of algorithm deals with situations such as data compression. 
This type of algorithm does not have an expected output (like a sorting algorithm), but instead tries to optimize some other criteria. 
In the case of data compression, the algorithm (LZW, for instance) tries to make the data use as few bytes as possible, in such a way that it can be decompressed to its original form. 
In some cases, this type of algorithm will use the same techniques as other algorithms, resulting in output that is good, but potentially sub-optimal. 
JPG and MP3 compression, for example, both compress data in a way that makes the final result somewhat lower quality than the original, but they create much smaller files. 
MP3 compression does not retain every feature of the original song file, but it attempts to maintain enough of the details to capture most of the quality, while at the same time ensuring the significantly reduced file size that we all know and love. 
The JPG image file format follows the same principle, but the details are significantly different since the goal is image rather than audio compression. 
Algorithms for finding the shortest path from one point to another have been researched for years. 
Applications abound, but lets keep things simple by saying we want to find the shortest path from point A to point B in a city with just a few streets and intersections. 
There are quite a few different algorithms that have been developed to solve such problems, all with different benefits and drawbacks. Before we delve into them though, lets consider how long a naive algorithm â€“ one that tries every conceivable option â€“ would take to run. 
If the algorithm considered every possible path from A to B (that didnâ€™t go in circles), it would not finish in our lifetimes, even if A and B were both in a small town. 
The runtime of this algorithm is exponential in the size of the input, meaning that it is O(CN) for some C. Even for small values of C, CN becomes astronomical when N gets even moderately large. 
One of the fastest algorithms for solving this problem has a runtime of O(E*V*Log(V)), where E is the number of road segments, and V is the number of intersections. 
To put this in perspective, the algorithm would take about 2 seconds to find the shortest path in a city with 10,000 intersections, and 20,000 road segments (there are usually about 2 road segments per intersection). 
The algorithm, known as Djikstraâ€™s Algorithm, is fairly complex, and requires the use of a data structure known as a priority queue. 
In some applications, however, even this runtime is too slow (consider finding the shortest path from New York City to San Francisco â€“ there are millions of intersections in the US), and programmers try to do better by using what are known as heuristics.
A heuristic is an approximation of something that is relevant to the problem, and is often computed by an algorithm of its own. 
In the shortest path problem, for example, it is useful to know approximately how far a point is from the destination. Knowing this allows for the development of faster algorithms (such as A*, an algorithm that can sometimes run significantly faster than Djikstraâ€™s algorithm) and so programmers come up with heuristics to approximate this value. 
Doing so does not always improve the runtime of the algorithm in the worst case, but it does make the algorithm faster in most real-world applications. 
Of course, there are often times when youâ€™ll run across a problem that has not been previously studied. 
In these cases, you have to come up with a new algorithm, or apply an old algorithm in a new way. 
The more you know about algorithms in this case, the better your chances are of finding a good way to solve the problem. 
In many cases, a new problem can be reduced to an old problem without too much effort, but you will need to have a fundamental understanding of the old problem in order to do this. 
As an example of this, lets consider what a switch does on the Internet. 
A switch has N cables plugged into it, and receives packets of data coming in from the cables. 
The switch has to first analyze the packets, and then send them back out on the correct cables. 
A switch, like a computer, is run by a clock with discrete steps â€“ the packets are send out at discrete intervals, rather than continuously. 
In a fast switch, we want to send out as many packets as possible during each interval so they donâ€™t stack up and get dropped. 
The goal of the algorithm we want to develop is to send out as many packets as possible during each interval, and also to send them out so that the ones that arrived earlier get sent out earlier. 
In this case it turns out that an algorithm for a problem that is known as "stable matching" is directly applicable to our problem, though at first glance this relationship seems unlikely. 
Only through pre-existing algorithmic knowledge and understanding can such a relationship be discovered. 
Many coders go their entire careers without ever having to implement an algorithm that uses dynamic programming. 
However, dynamic programming pops up in a number of important algorithms.
One algorithm that most programmers have probably used, even though they may not have known it, finds differences between two sequences. 
More specifically, it calculates the minimum number of insertions, deletions, and edits required to transform sequence A into sequence B. 
The different algorithms that people study are as varied as the problems that they solve.
However, chances are good that the problem you are trying to solve is similar to another problem in some respects. 
By developing a good understanding of a large range of algorithms, you will be able to choose the right one for a problem and apply it properly. 
Furthermore, solving problems like those found in TopCoderâ€™s competitions will help you to hone your skills in this respect.
Many of the problems, though they may not seem realistic, require the same set of algorithmic knowledge that comes up every day in the real world.
The maximum flow problem has to do with determining the best way to get some sort of stuff from one place to another, through a network of some sort. 
In more concrete terms, the problem first arose in relation to the rail networks of the Soviet Union, during the 1950â€²s. 
The US wanted to know how quickly the Soviet Union could get supplies through its rail network to its satellite states in Eastern Europe. 
In addition, the US wanted to know which rails it could destroy most easily to cut off the satellite states from the rest of the Soviet Union. 
It turned out that these two problems were closely related, and that solving the max flow problem also solves the min cut problem of figuring out the cheapest way to cut off the Soviet Union from its satellites. 
The first efficient algorithm for finding the maximum flow was conceived by two Computer Scientists, named Ford and Fulkerson. The algorithm was subsequently named the Ford-Fulkerson algorithm, and is one of the more famous algorithms in computer science.
In the last 50 years, a number of improvements have been made to the Ford-Fulkerson algorithm to make it faster, some of which are dauntingly complex. 
Since the problem was first posed, many additional applications have been discovered. 
The algorithm has obvious relevance to the Internet, where getting as much data as possible from one point to another is important. 
It also comes up in many business settings, and is an important part of operations research. 
For example, if you have N employees and N jobs that need to be done, but not every employee can do every job, the max flow algorithm will tell you how to assign your N employees to jobs in such a way that every job gets done, provided thatâ€™s possible. 
Graduation, from SRM 200, is a good example of a TopCoder problem that lends itself to a solution using max flow. 
Algorithms are one of the few mathematical/ logical constructs whose theoretical construct is instantiated in practical application by billions of people each day via the computer. 
Understanding the historical background, formulation, analysis, and legal issues regarding algorithms is useful for those in the computer science Field and curious outsiders in gaining an appreciation for what a computer can do as well as its limitations.
The word algorithm originated in ancient times to identify discrete and distinguishable symbols in mathematics and using them in arithmetic, based on a defined procedure. 
Eventually, unary numerical systems appeared during the times of the Romans and the abacus was introduced. 
Gottfried Liebniz contributed to development with his notion of calculus ratiocinator, which equated algebra with logic and claimed that there is a method by which logical concepts can be manipulated in the same way that mathematical concepts are. 
Scholars believe that the Jacquard loom and telephone switching technologies where the first fundamental developments leading to the creation of the first computers.
Mathematically, the development of algorithms is credited to the work of George Boole, Gottlob Frege, and Giuseppe Peano. 
Between them, they were able to reduce arithmetic to a collection of symbols manipulated by rules. 
Doing this however, resulted in a number of unsettling paradoxes such as Russell's paradox and Godel's paradox of the liar.
Algorithms also faced an issue of 'entscheidungsproblem', which is a problem of effective calculatablity. 
In this problem, an algorithm accepts as an input, some description of a formal language and a statement in that language. 
The algorithm must then produce a 'true' or 'false' statement based on the statement being true or false. 
It is possible for the algorithm to decide on problems for which there is no proof or disproof. 
It was later shown by Emil Post and Alan Turning that the entscheidungsproblem was unsolvable.
The next advancement in the history of algorithms is Alan Turning's 'turning machine' this anticipated the creation of computers. 
The first computer was then created in the early twentieth century, with ongoing development to software and hardware, particularly with the advent of the silicone chip.
As was explained algorithms specify instructions for a task that needs to be preformed, often times by computers.
Typically, data is read from some input, then it is written as some output or it is stored in memory. 
Often algorithms are not merely linear but the 'instructions' branch out to include a wide array of difference situations that may arise, this is formally described as flow of control. 
Algorithms are expressed in a variety of ways, most intuitively natural languages, which are spoken languages. 
The other way that they are commonly expressed is through artificial languages such as mathematics and programming languages. 
They are also expressed, for learning purposes using flowcharts to illustrate key algorithmic concepts in academic settings.
Algorithm analysis can take on a number of forms, depending on the medium, or language, that it is expressed in. 
In computer science binary search is superior to sequential search. 
Regarding analysis of programming languages it is not needed to analyze a specific language to conduct an analysis, since programming languages can be generalized in pseudo-code. 
The levels of description are high level description, implementation description, and formal description. 
Pseudo-code is a non-formal caricature of a programming language that one wishes to test. 
Normally, during the testing of a program testing is first done with pseudo-code and then with the actual code that is being tested. 
Overall, program optimization is a complex process that is gradually revised and refined throughout testing, at varying levels of complexity.
The concept becomes more precise with the use of variables in mathematics.
Algorithm in the sense of what is now used by computers appeared as soon as first mechanical engines were invented.
The word algorithm comes from the name of the 9th century Persian Muslim mathematician Abu Abdullah Muhammad ibn Musa Al-Khwarizmi. 
The word algorism originally referred only to the rules of performing arithmetic using Hindu-Arabic numerals but evolved via European Latin translation of Al-Khwarizmi's name into algorithm by the 18th century. 
The use of the word evolved to include all definite procedures for solving problems or performing tasks.
The work of the ancient Greek geometers, Persian mathematician Al-Khwarizmi -- often considered as the "father of algebra", Chinese and Western European mathematicans culiminated in Leibniz' notion of the "calculus ratiocinator", an algebra of logic.
"Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book... I assume then that the computation is carried out on one-dimensional paper, i.e. on a tape divided into squares. 
I shall also suppose that the number of symbols which may be printed is finite"
"The behavior of the computer at any moment is determined by the symbols which he is observing, and his "state of mind" at that moment.
We may suppose that there is a bound B to the number of symbols or squares which the computer can observe at one moment. 
If he wishes to observe more, he must use successive observations. 
We will also suppose that the number of states of mind which need be taken into account is finite"
'Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps. 
With this special meaning, three different precise definitions have been given to date.
The simplest of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. 
All three definitions are equivalent, so it doesn't matter which one is used. 
Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one."
The concept of algorithm was formalized in 1936 through Alan Turing's Turing machines and Alonzo Church's lambda calculus, which in turn formed the foundation of computer science.
"Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further divided".
Concisely stated, a genetic algorithm is a programming technique that mimics biological evolution as a problem-solving strategy. 
Given a specific problem to solve, the input to the genetic algorithm is a set of potential solutions to that problem, encoded in some fashion, and a metric called a fitness function that allows each candidate to be quantitatively evaluated. 
These candidates may be solutions already known to work, with the aim of the genetic algorithm being to improve them, but more often they are generated at random.
The genetic algorithm then evaluates each candidate according to the fitness function. 
In a pool of randomly generated candidates, of course, most will not work at all, and these will be deleted. 
However, purely by chance, a few may hold promise - they may show activity, even if only weak and imperfect activity, toward solving the problem.
These promising candidates are kept and allowed to reproduce. 
Multiple copies are made of them, but the copies are not perfect; random changes are introduced during the copying process. 
These digital offspring then go on to the next generation, forming a new pool of candidate solutions, and are subjected to a second round of fitness evaluation. 
Those candidate solutions which were worsened, or made no better, by the changes to their code are again deleted; but again, purely by chance, the random variations introduced into the population may have improved some individuals, making them into better, more complete or more efficient solutions to the problem at hand. 
Again these winning individuals are selected and copied over into the next generation with random changes, and the process repeats. 
The expectation is that the average fitness of the population will increase each round, and so by repeating this process for hundreds or thousands of rounds, very good solutions to the problem can be discovered.
As astonishing and counterintuitive as it may seem to some, genetic algorithms have proven to be an enormously powerful and successful problem-solving strategy, dramatically demonstrating the power of evolutionary principles. 
Genetic algorithms have been used in a wide variety of fields to evolve solutions to problems as difficult as or more difficult than those faced by human designers. 
Moreover, the solutions they come up with are often more efficient, more elegant, or more complex than anything comparable a human engineer would produce. 
In some cases, genetic algorithms have come up with solutions that baffle the programmers who wrote the algorithms in the first place!
Before a genetic algorithm can be put to work on any problem, a method is needed to encode potential solutions to that problem in a form that a computer can process. 
One common approach is to encode solutions as binary strings: sequences of 1's and 0's, where the digit at each position represents the value of some aspect of the solution. 
Another, similar approach is to encode solutions as arrays of integers or decimal numbers, with each position again representing some particular aspect of the solution. 
This approach allows for greater precision and complexity than the comparatively restricted method of using binary numbers only and often "is intuitively closer to the problem space".
This technique was used, for example, in the work of Steffen Schulze-Kremer, who wrote a genetic algorithm to predict the three-dimensional structure of a protein based on the sequence of amino acids that go into it.
Schulze-Kremer's genetic algorithm used real-valued numbers to represent the so-called "torsion angles" between the peptide bonds that connect amino acids. 
A protein is made up of a sequence of basic building blocks called amino acids, which are joined together like the links in a chain. 
Once all the amino acids are linked, the protein folds up into a complex three-dimensional shape based on which amino acids attract each other and which ones repel each other. 
The shape of a protein determines its function.) Genetic algorithms for training neural networks often use this method of encoding also.
A third approach is to represent individuals in a genetic algorithm as strings of letters, where each letter again stands for a specific aspect of the solution. 
One example of this technique is Hiroaki Kitano's "grammatical encoding" approach, where a GA was put to the task of evolving a simple set of rules called a context-free grammar that was in turn used to generate neural networks for a variety of problems.
The virtue of all three of these methods is that they make it easy to define operators that cause the random changes in the selected candidates: flip a 0 to a 1 or vice versa, add or subtract from the value of a number by a randomly chosen amount, or change one letter to another. 
Another strategy, developed principally by John Koza of Stanford University and called genetic programming, represents programs as branching data structures called trees. 
In this approach, random changes can be brought about by changing the operator or altering the value at a given node in the tree, or replacing one subtree with another.
It is important to note that evolutionary algorithms do not need to represent candidate solutions as data strings of fixed length. 
Some do represent them in this way, but others do not; for example, Kitano's grammatical encoding discussed above can be efficiently scaled to create large and complex neural networks, and Koza's genetic programming trees can grow arbitrarily large as necessary to solve whatever problem they are applied to.
A neural network, or neural net for short, is a problem-solving method based on a computer model of how neurons are connected in the brain.
A neural network consists of layers of processing units called nodes joined by directional links: one input layer, one output layer, and zero or more hidden layers in between. 
An initial pattern of input is presented to the input layer of the neural network, and nodes that are stimulated then transmit a signal to the nodes of the next layer to which they are connected. 
If the sum of all the inputs entering one of these virtual neurons is higher than that neuron's so-called activation threshold, that neuron itself activates, and passes on its own signal to neurons in the next layer. 
The pattern of activation therefore spreads forward until it reaches the output layer and is there returned as a solution to the presented input. 
Just as in the nervous system of biological organisms, neural networks learn and fine-tune their performance over time via repeated rounds of adjusting their thresholds until the actual output matches the desired output for any given input. 
This process can be supervised by a human experimenter or may run automatically using a learning algorithm. 
Genetic algorithms have been used both to build and to train neural networks.
Similar to genetic algorithms, though more systematic and less random, a hill-climbing algorithm begins with one initial solution to the problem at hand, usually chosen at random. 
The string is then mutated, and if the mutation results in higher fitness for the new solution than for the previous one, the new solution is kept; otherwise, the current solution is retained. 
The algorithm is then repeated until no mutation can be found that causes an increase in the current solution's fitness, and this solution is returned as the result.
To understand where the name of this technique comes from, imagine that the space of all possible solutions to a given problem is represented as a three-dimensional contour landscape.
A given set of coordinates on that landscape represents one particular solution. 
Those solutions that are better are higher in altitude, forming hills and peaks; those that are worse are lower in altitude, forming valleys. 
A "hill-climber" is then an algorithm that starts out at a given point on the landscape and moves inexorably uphill.) Hill-climbing is what is known as a greedy algorithm, meaning it always makes the best choice available at each step in the hope that the overall best result can be achieved this way. 
By contrast, methods such as genetic algorithms and simulated annealing, discussed below, are not greedy; these methods sometimes make suboptimal choices in the hopes that they will lead to better solutions later on.
Another optimization technique similar to evolutionary algorithms is known as simulated annealing. 
The idea borrows its name from the industrial process of annealing in which a material is heated to above a critical point to soften it, then gradually cooled in order to erase defects in its crystalline structure, producing a more stable and regular lattice arrangement of atoms. 
In simulated annealing, as in genetic algorithms, there is a fitness function that defines a fitness landscape; however, rather than a population of candidates as in genetic algorithm, there is only one candidate solution. 
Simulated annealing also adds the concept of "temperature", a global numerical quantity which gradually decreases over time. 
At each step of the algorithm, the solution mutates (which is equivalent to moving to an adjacent point of the fitness landscape). 
The fitness of the new solution is then compared to the fitness of the previous solution; if it is higher, the new solution is kept. 
Otherwise, the algorithm makes a decision whether to keep or discard it based on temperature. 
If the temperature is high, as it is initially, even changes that cause significant decreases in fitness may be kept and used as the basis for the next round of the algorithm, but as temperature decreases, the algorithm becomes more and more inclined to only accept fitness-increasing changes. 
Finally, the temperature reaches zero and the system "freezes"; whatever configuration it is in at that point becomes the solution. 
Simulated annealing is often used for engineering design applications such as determining the physical layout of components on a computer chip.
There are two main ways of achieving this. The first, which is used by most genetic algorithms, is to define individuals as lists of numbers - binary-valued, integer-valued, or real-valued - where each number represents some aspect of a candidate solution. 
If the individuals are binary strings, 0 or 1 could stand for the absence or presence of a given feature. 
If they are lists of numbers, these numbers could represent many different things: the weights of the links in a neural network, the order of the cities visited in a given tour, the spatial placement of electronic components, the values fed into a controller, the torsion angles of peptide bonds in a protein, and so on. 
Mutation then entails changing these numbers, flipping bits or adding or subtracting random values. 
In this case, the actual program code does not change; the code is what manages the simulation and keeps track of the individuals, evaluating their fitness and perhaps ensuring that only values realistic and possible for the given problem result.
In another method, genetic programming, the actual program code does change. 
As discussed in the section Methods of representation, GP represents individuals as executable trees of code that can be mutated by changing or swapping subtrees. 
Both of these methods produce representations that are robust against mutation and can represent many different kinds of problems, and as discussed in the section Some specific examples, both have had considerable success.
This issue of representing candidate solutions in a robust way does not arise in nature, because the method of representation used by evolution, namely the genetic code, is inherently robust: with only a very few exceptions, such as a string of stop codons, there is no such thing as a sequence of DNA bases that cannot be translated into a protein.
Therefore, virtually any change to an individual's genes will still produce an intelligible result, and so mutations in evolution have a higher chance of producing an improvement. 
This is in contrast to human-created languages such as English, where the number of meaningful words is small compared to the total number of ways one can combine letters of the alphabet, and therefore random changes to an English sentence are likely to produce nonsense.
The problem of how to write the fitness function must be carefully considered so that higher fitness is attainable and actually does equate to a better solution for the given problem. 
If the fitness function is chosen poorly or defined imprecisely, the genetic algorithm may be unable to find a solution to the problem, or may end up solving the wrong problem.
(This latter situation is sometimes described as the tendency of a generic algorithm to "cheat", although in reality all that is happening is that the generic algorithm is doing what it was told to do, not what its creators intended it to do.) 
An example of this can be found in Graham-Rowe 2002, in which researchers used an evolutionary algorithm in conjunction with a reprogrammable hardware array, setting up the fitness function to reward the evolving circuit for outputting an oscillating signal. 
At the end of the experiment, an oscillating signal was indeed being produced - but instead of the circuit itself acting as an oscillator, as the researchers had intended, they discovered that it had become a radio receiver that was picking up and relaying an oscillating signal from a nearby piece of electronic equipment.
In the laboratory of biological evolution there is only one fitness function, which is the same for all living things - the drive to survive and reproduce, no matter what adaptations make this possible. 
Those organisms which reproduce more abundantly compared to their competitors are more fit; those which fail to reproduce are unfit.
In addition to making a good choice of fitness function, the other parameters of a genetic algorithm - the size of the population, the rate of mutation and crossover, the type and strength of selection - must be also chosen with care. 
If the population size is too small, the genetic algorithm may not explore enough of the solution space to consistently find good solutions. 
If the rate of genetic change is too high or the selection scheme is chosen poorly, beneficial schema may be disrupted and the population may enter error catastrophe, changing too fast for selection to ever bring about convergence.
Living things do face similar difficulties, and evolution has dealt with them. 
It is true that if a population size falls too low, mutation rates are too high, or the selection pressure is too strong (such a situation might be caused by drastic environmental change), then the species may go extinct. 
The solution has been "the evolution of evolvability" - adaptations that alter a species' ability to adapt. 
For example, most living things have evolved elaborate molecular machinery that checks for and corrects errors during the process of DNA replication, keeping their mutation rate down to acceptably low levels.
conversely, in times of severe environmental stress, some bacterial species enter a state of hypermutation where the rate of DNA replication errors rises sharply, increasing the chance that a compensating mutation will be discovered. 
Of course, not all catastrophes can be evaded, but the enormous diversity and highly complex adaptations of living things today show that, in general, evolution is a successful strategy. 
Likewise, the diverse applications of and impressive results produced by genetic algorithms show them to be a powerful and worthwhile field of study.
One type of problem that genetic algorithms have difficulty dealing with are problems with "deceptive" fitness functions, those where the locations of improved points give misleading information about where the global optimum is likely to be found. 
For example, imagine a problem where the search space consisted of all eight-character binary strings, and the fitness of an individual was directly proportional to the number of 1s in it - i.e., 00000001 would be less fit than 00000011, which would be less fit than 00000111, and so on - with two exceptions: the string 11111111 turned out to have very low fitness, and the string 00000000 turned out to have very high fitness. 
In such a problem, a GA (as well as most other algorithms) would be no more likely to find the global optimum than random search.
The resolution to this problem is the same for both genetic algorithms and biological evolution: evolution is not a process that has to find the single global optimum every time. 
It can do almost as well by reaching the top of a high local optimum, and for most situations, this will suffice, even if the global optimum cannot easily be reached from that point. 
Evolution is very much a "satisficer" - an algorithm that delivers a "good enough" solution, though not necessarily the best possible solution, given a reasonable amount of time and effort invested in the search. 
The Evidence for Jury-Rigged Design in Nature FAQ gives examples of this very outcome appearing in nature. 
One well-known problem that can occur with a GA is known as premature convergence. 
If an individual that is more fit than most of its competitors emerges early on in the course of the run, it may reproduce so abundantly that it drives down the population's diversity too soon, leading the algorithm to converge on the local optimum that that individual represents rather than searching the fitness landscape thoroughly enough to find the global optimum.  
This is an especially common problem in small populations, where even chance variations in reproduction rate may cause one genotype to become dominant over others.
The most common methods implemented by GA researchers to deal with this problem all involve controlling the strength of selection, so as not to give excessively fit individuals too great of an advantage. 
Rank, scaling and tournament selection, discussed earlier, are three major means for accomplishing this; some methods of scaling selection include sigma scaling, in which reproduction is based on a statistical comparison to the population's average fitness, and Boltzmann selection, in which the strength of selection increases over the course of a run in a manner similar to the "temperature" variable in simulated annealing.
This should not be surprising; as discussed above, evolution as a problem-solving strategy is under no obligation to find the single best solution, merely one that is good enough. 
However, premature convergence in nature is less common since most beneficial mutations in living things produce only small, incremental fitness improvements; mutations that produce such a large fitness gain as to give their possessors dramatic reproductive advantage are rare.
Encryption algorithm, or cipher, is a mathematical function used in the encryption and decryption process series of steps that mathematically transforms plaintext or other readable information into unintelligible ciphertext. 
A cryptographic algorithm works in combination with a key (a number, word, or phrase) to encrypt and decrypt data. 
To encrypt, the algorithm mathematically combines the information to be protected with a supplied key. The result of this combination is the encrypted data. 
To decrypt, the algorithm performs a calculation combining the encrypted data with a supplied key. The result of this combination is the decrypted data. 
If either the key or the data is modified, the algorithm produces a different result. 
The goal of every encryption algorithm is to make it as difficult as possible to decrypt the generated ciphertext without using the key.
Encryption algorithm, or cipher, is a mathematical function used in the encryption and decryption process series of steps that mathematically transforms plaintext or other readable information into unintelligible ciphertext. 
A cryptographic algorithm works in combination with a key (a number, word, or phrase) to encrypt and decrypt data. 
To encrypt, the algorithm mathematically combines the information to be protected with a supplied key. 
The result of this combination is the encrypted data. 
To decrypt, the algorithm performs a calculation combining the encrypted data with a supplied key. 
The result of this combination is the decrypted data. 
If either the key or the data is modified, the algorithm produces a different result. 
The goal of every encryption algorithm is to make it as difficult as possible to decrypt the generated ciphertext without using the key.
Each algorithm uses a string of bits known as a "key" to perform the calculations. 
The larger the key (the more bits), the greater the number of potential patterns can be created, thus making it harder to break the code and descramble the contents. 
Most encryption algorithms use the block cipher method, which codes fixed blocks of input that are typically from 64 to 128 bits in length. 
Some use the stream method, which works with the continuous stream of input.
Some cryptographic methods rely on the secrecy of the encryption algorithms; such algorithms are only of historical interest and are not adequate for real-world needs. 
Instead of the secrecy of the method itself, all modern algorithms base their security on the usage of a key; a message can be decrypted only if the key used for decryption matches the key used for encryption.
There are two kinds of key-based encryption algorithms, symmetric encryption algorithms (secret key algorithms) and asymmetric encryption algorithms (or public key algorithms). 
The difference is that symmetric encryption algorithms use the same key for encryption and decryption  whereas asymmetric encryption algorithms use a different key for encryption and decryption, and the decryption key cannot be derived from the encryption key.
Symmetric encryption algorithms can be divided into stream ciphers and block ciphers. 
Stream ciphers encrypt a single bit of plaintext at a time, whereas block ciphers take a number of bits (typically 64 bits in modern ciphers), and encrypt them as a single unit.
AES stands for Advanced Encryption Standard. 
AES is a symmetric key encryption technique which will replace the commonly used Data Encryption Standard (DES). 
It was the result of a worldwide call for submissions of encryption algorithms issued by the US Government's National Institute of Standards and Technology (NIST) in 1997 and completed in 2000.
In response to the growing feasibility of attacks against DES, NIST launched a call for proposals for an official successor that meets 21st century security needs. 
This successor is called the Advanced Encryption Standard (AES).
Five algorithms were selected into the second round, from which Rijndael was selected to be the final standard. 
NIST gave as its reasons for selecting Rijndael that it performs very well in hardware and software across a wide range of environments in all possible modes. 
It has excellent key setup time and has low memory requirements, in addition its operations are easy to defend against power and timing attacks. 
NIST stated that all five finalists had adequate security and that there was nothing wrong with the other four ciphers. 
The winning algorithm, Rijndael, was developed by two Belgian cryptologists, Vincent Rijmen and Joan Daemen.
AES provides strong encryption and was selected by NIST as a Federal Information Processing Standard in November 2001 (FIPS-197).
Rijndael follows the tradition of square ciphers. AES algorithm uses three key sizes: a 128-, 192-, or 256-bit encryption key. 
Each encryption key size causes the algorithm to behave slightly differently, so the increasing key sizes not only offer a larger number of bits with which you can scramble the data, but also increase the complexity of the cipher algorithm.
Blowfish is a symmetric encryption algorithm designed in 1993 by Bruce Schneier as an alternative to existing encryption algorithms.
Blowfish has a 64-bit block size and a variable key length - from 32 bits to 448 bits. 
It is a 16-round Feistel cipher and uses large key-dependent S-boxes. 
While doing key scheduling, it generates large pseudo-random lookup tables by doing several encryptions. 
The tables depend on the user supplied key in a very complex way. 
This approach has been proven to be highly resistant against many attacks such as differential and linear cryptanalysis. 
Unfortunately, this also means that it is not the algorithm of choice for environments where a large memory space is not available. 
Blowfish is similar in structure to CAST-128, which uses fixed S-boxes.
Since then Blowfish has been analyzed considerably, and is gaining acceptance as a strong encryption algorithm.
Blowfish was designed in 1993 by Bruce Schneier as a fast, free alternative to existing encryption algorithms. 
Since then it has been analyzed considerably, and it is slowly gaining acceptance as a strong encryption algorithm. 
Blowfish is unpatented and license-free, and is available free for all uses.
The only known attacks against Blowfish are based on its weak key classes.
Diffie-Hellman (DH) is a widely used key exchange algorithm. 
In many cryptographical protocols, two parties wish to begin communicating. 
However, let's assume they do not initially possess any common secret and thus cannot use secret key cryptosystems. 
The key exchange by Diffie-Hellman protocol remedies this situation by allowing the construction of a common secret key over an insecure communication channel. 
It is based on a problem related to discrete logarithms, namely the Diffie-Hellman problem. 
This problem is considered hard, and it is in some instances as hard as the discrete logarithm problem.
The Diffie-Hellman protocol is generally considered to be secure when an appropriate mathematical group is used. 
In particular, the generator element used in the exponentiations should have a large period (i.e. order). 
Usually, Diffie-Hellman is not implemented on hardware.
Digital Signature Algorithm (DSA) is a United States Federal Government standard or FIPS for digital signatures. 
It was proposed by the National Institute of Standards and Technology (NIST) in August 1991 for use in their Digital Signature Algorithm (DSA), specified in FIPS 186, adopted in 1993. 
A minor revision was issued in 1996 as FIPS 186-1, and the standard was expanded further in 2000 as FIPS 186-2. 
Digital Signature Algorithm (DSA) is similar to the one used by ElGamal signature algorithm. 
It is fairly efficient though not as efficient as RSA for signature verification. 
The standard defines DSS to use the SHA-1 hash function exclusively to compute message digests.
The main problem with DSA is the fixed subgroup size (the order of the generator element), which limits the security to around only 80 bits. 
Hardware attacks can be menacing to some implementations of DSS. However, it is widely used and accepted as a good algorithm.
Symmetric encryption algorithms encrypt and decrypt with the same key. 
Main advantages of symmetric encryption algorithms are its security and high speed. 
Asymmetric encryption algorithms encrypt and decrypt with different keys. 
Data is encrypted with a public key, and decrypted with a private key. 
Asymmetric encryption algorithms (also known as public-key algorithms) need at least a 3,000-bit key to achieve the same level of security of a 128-bit symmetric algorithm. 
Asymmetric algorithms are incredibly slow and it is impractical to use them to encrypt large amounts of data. 
Generally, symmetric encryption algorithms are much faster to execute on a computer system than asymmetric ones. 
In practice they are often used together, so that a public-key algorithm is used to encrypt a randomly generated encryption key, and the random key is used to encrypt the actual message using a symmetric algorithm. 
This is sometimes called hybrid encryption.
Strong encryption algorithms should always be designed so that they are as difficult to break as possible. 
In theory, any encryption algorithm with a key can be broken by trying all possible keys in sequence. 
If using brute force to try all keys is the only option, the required computing power increases exponentially with the length of the key. 
A 32-bit key takes 232 (about 109) steps. This is something anyone can do on his/her home computer. 
An encryption algorithm with 56-bit keys, such as DES, requires a substantial effort, but using massive distributed systems requires only hours of computing. 
In 1999, a brute-force search using a specially designed supercomputer and a worldwide network of nearly 100,000 PCs on the Internet, found a DES key in 22 hours and 15 minutes. 
It is currently believed that keys with at least 128 bits (as in AES, for example) will be sufficient against brute-force attacks into the foreseeable future.
However, key length is not the only relevant issue. Many encryption algorithms can be broken without trying all possible keys. 
In general, it is very difficult to design ciphers that could not be broken more effectively using other methods.
The keys used in public-key encryption algorithms are usually much longer than those used in symmetric encryption algorithms. 
This is caused by the extra structure that is available to the cryptanalyst. 
There the problem is not that of guessing the right key, but deriving the matching private key from the public key. 
In the case of RSA encryption algorithm, this could be done by factoring a large integer that has two large prime factors. 
In the case of some other cryptosystems, it is equivalent to computing the discrete logarithm modulo a large integer (which is believed to be roughly comparable to factoring when the moduli is a large prime number).
Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. 
It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.
The algorithm exists in many variants; Dijkstra's original variant found the shortest path between two nodes,but a more common variant fixes a single node as the "source" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest path tree.
For a given source node in the graph, the algorithm finds the shortest path between that node and every other. 
It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. 
For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road, Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. 
As a result, the shortest path algorithm is widely used in network routing protocols, most notably IS-IS and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson's.
Suppose you would like to find the shortest path between two intersections on a city map, a starting point and a destination. 
The order is conceptually simple: to start, mark the distance to every intersection on the map with infinity. 
This is done not to imply there is an infinite distance, but to note that intersection has not yet been visited; some variants of this method simply leave the intersection unlabeled. 
Now, at each iteration, select a current intersection. 
For the first iteration, the current intersection will be the starting point and the distance to it (the intersection's label) will be zero. 
For subsequent iterations (after the first), the current intersection will be the closest unvisited intersection to the starting pointâ€”this will be easy to find.
From the current intersection, update the distance to every unvisited intersection that is directly connected to it. 
This is done by determining the sum of the distance between an unvisited intersection and the value of the current intersection, and relabeling the unvisited intersection with this value if it is less than its current value. 
In effect, the intersection is relabeled if the path to it through the current intersection is shorter than the previously known paths. To facilitate shortest path identification, in pencil, mark the road with an arrow pointing to the relabeled intersection if you label/relabel it, and erase all others pointing to it. 
After you have updated the distances to each neighboring intersection, mark the current intersection as visited and select the unvisited intersection with lowest distance (from the starting point) â€“ or the lowest labelâ€”as the current intersection. 
Nodes marked as visited are labeled with the shortest path from the starting point to it and will not be revisited or returned to.
Continue this process of updating the neighboring intersections with the shortest distances, then marking the current intersection as visited and moving onto the closest unvisited intersection until you have marked the destination as visited. 
Once you have marked the destination as visited (as is the case with any visited intersection) you have determined the shortest path to it, from the starting point, and can trace your way back, following the arrows in reverse.
This algorithm makes no attempt to direct "exploration" towards the destination as one might expect. 
Rather, the sole consideration in determining the next "current" intersection is its distance from the starting point. 
This algorithm therefore expands outward from the starting point, interactively considering every node that is closer in terms of shortest path distance until it reaches the destination. 
When understood in this way, it is clear how the algorithm necessarily finds the shortest path. 
However, it may also reveal one of the algorithm's weaknesses: its relative slowness in some topologies.
A more general problem would be to find all the shortest paths between source and target (there might be several different ones of the same length). 
Then instead of storing only a single node in each entry of prev[] we would store all nodes satisfying the relaxation condition. 
For example, if both r and source connect to target and both of them lie on different shortest paths through target (because the edge cost is the same in both cases), then we would add both r and source to prev[target]. 
When the algorithm completes, prev[] data structure will actually describe a graph that is a subset of the original graph with some edges removed. 
Its key property will be that if the algorithm was run with some starting node, then every path from that node to any other node in the new graph will be the shortest path between those nodes in the original graph, and all paths of that length from the original graph will be present in the new graph. 
Then to actually find all these shortest paths between two given nodes we would use a path finding algorithm on the new graph, such as depth-first search.
In common presentations of Dijkstra's algorithm, initially all nodes are entered into the priority queue. 
This is, however, not necessary: the algorithm can start with a priority queue that contains only one item, and insert new items as they are discovered (instead of doing a decrease-key, check whether the key is in the queue; if it is, decrease its key, otherwise insert it).
This variant has the same worst-case bounds as the common variant, but maintains a smaller priority queue in practice, speeding up the queue operations.
Moreover, not inserting all nodes in a graph makes it possible to extend the algorithm to find the shortest path from a single source to the closest of a set of target nodes on infinite graphs or those too large to represent in memory.
The functionality of Dijkstra's original algorithm can be extended with a variety of modifications. 
For example, sometimes it is desirable to present solutions which are less than mathematically optimal. 
To obtain a ranked list of less-than-optimal solutions, the optimal solution is first calculated. 
A single edge appearing in the optimal solution is removed from the graph, and the optimum solution to this new graph is calculated. 
Each edge of the original solution is suppressed in turn and a new shortest-path calculated. 
The secondary solutions are then ranked and presented after the first optimal solution.
Dijkstra's algorithm is usually the working principle behind link-state routing protocols, OSPF and IS-IS being the most common ones.
Unlike Dijkstra's algorithm, the Bellmanâ€“Ford algorithm can be used on graphs with negative edge weights, as long as the graph contains no negative cycle reachable from the source vertex s. 
The presence of such cycles means there is no shortest path, since the total weight becomes lower each time the cycle is traversed. 
It is possible to adapt Dijkstra's algorithm to handle negative weight edges by combining it with the Bellman-Ford algorithm (to remove negative edges and detect negative cycles), such an algorithm is called Johnson's algorithm.
The A* algorithm is a generalization of Dijkstra's algorithm that cuts down on the size of the subgraph that must be explored, if additional information is available that provides a lower bound on the "distance" to the target. 
This approach can be viewed from the perspective of linear programming: there is a natural linear program for computing shortest paths, and solutions to its dual linear program are feasible if and only if they form a consistent heuristic (speaking roughly, since the sign conventions differ from place to place in the literature). 
This feasible dual / consistent heuristic defines a non-negative reduced cost and A* is essentially running Dijkstra's algorithm with these reduced costs. 
If the dual satisfies the weaker condition of admissibility, then A* is instead more akin to the Bellmanâ€“Ford algorithm.
The process that underlies Dijkstra's algorithm is similar to the greedy process used in Prim's algorithm. 
Prim's purpose is to find a minimum spanning tree that connects all nodes in the graph; Dijkstra is concerned with only two nodes. 
Prim's does not evaluate the total weight of the path from the starting node, only the individual path.
Breadth-first search can be viewed as a special-case of Dijkstra's algorithm on unweighted graphs, where the priority queue degenerates into a FIFO queue.
Fast marching method can be viewed as a continuous version of Dijkstra's algorithm which computes the geodesic distance on a triangle mesh.
An algorithm is any well-defined procedure for solving a given class of problems. 
Ideally, when applied to a particular problem in that class, the algorithm would yield a full solution. 
Nonetheless, it makes sense to speak of algorithms that yield only partial solutions or yield solutions only some of the time. 
Such algorithms are sometimes called "rules of thumb" or "heuristics."
Algorithms have been around throughout recorded history. The ancient Hindus, Greeks, Babylonians, and Chinese all had algorithms for doing arithmetic computations. 
The actual term algorithm derives from ninth-century Arabic and incorporates the Greek word for number (arithmos ).
Algorithms are typically constructed on a case-by-case basis, being adapted to the problem at hand. 
Nonetheless, the possibility of a universal algorithm that could in principle resolve all problems has been a recurrent theme over the last millennium. 
Spanish theologian Raymond Lully (c. 1232â€“1315), in his Ars Magna, proposed to reduce all rational discussion to mechanical manipulations of symbolic notation and combinatorial diagrams. 
German philosopher Gottfried Wilhelm Leibniz (1646â€“1716) argued that Lully's project was overreaching but had merit when conceived more narrowly.
The idea of a universal algorithm did not take hold, however, until technology had advanced sufficiently to mechanize it. 
The Cambridge mathematician Charles Babbage (1791â€“1871) conceived and designed the first machine that could in principle resolve all well-defined arithmetic problems. 
Nevertheless, he was unable to build a working prototype. 
Over a century later another Cambridge mathematician, Alan Turing (1912â€“1954), laid the theoretical foundations for effectively implementing a universal algorithm.
Turing proposed a very simple conceptual device involving a tape with a movable reader that could mark and erase letters on the tape. 
Turing showed that all algorithms could be mapped onto the tape (as data) and then run by a universal algorithm already inscribed on the tape. 
This machine, known as a universal Turing machine, became the basis for the modern theory of computation (known as recursion theory) and inspired the modern digital computer.
Turing's universal algorithm fell short of Lully's vision of an algorithm that could resolve all problems. 
Turing's universal algorithm is not so much a universal problem solver as an empty box capable of housing and implementing the algorithms placed into it. 
Thus Turing invited into the theory of computing the very Cartesian distinction between hardware and software. 
Hardware is the mechanical device (i.e., the empty box) that houses and implements software (i.e., the algorithms) running on it.
Turing himself was fascinated with how the distinction between software and hardware illuminated immortality and the soul. 
Identifying personal identity with computer software ensured that humans were immortal, since even though hardware could be destroyed, software resided in a realm of mathematical abstraction and was thus immune to destruction.
It is a deep and much disputed question whether the essence of what constitutes the human person is at base computational and therefore an emergent property of algorithms, or whether it fundamentally transcends the capacity of algorithms.
Step by step procedure designed to perform an operation, and which (like a map or flowchart) will lead to the sought result if followed correctly. 
Algorithms have a definite beginning and a definite end, and a finite number of steps. 
An algorithm produces the same output information given the same input information, and several short algorithms can be combined to perform complex tasks such as writing a computer program. 
A cookbook recipe, a diagnosis, a problem solving routine, are some common examples of simple algorithms. 
Suitable for solving structured problems (amenable to sequential analysis) algorithms are, however, unsuitable for problems where value judgments are required. 
For example, you start working on a report, and once you have completed a paragraph, you perform a spell check. 
You open up a spreadsheet application to do some financial projections to see if you can afford a new car loan. 
You use a web browser to search online for a kind of car you want to buy.
You may not think about this very consciously, but all of these operations performed by your computer consist of algorithms. 
An algorithm is a well-defined procedure that is used by a computer to solve a problem. Another way to describe an algorithm is a sequence of unambiguous instructions. 
The use of the term 'unambiguous' indicates that there is no room for subjective interpretation. 
Every time you ask your computer to carry out the same algorithm, it will do it in exactly the same manner with the exact same result.
Consider the earlier examples again. Spell checking uses algorithms. 
Financial calculations use algorithms. A search engine uses algorithms. In fact, it is difficult to think of a task performed by your computer that does not use algorithms.
There are many different types of algorithms. Search algorithms are used to find an item with specific properties among a collection of items. 
For example, you may want to know if a particular word occurs in a list of words or not. 
Searching is closely related to the concept of dictionaries since it is like looking up a word in a dictionary. 
There are different approaches to searching, each representing a slightly different technical approach to the same problem.
Artificial intelligence (AI) is the intelligence exhibited by machines or software. 
It is an academic field of study which studies how to create computers and computer software that are capable of intelligent behavior. 
Major AI researchers and textbooks define this field as "the study and design of intelligent agents", in which an intelligent agent is a system that perceives its environment and takes actions that maximize its chances of success.
John McCarthy, who coined the term in 1955,[3] defines it as "the science and engineering of making intelligent machines".
AI research is highly technical and specialized, and is deeply divided into subfields that often fail to communicate with each other.
Some of the division is due to social and cultural factors: subfields have grown up around particular institutions and the work of individual researchers. 
AI research is also divided by several technical issues. Some subfields focus on the solution of specific problems. 
Others focus on one of several possible approaches or on the use of a particular tool or towards the accomplishment of particular applications.
The central problems (or goals) of AI research include reasoning, knowledge, planning, learning, natural language processing (communication), perception and the ability to move and manipulate objects.
Currently popular approaches include statistical methods, computational intelligence and traditional symbolic AI. 
There are a large number of tools used in AI, including versions of search and mathematical optimization, logic, methods based on probability and economics, and many others. 
The AI field is interdisciplinary, in which a number of sciences and professions converge, including computer science, mathematics, psychology, linguistics, philosophy and neuroscience, as well as other specialized fields such as artificial psychology.
The field was founded on the claim that a central property of humans, intelligenceâ€”the sapience of Homo sapiensâ€”"can be so precisely described that a machine can be made to simulate it.
This raises philosophical issues about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence, issues which have been addressed by myth, fiction and philosophy since antiquity.
Artificial intelligence has been the subject of tremendous optimism[10] but has also suffered stunning setbacks.
Today it has become an essential part of the technology industry, providing the heavy lifting for many of the most challenging problems in computer science.
Thinking machines and artificial beings appear in Greek myths, such as Talos of Crete, the bronze robot of Hephaestus, and Pygmalion's Galatea.
Human likenesses believed to have intelligence were built in every major civilization: animated cult images were worshiped in Egypt and Greece and humanoid automatons were built by Yan Shi, Hero of Alexandria and Al-Jazari.
It was also widely believed that artificial beings had been created by JÄbir ibn HayyÄn, Judah Loew and Paracelsus.
By the 19th and 20th centuries, artificial beings had become a common feature in fiction, as in Mary Shelley's Frankenstein or Karel ÄŒapek's R.U.R. (Rossum's Universal Robots).
Pamela McCorduck argues that all of these are some examples of an ancient urge, as she describes it, "to forge the gods".
Stories of these creatures and their fates discuss many of the same hopes, fears and ethical concerns that are presented by artificial intelligence.
Mechanical or "formal" reasoning has been developed by philosophers and mathematicians since antiquity. 
The study of logic led directly to the invention of the programmable digital electronic computer, based on the work of mathematician Alan Turing and others. 
Turing's theory of computation suggested that a machine, by shuffling symbols as simple as "0" and "1", could simulate any conceivable act of mathematical deduction.
This, along with concurrent discoveries in neurology, information theory and cybernetics, inspired a small group of researchers to begin to seriously consider the possibility of building an electronic brain.
The field of AI research was founded at a conference on the campus of Dartmouth College in the summer of 1956.
The attendees, including John McCarthy, Marvin Minsky, Allen Newell, Arthur Samuel, and Herbert Simon, became the leaders of AI research for many decades.
They and their students wrote programs that were, to most people, simply astonishing: computers were winning at checkers, solving word problems in algebra, proving logical theorems and speaking English.
By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world.
AI's founders were profoundly optimistic about the future of the new field: Herbert Simon predicted that "machines will be capable, within twenty years, of doing any work a man can do" and Marvin Minsky agreed, writing that "within a generation the problem of creating 'artificial intelligence' will substantially be solved".
They had failed to recognize the difficulty of some of the problems they faced.
In 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off all undirected exploratory research in AI. 
The next few years would later be called an "AI winter", a period when funding for AI projects was hard to find.
In the early 1980s, AI research was revived by the commercial success of expert systems,a form of AI program that simulated the knowledge and analytical skills of one or more human experts. 
By 1985 the market for AI had reached over a billion dollars. 
At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research in the field.
However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer lasting AI winter began.
In the 1990s and early 21st century, AI achieved its greatest successes, albeit somewhat behind the scenes. 
Artificial intelligence is used for logistics, data mining, medical diagnosis and many other areas throughout the technology industry.
The success was due to several factors: the increasing computational power of computers (see Moore's law), a greater emphasis on solving specific subproblems, the creation of new ties between AI and other fields working on similar problems, and a new commitment by researchers to solid mathematical methods and rigorous scientific standards.
On 11 May 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov.
In February 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin.
The Kinect, which provides a 3D bodyâ€“motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from lengthy AI research as do intelligent personal assistants in smartphones
Early AI researchers developed algorithms that imitated the step-by-step reasoning that humans use when they solve puzzles or make logical deductions.
By the late 1980s and 1990s, AI research had also developed highly successful methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.
For difficult problems, most of these algorithms can require enormous computational resources â€“ most experience a "combinatorial explosion": the amount of memory or computer time required becomes astronomical when the problem goes beyond a certain size. 
The search for more efficient problem-solving algorithms is a high priority for AI research.
Human beings solve most of their problems using fast, intuitive judgements rather than the conscious, step-by-step deduction that early AI research was able to model.
AI has made some progress at imitating this kind of "sub-symbolic" problem solving: embodied agent approaches emphasize the importance of sensorimotor skills to higher reasoning; neural net research attempts to simulate the structures inside the brain that give rise to this skill; statistical approaches to AI mimic the probabilistic nature of the human ability to guess.
Knowledge representation[44] and knowledge engineering are central to AI research.
Many of the problems machines are expected to solve will require extensive knowledge about the world. 
Among the things that AI needs to represent are: objects, properties, categories and relations between objects;  situations, events, states and time;  causes and effects;  knowledge about knowledge (what we know about what other people know); and many other, less well researched domains. 
A representation of "what exists" is an ontology: the set of objects, relations, concepts and so on that the machine knows about. 
The most general are called upper ontologies, which attempt to provide a foundation for all other knowledge.
Many of the things people know take the form of "working assumptions." 
For example, if a bird comes up in conversation, people typically picture an animal that is fist sized, sings, and flies. 
None of these things are true about all birds. 
John McCarthy identified this problem in 1969 as the qualification problem: for any commonsense rule that AI researchers care to represent, there tend to be a huge number of exceptions. 
Almost nothing is simply true or false in the way that abstract logic requires. AI research has explored a number of solutions to this problem.
The number of atomic facts that the average person knows is astronomical. 
Research projects that attempt to build a complete knowledge base of commonsense knowledge (e.g., Cyc) require enormous amounts of laborious ontological engineeringâ€”they must be built, by hand, one complicated concept at a time.
A major goal is to have the computer understand enough concepts to be able to learn by reading from sources like the internet, and thus be able to add to its own ontology
Much of what people know is not represented as "facts" or "statements" that they could express verbally. 
For example, a chess master will avoid a particular chess position because it "feels too exposed" or an art critic can take one look at a statue and instantly realize that it is a fake.
These are intuitions or tendencies that are represented in the brain non-consciously and sub-symbolically.
Knowledge like this informs, supports and provides a context for symbolic, conscious knowledge. 
As with the related problem of sub-symbolic reasoning, it is hoped that situated AI, computational intelligence, or statistical AI will provide ways to represent this kind of knowledge.
Intelligent agents must be able to set goals and achieve them.
They need a way to visualize the future (they must have a representation of the state of the world and be able to make predictions about how their actions will change it) and be able to make choices that maximize the utility (or "value") of the available choices.
In classical planning problems, the agent can assume that it is the only thing acting on the world and it can be certain what the consequences of its actions may be.
However, if the agent is not the only actor, it must periodically ascertain whether the world matches its predictions and it must change its plan as this becomes necessary, requiring the agent to reason under uncertainty.[60]
Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. 
Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence
Machine learning is the study of computer algorithms that improve automatically through experience and has been central to AI research since the field's inception.
Unsupervised learning is the ability to find patterns in a stream of input. Supervised learning includes both classification and numerical regression. 
Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. 
Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. 
In reinforcement learning[65] the agent is rewarded for good responses and punished for bad ones. 
The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space. 
These three types of learning can be analyzed in terms of decision theory, using concepts like utility. 
The mathematical analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory.
Within developmental robotics, developmental learning approaches were elaborated for lifelong cumulative acquisition of repertoires of novel skills by a robot, through autonomous self-exploration and social interaction with human teachers, and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation
Natural language processing gives machines the ability to read and understand the languages that humans speak. 
A sufficiently powerful natural language processing system would enable natural language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. 
Some straightforward applications of natural language processing include information retrieval (or text mining), question answeringand machine translation.
A common method of processing and extracting meaning from natural language is through semantic indexing. 
Increases in processing speeds and the drop in the cost of data storage makes indexing large volumes of abstractions of the user's input much more efficient.
Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human affects. 
It is an interdisciplinary field spanning computer sciences, psychology, and cognitive science.
While the origins of the field may be traced as far back as to early philosophical inquiries into emotion,the more modern branch of computer science originated with Rosalind Picard's 1995 paper on affective computing.
A motivation for the research is the ability to simulate empathy. The machine should interpret the emotional state of humans and adapt its behaviour to them, giving an appropriate response for those emotions.
Emotion and social skills play two roles for an intelligent agent. 
First, it must be able to predict the actions of others, by understanding their motives and emotional states. 
(This involves elements of game theory, decision theory, as well as the ability to model human emotions and the perceptual skills to detect emotions.) 
Also, in an effort to facilitate human-computer interaction, an intelligent machine might want to be able to display emotionsâ€”even if it does not actually experience them itselfâ€”in order to appear sensitive to the emotional dynamics of human interaction.
Many researchers think that their work will eventually be incorporated into a machine with general intelligence (known as strong AI), combining all the skills above and exceeding human abilities at most or all of them.
A few believe that anthropomorphic features like artificial consciousness or an artificial brain may be required for such a project.
Many of the problems above may require general intelligence to be considered solved. 
For example, even a straightforward, specific task like machine translation requires that the machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's intention (social intelligence). 
A problem like machine translation is considered "AI-complete". 
In order to solve this particular problem, you must solve all the problems.
There is no established unifying theory or paradigm that guides AI research. Researchers disagree about many issues.
A few of the most long standing questions that have remained unanswered are these: should artificial intelligence simulate natural intelligence by studying psychology or neurology? Or is human biology as irrelevant to AI research as bird ibology is to aeronautical engineering?
Can intelligent behavior be described using simple, elegant principles (such as logic or optimization)? Or does it necessarily require solving a large number of completely unrelated problems?
Can intelligence be reproduced using high-level symbols, similar to words and ideas? Or does it require "sub-symbolic" processing?John Haugeland, who coined the term GOFAI (Good Old-Fashioned Artificial Intelligence), also proposed that AI should more properly be referred to as synthetic intelligence,a term which has since been adopted by some non-GOFAI researchers.
When access to digital computers became possible in the middle 1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. 
The research was centered in three institutions: Carnegie Mellon University, Stanford and MIT, and each one developed its own style of research. 
John Haugeland named these approaches to AI "good old fashioned AI" or "GOFAI".
During the 1960s, symbolic approaches had achieved great success at simulating high-level thinking in small demonstration programs. 
Approaches based on cybernetics or neural networks were abandoned or pushed into the background.
Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.
Economist Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. 
Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. 
This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s
Researchers from the related field of robotics, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move and survive. 
Their work revived the non-symbolic viewpoint of the early cybernetics researchers of the 1950s and reintroduced the use of control theory in AI. 
This coincided with the development of the embodied mind thesis in the related field of cognitive science: the idea that aspects of the body (such as movement, perception and visua. 
For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the applicalization) are required for higher intelligence.
Many problems in AI can be solved in theory by intelligently searching through many possible solutions:Reasoning can be reduced to performing a searchtion of an inference rule.
Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.
Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.
Many learning algorithms use search algorithms based on optimization.
Simple exhaustive searches are rarely sufficient for most real world problems: the search space (the number of places to search) quickly grows to astronomical numbers. 
The result is a search that is too slow or never completes. 
The solution, for many problems, is to use "heuristics" or "rules of thumb" that eliminate choices that are unlikely to lead to the goal (called "pruning the search tree"). 
Heuristics supply the program with a "best guess" for the path on which the solution lies.
Heuristics limit the search for solutions into a smaller sample size.
A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. 
These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. 
Other optimization algorithms are simulated annealing, beam search and random optimization.
Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). 
Forms of evolutionary computation include swarm intelligence algorithms (such as ant colony or particle swarm optimization)[126] and evolutionary algorithms (such as genetic algorithms, gene expression programming, and genetic programming).
Logic[128] is used for knowledge representation and problem solving, but it can be applied to other problems as well. 
For example, the satplan algorithm uses logic for planning[129] and inductive logic programming is a method for learning.
Several different forms of logic are used in AI research. Propositional or sentential logic is the logic of statements which can be true or false. 
First-order logic also allows the use of quantifiers and predicates, and can express facts about objects, their properties, and their relations with each other. 
Fuzzy logic,[133] is a version of first-order logic which allows the truth of a statement to be represented as a value between 0 and 1, rather than simply True (1) or False (0). Fuzzy systems can be used for uncertain reasoning and have been widely used in modern industrial and consumer product control systems. 
Subjective logic models uncertainty in a different and more explicit manner than fuzzy-logic: a given binomial opinion satisfies belief + disbelief + uncertainty = 1 within a Beta distribution.
By this method, ignorance can be distinguished from probabilistic statements that an agent makes with high confidence.
Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem. 
Several extensions of logic have been designed to handle specific domains of knowledge, such as: description logics; situation calculus, event calculus and fluent calculus (for representing events and time);  causal calculus; belief calculus; and modal logics.
Many problems in AI (in reasoning, planning, learning, perception and robotics) require the agent to operate with incomplete or uncertain information. 
AI researchers have devised a number of powerful tools to solve these problems using methods from probability theory and economics.
Bayesian networks[136] are a very general tool that can be used for a large number of problems: reasoning (using the Bayesian inference algorithm),learning (using the expectation-maximization algorithm),planning (using decision networks)and perception (using dynamic Bayesian networks).
Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[140]
A key concept from the science of economics is "utility": a measure of how valuable something is to an intelligent agent. 
Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. 
These tools include models such as Markov decision processes,dynamic decision networks,[140] game theory and mechanism design.
The simplest AI applications can be divided into two types: classifiers ("if shiny then diamond") and controllers ("if shiny then pick up"). 
Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. 
Classifiers are functions that use pattern matching to determine a closest match. 
They can be tuned according to examples, making them very attractive for use in AI. 
These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. 
A class can be seen as a decision that has to be made. All the observations combined with their class labels are known as a data set. 
When a new observation is received, that observation is classified based on previous experience.
A classifier can be trained in various ways; there are many statistical and machine learning approaches. 
The most widely used classifiers are the neural network,kernel methods such as the support vector machine,k-nearest neighbor algorithm, Gaussian mixture model, naive Bayes classifier,and decision tree.
The performance of these classifiers have been compared over a wide range of tasks. 
Classifier performance depends greatly on the characteristics of the data to be classified. 
There is no single classifier that works best on all given problems; this is also referred to as the "no free lunch" theorem. 
Determining a suitable classifier for a given problem is still more an art than science.
The study of artificial neural networks[145] began in the decade before the field of AI research was founded, in the work of Walter Pitts and Warren McCullough. 
Other important early researchers were Frank Rosenblatt, who invented the perceptron and Paul Werbos who developed the backpropagation algorithm.
The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback). 
Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.
Among recurrent networks, the most famous is the Hopfield net, a form of attractor network, which was first described by John Hopfield in 1982.
Neural networks can be applied to the problem of intelligent control (for robotics) or learning, using such techniques as Hebbian learning and competitive learning.
Hierarchical temporal memory is an approach that models some of the structural and algorithmic properties of the neocortex.
The term "deep learning" gained traction in the mid-2000s after a publication by Geoffrey Hinton and Ruslan Salakhutdinov showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning.
Computationalism is the idea that â€œthe human mind or the human brain (or both) is an information processing system and that thinking is a form of computingâ€. 
AI, or implementing machines with human intelligence was founded on the claim that â€œa central property of humans, intelligence can be so precisely described that a machine can be made to simulate itâ€. 
A program can then be derived from this human human computer and implemented into an artificial one to, create efficient artificial intelligence. 
This program would act upon a set of outputs that result from set inputs of the internal memory of the computer, that is, the machine can only act with what it has implemented in it to start with. 
A long term goal for AI researchers is to provide machines with a deep understanding of the many abilities of a human being to replicate a general intelligence or STRONG AI, defined as a machine surpassing human abilities to perform the skills implanted in it, a scary thought to many, who fear losing control of such a powerful machine. 
Obstacles for researchers are mainly time contstraints. 
That is, AI scientists cant establish much of a database for commonsense knowledge because it must be ontologically crafted into the machine which takes up a tremendous amount of time, to combat this, AI research looks to have the machine able to understand enough concepts in order to add to its own ontology, but how can it do this when machine ethics is primarily concerned with behavior of machines towards humans or other machines, limiting the extent of developing AI. 
If research into Strong AI produced sufficiently intelligent software, it might be able to reprogram and improve itself. 
The improved software would be even better at improving itself, leading to recursive self-improvement.
The field of machine ethics is concerned with giving machines ethical principles, or a procedure for discovering a way to resolve the ethical dilemmas they might encounter, enabling them to function in an ethically responsible manner through their own ethical decision making.
The field was delineated in the AAAI Fall 2005 Symposium on Machine Ethics: "Past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings, with a few people being interested in how human beings ought to treat machines. 
In all cases, only human beings have engaged in ethical reasoning. The time has come for adding an ethical dimension to at least some machines. 
Recognition of the ethical ramifications of behavior involving machines, as well as recent and potential developments in machine autonomy, necessitate this. 
In contrast to computer hacking, software property issues, privacy issues and other topics normally ascribed to computer ethics, machine ethics is concerned with the behavior of machines towards human users and other machines. Research in machine ethics is key to alleviating concerns with autonomous systems â€” it could be argued that the notion of autonomous machines without such a dimension is at the root of all fear concerning machine intelligence. 
Further, investigation of machine ethics could enable the discovery of problems with current ethical theories, advancing our thinking about Ethics."
Machine ethics is sometimes referred to as machine morality, computational ethics or computational morality. 
A variety of perspectives of this nascent field can be found in the collected edition "Machine Ethics" that stems from the AAAI Fall 2005 Symposium on Machine Ethics.
A platform (or "computing platform") is defined as "some sort of hardware architecture or software framework (including application frameworks), that allows software to run." 
As Rodney Brooks pointed out many years ago, it is not just the artificial intelligence software that defines the AI features of the platform, but rather the actual platform itself that affects the AI that results, i.e., there needs to be work in AI problems on real-world platforms rather than in isolation.
A wide variety of platforms has allowed different aspects of AI to develop, ranging from expert systems, albeit PC-based but still an entire real-world system, to various robot platforms such as the widely available Roomba with open interface
A quite different approach measures machine intelligence through tests which are developed from mathematical definitions of intelligence. 
Examples of these kinds of tests start in the late nineties devising intelligence tests using notions from Kolmogorov complexity and data compression.
Two major advantages of mathematical definitions are their applicability to nonhuman intelligences and their absence of a requirement for human testers.
A derivative of the Turing test is the Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA). 
as the name implies, this helps to determine that a user is an actual person and not a computer posing as a human. 
In contrast to the standard Turing test, CAPTCHA administered by a machine and targeted to a human as opposed to being administered by a human and targeted to a machine. 
A computer asks a user to complete a simple test then generates a grade for that test. 
Computers are unable to solve the problem, so correct solutions are deemed to be the result of a person taking the test. 
A common type of CAPTCHA is the test that requires the typing of distorted letters, numbers or symbols that appear in an image undecipherable by a computer.
The new intelligence could thus increase exponentially and dramatically surpass humans. 
Science fiction writer Vernor Vinge named this scenario "singularity".
Technological singularity is when accelerating progress in technologies will cause a runaway effect wherein artificial intelligence will exceed human intellectual capacity and control, thus radically changing or even ending civilization. 
Because the capabilities of such an intelligence may be impossible to comprehend, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.
Ray Kurzweil has used Moore's law (which describes the relentless exponential improvement in digital technology) to calculate that desktop computers will have the same processing power as human brains by the year 2029, and predicts that the singularity will occur in 2045.
Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either.
This idea, called transhumanism, which has roots in Aldous Huxley and Robert Ettinger, has been illustrated in fiction as well, for example in the manga Ghost in the Shell and the science-fiction series Dune.
In the 1980s artist Hajime Sorayama's Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later "the Gynoids" book followed that was used by or influenced movie makers including George Lucas and other creatives. 
Sorayama never considered these organic robots to be real part of nature but always unnatural product of the human mind, a fantasy existing in the mind even when realized in actual form.
Edward Fredkin argues that "artificial intelligence is the next stage in evolution", an idea first proposed by Samuel Butler's "Darwin among the Machines" (1863), and expanded upon by George Dyson in his book of the same name in 1998
Artificial intelligence (AI), the ability of a digital computer or computer-controlled robot to perform tasks commonly associated with intelligent beings. 
The term is frequently applied to the project of developing systems endowed with the intellectual processes characteristic of humans, such as the ability to reason, discover meaning, generalize, or learn from past experience. 
Since the development of the digital computer in the 1940s, it has been demonstrated that computers can be programmed to carry out very complex tasksâ€”as, for example, discovering proofs for mathematical theorems or playing chessâ€”with great proficiency. 
Still, despite continuing advances in computer processing speed and memory capacity, there are as yet no programs that can match human flexibility over wider domains or in tasks requiring much everyday knowledge. 
On the other hand, some programs have attained the performance levels of human experts and professionals in performing certain specific tasks, so that artificial intelligence in this limited sense is found in applications as diverse as medical diagnosis, computer search engines, and voice or handwriting recognition.
All but the simplest human behaviour is ascribed to intelligence, while even the most complicated insect behaviour is never taken as an indication of intelligence. 
What is the difference? Consider the behaviour of the digger wasp, Sphex ichneumoneus. 
When the female wasp returns to her burrow with food, she first deposits it on the threshold, checks for intruders inside her burrow, and only then, if the coast is clear, carries her food inside. 
The real nature of the waspâ€™s instinctual behaviour is revealed if the food is moved a few inches away from the entrance to her burrow while she is inside: on emerging, she will repeat the whole procedure as often as the food is displaced. 
Intelligenceâ€”conspicuously absent in the case of Sphexâ€”must include the ability to adapt to new circumstances.
Psychologists generally do not characterize human intelligence by just one trait but by the combination of many diverse abilities. 
Research in AI has focused chiefly on the following components of intelligence: learning, reasoning, problem solving, perception, and using language.
There are a number of different forms of learning as applied to artificial intelligence. 
The simplest is learning by trial and error. 
For example, a simple computer program for solving mate-in-one chess problems might try moves at random until mate is found. 
The program might then store the solution with the position so that the next time the computer encountered the same position it would recall the solution. 
This simple memorizing of individual items and proceduresâ€”known as rote learningâ€”is relatively easy to implement on a computer. 
More challenging is the problem of implementing what is called generalization. 
Generalization involves applying past experience to analogous new situations. 
For example, a program that learns the past tense of regular English verbs by rote will not be able to produce the past tense of a word such as jump unless it previously had been presented with jumped, whereas a program that is able to generalize can learn the â€œadd edâ€ rule and so form the past tense of jump based on experience with similar verbs.
To reason is to draw inferences appropriate to the situation. 
Inferences are classified as either deductive or inductive. An example of the former is, â€œFred must be in either the museum or the cafÃ©. 
He is not in the cafÃ©; therefore he is in the museum,â€ and of the latter, â€œPrevious accidents of this sort were caused by instrument failure; therefore this accident was caused by instrument failure.â€ 
The most significant difference between these forms of reasoning is that in the deductive case the truth of the premises guarantees the truth of the conclusion, whereas in the inductive case the truth of the premise lends support to the conclusion without giving absolute assurance. 
Inductive reasoning is common in science, where data are collected and tentative models are developed to describe and predict future behaviourâ€”until the appearance of anomalous data forces the model to be revised. Deductive reasoning is common in mathematics and logic, where elaborate structures of irrefutable theorems are built up from a small set of basic axioms and rules.
There has been considerable success in programming computers to draw inferences, especially deductive inferences. 
However, true reasoning involves more than just drawing inferences; it involves drawing inferences relevant to the solution of the particular task or situation. 
This is one of the hardest problems confronting AI.
Problem solving, particularly in artificial intelligence, may be characterized as a systematic search through a range of possible actions in order to reach some predefined goal or solution. 
Problem-solving methods divide into special purpose and general purpose. 
A special-purpose method is tailor-made for a particular problem and often exploits very specific features of the situation in which the problem is embedded. 
In contrast, a general-purpose method is applicable to a wide variety of problems. 
One general-purpose technique used in AI is means-end analysisâ€”a step-by-step, or incremental, reduction of the difference between the current state and the final goal. 
The program selects actions from a list of meansâ€”in the case of a simple robot this might consist of PICKUP, PUTDOWN, MOVEFORWARD, MOVEBACK, MOVELEFT, and MOVERIGHTâ€”until the goal is reached.
Many diverse problems have been solved by artificial intelligence programs. 
Some examples are finding the winning move (or sequence of moves) in a board game, devising mathematical proofs, and manipulating â€œvirtual objectsâ€ in a computer-generated world.
In perception the environment is scanned by means of various sensory organs, real or artificial, and the scene is decomposed into separate objects in various spatial relationships. 
Analysis is complicated by the fact that an object may appear different depending on the angle from which it is viewed, the direction and intensity of illumination in the scene, and how much the object contrasts with the surrounding field.
At present, artificial perception is sufficiently well advanced to enable optical sensors to identify individuals, autonomous vehicles to drive at moderate speeds on the open road, and robots to roam through buildings collecting empty soda cans. 
One of the earliest systems to integrate perception and action was FREDDY, a stationary robot with a moving television eye and a pincer hand, constructed at the University of Edinburgh, Scotland, during the period 1966â€“73 under the direction of Donald Michie. 
FREDDY was able to recognize a variety of objects and could be instructed to assemble simple artifacts, such as a toy car, from a random heap of components.
Knowledge engineering is a core part of AI research. 
Machines can often act and react like humans only if they have abundant information relating to the world. 
Artificial intelligence must have access to objects, categories, properties and relations between all of them to implement knowledge engineering. 
Initiating common sense, reasoning and problem-solving power in machines is a difficult and tedious approach. 
Machine learning is another core part of AI. 
Learning without any kind of supervision requires an ability to identify patterns in streams of inputs, whereas learning with adequate supervision involves classification and numerical regressions. 
Classification determines the category an object belongs to and regression deals with obtaining a set of numerical input or output examples, thereby discovering functions enabling the generation of suitable outputs from respective inputs. 
Mathematical analysis of machine learning algorithms and their performance is a well-defined branch of theoretical computer science often referred to as computational learning theory. 
Machine perception deals with the capability to use sensory inputs to deduce the different aspects of the world, while computer vision is the power to analyze visual inputs with few sub-problems such as facial, object and speech recognition.
Robotics is also a major field related to AI. Robots require intelligence to handle tasks such as object manipulation and navigation, along with sub-problems of localization, motion planning and mapping. 
Emulators are part of past and present non-PC computers such as the Commodore 64 or the Apple Macintosh.
Artificial intelligence (AI) is the field within computer science that seeks to explain and to emulate, through mechanical or computational processes, some or all aspects of human intelligence. 
Included among these aspects of intelligence are the ability to interact with the environment through sensory means and the ability to make decisions in unforeseen circumstances without human intervention. 
Typical areas of research in AI include game playing, natural language understanding and synthesis, computer vision, problem solving, learning, and robotics.
The above is a general description of the field; there is no agreed upon definition of artificial intelligence, primarily because there is little agreement as to what constitutes intelligence. Interpretations of what it means to be intelligent vary, yet most can be categorized in one of three ways. 
Intelligence can be thought of as a quality, an individually held property that is separable from all other properties of the human person. 
Intelligence is also seen in the functions one performs, in actions or the ability to carry out certain tasks. Finally, some researchers see intelligence as a quality that can only be acquired and demonstrated through relationship with other intelligent beings. 
Each of these understandings of intelligence has been used as the basis of an approach to developing computer programs with intelligent characteristics.
The field of AI is considered to have its origin in the publication of British mathematician Alan Turing's (1912â€“1954) paper "Computing Machinery and Intelligence" (1950). 
The term itself was coined six years later by mathematician and computer scientist John McCarthy (b. 1927) at a summer conference at Dartmouth College in New Hampshire. 
The earliest approach to AI is called symbolic or classical AI and is predicated on the hypothesis that every process in which either a human being or a machine engages can be expressed by a string of symbols that is modifiable according to a limited set of rules that can be logically defined. 
Just as geometry can be built from a finite set of axioms and primitive objects such as points and lines, so symbolicists, following rationalist philosophers such as Ludwig Wittgenstein (1889â€“1951) and Alfred North Whitehead (1861â€“1947), predicated that human thought is represented in the mind by concepts that can be broken down into basic rules and primitive objects. 
Simple concepts or objects are directly expressed by a single symbol while more complex ideas are the product of many symbols, combined by certain rules. 
For a symbolicist, any patternable kind of matter can thus represent intelligent thought.
Symbolic AI met with immediate success in areas in which problems could be easily described using a limited domain of objects that operate in a highly rule-based manner, such as games. 
The game of chess takes place in a world where the only objects are thirty-two pieces moving on a sixty-four square board according to a limited number of rules. 
The limited options this world provides give the computer the potential to look far ahead, examining all possible moves and countermoves, looking for a sequence that will leave its pieces in the most advantageous position. 
Other successes for symbolic AI occurred rapidly in similarly restricted domains such as medical diagnosis, mineral prospecting, chemical analysis, and mathematical theorem proving.
Symbolic AI faltered, however, not on difficult problems like passing a calculus exam, but on the easy things a two year old child can do, such as recognizing a face in various settings or understanding a simple story. 
McCarthy labels symbolic programs as brittle because they crack or break down at the edges; they cannot function outside or near the edges of their domain of expertise since they lack knowledge outside of that domain, knowledge that most human "experts" possess in the form of what is known as common sense. 
Humans make use of general knowledgeâ€”the millions of things that are known and applied to a situationâ€”both consciously and subconsciously. 
Should it exist, it is now clear to AI researchers that the set of primitive facts necessary for representing human knowledge is exceedingly large.
Another critique of symbolic AI, advanced by Terry Winograd and Fernando Flores in their 1986 book Understanding Computers and Cognition is that human intelligence may not be a process of symbol manipulation; humans do not carry mental models around in their heads. 
Hubert Dreyfus makes a similar argument in Mind over Machine (1986); he suggests that human experts do not arrive at their solutions to problems through the application of rules or the manipulation of symbols, but rather use intuition, acquired through multiple experiences in the real world. 
He describes symbolic AI as a "degenerating research project," by which he means that, while promising at first, it has produced fewer results as time has progressed and is likely to be abandoned should other alternatives become available. 
By 2000 the once dominant symbolic approach had been all but abandoned in AI, with only one major ongoing project, Douglas Lenat's Cyc (pronounced "psych"). 
Lenat hopes to overcome the general knowledge problem by providing an extremely large base of primitive facts. 
Lenat plans to combine this large database with the ability to communicate in a natural language, hoping that once enough information is entered into Cyc, the computer will be able to continue the learning process on its own, through conversation, reading, and applying logical rules to detect patterns or inconsistencies in the data Cyc is given. 
Initially conceived in 1984 as a ten-year initiative, Cyc has not yet shown convincing evidence of extended independent learning.
In 1980, John Searle, in the paper "Minds, Brains, and Programs," introduced a division of the field of AI into "strong" and "weak" AI. 
Strong AI denoted the attempt to develop a full human-like intelligence, while weak AI denoted the use of AI techniques to either better understand human reasoning or to solve more limited problems. 
Although there was little progress in developing a strong AI through symbolic programming methods, the attempt to program computers to carry out limited human functions has been quite successful. 
Much of what is currently labeled AI research follows a functional model, applying particular programming techniques, such as knowledge engineering, fuzzy logic, genetic algorithms, neural networking, heuristic searching, and machine learning via statistical methods, to practical problems. 
This view sees AI as advanced computing. 
It produces working programs that can take over certain human tasks. Such programs are used in manufacturing operations, transportation, education, financial markets, "smart" buildings, and even household appliances.
For a functional AI, there need be no quality labeled "intelligence" that is shared by humans and computers. 
All computers need do is perform a task that requires intelligence for a human to perform. 
It is also unnecessary, in functional AI, to model a program after the thought processes that humans use. 
If results are what matters, then it is possible to exploit the speed and storage capabilities of the digital computer while ignoring parts of human thought that are not understood or easily modeled, such as intuition. 
This is, in fact, what was done in designing the chess-playing program Deep Blue, which in 1997 beat the reigning world chess champion, Gary Kasparov. 
Deep Blue does not attempt to mimic the thought of a human chess player. Instead, it capitalizes on the strengths of the computer by examining an extremely large number of moves, more moves than any human player could possibly examine.
There are two problems with functional AI. The first is the difficulty of determining what falls into the category of AI and what is simply a normal computer application. 
A definition of AI that includes any program that accomplishes some function normally done by a human being would encompass virtually all computer programs. 
Nor is there agreement among computer scientists as to what sorts of programs should fall under the rubric of AI. 
Once an application is mastered, there is a tendency to no longer define that application as AI. 
For example, while game playing is one of the classical fields of AI, Deep Blue's design team emphatically states that Deep Blue is not artificial intelligence, since it uses standard programming and parallel processing techniques that are in no way designed to mimic human thought. 
The implication here is that merely programming a computer to complete a human task is not AI if the computer does not complete the task in the same way a human would.
For a functional approach to result in a full human-like intelligence it would be necessary not only to specify which functions make up intelligence, but also to make sure those functions are suitably congruent with one another. 
Functional AI programs are rarely designed to be compatible with other programs; each uses different techniques and methods, the sum of which is unlikely to capture the whole of human intelligence. 
Many in the AI community are also dissatisfied with a collection of task-oriented programs. 
The building of a general human-like intelligence, as difficult a goal as it may seem, remains the vision.
A third approach is to consider intelligence as acquired, held, and demonstrated only through relationships with other intelligent agents. 
In "Computing Machinery and Intelligence" (1997), Turing addresses the question of which functions are essential for intelligence with a proposal for what has come to be the generally accepted test for machine intelligence. 
An human interrogator is connected by terminal to two subjects, one a human and the other a machine. If the interrogator fails as often as he or she succeeds in determining which is the human and which the machine, the machine could be considered as having intelligence. 
The Turing Test is not based on the completion of tasks or the solution of problems by the machine, but on the machine's ability to relate to a human being in conversation. Discourse is unique among human activities in that it subsumes all other activities within itself. 
Turing predicted that by the year 2000, there would be computers that could fool an interrogator at least thirty percent of the time. This, like most predictions in AI, was overly optimistic. No computer has yet come close to passing the Turing Test.
However, Turing also notes the importance of being in relationship for the acquisition of knowledge or intelligence. 
He estimates that the programming of the background knowledge needed for a restricted form of the game would take at a minimum three hundred person-years to complete. 
This is assuming that the appropriate knowledge set could be identified at the outset. 
Turing suggests that rather than trying to imitate an adult mind, computer scientists should attempt to construct a mind that simulates that of a child. 
Such a mind, when given an appropriate education, would learn and develop into an adult mind. 
One AI researcher taking this approach is Rodney Brooks of the Massachusetts Institute of Technology, whose lab has constructed several robots, including Cog and Kismet, that represent a new direction in AI in which embodiedness is crucial to the robot's design. 
Their programming is distributed among the various physical parts; each joint has a small processor that controls movement of that joint. 
These processors are linked with faster processors that allow for interaction between joints and for movement of the robot as a whole. 
These robots are designed to learn tasks associated with human infants, such as eye-hand coordination, grasping an object, and face recognition through social interaction with a team of researchers. 
Although the robots have developed abilities such as tracking moving objects with the eyes or withdrawing an arm when touched, Brooks's project is too new to be assessed. 
It may be no more successful than Lenat's Cyc in producing a machine that could interact with humans on the level of the Turing Test. 
However Brooks's work represents a movement toward Turing's opinion that intelligence is socially acquired and demonstrated.
The Turing Test makes no assumptions as to how the computer arrives at its answers; there need be no similarity in internal functioning between the computer and the human brain. 
However, an area of AI that shows some promise is that of neural networks, systems of circuitry that reproduce the patterns of neurons found in the brain. 
The human brain has billions of neurons and researchers have yet to understand both how these neurons are connected and how the various neurotransmitting chemicals in the brain function. 
Despite these limitations, neural nets have reproduced interesting behaviors in areas such as speech or image recognition, natural-language processing, and learning. 
Some researchers, including Hans Moravec and Raymond Kurzweil, see neural net research as a way to reverse engineer the brain. 
They hope that once scientists can design nets with a complexity equal to the human brain, the nets will have the same power as the brain and develop consciousness as an emergent property. 
Kurzweil posits that such mechanical brains, when programmed with a given person's memories and talents, could form a new path to immortality, while Moravec holds out hope that such machines might some day become our evolutionary children, capable of greater abilities than humans currently demonstrate.
Though researchers have continually projected that intelligent computers are immanent, progress in AI has been limited. 
Computers with intentionality and self consciousness, with fully human reasoning skills, or the ability to be in relationship, exist only in the realm of dreams and desires, a realm explored in fiction and fantasy.
The artificially intelligent computer in science fiction story and film is not a prop, but a character, one that has become a staple since the mid-1950s. 
These characters are embodied in a variety of physical forms, ranging from the wholly mechanical (computers and robots) to the partially mechanical (cyborgs) and the completely biological (androids). 
A general trend from the 1950s to the 1990s has been to depict intelligent computers in an increasingly anthropomorphic way. 
The robots and computers of early films, such as Maria in Fritz Lang's Metropolis (1926), Robby in Fred Wilcox's Forbidden Planet (1956), Hal in Stanley Kubrick's 2001: A Space Odyssey (1968), or R2D2 and C3PO in George Lucas's Star Wars (1977), were clearly constructs of metal. 
On the other hand, early science fiction stories, such as Isaac Asimov's I, Robot (1950), explored the question of how one might distinguish between robots that looked human and actual human beings. 
Films and stories from the 1980s through the early 2000s, including Ridley Scott's Blade Runner (1982) and Stephen Spielberg's A.I. (2001), pick up this question, depicting machines with both mechanical and biological parts that are far less easily distinguished from human beings.
Fiction that features AI can be classified in two general categories: cautionary tales (A.I., 2001 ) or tales of wish fulfillment (Star Wars ; I, Robot ). 
These present two differing visions of the artificially intelligent being, as a rival to be feared or as a friendly and helpful companion.
Artificial Intelligence (AI) is the key technology in many of today's novel applications, ranging from banking systems that detect attempted credit card fraud, to telephone systems that understand speech, to software systems that notice when you're having problems and offer appropriate advice. 
These technologies would not exist today without the sustained federal support of fundamental AI research over the past three decades.
Although there are some fairly pure applications of AI such as industrial robots, or the IntellipathTM pathology diagnosis system recently approved by the American Medical Association and deployed in hundreds of hospitals worldwide for the most part, AI does not produce stand-alone systems, but instead adds knowledge and reasoning to existing applications, databases, and environments, to make them friendlier, smarter, and more sensitive to user behavior and changes in their environments. 
The AI portion of an application (e.g., a logical inference or learning module) is generally a large system, dependent on a substantial infrastructure. 
Industrial R&D, with its relatively short time-horizons, could not have justified work of the type and scale that has been required to build the foundation for the civilian and military successes that AI enjoys today. 
And beyond the myriad of currently deployed applications, ongoing efforts that draw upon these decades of federally-sponsored fundamental research point towards even more impressive future capabilities:
Autonomous vehicles: A DARPA-funded onboard computer system from Carnegie Mellon University drove a van all but 52 of the 2849 miles from Washington, DC to San Diego, averaging 63 miles per hour day and night, rain or shine;
Computer chess: Deep Blue, a chess computer built by IBM researchers, defeated world champion Gary Kasparov in a landmark performance;
Mathematical theorem proving: A computer system at Argonne National Laboratories proved a long-standing mathematical conjecture about algebra using a method that would be considered creative if done by humans;
Scientific classification: A NASA system learned to classify very faint signals as either stars or galaxies with superhuman accuracy, by studying examples classified by experts;
Advanced user interfaces: PEGASUS is a spoken language interface connected to the American Airlines EAASY SABRE reservation system, which allows subscribers to obtain flight information and make flight reservations via a large, on-line, dynamic database, accessed through their personal computer over the telephone.
In a 1977 article, the late AI pioneer Allen Newell foresaw a time when the entire man-made world would be permeated by systems that cushioned us from dangers and increased our abilities: smart vehicles, roads, bridges, homes, offices, appliances, even clothes. 
Systems built around AI components will increasingly monitor financial transactions, predict physical phenomena and economic trends, control regional transportation systems, and plan military and industrial operations. 
Basic research on common sense reasoning, representing knowledge, perception, learning, and planning is advancing rapidly, and will lead to smarter versions of current applications and to entirely new applications. 
As computers become ever cheaper, smaller, and more powerful, AI capabilities will spread into nearly all industrial, governmental, and consumer applications.
Moreover, AI has a long history of producing valuable spin-off technologies. AI researchers tend to look very far ahead, crafting powerful tools to help achieve the daunting tasks of building intelligent systems. 
There is every reason to believe that AI will continue to produce such spin-off technologies.
Intellectually, AI depends on a broad intercourse with computing disciplines and with fields outside computer science, including logic, psychology, linguistics, philosophy, neuroscience, mechanical engineering, statistics, economics, and control theory, among others. 
This breadth has been necessitated by the grandness of the dual challenges facing AI: creating mechanical intelligence and understanding the information basis of its human counterpart. AI problems are extremely difficult, far more difficult than was imagined when the field was founded. 
However, as much as AI has borrowed from many fields, it has returned the favor: through its interdisciplinary relationships, AI functions as a channel of ideas between computing and other fields, ideas that have profoundly changed those fields. 
For example, basic notions of computation such as memory and computational complexity play a critical role in cognitive psychology, and AI theories of knowledge representation and search have reshaped portions of philosophy, linguistics, mechanical engineering and, control theory.
Early work in AI focused on using cognitive and biological models to simulate and explain human information processing skills, on "logical" systems that perform common-sense and expert reasoning, and on robots that perceive and interact with their environment. 
This early work was spurred by visionary funding from the Defense Advanced Research Projects Agency (DARPA) and Office of Naval Research (ONR), which began on a large scale in the early 1960's and continues to this day. 
Basic AI research support from DARPA and ONR -- as well as support from NSF, NIH, AFOSR, NASA, and the U.S. Army beginning in the 1970's -- led to theoretical advances and to practical technologies for solving military, scientific, medical, and industrial information processing problems.
By the early 1980's an "expert systems" industry had emerged, and Japan and Europe dramatically increased their funding of AI research. 
In some cases, early expert systems success led to inflated claims and unrealistic expectations: while the technology produced many highly effective systems, it proved very difficult to identify and encode the necessary expertise. 
The field did not grow as rapidly as investors had been led to expect, and this translated into some temporary disillusionment. 
AI researchers responded by developing new technologies, including streamlined methods for eliciting expert knowledge, automatic methods for learning and refining knowledge, and common sense knowledge to cover the gaps in expert information. 
These technologies have given rise to a new generation of expert systems that are easier to develop, maintain, and adapt to changing needs.
Today developers can build systems that meet the advanced information processing needs of government and industry by choosing from a broad palette of mature technologies. 
Sophisticated methods for reasoning about uncertainty and for coping with incomplete knowledge have led to more robust diagnostic and planning systems. 
Hybrid technologies that combine symbolic representations of knowledge with more quantitative representations inspired by biological information processing systems have resulted in more flexible, human-like behavior. 
AI ideas also have been adopted by other computer scientists -- for example, "data mining," which combines ideas from databases, AI learning, and statistics to yield systems that find interesting patterns in large databases, given only very broad guidelines.
Credit card providers, telephone companies, mortgage lenders, banks, and the U.S. Government employ AI systems to detect fraud and expedite financial transactions, with daily transaction volumes in the billions. 
These systems first use learning algorithms to construct profiles of customer usage patterns, and then use the resulting profiles to detect unusual patterns and take the appropriate action (e.g., disable the credit card). 
Such automated oversight of financial transactions is an important component in achieving a viable basis for electronic commerce.
AI systems configure custom computer, communications, and manufacturing systems, guaranteeing the purchaser maximum efficiency and minimum setup time, while providing the seller with superhuman expertise in tracking the rapid technological evolution of system components and specifications. 
These systems detect order incompletenesses and inconsistencies, employing large bodies of knowledge that describe the complex interactions of system components. 
Systems currently deployed process billions of dollars of orders annually; the estimated value of the market leader in this area is over a billion dollars.
Systems that diagnose and treat problems -- whether illnesses in people or problems in hardware and software -- are now in widespread use. 
Diagnostic systems based on AI technology are being built into photocopiers, computer operating systems, and office automation tools to reduce service calls. 
Stand-alone units are being used to monitor and control operations in factories and office buildings. 
AI-based systems assist physicians in many kinds of medical diagnosis, in prescribing treatments, and in monitoring patient responses. 
Microsoft's Office Assistant, an integral part of every Office 97 application, provides users with customized help by means of decision-theoretic reasoning.
The use of automatic scheduling for manufacturing operations is exploding as manufacturers realize that remaining competitive demands an ever more efficient use of resources. 
This AI technology -- supporting rapid rescheduling up and down the "supply chain" to respond to changing orders, changing markets, and unexpected events -- has shown itself superior to less adaptable systems based on older technology. 
This same technology has proven highly effective in other commercial tasks, including job shop scheduling, and assigning airport gates and railway crews. 
It also has proven highly effective in military settings -- DARPA reported that an AI-based logistics planning tool, DART, pressed into service for operations Desert Shield and Desert Storm, completely repaid its three decades of investment in AI research.
AI began as an attempt to answer some of the most fundamental questions about human existence by understanding the nature of intelligence, but it has grown into a scientific and technological field affecting many aspects of commerce and society.
Even as AI technology becomes integrated into the fabric of everyday life, AI researchers remain focused on the grand challenges of automating intelligence. 
Work is progressing on developing systems that converse in natural language, that perceive and respond to their surroundings, and that encode and provide useful access to all of human knowledge and expertise. 
The pursuit of the ultimate goals of AI -- the design of intelligent artifacts; understanding of human intelligence; abstract understanding of intelligence (possibly superhuman) -- continues to have practical consequences in the form of new industries, enhanced functionality for existing systems, increased productivity in general, and improvements in the quality of life. 
But the ultimate promises of AI are still decades away, and the necessary advances in knowledge and technology will require a sustained fundamental research effort.
The Enhanced Virtual Assistant, or Eva, enables members to do 200 transactions by just talking, including transferring money and paying bills. 
â€œIt makes search better and answers in a Siri-like voice. But this is a 1.0 version. 
Our next step is to create a virtual agent that is capable of learning. 
Most of our value is in moving money day-to-day for our members, but there are a lot of unique things we can do that happen less frequently with our 140 products. 
Our goal is to be our membersâ€™ personal financial agent for our full range of services.â€
In addition to working with large, established companies, IBM is also providing Watsonâ€™s capabilities to startups. 
IBM has set aside $100 million for investments in startups. 
One of the startups that is leveraging Watson is WayBlazer, a new venture in travel planning that is led by Terry Jones, a founder of Travelocity and Kayak. He told me:
â€œIâ€™ve spent my whole career in travel and IT. 
I started as a travel agent, and people would come in, and Iâ€™d send them a letter in a couple weeks with a plan for their trip. 
The Sabre reservation system made the process better by automating the channel between travel agents and travel providers. 
Then with Travelocity we connected travelers directly with travel providers through the Internet. 
Then with Kayak we moved up the chain again, providing offers across travel systems. 
Now with WayBlazer we have a system that deals with words. 
Nobody has helped people with a tool for dreaming and planning their travel. 
Our mission is to make it easy and give people several personalized answers to a complicated trip, rather than the millions of clues that search provides today. 
This new technology can take data out of all the silos and dark wells that companies donâ€™t even know they have and use it to provide personalized service.â€
Artificial Intelligence (AI) is an idea that has oscillated through many hype cycles over many years, as scientists and sci-fi visionaries have declared the imminent arrival of thinking machines. 
But it seems weâ€™re now at an actual tipping point. 
AI, expert systems, and business intelligence have been with us for decades, but this time the reality almost matches the rhetoric, driven by the exponential growth in technology capabilities (e.g., Mooreâ€™s Law), smarter analytics engines, and the surge in data.
Most people know the Big Data story by now: the proliferation of sensors (the â€œInternet of Thingsâ€) is accelerating exponential growth in â€œstructuredâ€ data. 
And now on top of that explosion, we can also analyze â€œunstructuredâ€ data, such as text and video, to pick up information on customer sentiment. 
Companies have been using analytics to mine insights within this newly available data to drive efficiency and effectiveness. 
For example, companies can now use analytics to decide which sales representatives should get which leads, what time of day to contact a customer, and whether they should e-mail them, text them, or call them.
Such mining of digitized information has become more effective and powerful as more info is â€œtaggedâ€ and as analytics engines have gotten smarter. 
As Dario Gil, Director of Symbiotic Cognitive Systems at IBM Research, told me:
â€œData is increasingly tagged and categorized on the Web â€“ as people upload and use data they are also contributing to annotation through their comments and digital footprints. 
This annotated data is greatly facilitating the training of machine learning algorithms without demanding that the machine-learning experts manually catalogue and index the world. 
Thanks to computers with massive parallelism, we can use the equivalent of crowdsourcing to learn which algorithms create better answers. 
For example, when IBMâ€™s Watson computer played â€˜Jeopardy!,â€™ the system used hundreds of scoring engines, and all the hypotheses were fed through the different engines and scored in parallel. 
It then weighted the algorithms that did a better job to provide a final answer with precision and confidence.â€
Interestingly, for a long time, doing detailed analytics has been quite labor- and people-intensive. 
You need â€œquants,â€ the statistically savvy mathematicians and engineers who build models that make sense of the data. 
As Babson professor and analytics expert Tom Davenport explained to me, humans are traditionally necessary to create a hypothesis, identify relevant variables, build and run a model, and then iterate it. 
Quants can typically create one or two good models per week.
However, machine learning tools for quantitative data â€“ perhaps the first line of AI â€“ can create thousands of models a week. 
For example, in programmatic ad buying on the Web, computers decide which ads should run in which publishersâ€™ locations. 
Massive volumes of digital ads and a never-ending flow of clickstream data depend on machine learning, not people, to decide which Web ads to place where. 
Firms like DataXu use machine learning to generate up to 5,000 different models a week, making decisions in under 15 milliseconds, so that they can more accurately place ads that you are likely to click on.
â€œI initially thought that AI and machine learning would be great for augmenting the productivity of human quants. 
One of the things human quants do, that machine learning doesnâ€™t do, is to understand what goes into a model and to make sense of it. 
Thatâ€™s important for convincing managers to act on analytical insights. 
For example, an early analytics insight at Osco Pharmacy uncovered that people who bought beer also bought diapers. 
But because this insight was counter-intuitive and discovered by a machine, they didnâ€™t do anything with it. 
But now companies have needs for greater productivity than human quants can address or fathom. 
They have models with 50,000 variables. These systems are moving from augmenting humans to automating decisions.â€
In business, the explosive growth of complex and time-sensitive data enables decisions that can give you a competitive advantage, but these decisions depend on analyzing at a speed, volume, and complexity that is too great for humans. 
AI is filling this gap as it becomes ingrained in the analytics technology infrastructure in industries like health care, financial services, and travel.
IBM is leading the integration of AI in industry. 
It has made a $1 billion investment in AI through the launch of its IBM Watson Group and has made many advancements and published research touting the rise of â€œcognitive computingâ€ â€“ the ability of computers like Watson to understand words (â€œnatural languageâ€), not just numbers. 
Rather than take the cutting edge capabilities developed in its research labs to market as a series of products, IBM has chosen to offer a platform of services under the Watson brand. 
It is working with an ecosystem of partners who are developing applications leveraging the dynamic learning and cloud computing capabilities of Watson.
The biggest application of Watson has been in health care. 
Watson excels in situations where you need to bridge between massive amounts of dynamic and complex text information (such as the constantly changing body of medical literature) and another mass of dynamic and complex text information (such as patient records  or genomic data), to generate and evaluate hypotheses. 
With training, Watson can provide recommendations for treatments for specific patients. 
Many prestigious academic medical centers, such as The Cleveland Clinic, The Mayo Clinic, MD Anderson, and Memorial Sloan-Kettering are working with IBM to develop systems that will help healthcare providers better understand patientsâ€™ diseases and recommend personalized courses of treatment. 
This has proven to be a challenging domain to automate and most of the projects are behind schedule.
Another large application area for AI is in financial services. 
Mike Adler, Global Financial Services Leader at The Watson Group, told me they have 45 clients working mostly on three applications: 
a â€œdigital virtual agentâ€ that enables banks and insurance companies to engage their customers in a new, personalized way.
a â€œwealth advisorâ€ that enables financial planning and wealth management, either for self-service or in combination with a financial advisor.
risk and compliance management.
For example, USAA, the $20 billion provider of financial services to people that serve, or have served, in the United States military, is using Watson to help their members transition from the military to civilian life. 
Neff Hudson, vice president of emerging channels at USAA, told me, â€œWeâ€™re always looking to help our members, and thereâ€™s nothing more critical than helping the 150,000+ people leaving the military every year. 
Their financial security goes down when they leave the military. 
Weâ€™re trying to use a virtual agent to intervene to be more productive for them.â€ 
USAA also uses AI to enhance navigation on their popular mobile app. 
As Mooreâ€™s Law marches on, we have more power in our smartphones than the most powerful supercomputers did 30 or 40 years ago. 
Ray Kurzweil has predicted that the computing power of a $4,000 computer will surpass that of a human brain in 2019 (20 quadrillion calculations per second). 
What does it all mean for the future of AI?
To get a sense, I talked to some venture capitalists, whose profession it is to keep their eyes and minds trained on the future. 
Mark Gorenberg, Managing Director at Zetta Venture Partners, which is focused on investing in analytics and data startups, told me, â€œAI historically was not ingrained in the technology structure. 
Now weâ€™re able to build on top of ideas and infrastructure that didnâ€™t exist before. 
Weâ€™ve gone through the change of Big Data. Now weâ€™re adding machine learning. 
AI is not the be-all and end-all; itâ€™s an embedded technology. Itâ€™s like taking an application and putting a brain into it, using machine learning. 
Itâ€™s the use of cognitive computing as part of an application.â€ 
Another veteran venture capitalist, Promod Haque, senior managing partner at Norwest Venture Partners, explained to me, â€œif you can have machines automate the correlations and build the models, you save labor and increase speed. 
With tools like Watson, lots of companies can do different kinds of analytics automatically.â€
Manoj Saxena, former head of IBMâ€™s Watson efforts and now a venture capitalist, believes that analytics is moving to the â€œcognitive cloudâ€ where massive amounts of first- and third-party data will be fused to deliver real-time analysis and learning. 
Companies often find AI and analytics technology difficult to integrate, especially with the technology moving so fast; thus, he sees collaborations forming where companies will bring their people with domain knowledge, and emerging service providers will bring system and analytics people and technology. 
Cognitive Scale (a startup that Saxena has invested in) is one of the new service providers adding more intelligence into business processes and applications through a model they are calling â€œCognitive Garages.â€ 
Using their â€œ10-10-10 methodâ€ they deploy a cognitive cloud in 10 seconds, build a live app in 10 hours, and customize it using their clientâ€™s data in 10 days. 
Saxena told me that the company is growing extremely rapidly.
Iâ€™ve been tracking AI and expert systems for years. 
What is most striking now is its genuine integration as an important strategic accelerator of Big Data and analytics. 
Applications such as USAAâ€™s Eva, healthcare systems using IBMâ€™s Watson, and WayBlazer, among others, are having a huge impact and are showing the way to the next generation of AI.
What was once just a figment of the imagination of some our most famous science fiction writers, artificial intelligence (AI) is taking root in our everyday lives. 
Weâ€™re still a few years away from having robots at our beck and call, but AI has already had a profound impact in more subtle ways. 
Weather forecasts, email spam filtering, Googleâ€™s search predictions, and voice recognition, such Appleâ€™s Siri, are all examples. 
What these technologies have in common are machine-learning algorithms that enable them to react and respond in real time. 
There will be growing pains as AI technology evolves, but the positive effect it will have on society in terms of efficiency is immeasurable.
AI isnâ€™t a new concept; its storytelling roots go as far back as Greek antiquity. 
However, it was less than a century ago that the technological revolution took off and AI went from fiction to very plausible reality. 
Alan Turing, British mathematician and WWII code-breaker, is widely credited as being one of the first people to come up with the idea of machines that think in 1950. 
He even created the Turing test, which is still used today, as a benchmark to determine a machineâ€™s ability to â€œthinkâ€ like a human. 
Though his ideas were ridiculed at the time, they set the wheels in motion, and the term â€œartificial intelligenceâ€ entered popular awareness in the mid- 1950s, after Turing died.
American cognitive scientist Marvin Minsky picked up the AI torch and co-founded the Massachusetts Institute of Technologyâ€™s AI laboratory in 1959, and he was one of the leading thinkers in the field through the 1960s and 1970s. 
He even advised Stanley Kubrick on â€œ2001: A Space Odyssey,â€ released in 1968, which gave the world one of the best representations of AI in the form of HAL 9000. The rise of the personal computer in the 1980s sparked even more interest in machines that think.
But it took a couple of decades for people to recognize the true power of AI. 
High-profile investors and physicists, like Elon Musk, founder of Tesla, and Stephen Hawking, are continuing the conversation about the potential for AI technology. 
While the discussion occasionally turns to potential doomsday scenarios, there is a consensus that when used for good, AI could radically change the course of human history. 
And that is especially true when it comes to big data.
The very premise of AI technology is its ability to continually learn from the data it collects. 
The more data there is to collect and analyze through carefully crafted algorithms, the better the machine becomes at making predictions. 
Not sure what movie to watch tonight? Donâ€™t worry; Netflix has some suggestions for you based on your previous viewing experiences. 
Donâ€™t feel like driving? Googleâ€™s working on a solution for that, too, racking up the miles on its driverless car prototype.
Nowhere has AI had a greater impact in the early stages of the 21st century than in the office. 
Machine-learning technologies are driving increases in productivity never before seen. 
From workflow management tools to trend predictions and even the way brands purchase advertising, AI is changing the way we do business. 
In fact, a Japanese venture capital firm recently became the first company in history to nominate an AI board member for its ability to predict market trends faster than humans.
Big data is a goldmine for businesses, but companies are practically drowning in it. 
Yet, itâ€™s been a primary driver for AI advancements, as machine-learning technologies can collect and organize massive amounts of information to make predictions and insights that are far beyond the capabilities of manual processing. 
Not only does it increase organizational efficiency, but it dramatically reduces the likelihood that a critical mistake will be made. 
AI can detect irregular patterns, such as spam filtering or payment fraud, and alert businesses in real time about suspicious activities. 
Businesses can â€œtrainâ€ AI machines to handle incoming customer support calls, reducing costs. 
It can even be used to optimize the sales funnel by scanning the database and searching the Web for prospects that exhibit the same buying patterns as existing customers.
There is so much potential for AI development that itâ€™s getting harder to imagine a future without it. 
Weâ€™re already seeing an increase in workplace productivity thanks to AI advancements. 
By the end of the decade, AI will become commonplace in everyday life, whether itâ€™s self-driving cars, more accurate weather predictions, or space exploration. 
We will even see machine-learning algorithms used to prevent cyberterrorism and payment fraud, albeit with increasing public debate over privacy implications. 
AI will also have a strong impact in healthcare advancements due to its ability to analyze massive amounts of genomic data, leading to more accurate prevention and treatment of medical conditions on a personalized level.
But donâ€™t expect a machine takeover any time soon. 
As easy as it is for machine-learning technology to self-improve, what it lacks is intuition. 
Thereâ€™s a gut instinct that canâ€™t be replicated via algorithms, making humans an important piece of the puzzle. 
The best way forward is for humans and machines to live harmoniously, leaning on one anotherâ€™s strengths. 
Advertising is a perfect example, where machines are now doing much of the purchasing through programmatic exchanges to maximize returns on investment, allowing advertisers to focus on creating more engaging content.
While early science fiction writers might have expected more from AI at this stage, the rest of the world is generally satisfied with our progress. 
After all, not everyone is ready for humanoid robots or self-learning spaceships.
Data communications refers to the transmission of this digital data between two or more computers and a computer network or data network is a telecommunications network that allows to exchange data. 
The physical connection between networked computing devices is established using either cable media or wireless media. 
The best-known computer network is the Internet.
This tutorial should teach you basics of Data Communication and Computer Network (DCN) and will also take you through various advance concepts related to Data Communication and Computer Network.
As one of the fastest growing technologies in our culture today, data communications and networking presents a unique challenge for instructors. 
As both the number and types of students are increasing, it is essential to have a textbook that provides coverage of the latest advances, while presenting the material in a way that is accessible to students with little or no background in the field. 
Using a bottom-up approach, Data Communications and Networking presents this highly technical subject matter without relying on complex formulas by using a strong pedagogical approach supported by more than 700 figures. 
Now in its Fourth Edition, this textbook brings the beginning student right to the forefront of the latest advances in the field, while presenting the fundamentals in a clear, straightforward manner. 
Students will find better coverage, improved figures and better explanations on cutting-edge material. 
The "bottom-up" approach allows instructors to cover the material in one course, rather than having separate courses on data communications and networking. 
The distance over which data moves within a computer may vary from a few thousandths of an inch, as is the case within a single IC chip, to as much as several feet along the backplane of the main circuit board. 
Over such small distances, digital data may be transmitted as direct, two-level electrical signals over simple copper conductors. 
Except for the fastest computers, circuit designers are not very concerned about the shape of the conductor or the analog characteristics of signal transmission.
Frequently, however, data must be sent beyond the local circuitry that constitutes a computer. 
In many cases, the distances involved may be enormous. 
Unfortunately, as the distance between the source of a message and its destination increases, accurate transmission becomes increasingly difficult. 
This results from the electrical distortion of signals traveling through long conductors, and from noise added to the signal as it propagates through a transmission medium. 
Although some precautions must be taken for data exchange within a computer, the biggest problems occur when data is transferred to devices outside the computer's circuitry. 
In this case, distortion and noise can become so severe that information is lost.
Data Communications concerns the transmission of digital messages to devices external to the message source. 
"External" devices are generally thought of as being independently powered circuitry that exists beyond the chassis of a computer or other digital message source. 
As a rule, the maximum permissible transmission rate of a message is directly proportional to signal power, and inversely proportional to channel noise. 
It is the aim of any communications system to provide the highest possible transmission rate at the lowest possible power and with the least possible noise.
Data communications and networking are changing the way we do business and the way we live.
Business decisions have to be made ever more quickly, and the decision makers require immediate access to accurate information. 
Data communication refers to the exchange of data between a source and a receiver. 
Data communication is said to be local if communicating devices are in the same building or a similarly restricted geographical area.
The meanings of source and receiver are very simple. 
The device that transmits the data is known as source and the device that receives the transmitted data is known as receiver. 
Data communication aims at the transfer of data and maintenance of the data during the process but not the actual generation of the information at the source and receiver. 
Datum mean the facts information statistics or the like derived by calculation or experimentation. 
The facts and information so gathered are processed in accordance with defined systems of procedure. 
Data can exist in a variety of forms such as numbers, text, bits and bytes. 
The Figure is an illustration of a simple data communication system.simple data communication system
A data communication system may collect data from remote locations through data transmission circuits, and then outputs processed results to remote locations. 
The different data communication techniques which are presently in widespread use evolved gradually either to improve the data communication techniques already existing or to replace the same with better options and features.
Then, there are data communication jargons to contend with such as baud rate, modems, routers, LAN, WAN, TCP/IP, ISDN, during the selection of communication systems. 
Hence, it becomes necessary to review and understand these terms and gradual development of data communication methods.
The distance over which data moves within a computer can be as small as thousandths of a centimetre, however, frequently data must be sent beyond the realms of the computers' circuitry and, in many cases, across vast distances. 
Data communications refers to the transmission of this digital data.
Before computer networks were based on telecommunication systems, communication between machines was performed by people carrying instructions from one machine to another, and many of the social aspects of the internet as we have come to know it started in this format. 
Throughout the 1940s, 50s and 60s computer and communications systems advanced exponentially and even today computer networks and the technology needed to communicate between them continues to drive computer hardware, software and service industries. 
Alongside the technological development has been an equally strong growth in the volume and types of users accessing data communications devices.
The scope of communication has increased enormously and this would not have been possible without the advancement of the computer network; as such this is not only an important but also an incredibly fast-paced arena to study.
Courses in data communications combine the study of networks, electronics, digital and analogue processes, software, system design, physical technology and signal processing. 
Courses are designed to be intellectually demanding whilst also providing a strong vocational element which will equip students with a high level of technical ability as well as the knowledge to forge a successful career managing the challenge of technological changes in communications.
Assessment of courses is by a variety of practical assignments as well as written examinations. 
Many courses also offer assistance in identifying industrial placements either during the summer breaks or as a sandwich year. 
Unusually for undergraduate level courses there are also a large number of sponsorship opportunities and institutions that offer data communications courses are often very flexible in accommodating a sponsors needs (for example if the sponsor requires you to defer for a year in order to embed you within their company prior to gaining your qualification) or in assisting in identifying the correct sponsor for you. 
It is worth bearing this in mind when considering where you want to study: links with industry will differ from institution to institution and you should consider your personal preferences before applying.
Data communications analysts, sometimes called computer network architects, use their knowledge of network structure and operations to create dynamic and efficient systems for use by businesses and organizations. 
These information technology professionals maintain and update systems in order to keep them working and secure.
Most often, data communication analysts hold a bachelor's degree and have experience working with network systems. 
Data communication analysts are professionals who deal with testing, proofing, and designing various network systems for LANs (local area networks), WANs (wide area networks), and the Internet. 
Some analysts use data obtained in the network design to model and upgrade future network systems. 
In addition, data communication analysts may work with computer programmers in interfacing programs and communication systems. 
The distance over which data moves within a computer may vary from a few thousandths of an inch, as is the case within a single IC chip, to as much as several feet along the backplane of the main circuit board. 
Over such small distances, digital data may be transmitted as direct, two-level electrical signals over simple copper conductors. 
Except for the fastest computers, circuit designers are not very concerned about the shape of the conductor or the analog characteristics of signal transmission.
Frequently, however, data must be sent beyond the local circuitry that constitutes a computer. 
In many cases, the distances involved may be enormous. Unfortunately, as the distance between the source of a message and its destination increases, accurate transmission becomes increasingly difficult. 
This results from the electrical distortion of signals traveling through long conductors, and from noise added to the signal as it propagates through a transmission medium. 
Although some precautions must be taken for data exchange within a computer, the biggest problems occur when data is transferred to devices outside the computer's circuitry. 
In this case, distortion and noise can become so severe that information is lost.
Data Communications concerns the transmission of digital messages to devices external to the message source. 
"External" devices are generally thought of as being independently powered circuitry that exists beyond the chassis of a computer or other digital message source. 
As a rule, the maximum permissible transmission rate of a message is directly proportional to signal power, and inversely proportional to channel noise. 
It is the aim of any communications system to provide the highest possible transmission rate at the lowest possible power and with the least possible noise. 
A communications channel is a pathway over which information can be conveyed. 
It may be defined by a physical wire that connects communicating devices, or by a radio, laser, or other radiated energy source that has no obvious physical presence. 
Information sent through a communications channel has a source from which the information originates, and a destination to which the information is delivered. 
Although information originates from a single source, there may be more than one destination, depending upon how many receive stations are linked to the channel and how much energy the transmitted signal possesses.
In a digital communications channel, the information is represented by individual data bits, which may be encapsulated into multibit message units. 
A byte, which consists of eight bits, is an example of a message unit that may be conveyed through a digital communications channel. 
A collection of bytes may itself be grouped into a frame or other higher-level message unit. 
Such multiple levels of encapsulation facilitate the handling of messages in a complex data communications network.
The message source is the transmitter, and the destination is the receiver. A channel whose direction of transmission is unchanging is referred to as a simplex channel. 
For example, a radio station is a simplex channel because it always transmits the signal to its listeners and never allows them to transmit back.
A half-duplex channel is a single physical channel in which the direction may be reversed. Messages may flow in two directions, but never at the same time, in a half-duplex system. 
In a telephone call, one party speaks while the other listens. 
After a pause, the other party speaks and the first party listens. 
Speaking simultaneously results in garbled sound that cannot be understood.
A full-duplex channel allows simultaneous message exchange in both directions. 
It really consists of two simplex channels, a forward channel and a reverse channel, linking the same points. 
The transmission rate of the reverse channel may be slower if it is used only for flow control of the forward channel.
Most digital messages are vastly longer than just a few bits. 
Because it is neither practical nor economic to transfer all bits of a long message simultaneously, the message is broken into smaller parts and transmitted sequentially. 
Bit-serial transmission conveys a message one bit at a time through a channel. Each bit represents a part of the message. 
The individual bits are then reassembled at the destination to compose the message. In general, one channel will pass only one bit at a time. 
Thus, bit-serial transmission is necessary in data communications if only a single channel is available. 
Bit-serial transmission is normally just called serial transmission and is the chosen communications method in many computer peripherals.
Byte-serial transmission conveys eight bits at a time through eight parallel channels. 
Although the raw transfer rate is eight times faster than in bit-serial transmission, eight channels are needed, and the cost may be as much as eight times higher to transmit the message. 
When distances are short, it may nonetheless be both feasible and economic to use parallel channels in return for high data rates. 
The popular Centronics printer interface is a case where byte-serial transmission is used. 
As another example, it is common practice to use a 16-bit-wide data bus to transfer data between a microprocessor and memory chips; this provides the equivalent of 16 parallel channels. 
On the other hand, when communicating with a timesharing system over a modem, only a single channel is available, and bit-serial transmission is required.                          
The baud rate refers to the signalling rate at which data is sent through a channel and is measured in electrical transitions per second. 
In the EIA232 serial interface standard, one signal transition, at most, occurs per bit, and the baud rate and bit rate are identical. 
In this case, a rate of 9600 baud corresponds to a transfer of 9,600 data bits per second with a bit period of 104 microseconds (1/9600 sec.). 
If two electrical transitions were required for each bit, as is the case in non-return-to-zero coding, then at a rate of 9600 baud, only 4800 bits per second could be conveyed. 
The channel efficiency is the number of bits of useful information passed through the channel per second. 
It does not include framing, formatting, and error detecting bits that may be added to the information bits before a message is transmitted, and will always be less than one.
The data rate of a channel is often specified by its bit rate (often thought erroneously to be the same as baud rate). 
However, an equivalent measure channel capacity is bandwidth. 
In general, the maximum data rate a channel can support is directly proportional to the channel's bandwidth and inversely proportional to the channel's noise level.
A communications protocol is an agreed-upon convention that defines the order and meaning of bits in a serial transmission. 
It may also specify a procedure for exchanging messages. 
A protocol will define how many data bits compose a message unit, the framing and formatting bits, any error-detecting bits that may be added, and other information that governs control of the communications hardware. 
Channel efficiency is determined by the protocol design rather than by digital hardware considerations. 
Note that there is a tradeoff between channel efficiency and reliability - protocols that provide greater immunity to noise by adding error-detecting and -correcting codes must necessarily become less efficient.
Serialized data is not generally sent at a uniform rate through a channel. 
Instead, there is usually a burst of regularly spaced binary data bits followed by a pause, after which the data flow resumes. 
Packets of binary data are sent in this manner, possibly with variable-length pauses between packets, until the message has been fully transmitted. 
In order for the receiving end to know the proper moment to read individual binary bits from the channel, it must know exactly when a packet begins and how much time elapses between bits. 
When this timing information is known, the receiver is said to be synchronized with the transmitter, and accurate data transfer becomes possible. 
Failure to remain synchronized throughout a transmission will cause data to be corrupted or lost.
Two basic techniques are employed to ensure correct synchronization. 
In synchronous systems, separate channels are used to transmit data and timing information. 
The timing channel transmits clock pulses to the receiver. 
Upon receipt of a clock pulse, the receiver reads the data channel and latches the bit value found on the channel at that moment. 
The data channel is not read again until the next clock pulse arrives. 
Because the transmitter originates both the data and the timing pulses, the receiver will read the data channel only when told to do so by the transmitter (via the clock pulse), and synchronization is guaranteed.
Techniques exist to merge the timing signal with the data so that only a single channel is required. 
This is especially useful when synchronous transmissions are to be sent through a modem. 
Two methods in which a data signal is self-timed are nonreturn-to-zero and biphase Manchester coding. 
These both refer to methods for encoding a data stream into an electrical waveform for transmission.
In asynchronous systems, a separate timing channel is not used. The transmitter and receiver must be preset in advance to an agreed-upon baud rate. 
A very accurate local oscillator within the receiver will then generate an internal clock signal that is equal to the transmitter's within a fraction of a percent. 
For the most common serial protocol, data is sent in small packets of 10 or 11 bits, eight of which constitute message information. 
When the channel is idle, the signal voltage corresponds to a continuous logic '1'. 
A data packet always begins with a logic '0' (the start bit) to signal the receiver that a transmission is starting.
The start bit triggers an internal timer in the receiver that generates the needed clock pulses. 
Following the start bit, eight bits of message data are sent bit by bit at the agreed upon baud rate. 
The packet length is short in asynchronous systems to minimize the risk that the local oscillators in the receiver and transmitter will drift apart. 
When high-quality crystal oscillators are used, synchronization can be guaranteed over an 11-bit period. 
Every time a new packet is sent, the start bit resets the synchronization, so the pause between packets can be arbitrarily long. 
Note that the EIA232 standard defines electrical, timing, and mechanical characteristics of a serial interface. 
However, it does not include the asynchronous serial protocol shown in the previous figure, or the ASCII alphabet described next.
This standard relates binary codes to printable characters and control codes. 
Fully 25 percent of the ASCII character set represents nonprintable control codes, such as carriage return (CR) and line feed (LF). 
Most modern character-oriented peripheral equipment abides by the ASCII standard, and thus may be used interchangeably with different computers.
Noise and momentary electrical disturbances may cause data to be changed as it passes through a communications channel. 
If the receiver fails to detect this, the received message will be incorrect, resulting in possibly serious consequences. 
As a first line of defense against data errors, they must be detected. 
If an error can be flagged, it might be possible to request that the faulty packet be resent, or to at least prevent the flawed data from being taken as correct. 
If sufficient redundant information is sent, one- or two-bit errors may be corrected by hardware within the receiver before the corrupted data ever reaches its destination.
A parity bit is added to a data packet for the purpose of error detection. 
In the even-parity convention, the value of the parity bit is chosen so that the total number of '1' digits in the combined data plus parity packet is an even number. 
Upon receipt of the packet, the parity needed for the data is recomputed by local hardware and compared to the parity bit received with the data. 
If any bit has changed state, the parity will not match, and an error will have been detected. 
In fact, if an odd number of bits (not just one) have been altered, the parity will not match. 
If an even number of bits have been reversed, the parity will match even though an error has occurred. 
However, a statistical analysis of data communication errors has shown that a single-bit error is much more probable than a multibit error in the presence of random noise. 
Another approach to error detection involves the computation of a checksum. 
In this case, the packets that constitute a message are added arithmetically. 
A checksum number is appended to the packet sequence so that the sum of data plus checksum is zero. 
When received, the packet sequence may be added, along with the checksum, by a local microprocessor. 
If the sum is nonzero, an error has occurred. 
As long as the sum is zero, it is highly unlikely (but not impossible) that any data has been corrupted during transmission.
Errors may not only be detected, but also corrected if additional code is added to a packet sequence. 
If the error probability is high or if it is not possible to request retransmission, this may be worth doing. 
However, including error-correcting code in a transmission lowers channel efficiency, and results in a noticeable drop in channel throughput.
data communications, application of telecommunications technology to the problem of transmitting data, especially to, from, or between computers
In popular usage, it is said that data communications make it possible for one computer to "talk" with another. 
Telephone circuits are often used to transfer data, although their relatively limited bandwidth makes them relatively slow paths for data. 
Recent techniques, however, have made it possible to send data over phone lines at rates of 28,800 bits per second and higher. 
A modem is required for such telephone communications when they occur over standard (analog) telephone circuits. 
Where cost can be justified, high speed data links are constructed; these are often fiber-optic or coaxial cables designed for wide frequency range, or microwave, radio links. 
Local-area and wide-area networks link computers together so that they can transfer and share data. 
Computer is connected to the network at any given time, techniques such as time-division multiplexing are used; each computer is assigned a short time slot during which it can use the full bandwidth of the network. 
Packet switching allows a single channel to be used for multiple concurrent transmissions. 
Data packets contain addresses that indicate the intended destination. 
To minimize data-communication errors, special codes are used.
The conveyance of information from a source to a destination. 
Data means the symbolic representation of information, generally in a digital (that is, discrete) form. 
Analog information refers to information encoded according to a continuous physical parameter, such as the height or amplitude of a waveform, while digital information is encoded into a discrete set of some parameter.) 
Usually, this digital information is composed of a sequence of binary digits (ones and zeros), called bits. 
The binary system is used because its simplicity is universally recognizable and because digital data have greater immunity to so-called noise than analog information and allow flexible processing of the information. 
Groups of eight bits create a data byte or character. 
These characters make up the so-called alphabets (including alphabetic, numeric, and special symbols) which are used in data communications. 
The most common data sources and destinations are computers and computer peripherals, and the data represent groups of characters to form text, hypertext (text organized according to topic rather than linear sequence), or multimedia information, including audio, graphics, animation, and video information. 
See Bit, Computer, Computer graphics, Computer peripheral devices, Digital computer, Multimedia technology
Data communications may be accomplished through two principal functions, data transmission and data switching.
Data transmission always involves at least three elements: a source of the information, a channel for the transmission of the information, and a destination for the information. 
In addition, sometimes the data are encoded. 
The codes can be used for error detection and correction, compression of the digital data, and so forth. See Data compression
The communications channel is carried over a transmission medium. 
Such media can be wired, as in the cases of twisted-pair telephone wires, coaxial cables, or fiber-optic cables, or they can be wireless, where the transmission is not confined to any physical medium, such as in radio, satellite, or infrared optical transmission. 
Sometimes, even when the source of the information is digital, the transmission medium requires analog signaling, and modems (modulators-demodulators) are required to convert the digital signals to analog, and vice versa. 
For example, data communication between personal computers transmitted over telephone lines normally uses modems. See Coaxial cable, Communications cable, Modem, Optical communications, Radio, Telephone service
The directionality of the information can be either one-way (simplex communications) or two-way. 
Two-way communications can be either half-duplex (information goes both ways over the communications link, but not at the same time) or full-duplex (information goes both ways at the same time).
The data channel can be a serial channel, in which the bits are transmitted one after another across a single physical connection; or a parallel channel, in which many bits are transmitted simultaneously (for instance, over parallel wires). 
Generally, parallel channels are used for short-distance links (less than 300 ft or 100 m), whereas serial links are used for larger distances and high data rates.
At low data rates (less than a few hundred kilobits per second) communications channels are typically dedicated, whereas at higher data rates, because of the cost of high-speed transmitters and receivers, shared channels are used by multiplexing the data streams.
For example, two independent data streams with constant data rates of 10 megabits per second (Mb/s) could use a shared channel having a data-rate capability of 20 Mb/s. 
The multiplexing system would select one bit from each of the two channels to time-multiplex the data together.
In many cases, the source, the destination, and the path taken by the data may vary; thus switching is required.
The two primary types of switching employed in data communications are circuit switching and packet switching. 
In circuit-switched data communications, an end-to-end connection is established prior to the actual transmission of the data and the communications channel is open (whether or not it is in use) until the connection is removed.
A packet is a group of data bytes which represents a specific information unit with a known beginning and end. 
The packet can be formed from either a fixed or variable number of bytes. 
Some of these bytes represent the information payload, while the rest represent the header, which contains address information to be used in routing the packet. 
In packet switching, unlike circuit switching, the packets are sent only when information transmission is required. 
The channel is not used when there is no information to be sent. 
Sharing the channel capacity through multiplexing is natural for packet-switched systems. 
Furthermore, the packet switches allow for temporary loading of the network beyond the transmission capacity of the channel. 
This information overload is stored (buffered) in the packet switches and sent on when the channel becomes available.
In order to transfer information from a sender to a receiver, a common physical transmission protocol must be used. 
Protocols can range from very simple to quite complex. 
The Open Systems Interconnect (OSI) model, developed by the International Standards Organization, reduces protocol complexity by breaking the protocol into smaller functional units which operate in conjunction with similar functional units at a peer-to-peer level. 
Each layer performs functions for the next higher layer by building on the functions provided by the layer below. 
The advantage of performing communications based on this model is that at the application layers (user processes) there is no concern with the communications mechanisms. 
See Electrical communications, Integrated services digital network (ISDN), Packet switching, Switching systems (communications).
data communications, application of telecommunications technology to the problem of transmitting data, especially to, from, or between computers. 
In popular usage, it is said that data communications make it possible for one computer to "talk" with another. 
Telephone circuits are often used to transfer data, although their relatively limited bandwidth makes them relatively slow paths for data. 
Recent techniques, however, have made it possible to send data over phone lines at rates of 28,800 bits per second and higher. 
A modem is required for such telephone communications when they occur over standard (analog) telephone circuits. 
Where cost can be justified, high speed data links are constructed; these are often fiber-optic or coaxial cables designed for wide frequency range, or microwave, radio links. 
Local-area and wide-area networks link computers together so that they can transfer and share data. 
Because many computers are on the network at any given time, techniques such as time-division multiplexing are used; each computer is assigned a short time slot during which it can use the full bandwidth of the network. 
Packet switching allows a single channel to be used for multiple concurrent transmissions. 
Data packets contain addresses that indicate the intended destination. 
To minimize data-communication errors, special codes are used.
Data communication refers to the exchange of data between a source and a receiver. 
Data communication is said to be local if communicating devices are in the same building or a similarly restricted geographical area.
The meanings of source and receiver are very simple. The device that transmits the data is known as source and the device that receives the transmitted data is known as receiver. 
Data communication aims at the transfer of data and maintenance of the data during the process but not the actual generation of the information at the source and receiver. 
Datum mean the facts information statistics or the like derived by calculation or experimentation. 
The facts and information so gathered are processed in accordance with defined systems of procedure. 
Data can exist in a variety of forms such as numbers, text, bits and bytes. 
A data communication system may collect data from remote locations through data transmission circuits, and then outputs processed results to remote locations. 
The different data communication techniques which are presently in widespread use evolved gradually either to improve the data communication techniques already existing or to replace the same with better options and features. 
Then, there are data communication jargons to contend with such as baud rate, modems, routers, LAN, WAN, TCP/IP, ISDN, during the selection of communication systems. 
Hence, it becomes necessary to review and understand these terms and gradual development of data communication methods
The International Journal of Business Data Communications and Networking (IJBDCN) examines the impact of data communications and networking technologies, policies, and management on business organizations, capturing their effect on IT-enabled management practices. 
This journal includes analytical and empirical research articles, business case studies, and surveys that provide solutions and insight into challenges facing telecommunication service providers, equipment manufacturers, enterprise users, and policy makers. 
IJBDCN covers the principles of both wired and wireless communications of voice, data, images, and video and the impact of their business values on the organizations in which they are used. 
This journal includes theoretical and practical works, relevant case studies, topical surveys, and research articles that address problems faced by telecommunication service providers, equipment manufacturers, enterprises, and policy makers in the areas of data communications and networking.
Data communications equipment (DCE) refers to computer hardware devices used to establish, maintain and terminate communication network sessions between a data source and its destination. 
DCE is connected to the data terminal equipment (DTE) and data transmission circuit (DTC) to convert transmission signals. 
IT vendors may also refer to data communications equipment as data circuit-terminating equipment or data carrier equipment
A modem is a typical example of data communications equipment. In general, data communications equipment is used to perform signal exchange, coding and line clocking tasks as a part of intermediate equipment or DTE. 
Some additional interfacing electronic equipment may also be needed to pair the DTE with a transmission channel or to connect a circuit to the DTE. 
DCE and DTE are often confused with each other, but these are two different device types that are interlinked with an RS-232 serial line. 
DTE and DCE connectors are wired differently if a single straight cable is employed. DCE generates internal clock signals, while DTE works with externally provided signals. 
Without employing a modem, the DCE and DTE can be connected through a crossable cable medium like a null modem for Ethernet or typical RS-232 serial line. 
Many modems are DCE, while the computer terminal is DTE.
In the current United States National Airspace System, all communications with airborne aircraft is by voice communications. 
Aircraft route of flight revisions must be communicated through multiple change-of-course instructions or lengthy verbal reroute instructions, which must be repeated; are prone to verbal communications errors; and entry errors into an aircraft's flight management system. 
The use of voice communication is labor and time intensive and will limit the ability of the Federal Aviation Administration (FAA) to effectively meet future traffic demand in the United States.
Adding air-to-ground and ground-to-ground data communications will significantly reduce controller-to-pilot communications and controller workload. 
The data communications will enable ground automated message generation and receipt, message routing and transmission, and direct communications with aircraft avionics.
Initially, data communications will be an additional means for two-way exchange between controllers and flight crews for air traffic control clearances, instructions, advisories, flight crew requests and reports. 
Eventually, the majority of communications will be handled by data communications for appropriately equipped ground and airborne stations. 
Data communications will enable air traffic control to issue an entire route of a flight with a single data transmission directly to the aircraft's flight management system.
NexCom will be an eventual replacement for the existing Future Air Navigation System that is currently used primarily by transoceanic commercial airliners.
Voice communications contribute to operational errors due to miscommunication, stolen clearances (an air traffic control clearance for one aircraft is heard and erroneously accepted by another aircraft) and delayed message transfers due to radio frequency congestion.
Data communications will enable air traffic controller productivity improvements and will permit capacity growth without requisite growth in costs associated with infrastructure equipment, maintenance, labor and training. 
As a result, the resources required to provide air traffic management service per aircraft operation will decrease. 
The use of real-time aircraft data by ground systems to plot 4-dimensional trajectories (lateral and vertical navigation, ground speed and longitudinal navigation), and perform conformance management, will shift air traffic operations from minute-by-minute tactical control, to more predictable and planned strategic traffic management.
If a typical message were statistically analyzed, it would be found that certain characters are used much more frequently than others. 
By analyzing a message before it is transmitted, short binary codes may be assigned to frequently used characters and longer codes to rarely used characters. 
In doing so, it is possible to reduce the total number of characters sent without altering the information in the message. Appropriate decoding at the receiver will restore the message to its original form. 
This procedure, known as data compression, may result in a 50 percent or greater savings in the amount of data transmitted. 
Even though time is necessary to analyze the message before it is transmitted, the savings may be great enough so that the total time for compression, transmission, and decompression will still be lower than it would be when sending an uncompressed message.
Some kinds of data will compress much more than others. Data that represents images, for example, will usually compress significantly, perhaps by as much as 80 percent over its original size. 
Data representing a computer program, on the other hand, may be reduced only by 15 or 20 percent.
A compression method called Huffman coding is frequently used in data communications, and particularly in fax transmission. 
Clearly, most of the image data for a typical business letter represents white paper, and only about 5 percent of the surface represents black ink. 
It is possible to send a single code that, for example, represents a consecutive string of 1000 white pixels rather than a separate code for each white pixel. 
Consequently, data compression will significantly reduce the total message length for a faxed business letter. 
Were the letter made up of randomly distributed black ink covering 50 percent of the white paper surface, data compression would hold no advantages.
Privacy is a great concern in data communications. 
Faxed business letters can be intercepted at will through tapped phone lines or intercepted microwave transmissions without the knowledge of the sender or receiver. 
To increase the security of this and other data communications, including digitized telephone conversations, the binary codes representing data may be scrambled in such a way that unauthorized interception will produce an indecipherable sequence of characters.
Authorized receive stations will be equipped with a decoder that enables the message to be restored. 
The process of scrambling, transmitting, and descrambling is known as encryption.
Custom integrated circuits have been designed to perform this task and are available at low cost. 
In some cases, they will be incorporated into the main circuitry of a data communications device and function without operator knowledge. 
In other cases, an external circuit is used so that the device, and its encrypting/decrypting technique, may be transported easily.
Normally, we think of communications science as dealing with the contemporaneous exchange of information between distant parties. 
However, many of the same techniques employed in data communications are also applied to data storage to ensure that the retrieval of information from a storage medium is accurate. 
We find, for example, that similar kinds of error-correcting codes used to protect digital telephone transmissions from noise are also used to guarantee correct readback of digital data from compact audio disks, CD-ROMs, and tape backup systems.
The transmit and receive switches shown above are electronic and operate in response to commands from a central control unit. 
It is possible that two or more destination registers will be switched on to receive data from a single source. 
However, only one source may transmit data onto the bus at any time. 
If multiple sources were to attempt transmission simultaneously, an electrical conflict would occur when bits of opposite value are driven onto a single bus conductor. 
Such a condition is referred to as a bus contention. Not only will a bus contention result in the loss of information, but it also may damage the electronic circuitry. 
As long as all registers in a system are linked to one central control unit, bus contentions should never occur if the circuit has been designed properly. 
Note that the data buses within a typical microprocessor are funda-mentally half-duplex channels.
When the source and destination registers are part of an integrated circuit (within a microprocessor chip, for example), they are extremely close (thousandths of an inch). 
Consequently, the bus signals are at very low power levels, may traverse a distance in very little time, and are not very susceptible to external noise and distortion. 
This is the ideal environment for digital communications. However, it is not yet possible to integrate all the necessary circuitry for a computer (i.e., CPU, memory, disk control, video and display drivers, etc.) on a single chip. 
When data is sent off-chip to another integrated circuit, the bus signals must be amplified and conductors extended out of the chip through external pins. 
Data communications has an ancient history, as people have always had an interest in communicating with each other. 
Different methods have been used and associated with each method are various advantages and disadvantages. 
A major problem with communications is ensuring that the receiver gets the message sent by the transmitter.
Data transmission, digital transmission, or digital communications is the physical transfer of data (a digital bit stream or a digitized analog signal over a point-to-point or point-to-multipoint communication channel. 
Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. 
The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal.
While analog transmission is the transfer of a continuously varying analog signal over an analog channel, digital communications is the transfer of discrete messages over a digital or an analog channel. 
The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying wave forms (passband transmission), using a digital modulation method. 
The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. 
According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion.
Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. 
It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream for example using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. 
This source coding and decoding is carried out by codec equipment.
Courses and textbooks in the field of data transmission[1] as well as digital transmission and digital communications have similar content.
Digital transmission or data transmission traditionally belongs to telecommunications and electrical engineering. Basic principles of data transmission may also be covered within the computer science/computer engineering topic of data communications, which also includes computer networking or computer communication applications and networking protocols, for example routing, switching and inter-process communication. 
Although the Transmission control protocol (TCP) involves the term "transmission", TCP and other transport layer protocols are typically not discussed in a textbook or course about data transmission, but in computer networking.
The term tele transmission involves the analog as well as digital communication. 
In most textbooks, the term analog transmission only refers to the transmission of an analog message signal (without digitization) by means of an analog signal, either as a non-modulated baseband signal, or as a passband signal using an analog modulation method such as AM or FM. 
It may also include analog-over-analog pulse modulatated baseband signals such as pulse-width modulation. 
In a few books within the computer networking tradition, "analog transmission" also refers to passband transmission of bit-streams using digital modulation methods such as FSK, PSK and ASK. 
Note that these methods are covered in textbooks named digital transmission or data transmission, for example.
The theoretical aspects of data transmission are covered by information theory and coding theory.
Data (mainly but not exclusively informational) has been sent via non-electronic (e.g. optical, acoustic, mechanical) means since the advent of communication. 
Analog signal data has been sent electronically since the advent of the telephone. 
However, the first data electromagnetic transmission applications in modern time were telegraphy (1809) and teletypewriters (1906), which are both digital signals. 
The fundamental theoretical work in data transmission and information theory by Harry Nyquist, Ralph Hartley, Claude Shannon and others during the early 20th century, was done with these applications in mind.
Data transmission is utilized in computers in computer buses and for communication with peripheral equipment via parallel ports and serial ports such as RS-232 (1969), Firewire (1995) and USB (1996). 
The principles of data transmission are also utilized in storage media for Error detection and correction since 1951.
Data transmission is utilized in computer networking equipment such as modems (1940), local area networks (LAN) adapters (1964), repeaters, hubs, microwave links, wireless network access points (1997), etc.
In telephone networks, digital communication is utilized for transferring many phone calls over the same copper cable or fiber cable by means of Pulse code modulation (PCM), i.e. sampling and digitization, in combination with Time division multiplexing (TDM) (1962). 
Telephone exchanges have become digital and software controlled, facilitating many value added services. 
For example the first AXE telephone exchange was presented in 1976. 
Since the late 1980s, digital communication to the end user has been possible using Integrated Services Digital Network (ISDN) services. 
Since the end of the 1990s, broadband access techniques such as ADSL, Cable modems, fiber-to-the-building (FTTB) and fiber-to-the-home (FTTH) have become widespread to small offices and homes. 
The current tendency is to replace traditional telecommunication services by packet mode communication such as IP telephony and IPTV.
Transmitting analog signals digitally allows for greater signal processing capability. 
The ability to process a communications signal means that errors caused by random processes can be detected and corrected. 
Digital signals can also be sampled instead of continuously monitored. 
The multiplexing of multiple digital signals is much simpler to the multiplexing of analog signals.
Because of all these advantages, and because recent advances in wideband communication channels and solid-state electronics have allowed scientists to fully realize these advantages, digital communications has grown quickly. 
Digital communications is quickly edging out analog communication because of the vast demand to transmit computer data and the ability of digital communications to do so.
The digital revolution has also resulted in many digital telecommunication applications where the principles of data transmission are applied. 
Examples are second-generation (1991) and later cellular telephony, video conferencing, digital TV (1998), digital radio (1999), telemetry, etc.
In telecommunications, serial transmission is the sequential transmission of signal elements of a group representing a character or other entity of data. 
Digital serial transmissions are bits sent over a single wire, frequency or optical path sequentially. 
Because it requires less signal processing and less chances for error than parallel transmission, the transfer rate of each individual path may be faster. 
This can be used over longer distances as a check digit or parity bit can be sent along it easily.
In telecommunications, parallel transmission is the simultaneous transmission of the signal elements of a character or other entity of data. 
In digital communications, parallel transmission is the simultaneous transmission of related signal elements over two or more separate paths. 
Multiple electrical wires are used which can transmit multiple bits simultaneously, which allows for higher data transfer rates than can be achieved with serial transmission. 
This method is used internally within the computer, for example the internal buses, and sometimes externally for such things as printers, 
The major issue with this is "skewing" because the wires in parallel data transmission have slightly different properties (not intentionally) so some bits may arrive before others, which may corrupt the message. 
A parity bit can help to reduce this. However, electrical wire parallel data transmission is therefore less reliable for long distances because corrupt transmissions are far more likely.
Asynchronous transmission uses start and stop bits to signify the beginning bit[citation needed] ASCII character would actually be transmitted using 10 bits. 
For example, "0100 0001" would become "1 0100 0001 0". The extra one (or zero, depending on parity bit) at the start and end of the transmission tells the receiver first that a character is coming and secondly that the character has ended. 
This method of transmission is used when data are sent intermittently as opposed to in a solid stream. 
In the previous example the start and stop bits are in bold. The start and stop bits must be of opposite polarity. 
This allows the receiver to recognize when the second packet of information is being sent.
Synchronous transmission uses no start and stop bits, but instead synchronizes transmission speeds at both the receiving and sending end of the transmission using clock signal(s) built into each component. 
A continual stream of data is then sent between the two nodes. 
Due to there being no start and stop bits the data transfer rate is quicker although more errors will occur, as the clocks will eventually get out of sync, and the receiving device would have the wrong time that had been agreed in the protocol for sending/receiving data, so some bytes could become corrupted (by losing bits).
Ways to get around this problem include re-synchronization of the clocks and use of check digits to ensure the byte is correctly interpreted and received
Data Communications is the transfer of data or information between a source and a receiver. 
The source transmits the data and the receiver receives it. 
The actual generation of the information is not part of Data Communications nor is the resulting action of the information at the receiver. 
Data Communication is interested in the transfer of data, the method of transfer and the preservation of the data during the transfer process.
In Local Area Networks, we are interested in "connectivity", connecting computers together to share resources. 
Even though the computers can have different disk operating systems, languages, cabling and locations, they still can communicate to one another and share resources.
The purpose of Data Communications is to provide the rules and regulations that allow computers with different disk operating systems, languages, cabling and locations to share resources. 
The rules and regulations are called protocols and standards in Data Communications.
What does networking have to do with telephones? 
Telephones and networking work hand in hand. 
The telecommunications industry has been gradually integrating with the computer industry and the computer industry has been gradually integrating with the telecommunications industry. 
The common goal is to join distantly located Local Area Networks into Metropolitan and Wide Area Networks (MANs and WANs).
First thing that comes to mind is telephone systems and the phone at home. Talking to someone on the phone uses Voice Channels. 
This doesn't seem to have much to do with Networks!
We do use voice channels for modem communications to connect to BBSs (Bulletin Board Services) or to connect to the Internet. 
We also use voice channels to connect LANs using remote access. 
Due to the bandwidth limits on the Voice Channel, the data transfer rate is relatively slow.
Voice Channel: Dial-up connection through a modem using standard telephone lines. 
Typical Voice Channel communication rates are: 300, 1200, 2400, 9600, 14.4k, 19.2k, 28.8k, 33.6k and 56 kbps (bits per second).
Data channels are dedicated lines for communicating digitized voice and data. 
At the end of 1996, there was a major milestone where more data was communicated in North America's telecommunications system than voice. 
 Data Communications (Data Comm) will assume an ever increasing role in controller to flight crew communication, contributing significantly to increased efficiency, capacity, and safety of the National Airspace (NAS). 
 The evolution of Data Comm in the operational environment will be based upon the incremental implementation of advanced communication capabilities. 
 Data Comm represents the first phase of the transition from the current analog voice system to an International Civil Aviation Organization (ICAO) compliant system in which digital communication becomes an alternate and eventually predominant mode of communication.
The Next Generation Air Transportation System (NextGen) permits a crucial operational shift from workload-intensive tactical control to automation-assisted strategic traffic management. 
Completion of this shift in the air traffic control paradigm by 2025 is necessary to handle the projected increases in traffic over the next two decades. 
To achieve a major part of this goal, NextGen envisions the introduction of air/ground trajectory automation capabilities. 
This capability, as well as other components of NextGen, depends on efficient data communications between aircraft and air traffic management. 
The Data Communication (Data Comm) program is a key element in the implementation of NextGen.
Formal verification seems indispensable, because concurrent programs are notorious for the hidden and sophisticated bugs they contain.
A mathematical approach to the study of concurrency and communication is referred to as Communicating Sequential Processes (CSP).
Concurrency can also be modelled using finite state machines like Mealy and Moore machines. 
Mealy and Moore machines are in use as design tools in digital electronics systems, which we encounter in the form of hardware used in telecommunications or electronic devices in general.
This kind of design can be a bit of a challenge to say the least, so it is important to keep things simple. 
For the Internet protocols, in particular and in retrospect, this meant a basis for protocol design was needed to allow decomposition of protocols into much simpler, cooperating protocols.
Systems do not use a single protocol to handle a transmission. 
Instead they use a set of cooperating protocols, sometimes called a protocol family or protocol suite.
To cooperate the protocols have to communicate with each other, so some kind of conceptual framework is needed to make this communication possible.
Also note that software is needed to implement both the 'xfer-mechanism' and a protocol (no protocol, no communication).
In literature there are numerous references to the analogies between computer communication and programming. 
By analogy we could say that the aforementioned 'xfer-mechanism' is comparable to a cpu; a 'xfer-mechanism' performs communications and a cpu performs computations and the 'framework' introduces something that allows the protocols to be designed independent of one another by providing separate execution environments for them. 
Furthermore, it is repeatedly stated that protocols are to computer communication what programming languages are to computation.
In telecommunications, a communication protocol is a system of rules that allow that two or more entities of a communication system to communicate between them to transmit information via any kind of variation of a physical quantity. 
These are the rules or standard that defines the syntax, semantics and synchronization of communication and possible error recovery methods. 
Protocols may be implemented by hardware, software, or a combination of both.
Communicating systems use well-defined formats (protocol) for exchanging messages. 
Each message has an exact meaning intended to elicit a response from a range of possible responses pre-determined for that particular situation. 
The specified behavior is typically independent of how it is to be implemented. 
Communication protocols have to be agreed upon by the parties involved.
To reach agreement, a protocol may be developed into a technical standard. 
A programming language describes the same for computations, so there is a close analogy between protocols and programming languages: protocols are to communications as programming languages are to computations.
The standard method of communication between an air traffic controller and a pilot is voice radio, using either VHF bands for line-of-sight communication or HF bands for long-distance communication (such as that provided by Shanwick Oceanic Control).
One of the major problems with voice radio communications used in this manner is that all pilots being handled by a particular controller are tuned to the same frequency. 
As the number of flights air traffic controllers must handle is steadily increasing (for instance, Shanwick handled 414,570 flights in 2007, an increase of 5% - or 22,000 flights - from 2006), the number of pilots tuned to a particular station also increases. 
This increases the chances that one pilot will accidentally override another, thus requiring the transmission to be repeated. 
In addition, each exchange between a controller and pilot requires a certain amount of time to complete; eventually, as the number of flights being controlled reaches a saturation point, the controller will not be able to handle any further aircraft.
Traditionally, this problem has been countered by dividing a saturated air traffic control sector into two smaller sectors, each with its own controller and each using a different voice communications channel. 
However, this strategy suffers from two problems:
Each sector division increases the amount of "handover traffic". 
That is the overhead involved in transferring a flight between sectors, which requires a voice exchange between the pilot and both controllers, plus co-ordination between the controllers.
The number of available voice channels is finite, and, in high density airspace, such as central Europe or the Eastern US Seaboard, there may not be a new channel available.
In some cases it may not be possible or feasible to further divide down a section.
A new strategy is needed to cope with increased demands on air traffic control, and data link based communications offers a possible strategy by increasing the effective capacity of the communications channel.
Controllerâ€“pilot data link communication (CPDLC) is a means of communication between controller and pilot, using data link for ATC communication. 
At the highest level, the concept is simple, with the emphasis on the continued involvement of the human at either end and the flexibility of use.
The CPDLC application provides air-ground data communication for the ATC service. This includes a set of clearance/information/request message elements which correspond to voice phraseology employed by air traffic control procedures. 
The controller is provided with the capability to issue level assignments, crossing constraints, lateral deviations, route changes and clearances, speed assignments, radio frequency assignments, and various requests for information. 
The pilot is provided with the capability to respond to messages, to request clearances and information, to report information, and to declare/rescind an emergency. 
The pilot is, in addition, provided with the capability to request conditional clearances (downstream) and information from a downstream air traffic service unit (ATSU). 
A â€œfree textâ€ capability is also provided to exchange information not conforming to defined formats. 
An auxiliary capability is provided to allow a ground system to use data link to forward a CPDLC message to another ground system.
The sequence of messages between the controller at an ATSU and a pilot relating to a particular transaction (for example request and receipt of a clearance) is termed a â€˜dialogueâ€™. 
There can be several sequences of messages in the dialogue, each of which is closed by means of appropriate messages, usually of acknowledgement or acceptance. 
Closure of the dialogue does not necessarily terminate the link, since there can be several dialogues between controller and pilot while an aircraft transits the ATSU airspace.
All exchanges of CPDLC messages between pilot and controller can be viewed as dialogues.
The CPDLC application has three primary functions: the exchange of controller/pilot messages with the current data authority, the transfer of data authority involving current and next data authority, and downstream clearance delivery with a downstream data authority.
Simulations carried out at the Federal Aviation Administration's William J. Hughes Technical Center have shown that the use of CPDLC meant that "the voice channel occupancy was decreased by 75 percent during realistic operations in busy en route airspace. 
The net result of this decrease in voice channel occupancy is increased flight safety and efficiency through more effective communications.
The information exchanged between devicesâ€”through a network, or other mediaâ€”is governed by rules and conventions that can be set out in technical specifications called communication protocol standards. 
The nature of a communication, the actual data exchanged and any state-dependent behaviors, is defined by its specification.
In digital computing systems, the rules can be expressed by algorithms and data structures. 
Expressing the algorithms in a portable programming language makes the protocol software operating system independent.
Operating systems usually contain of a set of cooperating processes that manipulate shared data to communicate with each other. 
This communication is governed by well-understood protocols, which can be embedded in the process code itself.
In contrast, because there is no common memory, communicating systems have to communicate with each other using a shared transmission medium. 
Transmission is not necessarily reliable, and individual systems may use different hardware and/or operating systems.
To implement a networking protocol, the protocol software modules are interfaced with a framework implemented on the machine's operating system. 
This framework implements the networking functionality of the operating system.
The best known frameworks are the TCP/IP model and the OSI model.
At the time the Internet was developed, layering had proven to be a successful design approach for both compiler and operating system design and, given the similarities between programming languages and communication protocols, layering was applied to the protocols as well.
This gave rise to the concept of layered protocols which nowadays forms the basis of protocol design.
Systems typically do not use a single protocol to handle a transmission. Instead they use a set of cooperating protocols, sometimes called a protocol family or protocol suite.
Some of the best known protocol suites include: IPX/SPX, X.25, AX.25, AppleTalk and TCP/IP.
The protocols can be arranged based on functionality in groups, for instance there is a group of transport protocols. 
The functionalities are mapped onto the layers, each layer solving a distinct class of problems relating to, for instance: application-, transport-, internet- and network interface-functions.
To transmit a message, a protocol has to be selected from each layer, so some sort of multiplexing/demultiplexing takes place. 
The selection of the next protocol is accomplished by extending the message with a protocol selector for each layer.
Messages are sent and received on communicating systems to establish communications. 
Protocols should therefore specify rules governing the transmission. In general, much of the following should be addressed:
Data formats for data exchange. Digital message bitstrings are exchanged. The bitstrings are divided in fields and each field carries information relevant to the protocol. 
Conceptually the bitstring is divided into two parts called the header area and the data area. 
The actual message is stored in the data area, so the header area contains the fields with more relevance to the protocol. 
Bitstrings longer than the maximum transmission unit (MTU) are divided in pieces of appropriate size.
Address formats for data exchange. Addresses are used to identify both the sender and the intended receiver(s). 
The addresses are stored in the header area of the bitstrings, allowing the receivers to determine whether the bitstrings are intended for themselves and should be processed or should be ignored. 
An all-1s address could be taken to mean an addressing of all stations on the network, so sending to this address would result in a broadcast on the local network. 
The rules describing the meanings of the address value are collectively called an addressing scheme.
Address mapping. Sometimes protocols need to map addresses of one scheme on addresses of another scheme. 
For instance to translate a logical IP address specified by the application to an Ethernet hardware address. 
Routing. When systems are not directly connected, intermediary systems along the route to the intended receiver(s) need to forward messages on behalf of the sender. 
On the Internet, the networks are connected using routers. This way of connecting networks is called internetworking.
Detection of transmission errors is necessary on networks which cannot guarantee error-free operation. 
In a common approach, CRCs of the data area are added to the end of packets, making it possible for the receiver to detect differences caused by errors. 
The receiver rejects the packets on CRC differences and arranges somehow for retransmission.
Acknowledgements of correct reception of packets is required for connection-oriented communication. 
Acknowledgements are sent from receivers back to their respective senders.
Loss of information - timeouts and retries. Packets may be lost on the network or suffer from long delays. 
To cope with this, under some protocols, a sender may expect an acknowledgement of correct reception from the receiver within a certain amount of time. 
On timeouts, the sender must assume the packet was not received and retransmit it. 
In case of a permanently broken link, the retransmission has no effect so the number of retransmissions is limited. 
Direction of information flow needs to be addressed if transmissions can only occur in one direction at a time as on half-duplex links. 
This is known as Media Access Control. Arrangements have to be made to accommodate the case when two parties want to gain control at the same time.
Sequence control. We have seen that long bitstrings are divided in pieces, and then sent on the network individually. 
The pieces may get lost or delayed or take different routes to their destination on some types of networks. 
By marking the pieces with sequence information at the sender, the receiver can determine what was lost or duplicated, ask for necessary retransmissions and reassemble the original message.
Flow control is needed when the sender transmits faster than the receiver or intermediate network equipment can process the transmissions. 
Flow control can be implemented by messaging from receiver to sender.
Getting the data across a network is only part of the problem for a protocol. 
The data received has to be evaluated in the context of the progress of the conversation, so a protocol has to specify rules describing the context. 
These kind of rules are said to express the syntax of the communications. Other rules determine whether the data is meaningful for the context in which the exchange takes place. 
These kind of rules are said to express the semantics of the communications.
Protocols are to communications what algorithms or programming languages are to computations.
This analogy has important consequences for both the design and the development of protocols. 
One has to consider the fact that algorithms, programs and protocols are just different ways of describing expected behavior of interacting objects. 
A familiar example of a protocolling language is the HTML language used to describe web pages which are the actual web protocols.
In programming languages the association of identifiers to a value is termed a definition. Program text is structured using block constructs and definitions can be local to a block. 
The localized association of an identifier to a value established by a definition is termed a binding and the region of program text in which a binding is effective is known as its scope.
The computational state is kept using two components: the environment, used as a record of identifier bindings, and the store, which is used as a record of the effects of assignments.
In communications, message values are transferred using transmission media. 
By analogy, the equivalent of a store would be a collection of transmission media, instead of a collection of memory locations. 
A valid assignment in a protocol (as an analog of programming language) could be Ethernet:='message' , meaning a message is to be broadcast on the local ethernet.
On a transmission medium there can be many receivers. For instance a mac-address identifies an ether network card on the transmission medium (the 'ether'). 
In our imaginary protocol, the assignment ethernet[mac-address]:=message value could therefore make sense.[28]
By extending the assignment statement of an existing programming language with the semantics described, a protocolling language could easily be imagined.
Operating systems provide reliable communication and synchronization facilities for communicating objects confined to the same system by means of system libraries.
A programmer using a general purpose programming language (like C or ADA) can use the routines in the libraries to implement a protocol, instead of using a dedicated protocolling language.
The nice thing about standards is that you have so many to choose from.
Despite their numbers, networking protocols show little variety, because all networking protocols use the same underlying principles and concepts, in the same way. 
So, the use of a general purpose programming language would yield a large number of applications only differing in the details.
A suitably defined (dedicated) protocolling language would therefore have little syntax, perhaps just enough to specify some parameters or optional modes of operation, because its virtual machine would have incorporated all possible principles and concepts making the virtual machine itself a universal protocol. 
The protocolling language would have some syntax and a lot of semantics describing this universal protocol and would therefore in effect be a protocol, hardly differing from this universal networking protocol. 
In this (networking) context a protocol is a language.
The notion of a universal networking protocol provides a rationale for standardization of networking protocols; assuming the existence of a universal networking protocol, development of protocol standards using a consensus model (the agreement of a group of experts) might be a viable way to coordinate protocol design efforts.
Networking protocols operate in very heterogeneous environments consisting of very different network technologies and a (possibly) very rich set of applications, so a single universal protocol would be very hard to design and implement correctly. 
Instead, the IETF decided to reduce complexity by assuming a relatively simple network architecture allowing decomposition of the single universal networking protocol into two generic protocols, TCP and IP, and two classes of specific protocols, one dealing with the low-level network details and one dealing with the high-level details of common network applications. 
ISO choose a similar but more general path, allowing other network architectures, to standardize protocols.
The programming tools and techniques for dealing with parallel processes are collectively called concurrent programming. 
Concurrent programming only deals with the synchronization of communication. 
The syntax and semantics of the communication governed by a low-level protocol usually have modest complexity, so they can be coded with relative ease. 
High-level protocols with relatively large complexity could however merit the implementation of language interpreters. 
Concurrent programming has traditionally been a topic in operating systems theory texts. 
Desktop publishing is the creation of documents using page layout skills on a personal computer.
Desktop publishing software can generate layouts and produce typographic quality text and images comparable to traditional typography and printing. 
This technology allows individuals, businesses, and other organizations to self-publish a wide range of printed matter. 
Desktop publishing is also the main reference for digital typography. 
When used skillfully, desktop publishing allows the user to produce a wide variety of materials, from menus to magazines and books, without the expense of commercial printing. 
Others focus on one of several possible approaches or on the use of a particular tool or towards the accomplishment of particular applications.
Desktop publishing methods provide more control over design, layout, and typography than word processing. 
However, word processing software has evolved to include some, though by no means all, capabilities previously available only with professional printing or desktop publishing.
The same DTP skills and software used for common paper and book publishing are sometimes used to create graphics for point of sale displays, promotional items, trade show exhibits, retail package designs and outdoor signs. 
Although what is classified as "DTP software" is usually limited to print and PDF publications, DTP skills aren't limited to print. 
The content produced by desktop publishers may also be exported and used for electronic media. 
The job descriptions that include "DTP", such as DTP artist, often require skills using software for producing e-books, web content, and web pages, which may involve web design or user interface design for any graphical user interface.
Desktop publishing began in 1983 with a program developed by James Bessen at a community newspaper in Philadelphia.
That program, Type Processor One, ran on a PC using a graphics card for a WYSIWYG display and was offered commercially by Best info in 1984. 
(Desktop typesetting, with only limited page makeup facilities, had arrived in 1978â€“9 with the introduction of TeX, and was extended in the early 1980s by LaTeX.) 
The DTP market exploded in 1985 with the introduction in January of the Apple LaserWriter printer, and later in July with the introduction of PageMaker software from Aldus which rapidly became the DTP industry standard software.
The term "desktop publishing" is attributed to Aldus Corporation founder Paul Brainerd,who sought a marketing catch-phrase to describe the small size and relative affordability of this suite of products in contrast to the expensive commercial phototypesetting equipment of the day.
By the standards of today, early desktop publishing was a primitive affair. 
Users of the PageMaker-LaserWriter-Macintosh 512K system endured frequent software crashes,cramped display on the Mac's tiny 512 x 342 1-bit monochrome screen, the inability to control letter-spacing, kerning  and other typographic features, and discrepancies between the screen display and printed output. 
However, it was a revolutionary combination at the time, and was received with considerable acclaim.
Behind-the-scenes technologies developed by Adobe Systems set the foundation for professional desktop publishing applications. 
The LaserWriter and LaserWriter Plus printers included high quality, scalable Adobe PostScript fonts built into their ROM memory. 
The LaserWriter's PostScript capability allowed publication designers to proof files on a local printer then print the same file at DTP service bureaus using optical resolution 600+ ppi PostScript printers such as those from Linotronic. 
Later, the Macintosh II was released which was much more suitable for desktop publishing because of its greater expandability, support for large color multi-monitor displays, and its SCSI storage interface which allowed fast high-capacity hard drives to be attached to the system.
Although Macintosh-based systems would continue to dominate the market, in 1986, the GEM-based Ventura Publisher was introduced for MS-DOS computers. 
While PageMaker's pasteboard metaphor closely simulated the process of creating layouts manually, Ventura Publisher automated the layout process through its use of tags/style sheets and automatically generated indices and other body matter. 
This made it suitable for manuals and other long-format documents. Desktop publishing moved into the home market in 1986 with Professional Page for the Amiga, 
Publishing Partner (now PageStream) for the Atari ST, GST's Timeworks Publisher on the PC and Atari ST and Calamus for the Atari TT030. 
Even for 8-bit computers like the Apple II and Commodore 64 software was published: Home Publisher, The Newsroom and geoPublish.
During its early years, desktop publishing acquired a bad reputation as a result of untrained users who created poorly organized ransom note effect layouts â€” similar criticism would be levied again against early World Wide Web publishers a decade later. 
However, some were able to realize truly professional results.
Once considered a primary skill, increased accessibility to more user-friendly DTP software has made DTP a secondary skill to art direction, graphic design, multimedia development, marketing communications, and administrative careers. 
DTP skill levels range from what may be learned in a few hours (e.g. learning how to put clip art in a word processor) to what requires a college education. 
The discipline of DTP skills range from technical skills such as prepress production and programming to creative skills such as communication design and graphic image development.
There are two types of pages in desktop publishing, electronic pages and virtual paper pages to be printed on physical paper pages. 
All computerized documents are technically electronic, which are limited in size only by computer memory or computer data storage space.
Virtual paper pages will ultimately be printed, and therefore require paper parameters that coincide with international standard physical paper sizes such as "A4," "letter," etc., if not custom sizes for trimming. 
Some desktop publishing programs allow custom sizes designated for large format printing used in posters, billboards and trade show displays. 
A virtual page for printing has a predesignated size of virtual printing material and can be viewed on a monitor in WYSIWYG format. 
Each page for printing has trim sizes (edge of paper) and a printable area if bleed printing is not possible as is the case with most desktop printers.
A web page is an example of an electronic page that is not constrained by virtual paper parameters. 
Most electronic pages may be dynamically re-sized, causing either the content to scale in size with the page or causing the content to re-flow.
Master pages are templates used to automatically copy or link elements and graphic design styles to some or all the pages of a multipage document. 
Linked elements can be modified without having to change each instance of an element on pages that use the same element. 
Master pages can also be used to apply graphic design styles to automatic page numbering. 
Cascading Style Sheets can provide the same global formatting functions for web pages that master pages provide for virtual paper pages.
Page layout is the process by which the elements are laid on the page orderly, aesthetically, and precisely. 
Main types of components to be laid out on a page include text, linked images that can only be modified as an external source, and embedded images that may be modified with the layout application software. 
Some embedded images are rendered in the application software, while others can be placed from an external source image file. 
Text may be keyed into the layout, placed, or (with database publishing applications) linked to an external source of text which allows multiple editors to develop a document at the same time.
Graphic design styles such as color, transparency, and filters, may also be applied to layout elements. 
Typography styles may be applied to text automatically with style sheets. 
Some layout programs include style sheets for images in addition to text. 
Graphic styles for images may be border shapes, colors, transparency, filters, and a parameter designating the way text flows around the object called "wraparound" or "runaround."
While desktop publishing software still provides extensive features necessary for print publishing, modern word processors now have publishing capabilities beyond those of many older DTP applications, blurring the line between word processing and desktop publishing.
In the early days of graphical user interfaces, DTP software was in a class of its own when compared to the fairly spartan word processing applications of the time. 
Programs such as WordPerfect and WordStar were still mainly text-based and offered little in the way of page layout, other than perhaps margins and line spacing. 
On the other hand, word processing software was necessary for features like indexing and spell checking, features that are common in many applications today.
As computers and operating systems have become more powerful, vendors have sought to provide users with a single application platform that can meet all needs.
In modern usage, DTP is not generally said to include tools such as TeX or troff, though both can easily be used on a modern desktop system and are standard with many Unix-like operating systems and readily available for other systems. 
The key difference between electronic typesetting software and DTP software is that DTP software is generally interactive and WYSIWYG in design, while other electronic typesetting software, such as TeX, LaTeX and other variants, tends to operate in batch mode, requiring the user to enter the processing program's markup language without immediate visualization of the finished product. 
This kind of workflow is less user-friendly than WYSIWYG, but more suitable for conference proceedings and scholarly articles as well as corporate newsletters or other applications where consistent, automated layout is important. 
Recent[when?] interactive front-ends to TeX such as LyX have produced WYSIWYM (what you see is what you mean) hybrids of DTP and batch processing, focussed more on semantics than traditional DTP.
There is some overlap between desktop publishing and what is known as Hypermedia publishing (i.e. Web design, Kiosk, CD-ROM). 
Many graphical HTML editors such as Microsoft FrontPage and Adobe Dreamweaver use a layout engine similar to a DTP program. 
However, some Web designers still prefer to write HTML without the assistance of a WYSIWYG editor, for greater control and because these editors often result in code bloat.
Desktop publishing is a term coined after the development of a specific type of software. 
It's about using that software to combine and rearrange text and images and creating digital files.
At one time, only professional graphic designers used desktop publishing software. 
Then along came the consumer level desktop publishing software and an explosion of people who did desktop publishing for fun and profit, with or without a background in traditional design. 
Today, desktop publishing is still a career choice for some but it is also increasingly a required skill for a wide range of jobs and careers. 
When documents and images are printed, they are "published." 
Before computers became commonplace, the publishing process required large print presses that copied and duplicated pages. 
In order to print images and words on the same page, the text and graphics would have to printed separately, cut out, placed on a single sheet, taped in place, then copied and printed. 
Fortunately, computers with graphical user interfaces have enabled desktop publishing, which allows this process to be done electronically.
Complete desktop publishing involves the combination of typesetting (choosing fonts and the text layout), graphic design, page layout (how it all fits on the page), and printing the document. 
However, as mentioned before, desktop publishing can also be as simple as typing and printing a school paper. 
In order to desktop publish, all you need is a computer, monitor, printer, and software that can create a printable document. 
While that might cost more than a pen and paper, it certainly is cheaper than a printing press.
They also need word processing, computer graphics, and desktop publishing skills.
Personal computers and desktop publishing software help scam artists produce slick promotional materials.
Provide professional support services for central office employees with desktop publishing software.
The process he envisages is a technical procession similar to that which happened to desktop publishing and printing.
We work with you from developing ideas and determining what products you need through desktop publishing and printing.
They are stored in plain text and can be imported into any word processor or desktop publishing program.
Ability to use application tools for desktop publishing and graphic design.
Using computers to lay out text and images for printing in magazines, newsletters, brochures, etc. 
A good DTP system provides precise control over templates, styles, fonts, sizes, colour, paragraph formatting, images and fitting text into irregular shapes. 
Alternatively referred to as DTP, desktop publishing was first created by Paul Brainerd at Aldus after creating and releasing the Aldus Pagemaker software program in 1985 for the Macintosh. 
Desktop publishing is the use of a computer to produce documents for publication. 
This document is a high quality document containing text and graphics formatted on a single page. 
For example, desktop publishing is needed to create printed material as book covers, brochure, fliers, etc.
Using specialized software and high resolution printer(s) to assemble, design, and produce commercial quality printed material. 
DTP is usually cost effective only in very specialized jobs (such as multicolor network diagrams) or in short print runs. See also on demand printing.
Having a good desktop publishing set up at your work will make sure that all the things your company puts out look very professional.
A word processor is not always the best package to use to produce a document. 
If more control over the layout of a page is required or the document is to include a lot of graphics, then a desktop publishing package such as Microsoft Publisher would be more suitable. 
Desktop publishers are often used to produce newspapers, magazines, newsletters and leaflets.
Desktop publishing began in 1985 with the introduction of PageMaker software from Aldus and the LaserWriter printer from Apple Computer for the Apple Macintosh computer. 
The ability to create WYSIWYG page layouts on screen and then print pages at crisp 300 ppi resolution was revolutionary for both the typesetting industry as well as the personal computer industry. 
The term "desktop publishing" is attributed to Aldus Corporation founder Paul Brainerd, who sought a marketing catch phrase to describe the small size and relative affordability of this suite of products in contrast to the expensive commercial phototypesetting equipment of the day. 
Often considered a primary skill, increased accessibility to more user friendly DTP software has made DTP a secondary skill to art direction, graphic design, multimedia development, marketing communications, administrative careers and advanced high school literacy in thriving economies. 
DTP skill levels range from what may be learned in a few hours  to what requires a college education and years of experience  
By today's standards, early desktop publishing was a primitive affair. 
Users of the PageMaker-LaserWriter-Macintosh 512K system endured frequent software crashes, the Mac's tiny 512 x 342 1-bit black and white screen, the inability to control letter spacing, kerning and other typographic features, and discrepancies between the screen display and printed output.
 However, for that moment in time, it was received like a magic trick: difficult to believe, but everyone wants to know how to do the trick. 
Behind-the-scenes technologies developed by Adobe Systems set the foundation for professional desktop publishing applications. 
The LaserWriter and LaserWriter Plus printers included high quality, scalable Adobe fonts built into their ROM memory. 
The LaserWriter's additional PostScript capability allowed publication designers to proof files on a local printer then print the same file at DTP service bureaus using optical resolution 600+ ppi PostScript-printers such as those from Linotronic. 
Later, the Macintosh II was released which was much more suitable for desktop publishing because of its larger, color screen. In 1986, the GEM-based Ventura Publisher was introduced for MS-DOS computers. 
While PageMaker's pasteboard metaphor closely simulated the process of creating layouts manually, Ventura Publisher automated the layout process through its use of tags/style sheets and automatically generated indices and other body matter.
This made it suitable for manuals and other long-format documents. 
Desktop publishing moved into the home market with Publishing Partner for the Atari ST in 1986 and later for the Amiga, GST's Timeworks Publisher on the PC and Atari ST, Calamus for the Atari TT030, Home Publisher and Newsroom for 8 bit computers like the Apple II. 
During these early years, desktop publishing acquired a bad reputation from untrained users who created chaotically organized ransom note effect layouts - criticisms that would be levied again against early web publishers a decade later. 
The improved typographic controls and image handling of PC and Mac-based publishing systems increasingly attracted the attention of professional publishers. 
The turning point was the introduction of Quark XPress in the 1990s and an ever increasing number of digital typefaces. 
Xpress became dominant in the publishing world until the early 2000s when Adobe InDesign grew in popularity for its powerful typographic controls and integration with other Adobe publishing products, especially those which were predominate within the design, photography, publishing, printing, and digital media industries. 
By the late 1990s, virtually all publishing had become "desktop publishing." 
The superior flexibility and speed of desktop publishing systems has greatly reduced the lead time for all forms of publication and accommodates elaborate designs and layouts that were unfathomable in the decades before DTP. 
Database publishing has further reduced the time required to develop thick manuals and catalog publications. 
Desktop publishing helped condition a generation of personal computer users to be on the lookout for "the next big thing." 
In the late 1980s, developers hopefully applied the "desktop" prefix to potential new markets like "desktop presentations," "desktop forms" and "desktop video." 
All of these markets proved to be important (see PowerPoint, Adobe Acrobat, and miniDV for example), especially desktop video editing. 
Many cinema length movies are now edited on Apple Final Cut Pro on a desktop computer, replacing equipment and software that would have cost a hundred thousand dollars in the 1980s. 
While desktop publishing software still provides extensive features necessary for print publishing, modern word processors now have publishing capabilities beyond those of many older DTP applications, blurring the line between word processing and desktop publishing. 
In the early days of graphical user interfaces, DTP software was in a class of its own when compared to the fairly spartan word processing applications of the time. 
Programs such as WordPerfect and WordStar were still mainly text-based and offered little in the way of page layout, other than perhaps margins and line spacing. 
On the other hand, word processing software was necessary for features like indexing and spell checking, features that are today taken for granted. 
As computers and operating systems have become more powerful, vendors have sought to provide users with a single application platform that can meet all needs. 
Software such as Microsoft Word offers advanced layouts and linking between documents, and DTP applications have added in common word processor features. 
In modern usage, DTP is not generally said to include tools such as TeX or troff, though both can easily be used on a modern desktop system and are standard with many Unix-like operating systems and readily available for other systems. 
The key difference between electronic typesetting software and DTP software is that DTP software is generally interactive and WYSIWIG in design, while older electronic typesetting software tends to operate in batch mode, requiring the user to enter the processing program's markup language manually without a direct visualization of the finished product. 
The older style of typesetting software occupies a substantial but shrinking niche in technical writing and textbook publication; however, since much software in this genre is now open source, it can be more cost-effective than the professionally-oriented DTP systems. 
There is some overlap between desktop publishing and what is known as Hypermedia publishing (i.e. Web design, Kiosk, CD-ROM.) 
Many graphical HTML editors such as Microsoft FrontPage and Dreamweaver use a layout engine similar to a DTP program. 
However, some Web designers still prefer to write HTML without the assistance of a WYSIWIG editor and resort to such software, if at all, solely for complex layout that cannot easily be rendered in hand-written HTML code. 
Desktop publishing used to be the primary reason for owning a PCâ€”it was a business in and of itself. 
I know I am dating myself, but I remember when someone with a PC, a printer, and a desktop publishing application could actually make decent money producing flyers, menus, and other simple publications for others. 
Those days are long gone, but the concept of desktop publishing has also evolved over time, and software like Adobe InDesign has managed somehow to keep up and remain relevant.
It would be anomalous today to find someone so Luddite that they donâ€™t have access to a PC, printer, and basic page editing tools to produce their own flyers and such. 
However, once popular applications like PrintMaster, 
The Print Shop, or Microsoft Publisher have all but faded out of existence. 
That is partly a function of the fact that the basic page editing tools are available in most word processing software as wellâ€”so there is no need for a specialized tool, and it is partly a reflection of how technologyâ€”and how we use itâ€”has evolved over time. 
Simply put, we have apps and websites, and people are much less likely to need dedicated software to create a flyer today.
I had a chance to spend some time talking to Chris Kitchener, a product manager for Adobe InDesign. 
It was evident from his respect for the history and evolution from desktop publishing to Web design, apps, and eBooks that he is passionate about the topic. 
Kitchener was also very excited about Adobe Creative Cloud, and how the cloud-based subscription model compresses the development lifecycle and enables Adobe to roll out changes and updates more frequently.
Adobe demonstrated that fact a couple months ago with the launch of InDesign CC 2014. 
The latest version of InDesign includes a variety of new and enhanced features: simplified tables, enhanced search, enhanced footnotes, scaling of effects, and more. 
Adobe also introduced seamless updates, so your presets, preferences, and shortcuts are not affected when a new update is applied.
It shouldnâ€™t be any surprise that the Adobe InDesign product manager loves the product. 
Itâ€™s sort of part of the job description. 
To get some real-world perspective, I also spoke with an Adobe InDesign userâ€”Kelly Vaughn of Verity Yacht Publications.
Vaughn started off her desktop publishing career in 2001 using PageMaker. 
Eventually, she decided to fly solo as a one-woman publishing company catering to a very unique market segment. 
Verity Yacht Publications creates ownerâ€™s manuals for custom yachtsâ€”providing detailed documentation for marine craft that are unique and specialized so there is no mass produced manual.
â€œAdobe InDesign is the most powerful page publishing program on the planet,â€ explained Vaughn. She told me that she loves the seamless integration with the rest of the Adobe Creative Cloud suite, and TypeKit integration.
Vaughn is also a fan of the frequent updates across the Creative Cloud portfolio. 
One of the primary reasons she loves InDesign, though, is that it makes her job easier. 
The combination of power and simplicity in Adobe InDesign enable her to be more effective and more efficient in producing professional finished documents for her clients.
The world may have evolved beyond the need for specialized software to produce flyers for a party, but it hasnâ€™t evolved beyond the need for page publishing tools like Adobe InDesign. 
On the contrary, it seems that Adobe has managed to adapt to changing demand, and evolve InDesign in ways that make it just as necessary, and just as relevantâ€”perhaps even more soâ€”than it ever was.
High-end desktop publishing programs, such as Adobe InDesign and QuarkXPress, feature lots of tools to help designers produce stunning pages. 
But these programs are expensive, and novices require training to use them, factors that render their acquisition difficult to justify for most small businesses.
Microsoftâ€™s own Publisher program is a step down from those applications in both power and price, but not every version of Office includes Publisher, and it costs $140 to purchase separately. 
However, chances are good that you already own a copy of Microsoft Word, and that software has a host of desktop publishing tools that you can use to produce pages that rival the output of the best layout artist.
If you need to create documents with drop caps, pull quotes, columns, text that wraps around images, and similar desktop publishing elements, you can do so in Word. 
The only problem is that these tools are scattered all across Wordâ€™s Ribbon user interface, and some are buried deep in arcane menus. 
I'll show you where to find them, and explain how to make the most of them.
One way to ensure that a document looks professional and smart is to use the same formatting throughout. 
You should format every heading the same way, and make all of your body text look the same. 
You can use Word's styles to apply formats quickly.
First, choose a Style Set for your document from the Home tab on the Ribbon by clicking Change Styles > Style Set. 
Youâ€™ll see a number of possibilities in the menu that pops up. 
Choose the look that's closest to how you want your document to appear.
Once youâ€™ve selected a Style Set, the Styles gallery on the Home tab will display a series of styles that you can use to format text in your document. 
To apply a style, select a block of text (such as a heading) and click an item, such as Heading 1, in the Style gallery. 
Typically youâ€™ll use Normal for body text and Heading 1 for headings. 
You can use other styles for special elements in the document.
If youâ€™re not satisfied with these prefab styles, you can easily modify them: Right-click the style name in the Style gallery, and choose Modify. 
Make whatever changes you want (click Bold to render all the text in that style in bold type, for example), and click OK. 
Now all of the text in the document that you have formatted using that style will automatically update to reflect your change.
2. Align and Distribute Objects Evenly
When you embed a series of images on a page, they typically look best when you align each image's left or right edge along the respective edge of the page. 
If you place them across the width of a document, they usually look best when their top or bottom edges are aligned. 
To align a series of images to the left or right down the page margin, click on the first image and then hold down the Shift key while clicking on each additional image until youâ€™ve selected all of them. 
Next, click the Picture Tools tab on the Ribbon and click Format > Align > Align To Margin. 
Now click Format > Align > Align Left to align the images down the left margin, or Align Right to line them up down the right margin.
To line up images relative to each other across the page, select the images and click the Picture Tools tab on the Ribbon; then click Format > Align > Align Selected Objects. 
Finally, click Format > Align once more, and click Align Top (to align their top edges) or Align Bottom (to align their bottom edges). 
When you click Format > Align, youâ€™ll see that you can also choose Distribute Vertically or Distribute Horizontally to space images evenly down the page margin or space them evenly relative to each other (depending on whether you select Align to Page or Align Selected Objects).
3. Flow Text From One Page to the Next Using a Text Box
To make the best use of the first few pages of a newsletter, you should start a long story on one page and finish it on a later page. 
That way, you can fit more stories on the front page, which is what your readers will see first. 
You can accomplish this by placing the story in linked text boxes, so that when the first text box is full, excess text will automatically flow into the second text box.
First, create the text boxes by clicking the Insert tab on the Ribbon, clicking Text Box > Draw Text Box, and then dragging your mouse to draw a text box on the page. 
Repeat this step to create a second text box on a later page. 
Next, select the first text box and click Drawing Tools > Format > Create Link. 
The cursor will change to resemble a jug with a down-pointing arrow in it. 
Position the cursor over the second empty text box, and click once to link the two text boxes. 
Now when you type or paste text into the first text box, and thereâ€™s too much to fit in the first box, it will overflow into the second box. 
The best part is that you can edit within either box, and the text will automatically flow back and forth as you cut or pad the story.
When it comes to wrapping text creatively around an image, Word's tools are superior to those of its Office sibling Publisher. 
This is the feature to use when you're working with an image that contains a plain or light-colored area to accommodate text (called copy space).
First, add the image to your Word document, select the image, and choose Picture Tools on the Ribbon toolbar. 
Click Format > Wrap Text > Tight. 
Now, with the image still selected, click Format once more and choose Edit Wrap Points. 
A red line with black markers, called wrap points, will appear around the image. 
Adjust this line by dragging the wrap points: You can drag the wrap points inward to wrap text over the image, or drag them outward so that the text moves away from the image. 
Drag on the line itself to create additional wrap points, as desired. 
When youâ€™re done, click away from the image, and the wrap points will disappear.
The history and future of desktop publishing and electronic publishing. 
A primer of the history of DTP hardware and software as it has affected professional graphic designers, printers and pre-press professionals.
Over twenty five years ago several computing technologies, hardware and software, combined to irreversibly change the design and publishing industry.
Did we say twenty five years? Well let's not quibble about a couple of years here or there. 
The point is that over a short period in the mid 1980s something quite dramatic happened to the graphic design, prepress and printing industry.
In the mid 1980s, Apple Computer, Adobe, Aldus and Hewlett-Packard each produced key technologies that, when combined, allowed graphic designers, publishers and pre-press professionals to bring the whole publishing process in-house.
Those four companies were responsible for the hardware and software that, to a large degree, still drives the electronic publishing industry. 
This literally created desktop publishing, or DTP.
Although hot metal typesetting and manual publishing techniques had long been replaced by phototypesetting, it was not until the mid 1980's that design and publishing was truly brought 'in-house'.
Although the first laser printers were built by Canon, it was Hewlett-Packard's LaserJet desktop laser printer, developed in 1984, combined with the Apple Macintosh computer and Adobe's PostScript page description language and Aldus's PageMaker software, that is generally acknowledged as the cornerstones of DTP.
The Macintosh, with its easy to use graphics user interface (GUI), allowed non-computer literate designers to simulate their normal working environment with its desktop as metaphor approach.
Many design companies and printers have remained loyal to Apple and standardized on the Macintosh. 
However, with the release of Windows 95 and it's successors, it is now just as possible to use the same software tools on a Windows-based PC as it is on a Mac. 
Whilst there used to be much debate as to the advantages of Apple versus Windows for graphic designers, especially around the areas of color accuracy and prepress, it is now generally accepted that, for the most part, the choice of computing platform is now more of a preference, than a requirement.
In 1985 Aldus, a company later bought by Adobe, released the first desktop publishing software. 
Called PageMaker, it allowed designers to layout pages in WYSIWYG mode, rather than having to type in arcane typesetting code commands.
Although PageMaker was the first professional desktop publishing layout tool, it was soon usurped by a company called Quark, who had developed their own layout package called QuarkXpress. 
One major advantage of QuarkXpress was its plugin system, known as Xtensions, which allowed publishing companies to purchase add-on technology to suit their particular workflow or industry.
In recent years â€“ well after Adobe had purchased Aldus â€“ Adobe released InDesign, which has been steadily challenging and even overtaking Quark's dominance of the DTP industry. 
Adobe also uses software plug-ins for many of its applications.
Apart from layout applications, other desktop publishing software tools were introduced that allowed publishers to take on increasing amounts of the design and production workload. 
High resolution drum scanners â€“ and later desktop scanners â€“ alongside Adobe Photoshop, soon put paid to the need for enormous film cameras.
Adobe's PostScript software allowed the designers' creations to be output accurately to a PostScript enabled device, such as the Apple Laserwiter (the first PostScript enabled desktop laser printer). 
PostScript was now also being built into high end imagesetters, which allowed for printers and pre-press bureaus to output press quality film, directly from a publishers digital files.
PostScript technology was now also being built into fonts and other DTP publishing tools, such as vector drawing applications like Adobe Illustrator. 
Indeed it is only recently that PostScript font technology has begun to be replaced by other formats such as OpenType fonts.
Desk-top publishing has often been criticized by graphic designers as being responsible for lowering design standards. 
The reasons for this are often related to the ease with which DTP has made it for amateur 'designers' to produce published documents. 
Indeed, the term 'desktop' has been criticized as somewhat misleading.
Many classically trained graphic designers point to increasing reliance by businesses on untrained in-house staff to produce everything from newsletter, to designing logos, stationary and even mass distribution brochures and promotional material. 
The point made is that, whilst DTP technology may allow more and more untrained people to produce publications, it matters little if the documents produced are badly designed and fail to achieve their purpose.
The number of programs coming under the banner of 'desktop publishing' has also risen and can sometimes include applications as diverse as PowerPoint, Microsoft's Publisher and Serif's PagePlus. 
Indeed, many word processing programs now claim to include desktop publishing features.
Many printers and pre-press bureaus also complain of receiving incorrectly formatted files from designers who have been taught how to use dtp software to produce layouts, but have not been taught how to process those documents properly for offset-litho commercial printing.
The plethora of non-professional DTP software tools has also led to some printers raising their hands in despair. 
With many of these tools not supporting basic print production formats, such as CMYK color separation.
The availability of color desktop printers has also led to some inexperienced designers sending 'desktop proofs' along with their artwork, without realizing that the colors can often not be matched when printed on a commercial printing press. 
In much the same way, the colors reproduced on a DTP computer monitor will rarely match those produced when the job is printed.
The design and publishing business is constantly changing. But most recent changes have yet to be as dramatic as those brought about by the events of the mid 1980s. 
Computers are constantly getting faster, allowing for more graphically intensive procedures to be performed on the 'desktop' by designers. 
Digital commercial printing machines have reduced the price of short-run full color printing. Albeit perhaps at the cost of quality. 
And complete on-screen electronic proofing (generally PDF driven) is now commonplace, speeding up the whole production process for non-color critical work.
But perhaps the most dramatic development in desktop publishing has been the web. 
When companies began to realize that an effective web presence was a crucial marketing tool, many graphic designers eagerly jumped aboard the web design ship. 
And WYSIWYG web design tools, such as Freeway, Dreamweaver and GoLive, have provided designers with an inroad into the once technically exclusive world of web design, in much the same way that QuarkXpress and Pagemaker opened up the publishing market in the mid-eighties.
Of course, this has sometimes raised many of the same questions of professionalism as were raised by the incorrect use of DTP print publishing tools. 
Many web developers, who were used to hand coding web sites, have complained that WYSIWYG web design software applications create bloated or non-compliant code.
The ongoing interaction between the printed design world and the interactive digital world will no-doubt continue. 
The overlap with interactive television technologies has already begun, as has desktop video, design for mobile phones and PDAs. 
It's going to be interesting.
Desktop publishing (DTP) was the catch-all term applied to the introduction of digital publishing systems in the 1980's. 
These systems replaced large, specialist design, pre-press and compositing systems.
DTP encompassed a mixture of tolls and technologies, but none more so than the combination of the Apple Macintosh computer, with its WYSIWYG interface, page layout software such as Pagemaker and PostScript laser printers. 
In practice DTP was, and remains, somewhat of a misnomer. Although a lot of the design and pre-press can be done on small 'desktop' systems, the actual printing still requires large and expensive output systems. 
Although, this is now gradually changing in some areas, with the advent of shorter run digital printing machines.
WYSIWYG (what you see is what you get) is one of the terms that came into use with the Desktop Publishing (DTP) revolution and refers to the display system whereby what is seen on a computer screen, using a DTP package, is what you will get when the document is printed. 
The use of WYSIWYG has now been expanded to include web site design software, such as Dreamweaver, that allow web designers to build web sites, without having to hand-code HTML or javascript.
All the DTP page layout programs such as Pagemaker, QuarkXpress and so on, all claimed to be WYSIWYG. 
In practice, the process of duplicating on-screen content into the 'real' or printed medium remains easier said than done. 
Similarly, this could also be said about WYSIWYG web development tools. 
Although, the criticism here was often that the code that they create is not as 'clean' as it would be if it were hand coded by a programmer.
DTP - Desktop Publishing - Prepress! All of us prefer to use or play with the most interesting software rather than learn the tedious underlying technicalities. 
I have heard the following statement expelled by Designers and Desktop Artists several times in my career..."I can get my page on the @&!%# screen, why can't you output it ?".
As if it is always someone else's fault, all people have to do is swallow a little pride and learn why millions of dollars are wasted every year because... 
"to create a page or image is one thing, to create a page or image correctly and methodically for predictable output is another". 
The importance of understanding, as a desktop operator, both the previous operations applied to page objects and the operations that will follow, cannot be stressed enough.
We have all been there before and the efficient, proficient, professional desktop operator will always stand out. 
VET colleges know this to be true from the requests of industry employers.
Professional tips and hints abound throughout the lesson and theory pages so spend your time wisely and learn, not just in areas that you enjoy. 
Colour, Halftones, Scanning, Photoshop images, theory and help tutorials. HTML page design is an extension of DTP, so comeback often and read the Web tutorial classrooms too. 
It is all designed to fit together.
Having said that, considering some of the many other advances in computing, the quantity of input devices for graphic designers has remained comparatively static. 
However, the quality and price of these devices has improved dramatically. 
The choice of keyboard and mouse are very much a personal thing and often the input devices supplied with the computer system are more than adequate for graphic design use. 
However many designers, especially illustrators, product designers and artists will swear by graphics tablets.
Graphics tablets are sold by a number of manufacturers, including Wacom, Adesso, Crossfield and Aiptek. 
Graphics tablets come in many sizes, ranging from small pads for amatiers, to table-top sized tablets for CAD designers.
Graphics tablets come with a variety of pen devices, the more advanced of which are pressure sensitive to simulate real life drawing tools. 
Most pressure sensitive pens will integrate with graphics applications such as Adobe Photoshop, Illustrator and Corel Painter.
The advance in desktop scanner technology since the early days of desktop publishing (DTP) has been phenomenal. 
It is now possible to buy a flatbed scanner for a few hundred dollars, that has almost the same optical resolution and color range as drum scanners costing tens of thousands only a few years ago.
Many designers and pre-press professionals will argue that most flatbed scanners are still not ideal for high-end graphic design work, such as quality brochures and art books. 
However, many flatbed scanners are certainly suitable for general magazine reproduction and for outputting client proofs.
Manufacturers that produce scanners for graphic arts use include Epson, Canon, Hewlett Packard, Agfa and Umax.
The debate is still raging as to whether digital cameras have evolved to the point where they can produce photographs to the quality of traditional film and high-end drum scanning. 
But for graphic designers and art directors, if perhaps not for professional photographers, the quality of even many of the cheaper digital SLR cameras is certainly good enough for producing concepts, client proofs and even finished high resolution artwork.
When it comes to deciding how much to spend on digital input devices such as desktop scanners and digital cameras, the trade-off is whether the cost in time, for retouching scans and photographs, is worth it. 
Or whether this is a justification for upgrading to a more expensive model of scanner or digital camera. 
Or whether to oursource the job for professional drum scanning, or to a professional photographer.
Desktop publishing is a term coined in the 1980s after the development of a specific type of software for creating materials for (primarily) commercial printing; however, today desktop publishing encompasses more than just print communication and utilizes many different types of software. 
The basic goal of desktop publishing remains the same: a process for creating materials for visual communication. 
The main change is in how those materials are disseminated.
For the past dozen years I've used this definition of desktop publishing to inform and develop most of the content on my site:
Desktop publishing is the use of the computer and specialized software to create documents for desktop or commercial printing. 
Desktop publishing refers to the process of using the computer to produce documents such as newsletters, brochures, books, and other publications that were once created manually using a variety of non-computer techniques along with large complex phototypesetting machines. 
Today desktop publishing software does it all - almost. 
But before PageMaker and other desktop publishing software there were e-scales, paste-up, and other non-desktop computer ways of putting together a design for printing.
Properly speaking, desktop publishing is the technical assembly of digital files in the proper format for printing. 
In practical use, much of the "graphic design" process is also accomplished using desktop publishing and graphics software and is sometimes included in the definition of desktop publishing.
Although still technically correct and based on the history of desktop publishing, this definition puts desktop publishing squarely in the realm of print design. 
It also refers almost exclusively to work created using page layout software, with perhaps some word processing and graphics software uses as well.
But it's been over 20 years since desktop publishing was invented. 
It's time to redefine what it means.
Coming up with a new, more current and complete definition for desktop publishing means looking at the way people â€” all kinds of people â€” use desktop publishing software as well as looking at all the different kinds of software used to do desktop publishing tasks. 
Desktop publishing is the use of the computer and software to create visual displays of ideas and information. 
Desktop publishing documents may be for desktop or commercial printing or electronic distribution including PDF, slide shows, email newsletters, and the Web.
Desktop publishing is for producing documents such as newsletters, brochures, books, ads, business cards, posters, and other publications that were once created manually using a variety of non-computer techniques along with large complex phototypesetting machines. 
Desktop publishing is also for producing Web pages and PDF documents to disseminate information, including re-purposing or reformatting print materials for on-screen presentation or online display. 
Desktop publishing principles apply to many types of business and non-business documents created using the computer including greeting cards, digital scrapbooks, email newsletters, Web sites, blogs, eBooks, and computer crafts.
Today's desktop publishing software does it all - almost. 
Before PageMaker and other desktop publishing software programs there were e-scales, paste-up, and other non-desktop computer ways of putting together a design for printing. 
Since PageMaker, individuals and businesses have adopted many types of software to do desktop publishing.
Some desktop publishing software is best suited to print publishing. Some software is more suited to on-screen or electronic distribution. 
Many of the traditional software programs used for desktop publishing combine all these tasks into one package or one suite of tools, such as the Adobe Creative Suite.
Professional designers tend to use certain high-end software for desktop publishing tasks, such as InDesign, QuarkXPress, Photoshop, and Dreamweaver. 
Businesses and individuals may use these same programs or choose from a great variety of programs geared more to business documents or creative printing projects or that offer more ease-of-use for casual users.
Desktop publishing is the aesthetic arrangement of text and graphics and the technical assembly of digital files in the proper format for printing or electronic publishing including PDF, on-screen, and online presentation. 
Desktop publishing involves the use of a variety of software tools, using the tool most suitable for the job whether it be word processing, page layout, graphics, Web, or presentation software. 
Desktop publishing draws from many disciplines including traditional principles of graphic design, Web design, typography, and the graphic arts.
It is difficult to create a linear outline of the tasks involved in desktop publishing. 
Depending on who is doing the work and how it will be used, some of the tasks set out here may be omitted, rearranged, or combined. 
Desktop publishing is the process used by individuals with or without formal design training -- including graphic designers, Web designers, journalists for large and small publications, self-publishers, businesses of all sizes, and ordinary citizens with something to say -- to visually communicate information in print, on-screen, and online. 
Desktop publishing is found in offices and homes.
Among a certain segment of the graphic design community, the term desktop publishing is looked at with derision, associating it with amateur design. 
Desktop publishing is merely a means to an end. 
Designers and non-designers alike use many of the same tools and create many of the same documents. 
However, how well each one does it can differ greatly. 
Desktop publishing isn't the villain. 
It's simply a visual communication process that some people do better than others.
Desktop publishing is a term coined after the development of a specific type of software. 
It's about using that software to combine and rearrange text and images and creating digital files.
Before the invention of desktop publishing software the tasks involved in desktop publishing were done manually, by a variety of people and involved graphic design, typesetting, and prepress tasks which sometimes leads to confusion about what desktop publishing is and how it is done.
Desktop publishing is the use of the computer and software to create visual displays of ideas and information. 
Desktop publishing documents may be for desktop or commercial printing or electronic distribution including PDF, slide shows, email newsletters, epub, and the Web.
Properly speaking, desktop publishing is the technical assembly of digital files in the proper format for printing or for electronic distribution. 
In practical use, much of the "graphic design" process is also accomplished using desktop publishing and graphics software and Web design software and is sometimes included in the definition of desktop publishing.
It is the process of using the computer and specific types of software to combine text and graphics to produce documents such as newsletters, brochures, books, Web pages, etc.
At one time, only professional graphic designers used desktop publishing software. 
Then along came the consumer level desktop publishing software and an explosion of people who did desktop publishing for fun and profit, with or without a background in traditional design. 
Today, desktop publishing is still a career choice for some but it is also increasingly a required skill for a wide range of jobs and careers. 
Find out why everyone needs to know desktop publishing.
Some may argue that desktop publishing is as dead as print. Not so fast. 
Find out the reasons put forward for why we don't need desktop publishing and counter-arguments for why it's still here.
With the right design software you can create almost any print or Web project imaginable. 
For print projects you generally need word processing, page layout, and graphics applications. 
For the Web, some of those same programs work but there's also specialized Web design software as well. 
Creative printing programs feature clip art and templates for a variety of home, school, and office projects. 
Discover what specific design software works best for each use.
Graphic design software and desktop publishing software are essentially the same thing. 
The primary difference is that the desktop publishing category includes some consumer level programs not typically used in professional graphic design.
Adobe InDesign and QuarkXPress are two of the main players in this field. 
These programs are geared toward producing documents for commercial printing and high-end Web publishing. 
Both freelance and in-house graphic designers and businesses use graphic design software.
Identity systems encompass logos, letterhead, and business cards, and spills over into other areas such as business forms, brochures, and signage as well. 
Although there are specialized programs available for all these documents -- most geared toward small businesses -- most of these materials can be easily created in almost any design software. 
For logo design, look specifically at illustration software such as Adobe Illustrator or CorelDRAW.
Many of today's page layout programs for print have Web publishing features as well. 
But are they the best tools for the job, or do you need a program specifically for Web design such as Dreamweaver, Homesite, Expression, or Frontpage? Adobe Creative Suite includes two editions geared specifically for the Web that give you Dreamweaver along with various tools for Web graphics and animation.
What programs do potential employers expect you to know how to use? Use this questionnaire to determine the best Web design software for your needs or browse this comprehensive listing of HTML text editors and WYSIWYG editors for Mac, Windows, and Unix/Linux.
There are many reasons to consider using free design software beyond just the cost-savings. 
Programs such as Scribus, OpenOffice, and even the free version of PagePlus are powerful programs, often comparable in features to some of the most expensive applications from Adobe or Microsoft.
In order to do your job effectively, you want to choose the best print design software. 
But design software is often very, very expensive. 
There are several ways to save money on design software without turning to illegal copies. 
The creative printing titles generally cost less than the professional graphic design software. 
The free software is quite powerful too. You may qualify for academic pricing. 
Using older versions can save money and often do exactly what you need.
From the standard of Fontographer to up-and-coming contenders and specialty font editors for beginners and pros, font design software lets you make your own fonts. 
Some programs are aimed at professional type designers while others let anyone turn their handwriting into a font, apply special effects to a basic font, convert fonts, or add special characters to an existing font.
Desktop publishing and graphic design can make documents look better, prettier. 
But it's about more than just appearance. 
Desktop publishing, used properly, enhances visual communication and streamlines the process of disseminating information of all kinds. 
It's also the method of file preparation that ensures files print properly so that the communications get out in a timely manner.
Answer: Desktop publishing is important as a tool that can enhance communication by making it possible to quickly and efficiently produce printed and electronic (online or on-screen) documents.
Desktop publishing software allows the user to rearrange text and graphics on screen, change typefaces as easily as changing shoes, and resize graphics on the fly, before finally committing a design to paper.
There are drawbacks to desktop publishing in that it also makes it easier and less expensive to produce really bad designs. 
So, while desktop publishing is important, education in basic principles of graphic design and desktop publishing techniques is equally important.
And desktop publishing is important because...
Employers are looking for employees with desktop publishing skills for all their job openings. 
That means office managers, teachers, administrative assistants, real estate agents, restaurant managers, and just about any office or clerical job (and many that aren't) require some level of desktop publishing skills.
Why Everyone Needs to Know Desktop Publishing describes how desktop publishing skills can not only help employees get a better job but also help students get better grades, and help individuals and small businesses save money.
Desktop publishing software can be found in both graphic design firms and other types of businesses of all sizes, homes, schools, quick copy centers, service bureaus, and print shops.
Just about anywhere there is a need to produce printed communications from billboards to baby announcements to business cards, you can find desktop publishing software and people using it.
The type of desktop publishing software in use varies. 
Homes and small businesses may utilize consumer-level desktop publishing software packages designed for non-designers while freelance graphic designers, graphic design firms, corporations, and print shops utilize high-end applications designed for professional and high-volume use. 
Despite the differences in the software, types of documents created, and where the software is used, all these types of uses fall under the umbrella of desktop publishing.
Several events of the mid-1980s including the development of Aldus PageMaker (now Adobe PageMaker) ushered in the era of desktop publishing.
It was primarily the introduction of both the Apple LaserWriter, a PostScript desktop printer, and PageMaker for the Mac that kicked off the desktop publishing revolution. 
Aldus Corporation founder Paul Brainerd, is generally credited for coining the phrase, "desktop publishing." 
1985 was a very good year.
1984 - The Apple Macintosh debuts.
1984 - Hewlett-Packard introduces the LaserJet, the first desktop laser printer.
1985 - Adobe introduces PostScript, the industry standard Page Description Language (PDL) for professional typesetting.
1985 - Aldus develops PageMaker for the Mac, the first "desktop publishing" application.
1985 - Apple produces the LaserWriter, the first desktop laser printer to contain PostScript.
1987 - PageMaker for the Windows platform is introduced.
1990 - Microsoft ships Windows 3.0.
Fast forward to 2003 and beyond. You can still buy Hewlett-Packard LaserJets and Apple LaserWriters but there are hundreds of other printers and printer manufacturers to choose from as well. 
PostScript is at level 3 while PageMaker is at version 7 but is now marketed to the business sector.
In the intervening years since PageMaker's introduction and purchase by Adobe, Quark, Inc.'s QuarkXPress took over as the sweetheart of desktop publishing applications. 
But today Adobe's InDesign is firmly planted in the professional sector and wooing over many converts on both the PC and Mac platforms.
While Macintosh is still considered by some to be the platform of choice for professional desktop publishing (that is changing slowly), dozens of consumer and small business desktop publishing packages hit the shelves in the 1990s, catering to the growing legions of PC/Windows users. 
Most notable among these low-cost Windows desktop publishing options, Microsoft Publisher and Serif PagePlus continue to add features that make them more and more viable as contenders to the traditional professional apps.
Dektop Publishing in the 21st Century has seen a change in the way we define desktop publishing including who does desktop publishing and the software used, even if many of the original players remain.
There is no single best desktop publishing software program. 
There are, however, specific programs that are better suited for certain tasks than others. 
But if you insist, I'll drop a few names down below.
Few desktop publishers rely on a single program to do it all. 
You'll also want to look at the variety of tools that are all part of desktop publishing.
This list of the four types of software used in desktop publishing describes the kinds of software, with examples.
But, if you still just want a quick list of the best desktop publishing software in a few different categories:
Top Desktop Publishing Software for Professional Use: Windows | Mac
If I had to choose just one I'd go with Adobe InDesign.
Best Free Desktop Publishing Software for General Use: Windows | Mac
The top of the freebies is Scribus unless your needs are very basic then Windows users might want to go with Serif PagePlus Starter Edition.
Best Desktop Publishing / Print Creativity Software for scrapbooks, calendars, iron-on transfers.
PrintMaster used to be a top choice but I can't recommend the 2.0/2011/2012 editions. 
Serif PagePlus and Microsoft Publisher are good all-around choices with Xara Software being very popular as well. 
And don't overlook titles from Nova Development such as Print Artist, a former top choice that could benefit from Broderbund's missteps with PrintMaster and The Print Shop.
Best Graphics Software for use in desktop publishing.
If you can't afford Photoshop then the top contenders are Photoshop Elements, the GIMP (free!), and Inkscape (free, and good for page layout too.)
Our readers aren't shy about proclaiming the BEST desktop publishing software. See what they choose.
Before you start comparing products to each other, analyze how you plan to use your desktop publishing software. 
Your planned use and your current desktop publishing and design knowledge can help you find the software with just the right combination of features. 
The best desktop publishing software program is the one (or more) that does what you want and need it to do.
Graphic design and desktop publishing share so many similarities that people often use the terms interchangeably.
There's not really anything terribly wrong with that but it is helpful to know and understand how they differ and how some people use and confuse the terms.
graphic design jobs involve the creative process of coming up with the concepts and ideas and arrangements for visually communicating a specific message
desktop publishing is the mechanical process that the designer and the non-designer use to turn their ideas for newsletters, brochures, ads, posters, greeting cards, and other projects into digital files for desktop or commercial printing
While desktop publishing does require a certain amount of creativity, it is more production-oriented than design-oriented.
Graphic designers use desktop publishing software and techniques to create the print materials they envision. 
The computer and desktop publishing software also aids in the creative process by allowing the designer to easily try out various page layouts, fonts, colors, and other elements.
Non-designers also use desktop publishing software and techniques to create print projects for business or pleasure. 
The amount of creative design that goes into these projects varies greatly. 
The computer and desktop publishing software, along with professionally-designed templates, allow consumers to construct and print the same type of projects as graphic designers although the overall product may not be as well-thought out, carefully crafted, or polished as the work of a professional designer.
Graphic design is the process and art of combining text and graphics and communicating an effective message in the design of logos, graphics, brochures, newsletters, posters, signs, and any other type of visual communication.
Desktop publishing is the process of using the computer and specific types of software to combine text and graphics to produce documents such as newsletters, brochures, books, etc.
Graphic Design = "Good" and Desktop Publishing = "Evil" Myth
Graphic design and desktop publishing are often used interchangeably but, in part because it is an activity also used by non-designers, desktop publishing is often considered a lesser activity than graphic design. 
In truth, the two are separate but intertwined disciplines.
Not everyone who does desktop publishing does graphic design, but most graphic designers are involved in desktop publishing - the production side of design. 
The term desktop publisher can refer to a designer or a non-designer but it often carries negative connotations of an amateur.
Some graphic designers are quite vocal about their distaste for desktop publishing, which is somewhat amusing since much of what they do does involve desktop publishing. 
What they are really upset about is not desktop publishing itself - it's an invaluable part of the entire graphic design process - but rather the misuse (real or perceived) of desktop publishing software by non-designers.
These pages give an overview of the history of prepress from 1984 onwards. For a more elaborate list of events, click on one of the years in the lists below. 
This allows you to go back to the days when phototypesetting took off and machines from Compugraphic, Berthold or Scangraphic ruled the industry. 
If you are in a hurry or just want to enjoy the pictures, watch the visual guide to prepress. 
Enjoy your visit to this virtual prepress museum. 
Donâ€™t forget to also visit the pages about the history of printing.
In the early 80s many of the technologies that are still in use today first appear on the market. IBM launches its Personal Computer. 
The Apple Lisa offers a first glimpse at the graphical user interface that will later be made popular by the Macintosh.
In 1985 the Apple LaserWriter and Aldus PageMaker are thrown in the mix and the desktop publishing revolution can start. 
A designer now has the possibility to create a full page design using standard computers and off-the-shelf software. 
Linotypeâ€™s Linotronic assures high-quality output on film or paper. 
Pretty soon other publishing applications appear for both Mac and PC, closely followed by drawing programs such as Illustrator and freehand. 
Larger screens, faster networking and improved support for peripherals through standards such as SCSI make sure the market matures rapidly.
Artistically the new found freedom often leads to pages that contain at least a dozen different fonts in two or three different typefaces, mixed with fairly low-resolution graphics.
The Ethernet specifications are published.
The IBM PC legitimizes personal computers in the business market.
Adobe is founded, Sony releases its first Trinitron monitor, Sun is incorporated.
The Apple Lisa introduces the graphical user interface and mouse, Creo is incorporated.
The Apple Macintosh is launched, Adobe releases PostScript, Linotype introduces the Linotronic 300 imagesetter.
Once desktop publishing becomes an established phenomenon, a few battles are fought over some of its fundamentals.
Traditional prepress vendors such as Crossfield, Scitex and Dainippon-Screen hope to maintain their lead by using their expensive systems to put the final touches to designs created on Mac. 
Aldus has already developed a technology called OPI to facilitate such workflows. Things turn out differently as Macs and networks within a few years become powerful enough to handle large files. CEPS systems quickly disappear off the market.
One of the traditional weaknesses of PostScript level 1 is its screening technology which isnâ€™t really suitable for 4-color jobs. 
In the screening wars vendors like Linotype-Hell and Agfa try to win sales by offering improved or radically new screening technologies such as stochastic screening.
In the early nineties Adobe, Apple and Microsoft also fight over file formats. 
Adobe looses its absolute control over font formats but manages to maintain PostScript as the standard page description language.
In the early nineties the imagesetter market gradually moves from small 1-up devices to larger systems that are capable of imaging an entire press sheet. 
In the second half of the decade computer-to-plate technology starts taking over but it follows a different pattern: first the 8-up (B1) market moves to CtP, then the even larger VLF (very large format) systems while smaller systems only become popular in the first half of the new millennium.
Weak Apple management and the continuous efforts of Microsoft make PCâ€™s an acceptable alternatives to Macs in the second half of the nineties. 
A lot of back-end processes migrate to the PC platforms, including most of the workflow systems that now enter the market. 
The resurrection of Apple in 1998 makes a lot of designers stick to their Macs for the creative side of things.
The enhanced power of hard- and software leads to more sophisticated designs. 
The wild typography of some designs is no longer due to a lack of artistic insight but inspired by the grunge movement. 
Easy to achieve effects, such as blends in QuarkXPress 2, occasionally still take the design world by storm.
The new millennium isnâ€™t as much about technology as it is about business.
There are some interesting technical trends such as the increased adoption of PDF as an exchange format and Adobeâ€™s move to capture the entire content creation market. 
Process-free technology becomes widely adopted for CtP system.
The majority of changes however are business oriented: the move to CTP kills many of the remaining trade shops, the internet starts having a negative impact on some markets, increased competition forces some printing companies out of business while others merge. 
Many vendors actually go through the same merger frenzy.
In word processing and desktop publishing applications, WYSIWYG means that the display simulates the appearance and represents the effect of fonts and line breaks on the final pagination using a specific printer configuration, so that, for example, a citation on page 1 of a 500-page document can accurately refer to a reference three hundred pages later.
Modern software does a good job of optimizing the screen display for a particular type of output. 
For example, a word processor is optimized for output to a typical printer. 
The software often emulates the resolution of the printer in order to get as close as possible to WYSIWYG. 
However, that is not the main attraction of WYSIWYG, which is the ability of the user to be able to visualize what they are producing.
In the computer security context, a hacker is someone who seeks and exploits weaknesses in a computer system or computer network. Hackers may be motivated by a multitude of reasons, such as profit, protest, challenge. enjoyment,or to evaluate those weaknesses to assist in removing them. 
The subculture that has evolved around hackers is often referred to as the computer underground and is now a known community.
While other uses of the word hacker exist that are related to computer security, such as referring to someone with an advanced understanding of computers and computer networks, they are rarely used in mainstream context. 
They are subject to the longstanding hacker definition controversy about the term's true meaning.
 In this controversy, the term hacker is reclaimed by computer programmers who argue that someone who breaks into computers, whether computer criminal (black hats) or computer security expert (white hats),is more appropriately called a cracker instead.
 Some white hat hackers claim that they also deserve the title hacker, and that only black hats should be called "crackers".
Bruce Sterling traces part of the roots of the computer underground to the Yippies, a 1960s counterculture movement that published the Technological Assistance Program (TAP) newsletter.
TAP was a phone phreaking newsletter that taught techniques for unauthorized exploration of the telephone network. 
Many people from the phreaking community are also active in the hacking community even today, and vice versa.
Several subgroups of the computer underground with different attitudes use different terms to demarcate themselves from each other, or try to exclude some specific group with whom they do not agree.
Eric S. Raymond, author of The New Hacker's Dictionary, advocates that members of the computer underground should be called crackers. 
Yet, those people see themselves as hackers and even try to include the views of Raymond in what they see as a wider hacker culture, a view that Raymond has harshly rejected. 
Instead of a hacker/cracker dichotomy, they emphasize a spectrum of different categories, such as white hat, grey hat, black hat and script kiddie. 
In contrast to Raymond, they usually reserve the termcracker for more malicious activity.
According to Ralph D. Clifford, a cracker or cracking is to "gain unauthorized access to a computer in order to commit another crime such as destroying information contained in that system".
These subgroups may also be defined by the legal status of their activities.
A white hat hacker breaks security for non-malicious reasons, perhaps to test their own security system or while working for a security company which makes security software. 
The term "white hat" in Internet slang refers to an ethical hacker.
 This classification also includes individuals who perform penetration tests and vulnerability assessments within a contractual agreement. 
The EC-Council, also known as the International Council of Electronic Commerce Consultants, is one of those organizations that have developed certifications, courseware, classes, and online training covering the diverse arena of ethical hacking.
A "black hat" hacker is a hacker who "violates computer security for little reason beyond maliciousness or for personal gain" (Moore, 2005).
Black hat hackers form the stereotypical, illegal hacking groups often portrayed in popular culture, and are "the epitome of all that the public fears in a computer criminal".
Black hat hackers break into secure networks to destroy, modify, or steal data; or to make the network unusable for those who are authorized to use the network. 
Black hat hackers are also referred to as the "crackers" within the security industry and by modern programmers. 
Crackers keep the awareness of the vulnerabilities to themselves and do not notify the general public or the manufacturer for patches to be applied.
 Individual freedom and accessibility is promoted over privacy and security. 
Once they have gained control over a system, they may apply patches or fixes to the system only to keep their reigning control.
 Richard Stallman invented the definition to express the maliciousness of a criminal hacker versus a white hat hacker who performs hacking duties to identify places to repair. 
A grey hat hacker lies between a black hat and a white hat hacker. 
A grey hat hacker may surf the Internet and hack into a computer system for the sole purpose of notifying the administrator that their system has a security defect, for example. 
They may then offer to correct the defect for a fee.
Grey hat hackers sometimes find the defect of a system and publish the facts to the world instead of a group of people. 
Even though grey hat hackers may not necessarily perform hacking for their personal gain, unauthorized access to a system can be considered illegal and unethical.
A social status among hackers, elite is used to describe the most skilled. 
Newly discovered exploits circulate among these hackers. Elite groups such as Masters of Deceptionconferred a kind of credibility on their members.
A script kiddie (also known as a skid or skiddie) is an unskilled hacker who breaks into computer systems by using automated tools written by others (usually by other black hat hackers), hence the term script (i.e. a prearranged plan or set of activities) kiddie (i.e. kid, childâ€”an individual lacking knowledge and experience, immature), usually with little understanding of the underlying concept.
A neophyte ("newbie", or "noob") is someone who is new to hacking or phreaking and has almost no knowledge or experience of the workings of technology and hacking. 
A blue hat hacker is someone outside computer security consulting firms who is used to bug-test a system prior to its launch, looking for exploits so they can be closed. Microsoftalso uses the term BlueHat to represent a series of security briefing events.
 A hacktivist is a hacker who utilizes technology to publicize a social, ideological, religious or political message.
Cyberterrorism â€” Activities involving website defacement or denial-of-service attacks.
Freedom of information â€” Making information that is not public, or is public in non-machine-readable formats, accessible to the public.
Intelligence agencies and cyberwarfare operatives of nation states.
Groups of hackers that carry out organized criminal activities for profit. 
Network enumeration: Discovering information about the intended target.
Vulnerability analysis: Identifying potential ways of attack.
Exploitation: Attempting to compromise the system by employing the vulnerabilities found through the vulnerability analysis.
In order to do so, there are several recurring tools of the trade and techniques used by computer criminals and security experts.
A security exploit is a prepared application that takes advantage of a known weakness. 
Common examples of security exploits areSQL injection, cross-site scripting and cross-site request forgery which abuse security holes that may result from substandard programming practice. 
Other exploits would be able to be used through File Transfer Protocol (FTP), Hypertext Transfer Protocol(HTTP), PHP, SSH, Telnet and some Web pages. 
These are very common in Web site and Web domain hacking.
A vulnerability scanner is a tool used to quickly check computers on a network for known weaknesses.
Hackers also commonly use port scanners. 
These check to see which ports on a specified computer are "open" or available to access the computer, and sometimes will detect what program or service is listening on that port, and its version number. 
(Firewalls defend computers from intruders by limiting access to ports and machines, but they can still be circumvented.)
Hackers may also attempt to find vulnerabilities manually. 
A common approach is to search for possible vulnerabilities in the code of the computer system then test them, sometimes reverse engineering the software if the code is not provided.
Password guessing. 
This method is very fast when used to check all short passwords, but for longer passwords other methods such as the dictionary attack are used, because of the time a brute-force search takes.
Password cracking is the process of recovering passwords from data that has been stored in or transmitted by a computer system. 
Common approaches include repeatedly trying guesses for the password, trying the most common passwords by hand, and repeatedly trying passwords from a "dictionary", or a text file with many passwords.
A packet analyzer ("packet sniffer") is an application that captures data packets, which can be used to capture passwords and other data in transit over the network.
A spoofing attack involves one program, system or website that successfully masquerades as another by falsifying data and is thereby treated as a trusted system by a user or another program â€” usually to fool programs, systems or users into revealing confidential information, such as user names and passwords.
A rootkit is a program that uses low-level, hard-to-detect methods to subvert control of an operating system from its legitimate operators.
 Rootkits usually obscure their installation and attempt to prevent their removal through a subversion of standard system security. 
They may include replacements for system binaries, making it virtually impossible for them to be detected by checking process tables.
In the second stage of the targeting process, hackers often use Social engineering tactics to get enough information to access the network. 
They may contact the system administrator and pose as a user who cannot get access to his or her system. 
This technique is portrayed in the 1995 film Hackers, when protagonist Dade "Zero Cool" Murphy calls a somewhat clueless employee in charge of security at a television network. 
Posing as an accountant working for the same company, Dade tricks the employee into giving him the phone number of a modem so he can gain access to the company's computer system.
Hackers who use this technique must have cool personalities, and be familiar with their target's security practices, in order to trick the system administrator into giving them information. 
In some cases, a help-desk employee with limited security experience will answer the phone and be relatively easy to trick. Another approach is for the hacker to pose as an angry supervisor, and when his/her authority is questioned, threaten to fire the help-desk worker. 
Social engineering is very effective, because users are the most vulnerable part of an organization. 
No security devices or programs can keep an organization safe if an employee reveals a password to an unauthorized person.
Intimidation As in the "angry supervisor" technique above, the hacker convinces the person who answers the phone that their job is in danger unless they help them. 
At this point, many people accept that the hacker is a supervisor and give them the information they seek.
Helpfulness The opposite of intimidation, helpfulness exploits many people's natural instinct to help others solve problems.
 Rather than acting angry, the hacker acts distressed and concerned. 
The help desk is the most vulnerable to this type of social engineering, as its general purpose is to help people; and it usually has the authority to change or reset passwords, which is exactly what the hacker wants.
Name-dropping The hacker uses names of authorized users to convince the person who answers the phone that the hacker is a legitimate user him or herself.
 Some of these names, such as those of webpage owners or company officers, can easily be obtained online. 
Hackers have also been known to obtain names by examining discarded documents (so-called "dumpster diving").
Technical Using technology is also a way to get information. 
A hacker can send a fax or email to a legitimate user, seeking a response that contains vital information. 
The hacker may claim that he or she is involved in law enforcement and needs certain data for an investigation, or for record-keeping purposes.
A Trojan horse is a program that seems to be doing one thing but is actually doing another. 
It can be used to set up a back door in a computer system, enabling the intruder to gain access later. 
The name refers to the horse from the Trojan War, with the conceptually similar function of deceiving defenders into bringing an intruder into a protected area.
A virus is a self-replicating program that spreads by inserting copies of itself into other executable code or documents. 
By doing this, it behaves similarly to a biological virus, which spreads by inserting itself into living cells. 
While some viruses are harmless or mere hoaxes, most are considered malicious.
Like a virus, a worm is also a self-replicating program.
 It differs from a virus in that it propagates through computer networks without user intervention; and does not need to attach itself to an existing program. 
Nonetheless, many people use the terms "virus" and "worm" interchangeably to describe any self-propagating program.
A keylogger is a tool designed to record ("log") every keystroke on an affected machine for later retrieval, usually to allow the user of this tool to gain access to confidential information typed on the affected machine. 
Some keyloggers use virus-, trojan-, and rootkit-like methods to conceal themselves. 
However, some of them are used for legitimate purposes, even to enhance computer security.
 For example, a business may maintain a keylogger on a computer used at a point of sale to detect evidence of employee fraud.
A thorough examination of hacker tools and procedures may be found in Cengage Learning's E|CSA certification workbook. 
Jacob Appelbaum is an advocate, security researcher, and developer for the Tor project. 
He speaks internationally for usage of Tor by human rights groups and others concerned about Internet anonymity and censorship.
Rakshit Tandon is an prominent cyber security researcher from India with primary focus on combating online abuse of women and children.
Eric Corley (also known as Emmanuel Goldstein) is the longstanding publisher of 2600: The Hacker Quarterly.
 He is also the founder of the Hackers on Planet Earth (HOPE) conferences. 
He has been part of the hacker community since the late 1970s.
Ed Cummings (also known as Bernie S) is a longstanding writer for 2600: The Hacker Quarterly. 
In 1995, he was arrested and charged with possession of technology that could be used for fraudulent purposes, and set legal precedents after being denied both a bail hearing and a speedy trial.
Dan Kaminsky is a DNS expert who exposed multiple flaws in the protocol and investigated Sony's rootkit security issues in 2005. 
He has spoken in front of the United States Senate on technology issues.
Andrew Auernheimer, sentenced to 3 years in prison, is a grey hat hacker whose security group Goatse Security exposed a flaw in AT&T's iPad security.
Gordon Lyon, known by the handle Fyodor, authored the Nmap Security Scanner as well as many network security books and web sites. 
He is a founding member of theHoneynet Project and Vice President of Computer Professionals for Social Responsibility.
Gary McKinnon is a Scottish hacker facing extradition to the United States to face criminal charges. 
Many people in the UK have called on the authorities to be lenient with McKinnon, who suffers from Asperger syndrome. 
Kevin Mitnick is a computer security consultant and author, formerly the most wanted computer criminal in United States history. 
Rafael NÃºÃ±ez, a.k.a. RaFa, was a notorious hacker who was sought by the Federal Bureau of Investigation in 2001. 
He has since become a respected computer security consultant and an advocate of children's online safety.
Meredith L. Patterson is a well-known technologist and biohacker who has presented research with Dan Kaminsky and Len Sassaman at many international security and hacker conferences.
Len Sassaman was a Belgian computer programmer and technologist who was also a privacy advocate.
Solar Designer is the pseudonym of the founder of the Openwall Project.
MichaÅ‚ Zalewski (lcamtuf) is a prominent security researcher.
The computer underground has produced its own specialized slang, such as 1337speak. 
Its members often advocate freedom of information, strongly opposing the principles of copyright, as well as the rights of free speech and privacy.
Writing software and performing other activities to support these views is referred to as hacktivism. 
Some consider illegal cracking ethically justified for these goals; a common form is website defacement. 
The computer underground is frequently compared to the Wild West.
It is common for hackers to use aliases to conceal their identities.
The computer underground is supported by regular real-world gatherings called hacker conventions or "hacker cons". 
These events include SummerCon (Summer), DEF CON,HoHoCon (Christmas), ShmooCon (February), BlackHat, Chaos Communication Congress, AthCon, Hacker Halted, and HOPE.
 Local Hackfest groups organize and compete to develop their skills to send a team to a prominent convention to compete in group pentesting, exploit and forensics on a larger scale. 
Hacker groups became popular in the early 1980s, providing access to hacking information and resources and a place to learn from other members. Computer bulletin board systems (BBSs), such as the Utopias, provided platforms for information-sharing via dial-up modem.
 Hackers could also gain credibility by being affiliated with elite groups.
However since the mid-1990s, with home computers that could run Unix-like operating systems and with inexpensive internet home access being available for the first time, many people from outside of the academic world started to take part in the programmer subculture of hacking.
Since the mid-1980s, there are some overlaps in ideas and members with the computer security hacking community. 
The most prominent case is Robert T. 
Morris, who was a user of MIT-AI, yet wrote the Morris worm. 
The Jargon File hence calls him "a true hacker who blundered".
 Nevertheless, members of the programmer subculture have a tendency to look down on and disassociate from these overlaps. 
They commonly refer disparagingly to people in the computer security subculture as crackers, and refuse to accept any definition of hacker that encompasses such activities.
 The computer security hacking subculture on the other hand tends not to distinguish between the two subcultures as harshly, instead acknowledging that they have much in common including many members, political and social goals, and a love of learning about technology.
They restrict the use of the term cracker to their categories of script kiddies and black hat hackers instead.
All three subcultures have relations to hardware modifications. 
In the early days of network hacking, phreaks were building blue boxes and various variants. 
The programmer subculture of hackers has stories about several hardware hacks in its folklore, such as a mysterious 'magic' switch attached to a PDP-10 computer in MIT's AI lab, that, when turned off, crashed the computer.
The early hobbyist hackers built their home computers themselves, from construction kits. 
The only kind of widespread hardware modification nowadays is case modding.
An encounter of the programmer and the computer security hacker subculture occurred at the end of the 1980s, when a group of computer security hackers, sympathizing with the Chaos Computer Club (who disclaimed any knowledge in these activities), broke into computers of American military organizations and academic institutions. 
They sold data from these machines to the Soviet secret service, one of them in order to fund his drug addiction. 
The case could be solved when Clifford Stoll, a scientist working as a system administrator, found ways to log the attacks and to trace them back (with the help of many others). 23, a German film adaption with fictional elements, shows the events from the attackers' perspective. 
Stoll described the case in his book The Cuckoo's Egg and in the TV documentary The KGB, the Computer, and Me from the other perspective. 
According to Eric S. Raymond, it "nicely illustrates the difference between 'hacker' and 'cracker'. 
Stoll's portrait of himself, his lady Martha, and his friends at Berkeley and on the Internet paints a marvelously vivid picture of how hackers and the people around them like to live and how they think.
Hacker is a term that is used to mean a variety of different things in computing.
 Depending on the context, the term can refer to a person in any one of several distinct (but not completely disjoint) communities and subcultures.
People committed to circumvention of computer security. 
This primarily concerns unauthorized remote computer break-ins via communication networks such as the Internet (Black hats), but also includes those who debug or fix security problems (White hats), and the morally ambiguous Grey hats.
A community of enthusiast computer programmers and systems designers, originated in the 1960s around the Massachusetts Institute of Technology's (MIT's) Tech Model Railroad Club (TMRC) and MIT Artificial Intelligence Laboratory.
his community is notable for launching the free software movement. 
The World Wide Web and Internetare hacker artifacts.
 The Request for Comments RFC 1392 amplifies this meaning as "[a] person who delights in having an intimate understanding of the internal workings of a system, computers and computer networks in particular."
The hobbyist home computing community, focusing on hardware in the late 1970s (e.g. the Homebrew Computer Club) and on software (video games,[5] software cracking, the demoscene) in the 1980s/1990s. 
The community included Steve Wozniak, Bill Gates and Paul Allen and created the personal computing industry.
Today, mainstream usage of "hacker" mostly refers to computer criminals, due to the mass media usage of the word since the 1980s. This includes what hacker slang calls "script kiddies," people breaking into computers using programs written by others, with very little knowledge about the way they work. 
This usage has become so predominant that the general public is unaware that different meanings exist.
While the self-designation of hobbyists as hackers is acknowledged by all three kinds of hackers, and the computer security hackers accept all uses of the word, people from the programmer subculture consider the computer intrusion related usage incorrect, and emphasize the difference between the two by calling security breakers "crackers" (analogous to a safecracker).
as someone who is able to subvert computer security; if doing so for malicious purposes, the person can also be called a cracker.
an adherent of the technology and programming subculture.
The controversy is usually based on the assumption that the term originally meant someone messing about with something in a positive sense, that is, using playful cleverness to achieve a goal. 
But then, it is supposed, the meaning of the term shifted over the decades since it first came into use in a computer context and came to refer to computer criminals.
As usage has spread more widely, the primary misunderstanding of newer users conflicts with the original primary emphasis. 
In popular usage and in the media, computer intruders or criminals is the exclusive meaning today, with associated pejorative connotations. 
For example, "An Internet 'hacker' broke through state government security systems in March."
In the computing community, the primary meaning is a complimentary description for a particularly brilliant programmer or technical expert. 
For example, "Linus Torvalds, the creator of Linux, is considered by some to be a hacker."
 A large segment of the technical community insist the latter is the "correct" usage of the word (see the Jargon File definition below).
The mainstream media's current usage of the term may be traced back to the early 1980s. 
When the term was introduced to wider society by the mainstream media in 1983, even those in the computer community referred to computer intrusion as "hacking", although not as the exclusive use of that word.
 In reaction to the increasing media use of the term exclusively with the criminal connotation, the computer community began to differentiate their terminology. 
Alternative terms such as "cracker" were coined in an effort to distinguish between those adhering to the historical use of the term "hack" within the programmer community and those performing computer break-ins.
 Further terms such as "black hat", "white hat" and "gray hat" developed when laws against breaking into computers came into effect, to distinguish criminal activities and those activities which were legal.
However, since network news use of the term pertained primarily to the criminal activities despite this attempt by the technical community to preserve and distinguish the original meaning, the mainstream media and general public continue to describe computer criminals with all levels of technical sophistication as "hackers" and do not generally make use of the word in any of its non-criminal connotations. 
Members of the media sometimes seem unaware of the distinction, grouping legitimate "hackers" such as Linus Torvalds andSteve Wozniak along with criminal "crackers".
As a result of this difference, the definition is the subject of heated controversy.
 The wider dominance of the pejorative connotation is resented by many who object to the term being taken from their cultural jargon and used negatively, including those who have historically preferred to self-identify as hackers. 
Many advocate using the more recent and nuanced alternate terms when describing criminals and others who negatively take advantage of security flaws in software and hardware. 
Others prefer to follow common popular usage, arguing that the positive form is confusing and unlikely to become widespread in the general public. 
A minority still use the term in both original senses despite the controversy, leaving context to clarify (or leave ambiguous) which meaning is intended.
However, the positive definition of hacker was widely used as the predominant form for many years before the negative definition was popularized. 
"Hacker" can therefore be seen as a shibboleth, identifying those who use the technically oriented sense (as opposed to the exclusively intrusion-oriented sense) as members of the computing community.
A possible middle ground position has been suggested, based on the observation that "hacking" describes a collection of skills which are used by hackers of both descriptions for differing reasons. 
The analogy is made to locksmithing, specifically picking locks, whichâ€”aside from its being a skill with a fairly high tropism to 'classic' hackingâ€”is a skill which can be used for good or evil. 
The primary weakness of this analogy is the inclusion of script kiddies in the popular usage of "hacker", despite the lack of an underlying skill and knowledge base. 
Sometimes, hacker also is simply used synonymous to geek: "A true hacker is not a group person. 
He's a person who loves to stay up all night, he and the machine in a love-hate relationship.
They're kids who tended to be brilliant but not very interested in conventional goals.
It's a term of derision and also the ultimate compliment.
Fred Shapiro thinks that "the common theory that 'hacker' originally was a benign term and the malicious connotations of the word were a later perversion is untrue." 
He found out that the malicious connotations were present at MIT in 1963 already (quoting The Tech, an MIT student newspaper) and then referred to unauthorized users of the telephone network, that is, the phreaker movement that developed into the computer security hacker subculture of today.
In computer security, a hacker is someone who focuses on security mechanisms of computer and network systems. 
While including those who endeavor to strengthen such mechanisms, it is more often used by the mass media and popular culture to refer to those who seek access despite these security measures. 
That is, the media portrays the 'hacker' as a villain.
 Nevertheless, parts of the subculture see their aim in correcting security problems and use the word in a positive sense. White hat is the name given to ethical computer hackers, who utilize hacking in a helpful way. 
White hats are becoming a necessary part of the information security field.
 They operate under a code, which acknowledges that breaking into other people's computers is bad, but that discovering and exploiting security mechanisms and breaking into computers is still an interesting activity that can be done ethically and legally. 
Accordingly, the term bears strong connotations that are favorable or pejorative, depending on the context.
The subculture around such hackers is termed network hacker subculture, hacker scene or computer underground.
It initially developed in the context of phreaking during the 1960s and the microcomputer BBS scene of the 1980s. 
It is implicated with 2600: The Hacker Quarterly and the alt.2600 newsgroup.
In 1980, an article in the August issue of Psychology Today (with commentary by Philip Zimbardo) used the term "hacker" in its title: "The Hacker Papers". 
It was an excerpt from a Stanford Bulletin Board discussion on the addictive nature of computer use. In the 1982 filmTron, Kevin Flynn (Jeff Bridges) describes his intentions to break into ENCOM's computer system, saying "I've been doing a little hacking here". 
CLU is the software he uses for this. 
By 1983, hacking in the sense of breaking computer security had already been in use as computer jargon, but there was no public awareness about such activities.
However, the release of the film WarGames that year, featuring a computer intrusion into NORAD, raised the public belief that computer security hackers (especially teenagers) could be a threat to national security. 
This concern became real when, in the same year, a gang of teenage hackers in Milwaukee, Wisconsin, known as The 414s, broke into computer systems throughout the United States and Canada, including those of Los Alamos National Laboratory, Sloan-Kettering Cancer Center and Security Pacific Bank.
 The case quickly grew media attention, and 17-year-old Neal Patrick emerged as the spokesman for the gang, including a cover story in Newsweek entitled "Beware: Hackers at play", with Patrick's photograph on the cover.
 The Newsweek article appears to be the first use of the word hacker by the mainstream media in the pejorative sense.
Pressured by media coverage, congressman Dan Glickman called for an investigation and began work on new laws against computer hacking.
Neal Patrick testified before the U.S. House of Representatives on September 26, 1983, about the dangers of computer hacking, and six bills concerning computer crime were introduced in the House that year.
 As a result of these laws against computer criminality, white hat, grey hat and black hat hackers try to distinguish themselves from each other, depending on the legality of their activities. 
These moral conflicts are expressed in The Mentor's "The Hacker Manifesto", published 1986 in Phrack.
Use of the term hacker meaning computer criminal was also advanced by the title "Stalking the Wily Hacker", an article by Clifford Stoll in the May 1988 issue of the Communications of the ACM. 
Later that year, the release by Robert Tappan Morris, Jr. of the so-called Morris worm provoked the popular media to spread this usage. The popularity of Stoll's book The Cuckoo's Egg, published one year later, further entrenched the term in the public's consciousness.
In the programmer subculture of hackers, a hacker is a person who follows a spirit of playful cleverness and loves programming.
 It is found in an originally academic movement unrelated to computer security and most visibly associated with free software and open source.
 It also has a hacker ethic, based on the idea that writing software and sharing the result on a voluntary basis is a good idea, and that information should be free, but that it's not up to the hacker to make it free by breaking into private computer systems. 
This hacker ethic was publicized and perhaps originated in Steven Levy's Hackers: Heroes of the Computer Revolution (1984).
 It contains a codification of its principles.
The programmer subculture of hackers disassociates from the mass media's pejorative use of the word 'hacker' referring to computer security, and usually prefer the term 'cracker' for that meaning.
 Complaints about supposed mainstream misuse started as early as 1983, when media used "hacker" to refer to the computer criminals involved in the 414s case.
 In the programmer subculture of hackers, a computer hacker is a person who enjoys designing software and building programs with a sense for aesthetics and playful cleverness.
 The term hack in this sense can be traced back to "describe the elaborate college pranks that...students would regularly devise".
To be considered a 'hack' was an honor among like-minded peers as "to qualify as a hack, the feat must be imbued with innovation, style and technical virtuosity".
  The MIT Tech Model Railroad Club Dictionary defined hack in 1959 (not yet in a computer context) as "an article or project without constructive end; a project undertaken on bad self-advice; an entropy booster; to produce, or attempt to produce, a hack(3)", and "hacker" was defined as "one who hacks, or makes them". 
Much of TMRC's jargon was later imported into early computing culture, because the club started using a DEC PDP-1 and applied its local model railroad slang in this computing context. 
Initially incomprehensible to outsiders, the slang also became popular in MIT's computing environments beyond the club. Other examples of jargon imported from the club are 'losing' {"when a piece of equipment is not working")[21] and 'munged' ("when a piece of equipment is ruined").
According to Eric S. Raymond,[22] the Open Source and Free Software hacker subculture developed in the 1960s among 'academic hackers'[23] working on early minicomputers in computer science environments in the United States.
Hackers were influenced by and absorbed many ideas of key technological developments and the people associated with them. 
Most notable is the technical culture of the pioneers of the Arpanet, starting in 1969. 
The PDP-10 machine AI at MIT, which was running the ITSoperating system and which was connected to the Arpanet, provided an early hacker meeting point. After 1980 the subculture coalesced with the culture of Unix. Since the mid-1990s, it has been largely coincident with what is now called the free software and open source movement.
Many programmers have been labeled "great hackers",[24] but the specifics of who that label applies to is a matter of opinion. 
Certainly major contributors to computer science such as Edsger Dijkstra and Donald Knuth, as well as the inventors of popular software such asLinus Torvalds (Linux), and Dennis Ritchie and Ken Thompson (the C programming language) are likely to be included in any such list; see also List of programmers. 
Within the computer programmer subculture of hackers, the term hacker is also used for a programmer who reaches a goal by employing a series of modifications to extend existing code or resources.
 In this sense, it can have a negative connotation of using inelegant kludges to accomplish programming tasks that are quick, but ugly, inelegant, difficult to extend, hard to maintain and inefficient. 
This derogatory form of the noun "hack" derives from the everyday English sense "to cut or shape by or as if by crude or ruthless strokes" [Merriam-Webster] and is even used among users of the positive sense of "hacker" who produces "cool" or "neat" hacks.
 In other words to "hack" at an original creation, as if with an axe, is to force-fit it into being usable for a task not intended by the original creator, and a "hacker" would be someone who does this habitually.
 (he original creator and the hacker may be the same person.
This usage is common in both programming, engineering and building. In programming, hacking in this sense appears to be tolerated and seen as a necessary compromise in many situations. 
Some argue that it should not be, due to this negative meaning; others argue that some kludges can, for all their ugliness and imperfection, still have "hack value".
In non-software engineering, the culture is less tolerant of unmaintainable solutions, even when intended to be temporary, and describing someone as a "hacker" might imply that they lack professionalism. 
In this sense, the term has no real positive connotations, except for the idea that the hacker is capable of doing modifications that allow a system to work in the short term, and so has some sort of marketable skills. 
However, there is always the understanding that a more skillful or technical logician could have produced successful modifications that would not be considered a "hack-job". 
The definition is similar to other, non-computer based uses of the term "hack-job". 
For instance, a professional modification of a production sports car into a racing machine would not be considered a hack-job, but a cobbled together backyard mechanic's result could be. 
Even though the outcome of a race of the two machines could not be assumed, a quick inspection would instantly reveal the difference in the level of professionalism of the designers. 
The adjective associated with hacker is "hackish" (see the Jargon file).
In a very universal sense, hacker also means someone who makes things work beyond perceived limits in a clever way in general, without necessarily referring to computers, especially at MIT.
 That is, people who apply the creative attitude of software hackers in fields other than computing.
 This includes even activities that predate computer hacking, for example reality hackers or urban spelunkers (exploring undocumented or unauthorized areas in buildings). 
One specific example is clever pranks traditionally perpetrated by MIT students, with the perpetrator being called hacker. 
For example, when MIT students surreptitiously put a fake police car atop the dome on MIT's Building 10, that was a hack in this sense, and the students involved were therefore hackers.
 Another type of hacker is now called a reality hacker. 
More recent examples of usage for almost any type of playful cleverness are wetware hackers ("hack your brain"), media hackers and "hack your reputation". 
In a similar vein, a "hack" may refer to a math hack, that is, a clever solution to a mathematical problem. 
The GNU General Public License has been described as a copyright hack because it cleverly uses the copyright laws for a purpose the lawmakers did not foresee. All of these uses now also have spread beyond MIT as well.
In yet another context, a hacker is a computer hobbyist who pushes the limits of software or hardware.
 The home computer hacking subculture relates to the hobbyist home computing of the late 1970s, beginning with the availability of MITS Altair. 
An influential organization was the Homebrew Computer Club. 
However, its roots go back further toamateur radio enthusiasts.
 The amateur radio slang referred to creatively tinkering to improve performance as "hacking" already in the 1950s.
A large overlaps between hobbyist hackers and the programmer subculture hackers existed during the Homebrew Club's days, but the interests and values of both communities somewhat diverged. 
Today, the hobbyists focus on commercial computer and video games, software cracking and exceptional computer programming (demo scene). 
Also of interest to some members of this group is the modification of computer hardware and other electronic devices, see modding.
Electronics hobbyists working on machines other than computers also fall into this category.
 This includes people who do simple modifications to graphing calculators, video game consoles, electronic musical keyboards or other device (see CueCat for a notorious example) to expose or add functionality to a device that was unintended for use by end users by the company who created it. 
A number oftechno musicians have modified 1980s-era Casio SK-1 sampling keyboards to create unusual sounds by doing circuit bending: connecting wires to different leads of the integrated circuit chips. 
The results of these DIY experiments range from opening up previously inaccessible features that were part of the chip design to producing the strange, dis-harmonic digital tones that became part of the techno music style. 
Companies take different attitudes towards such practices, ranging from open acceptance (such as Texas Instruments for its graphing calculators and Lego for its Lego Mindstorms robotics gear) to outright hostility (such as Microsoft's attempts to lock out Xbox hackers or the DRM routines on Blu-ray Disc players designed to sabotage compromised players.
In this context, a "hack" refers to a program that (sometimes illegally) modifies another program, often a video game, giving the user access to features otherwise inaccessible to them. 
As an example of this use, for Palm OS users (until the 4th iteration of this operating system), a "hack" refers to an extension of the operating system which provides additional functionality.
 Term also refers to those people who cheat on video games using special software. This can also refer to the jailbreaking of iPhones.
The main basic difference between programmer subculture and computer security hackers is their mostly separate historical origin and development. 
However, the Jargon Filereports that considerable overlap existed for the early phreaking at the beginning of the 1970s. 
An article from MIT's student paper The Tech used the term hacker in this context already in 1963 in its pejorative meaning for someone messing with the phone system.
The overlap quickly started to break when people joined in the activity who did it in a less responsible way.
 This was the case after the publication of an article exposing the activities of Draper and Engressia.
According to Raymond, hackers from the programmer subculture usually work openly and use their real name, while computer security hackers prefer secretive groups and identity-concealing aliases.
 Also, their activities in practice are largely distinct.
 The former focus on creating new and improving existing infrastructure (especially the software environment they work with), while the latter primarily and strongly emphasize the general act of circumvention of security measures, with the effective use of the knowledge (which can be to report and help fixing the security bugs, or exploitation for criminal purpose) being only rather secondary. 
The most visible difference in these views was in the design of the MIT hackers' Incompatible Timesharing System, which deliberately did not have any security measures.
There are some subtle overlaps, however, since basic knowledge about computer security is also common within the programmer subculture of hackers.
 For example, Ken Thompson noted during his 1983 Turing Award lecture that it is possible to add code to the UNIX "login" command that would accept either the intended encrypted password or a particular known password, allowing a back door into the system with the latter password. He named his invention the "Trojan horse". 
Furthermore, Thompson argued, the C compiler itself could be modified to automatically generate the rogue code, to make detecting the modification even harder. 
Because the compiler is itself a program generated from a compiler, the Trojan horse could also be automatically installed in a new compiler program, without any detectable modification to the source of the new compiler.
 However, Thompson disassociated himself strictly from the computer security hackers: "I would like to criticize the press in its handling of the 'hackers,' the 414 gang, the Dalton gang, etc. 
The acts performed by these kids are vandalism at best and probably trespass and theft at worst.. 
I have watched kids testifying before Congress. It is clear that they are completely unaware of the seriousness of their acts.
The programmer subculture of hackers sees secondary circumvention of security mechanisms as legitimate if it is done to get practical barriers out of the way for doing actual work. 
In special forms, that can even be an expression of playful cleverness.
 However, the systematic and primary engagement in such activities is not one of the actual interests of the programmer subculture of hackers and it does not have significance in its actual activities, either.
A further difference is that, historically, members of the programmer subculture of hackers were working at academic institutions and used the computing environment there. 
In contrast, the prototypical computer security hacker had access exclusively to a home computer and a modem. 
In home computing, a hacker is someone who modifies software or hardware of their own private computer system.
 It includes building, rebuilding, modifying, and creating software (software cracking, demoscene), electronic hardware (hardware hacking, overclocking, modding), either to make it better, faster, to give it added features or to make it do something it was not originally intended to do. 
Hacking in this sense originated around hobbyist circles discussing the MITS Altair at the homebrew computer club.
Hardware hackers are those who modify hardware (not limited to computers) to expand capabilities; this group blurs into the culture of hobbyist inventors and professional electronics engineering. 
A sample of such modification includes the addition of TCP/IP Internet capabilities to a number of vending machines and coffee makers during the late 1980s and early 1990s.
 Hackers who have the ability to write circuit-level code, device drivers, firmware, low-level networking, (and even more impressively, using these techniques to make devices do things outside of their spec sheets), are typically in very high regard among hacker communities. 
This is primarily due to the difficulty and enormous complexity of this type of work, and the electrical engineering knowledge required to do so.
Hardware hacking can consist of either making new hardware, or simply modifying existing hardware (known as "modding"). 
Some hardware hackers perform novel and perhaps dangerous modifications, to make the hardware suit their needs or to simply test its limits.
Hacker artists create art by hacking on technology as an artistic medium. 
This has extended the definition of the term and what it means to be a hacker. 
Such artists may work with graphics, computer hardware, sculpture, music and other audio, animation, video, software, simulations, mathematics, reactive sensory systems, text, poetry, literature, or any combination thereof.
Dartmouth College musician Larry Polansky states: "Technology and art are inextricably related. 
Many musicians, video artists, graphic artists, and even poets who work with technologyâ€”whether designing it or using itâ€”consider themselves to be part of the 'hacker community.'
 Computer artists, like non-art hackers, often find themselves on societyâ€™s fringes, developing strange, innovative uses of existing technology. There is an empathetic relationship between those, for example, who design experimental music software and hackers who write communications freeware." [3]
Another description is offered by Jenny Marketou: "Hacker artists operate as culture hackers who manipulate existing techno-semiotic structures towards a different end, to get inside cultural systems on the net and make them do things they were never intended to do.
A successful software and hardware hacker artist is Mark Lottor (mkl), who has created the 3-D light art projects entitled the Cubatron, and the Big Round Cubatron. 
This art is made using custom computer technology, with specially designed circuit boards and programming for microprocessor chips to manipulate the LED lights.
Don Hopkins is a software hacker artist well known for his artistic cellular automata. 
This art, created by a cellular automata computer program, generates objects which randomly bump into each other and in turn create more objects and designs, similar to a lava lamp, except that the parts change color and form through interaction. 
Says Hopkins, "Cellular automata are simple rules that are applied to a grid of cells, or the pixel values of an image. 
The same rule is applied to every cell, to determine its next state, based on the previous state of that cell and its neighboring cells. 
There are many interesting cellular automata rules, and they all look very different, with amazing animated dynamic effects. 'Life' is a widely known cellular automata rule, but many other lesser known rules are much more interesting."
Some hacker artists create art by writing computer code, and others, by developing hardware. 
Some create with existing software tools such as Adobe Photoshop or GIMP.
The creative process of hacker artists can be more abstract than artists using non-technological media. 
For example, mathematicians have produced visually stunning graphic presentations of fractals, which hackers have further enhanced, often producing detailed and intricate graphics and animations from simple mathematical formulas.
A hacker is an adherent of the subculture that originally emerged in academia in the 1960s, around the Massachusetts Institute of Technology (MIT)'s Tech Model Railroad Club(TMRC) and MIT Artificial Intelligence Laboratory. 
A hacker is one who enjoys the intellectual challenge of creatively overcoming and circumventing limitations of programming systems and who tries to extend their capabilities.
 The act of engaging in activities (such as programming or other media) in a spirit of playfulness and exploration is termed hacking. 
However the defining characteristic of a hacker is not the activities performed themselves (e.g. programming), but the manner in which it is done: Hacking entails some form of excellence, for example exploring the limits of what is possible,thereby doing something exciting and meaningful.
 Activities of playful cleverness can be said to have "hack value" and are termed hacks (examples include pranks at MIT intended to demonstrate technical aptitude and cleverness).
What they had in common was mainly love of excellence and programming. 
They wanted to make their programs that they used be as good as they could.
 They also wanted to make them do neat things.
 They wanted to be able to do something in a more exciting way than anyone believed possible and show "Look how wonderful this is.
 I bet you didn't believe this could be done.
Hackers from this subculture tend to emphatically differentiate themselves from what they pejoratively call "crackers"; those who are generally referred to by media and members of the general public using the term "hacker", and whose primary focusâ€”be it to malign or benevolent purposesâ€”lies in exploiting weaknesses in computer security. 
The Jargon File, an influential but not universally accepted compendium of hacker slang, defines hacker as "A person who enjoys exploring the details of programmable systems and stretching their capabilities, as opposed to most users, who prefer to learn only the minimum necessary.
The Request for Comments (RFC) 1392, the Internet Users' Glossary, amplifies this meaning as "A person who delights in having an intimate understanding of the internal workings of a system, computers and computer networks in particular."[9]
As documented in the Jargon File, these hackers are disappointed by the mass media and general public's usage of the word hacker to refer to security breakers, calling them "crackers" instead. 
This includes both "good" crackers ("white hat hackers") who use their computer security related skills and knowledge to learn more about how systems and networks work and to help to discover and fix security holes, as well as those more "evil" crackers  who use the same skills to author harmful software  and illegally infiltrate secure systems with the intention of doing harm to the system.
 The programmer subculture of hackers, in contrast to the cracker community, generally sees computer security related activities as contrary to the ideals of the original and true meaning of the hacker term that instead related to playful cleverness.
The word "hacker" derives from the seventeenth century word of a "lusty laborer" who harvested fields by dogged and rough swings of his hoe. Although the idea of "hacking" has existed long before the term "hacker"â€”with the most notable example of Lightning Ellsworth, it was not a word that the first programmers used to describe themselves.
 In fact, many of the first programmers were oftentimes from the engineering or physics background. 
"But from about 1945 onward (and especially during the creation of the first ENIAC computer) some programmers realized that their expertise in computer software and technology had evolved not just into a profession, but into a passion" (46).[3]
It was not until the 1960s that the term hackers began to be used to describe proficient computer programmers.
 Therefore, the fundamental characteristic that links all who identify themselves as hackers are ones who enjoy "â€¦the intellectual challenge of creatively overcoming and circumventing limitations of programming systems and who tries to extend their capabilities".
 With this definition in mind, it can be clear where the negative implications of the word "hacker" and the subculture of "hackers" came from.
Some common nicknames among this culture include "crackers" who are unskilled thieves who mainly rely on luck. 
Others include "phreak"â€”which refers to a type of skilled crackers and "warez d00dz"â€”which is a kind of cracker that acquires reproductions of copyrighted software. 
Within all hackers are tiers of hackers such as the "samurai" who are hackers that hire themselves out for legal electronic locksmith work. Furthermore, there are other hackers that are hired to test security which are called "sneakers" or "tiger teams".
Before communications between computers and computer users were as networked as they are now, there were multiple independent and parallel hacker subcultures, often unaware or only partially aware of each other's existence.
Creating software and sharing it with each other.
Placing a high value on freedom of inquiry.
Hostility to secrecy.
Information-sharing as both an ideal and a practical strategy.
Upholding the right to fork.
Emphasis on rationality.
Distaste for authority.
Playful cleverness, taking the serious humorously and the humor seriously.
These sorts of subcultures were commonly found at academic settings such as college campuses. 
The MIT Artificial Intelligence Laboratory, the University of California, Berkeley and Carnegie Mellon University were particularly well-known hotbeds of early hacker culture. 
They evolved in parallel, and largely unconsciously, until the Internet, where a legendary PDP-10 machine at MIT, called AI, that was running ITS, provided an early meeting point of the hacker community. 
This and other developments such as the rise of the free software movement and community drew together a critically large population and encouraged the spread of a conscious, common, and systematic ethos. 
Symptomatic of this evolution were an increasing adoption of common slang and a shared view of history, similar to the way in which other occupational groups have professionalized themselves but without the formal credentialing process characteristic of most professional groups.
Over time, the academic hacker subculture has tended to become more conscious, more cohesive, and better organized.
 The most important consciousness-raising moments have included the composition of the first Jargon File in 1973, the promulgation of the GNU Manifesto in 1985, and the publication of Eric Raymond's The Cathedral and the Bazaar in 1997.
 Correlated with this has been the gradual recognition of a set of shared culture heroes, including: Bill Joy, Donald Knuth, Dennis Ritchie, Paul Graham, Alan Kay, Ken Thompson, Richard M. Stallman, Linus Torvalds, Larry Wall, and Guido Van Rossum.
The concentration of academic hacker subculture has paralleled and partly been driven by the commoditization of computer and networking technology, and has in turn accelerated that process.
 In 1975, hackerdom was scattered across several different families of operating systems and disparate networks; today it is largely a Unix and TCP/IP phenomenon, and is concentrated around various operating systems based on free software and open-source softwaredevelopment.
Many of the values and tenets of the free and open source software movement stem from the hacker ethics that originated at MITand at the Homebrew Computer Club. 
The hacker ethics were chronicled by Steven Levy in Hackers: Heroes of the Computer Revolution and in other texts in which Levy formulates and summarizes general hacker attitudes.
Access to computers-and anything that might teach you something about the way the world works-should be unlimited and total.
All information should be free.
Hackers should be judged by their hacking, not bogus criteria such as degrees, age, race, or position.
You can create art and beauty on a computer.
Computers can change your life for the better.
Hacker ethics are concerned primarily with sharing, openness, collaboration, and engaging in the hands-on imperative.
Linus Torvalds, one of the leaders of the open source movement (known primarily for developing the Linux kernel), has noted in the book The Hacker Ethic] that these principles have evolved from the known Protestant ethics and incorporates the spirits of capitalism, as introduced in the early 20th century by Max Weber.
While using hacker to refer to someone who enjoys playful cleverness is most often applied to computer programmers, it is sometimes used for people who apply the same attitude to other fields. 
 For example, Richard Stallman describes the silent composition 4â€²33â€³ by John Cage and the 14th century palindromic three-part piece "Ma Fin Est Mon Commencement" by Guillaume de Machaut as hacks.
 According to the Jargon File,the word hacker was used in a similar sense among radio amateurs in the 1950s, predating the software hacking community.
The book Inside Narcotics, a semi-clandestine work appearing in 1990 and in its fifth English edition as of 2007 which is a compendium of scientific, historical, and cultural information about the opiates and related drugs and includes historical and scientific research on more than 150 drugs of this type, includes a discussion of the term in its Introduction. 
After making the above-mentioned distinction betwixt crackers and hackers ("a hacker is simply an autodidact, someone who doesn't feel satisfied with the information spoon-fed to the masses by the grey forces of mediocrity...") it goes on to say "it is therefore possible to be a phone hacker [ phreaker ], music hacker, sex hacker, drugs hacker, politics hacker, religion hacker..."
Hack value is the notion used by hackers to express that something is worth doing or is interesting.
This is something that hackers often feel intuitively about a problem or solution.
An aspect of hack value is performing feats for the sake of showing that they can be done, even if others think it is difficult. Using things in a unique way outside their intended purpose is often perceived as having hack value. 
Examples are using a dot matrix impact printer to produce musical notes, using a flatbed scanner to take ultra-high-resolution photographs or using an optical mouse as barcode reader.
A solution or feat has "hack value" if it is done in a way that has finesse, cleverness or brilliance, which makes creativity an essential part of the meaning. For example, picking a difficult lock has hack value; smashing a lock does not. 
As another example, proving Fermat's last theorem by linking together most of modern mathematics has hack value; solving a combinatorial problem by exhaustively trying all possibilities does not. Hacking is not using process of elimination to find a solution; it's the process of finding a clever solution to a problem.
Hacker groups began to flourish in the early 1980s, with the advent of the home computer. 
Prior to that, the term hacker was simply a referral to any computer hobbyist. 
The hacker groups were out to make names for themselves, and were often spurred on by their own press. 
This was a heyday of hacking, at a time before there was much law against computer crime. 
Hacker groups provided access to information and resources, and a place to learn from other members.
Hackers could also gain credibility by being affiliated with an elite group.
The names of hacker groups parody large corporations, governments, police and criminals;[2] and often used specialized orthography. 
In one sense it's silly to argue about the ``true'' meaning of a word. 
A word means whatever people use it to mean.
 I am not the Academie FranÃ§aise; I can't force Newsweek to use the word ``hacker'' according to my official definition.
Still, understanding the etymological history of the word ``hacker'' may help in understanding the current social situation.
The concept of hacking entered the computer culture at the Massachusetts Institute of Technology in the 1960s. 
Popular opinion at MIT posited that there are two kinds of students, tools and hackers. 
A ``tool'' is someone who attends class regularly, is always to be found in the library when no class is meeting, and gets straight As. 
A ``hacker'' is the opposite: someone who never goes to class, who in fact sleeps all day, and who spends the night pursuing recreational activities rather than studying. 
But there are standards for success as a hacker, just as grades form a standard for success as a tool. 
The true hacker can't just sit around all night; he must pursue some hobby with dedication and flair. 
It can be telephones, or railroads (model, real, or both), or science fiction fandom, or ham radio, or broadcast radio. 
In 1986, the word ``hacker'' is generally used among MIT students to refer not to computer hackers but to building hackers, people who explore roofs and tunnels where they're not supposed to be.
A ``computer hacker,'' then, is someone who lives and breathes computers, who knows all about computers, who can get a computer to do anything. 
Equally important, though, is the hacker's attitude.
 Computer programming must be a hobby, something done for fun, not out of a sense of duty or for the money. 
It's okay to make money, but that can't be the reason for hacking.
A hacker is an aesthete.
There are specialties within computer hacking. 
An algorithm hacker knows all about the best algorithm for any problem. 
A system hacker knows about designing and maintaining operating systems. 
And a ``password hacker'' knows how to find out someone else's password. 
That's what Newsweek should be calling them.
Someone who sets out to crack the security of a system for financial gain is not a hacker at all.
 It's not that a hacker can't be a thief, but a hacker can't be a professional thief.
 A hacker must be fundamentally an amateur, even though hackers can get paid for their expertise. 
A password hacker whose primary interest is in learning how the system works doesn't therefore necessarily refrain from stealing information or services, but someone whose primary interest is in stealing isn't a hacker. 
It's a matter of emphasis.
Throughout most of the history of the human race, right and wrong were relatively easy concepts. 
Each person was born into a particular social role, in a particular society, and what to do in any situation was part of the traditional meaning of the role. 
This social destiny was backed up by the authority of church or state.
This simple view of ethics was destroyed about 200 years ago, most notably by Immanuel Kant (1724-1804). 
Kant is in many ways the inventor of the 20th Century. 
He rejected the ethical force of tradition, and created the modern idea of autonomy. Along with this radical idea, he introduced the centrality of rational thought as both the glory and the obligation of human beings. 
There is a paradox in Kant: Each person makes free, autonomous choices, unfettered by outside authority, and yet each person is compelled by the demands of rationality to accept Kant's ethical principle, the Categorical Imperative. 
This principle is based on the idea that what is ethical for an individual must be generalizable to everyone.
Modern cognitive psychology is based on Kant's ideas. 
Central to the functioning of the mind, most people now believe, is information processing and rational argument. 
Even emotions, for many psychologists, are a kind of theorem based on reasoning from data. 
Kohlberg's theory of moral development interprets moral weakness as cognitive weakness, the inability to understand sophisticated moral reasoning, rather than as a failure of will. 
Disputed questions of ethics, like abortion, are debated as if they were questions of fact, subject to rational proof.
Since Kant, many philosophers have refined his work, and many others have disagreed with it. 
For our purpose, understanding what a hacker is, we must consider one of the latter, SÃ¶ren Kierkegaard (1813-1855). 
A Christian who hated the established churches, Kierkegaard accepted Kant's radical idea of personal autonomy. 
But he rejected Kant's conclusion that a rational person is necessarily compelled to follow ethical principles. 
In the book Either-Or he presents a dialogue between two people. 
One of them accepts Kant's ethical point of view.
 The other takes an aesthetic point of view: what's important in life is immediate experience.
The choice between the ethical and the aesthetic is not the choice between good and evil, it is the choice whether or not to choose in terms of good and evil. 
At the heart of the aesthetic way of life, as Kierkegaard characterises it, is the attempt to lose the self in the immediacy of present experience. 
The paradigm of aesthetic expression is the romantic lover who is immersed in his own passion. 
By contrast the paradigm of the ethical is marriage, a state of commitment and obligation through time, in which the present is bound by the past and to the future. Each of the two ways of life is informed by different concepts, incompatible attitudes, rival premises. 
Kierkegaard's point is that no rational argument can convince us to follow the ethical path. That decision is a radically free choice. 
He is not, himself, neutral about it; he wants us to choose the ethical. 
But he wants us to understand that we do have a real choice to make.
 The basis of his own choice, of course, was Christian faith. 
That's why he sees a need for religious conviction even in the post-Kantian world. But the ethical choice can also be based on a secular humanist faith.
A lesson on the history of philosophy may seem out of place in a position paper by a computer scientist about a pragmatic problem. 
But Kierkegaard, who lived a century before the electronic computer, gave us the most profound understanding of what a hacker is. 
A hacker is an aesthete.
The life of a true hacker is episodic, rather than planned. 
Hackers create ``hacks.''
 A hack can be anything from a practical joke to a brilliant new computer program. (VisiCalc was a great hack. Its imitators are not hacks.) 
But whatever it is, a good hack must be aesthetically perfect. 
If it's a joke, it must be a complete one. 
If you decide to turn someone's dorm room upside-down, it's not enough to epoxy the furniture to the ceiling. 
You must also epoxy the pieces of paper to the desk.
Steven Levy, in the book Hackers, talks at length about what he calls the ``hacker ethic.'' This phrase is very misleading. 
What he has discovered is the Hacker Aesthetic, the standards for art criticism of hacks. 
For example, when Richard Stallman says that information should be given out freely, his opinion is not based on a notion of property as theft, which (right or wrong) would be an ethical position. 
His argument is that keeping information secret is inefficient; it leads to unaesthetic duplication of effort.
The original hackers at MIT-AI were mostly undergraduates, in their late teens or early twenties. 
The aesthetic viewpoint is quite appropriate to people of that age.
 An epic tale of passionate love between 20-year-olds can be very moving. 
A tale of passionate love between 40-year-olds is more likely to be comic. 
To embrace the aesthetic life is not to embrace evil; hackers need not be enemies of society. 
They are young and immature, and should be protected for their own sake as well as ours.
In practical terms, the problem of providing moral education to hackers is the same as the problem of moral education in general. 
Real people are not wholly ethical or wholly aesthetic; they shift from one viewpoint to another. (They may not recognize the shifts.
 That's why Levy says ``ethic'' when talking about an aesthetic.)
 Some tasks in moral education are to raise the self-awareness of the young, to encourage their developing ethical viewpoint, and to point out gently and lovingly the situations in which their aesthetic impulses work against their ethical standards.
The Conscience of a Hacker (also known as The Hacker Manifesto) is a small essay written January 8, 1986 by a computer security hacker who went by the handle (or pseudonym) of The Mentor (born Loyd Blankenship), who belonged to the 2nd generation of Legion of Doom. 
It was written after the author's arrest, and first published in the underground hacker ezine Phrack and can be found on many websites, as well as on T-shirts and in films.Considered a cornerstone of hacker culture,The Manifesto acts as a guideline to hackers across the globe, especially those new to the field. 
It serves as an ethical foundation for hacking, and asserts that there is a point to hacking that supersedes selfish desires to exploit or harm other people, and that technology should be used to expand our horizons and try to keep the world free.
I was going through hacking withdrawal, and Craig/Knight Lightning needed something for an upcoming issue of Phrack. I was reading The Moon is a Harsh Mistress and was very taken with the idea of revolution. 
And [had] a great deal of empathy for my friends around the nation that were also in the same situation.
This was post-WarGames, the movie, so pretty much the only public perception of hackers at that time was â€˜hey, weâ€™re going to start a nuclear war, or play tic-tac-toe, one of the two,â€™ and so I decided I would try to write what I really felt was the essence of what we were doing and why we were doing it.
The article is quoted several times in the 1995 movie Hackers, although in the movie it is being read from an issue of the hacker magazine 2600, not the historically accuratePhrack. 
It is also reproduced inside the CD case of the computer game Uplink.
The Mentor gave a reading of The Hacker Manifesto and offered additional insight at H2K2.
It is also an item in the game Culpa Innata.
"A Hacker Manifesto" is also the name of a book written by The New School media studies professor McKenzie Wark.
A poster of the Hacker Manifesto is displayed in The Social Network in Mark Zuckerberg's dorm room.
An intranet is a computer network that uses Internet Protocol technology to share information, operational systems, or computing services within an organization. 
This term is used in contrast to extranet, a network between organizations, and instead refers to a network within an organization. 
Sometimes, the term refers only to the organization's internal website, but may be a more extensive part of the organization's information technology infrastructure, and may be composed of multiple local area networks. 
The objective is to organize each individual's desktop with minimal cost, time and effort to be more productive, cost efficient, timely, and competitive.
An intranet may host multiple private websites and constitute an important component and focal point of internal communication and collaboration. 
Any of the well known Internet protocols may be found in an intranet, such as HTTP (web services), SMTP (e-mail), and FTP (file transfer protocol).
 Internet technologies are often deployed to provide modern interfaces to legacy information systems hosting corporate data.
An intranet can be understood as a private analog of the Internet, or as a private extension of the Internet confined to an organization. 
The first intranet websites and home pages were published in 1991, and began to appear in non-educational organizations in 1994. 
Intranets are sometimes contrasted to extranets.
 While intranets are generally restricted to employees of the organization, extranets may also be accessed by customers, suppliers, or other approved parties.
 Extranets extend a private network onto the Internet with special provisions for authentication, authorization and accounting (AAA protocol).
In many organizations, intranets are protected from unauthorized external access by means of a network gateway and firewall. 
For smaller companies, intranets may be created simply by using private IP address ranges. 
However, companies may provide access to off-site employees by using a virtual private network, or by other access methods, requiring user authentication and encryption.
Increasingly, intranets are being used to deliver tools, e.g. collaboration (to facilitate working in groups and teleconferencing) or sophisticated corporate directories, sales andcustomer relationship management tools, project management etc., to advance productivity.
 Intranet is a interconnection of computers that enables peer groups in institutes and industries to interact among themselves.
Intranets are also being used as corporate culture-change platforms. 
For example, large numbers of employees discussing key issues in an intranet forum application could lead to new ideas in management, productivity, quality, and other corporate issues.
In large intranets, website traffic is often similar to public website traffic and can be better understood by using web metrics software to track overall activity. 
User surveys also improve intranet website effectiveness. 
Larger businesses allow users within their intranet to access public internet through firewall servers. 
They have the ability to screen messages coming and going keeping security intact.
When part of an intranet is made accessible to customers and others outside the business, that part becomes part of an extranet. 
Businesses can send private messages through the public network, using special encryption/decryption and other security safeguards to connect one part of their intranet to another.
Intranet user-experience, editorial, and technology teams work together to produce in-house sites. 
Most commonly, intranets are managed by the communications, HR or CIOdepartments of large organizations, or some combination of these.
Because of the scope and variety of content and the number of system interfaces, intranets of many organizations are much more complex than their respective public websites.
Intranets and their use are growing rapidly.
According to the Intranet design annual 2007 from Nielsen Norman Group, the number of pages on participants' intranets averaged 200,000 over the years 2001 to 2003 and has grown to an average of 6 million pages over 2005â€“2007.[5]
Workforce productivity: Intranets can help users to locate and view information faster and use applications relevant to their roles and responsibilities. 
With the help of a web browser interface, users can access data held in any database the organization wants to make available, anytime and â€” subject to security provisions â€” from anywhere within the company workstations, increasing employees' ability to perform their jobs faster, more accurately, and with confidence that they have the right information. 
It also helps to improve the services provided to the users.
Time: Intranets allow organizations to distribute information to employees on an as-needed basis; Employees may link to relevant information at their convenience, rather than being distracted indiscriminately by email.
Communication: Intranets can serve as powerful tools for communication within an organization, vertically strategic initiatives that have a global reach throughout the organization. 
The type of information that can easily be conveyed is the purpose of the initiative and what the initiative is aiming to achieve, who is driving the initiative, results achieved to date, and who to speak to for more information.
By providing this information on the intranet, staff have the opportunity to keep up-to-date with the strategic focus of the organization. 
Some examples of communication would be chat, email, and/or blogs. 
A great real world example of where an intranet helped a company communicate is when Nestle had a number of food processing plants in Scandinavia. 
Their central support system had to deal with a number of queries every day.
When Nestle decided to invest in an intranet, they quickly realized the savings.
McGovern says the savings from the reduction in query calls was substantially greater than the investment in the intranet.
Web publishing allows cumbersome corporate knowledge to be maintained and easily accessed throughout the company using hypermedia and Web technologies.
Examples include: employee manuals, benefits documents, company policies, business standards, news feeds, and even training, can be accessed using common Internet standards (Acrobat files, Flash files, CGI applications). 
Because each business unit can update the online copy of a document, the most recent version is usually available to employees using the intranet.
Business operations and management: Intranets are also being used as a platform for developing and deploying applications to support business operations and decisions across the internetworked enterprise.]
Cost-effective: Users can view information and data via web-browser rather than maintaining physical documents such as procedure manuals, internal phone list and requisition forms.
This can potentially save the business money on printing, duplicating documents, and the environment as well as document maintenance overhead. 
For example, the HRM company PeopleSoft "derived significant cost savings by shifting HR processes to the intranet".
McGovern goes on to say the manual cost of enrolling in benefits was found to be USD109.48 per enrollment. "Shifting this process to the intranet reduced the cost per enrollment to $21.79; a saving of 80 percent". 
Another company that saved money on expense reports was Cisco.
 "In 1996, Cisco processed 54,000 reports and the amount of dollars processed was USD19 million".
Enhance collaboration: Information is easily accessible by all authorised users, which enables teamwork.
Cross-platform capability: Standards-compliant web browsers are available for Windows, Mac, and UNIX.
Built for one audience: Many companies dictate computer specifications which, in turn, may allow Intranet developers to write applications that only have to work on one browser (no cross-browser compatibility issues).
Being able to specifically address your "viewer" is a great advantage.
Since Intranets are user-specific (requiring database/network authentication prior to access), you know exactly who you are interfacing with and can personalize your Intranet based on role (job title, department) or individual ("Congratulations Jane, on your 3rd year with our company!").
Promote common corporate culture: Every user has the ability to view the same information within the Intranet.
Immediate updates: When dealing with the public in any capacity, laws, specifications, and parameters can change. 
Intranets make it possible to provide your audience with "live" changes so they are kept up-to-date, which can limit a company's liability.
Supports a distributed computing architecture: The intranet can also be linked to a companyâ€™s management information system, for example a time keeping system.
Most organizations devote considerable resources into the planning and implementation of their intranet as it is of strategic importance to the organization's success. 
Some of the planning would include topics such as:
The purpose and goals of the intranet
Persons or departments responsible for implementation and management
Functional plans, information architecture, page layouts, design
Implementation schedules and phase-out of existing systems
Defining and implementing security of the intranet
How to ensure it is within legal boundaries and other constraints
Level of interactivity (e.g. wikis, on-line forms) desired.
Is the input of new data and updating of existing data to be centrally controlled or devolved
These are in addition to the hardware and software decisions (like content management systems), participation issues (like good taste, harassment, confidentiality), and features to be supported.
Intranets are often static sites.
Essentially they are a shared drive, serving up centrally stored documents alongside internal articles or communications (often one-way communication).
However organisations are now starting to think of how their intranets can become a 'communication hub' for their team by using companies specialising in 'socialising' intranets
The actual implementation would include steps such as:
Securing senior management support and funding. 
Business requirements analysis.
Identify users' information needs.
Installation of web server and user access network.
Installing required user applications on computers.
Creation of document framework for the content to be hosted. 
User involvement in testing and promoting use of intranet.
Ongoing measurement and evaluation, including through benchmarking against other intranets. 
Another useful component in an intranet structure might be key personnel committed to maintaining the Intranet and keeping content current. 
For feedback on the intranet, social networking can be done through a forum for users to indicate what they want and what they do not like.
Microsoft SharePoint is the dominant software which used for intranets.
 Estimates indicate that around 50% of all intranets are developed using SharePoint,however there are many alternatives.
An intranet portal is the gateway that unifies access to all enterprise information and applications on an intranet. 
It is a tool that helps a company manage its data, applications, and information more easily, and through personalized views. 
Some portal solutions today are able to integrate legacy applications, objects from other portals, and handle thousands of user requests. 
In a corporate enterprise environment, it is also known as an enterprise portal.
Corporate intranets began gaining popularity during the 1990s. 
Intranets quickly grew more complex as the result the concept of intranet portal was born. 
Today, intranet portals provide value-added capabilities such as managing workflows, increasing collaboration between work groups, and allowing content creators to self-publish their information.
One typical example of an intranet portal is Microsoft Sharepoint, which is used by 46% of organizations.
It provides a lot of features necessary for collaboration, integration and customization.
Integration â€” Ability to integrate with your current tools or the possibility of adding new tools. 
You have your outlook calendar and email integrated within intranet.
Security â€” Enable user or group based security to secure documents and sites throughout the intranet portal.
Customization â€” Software that is flexible to allow for organization.
 Web Parts can be used to create custom modules which can make interaction easier with the site. 
Ability for users to customize tools and resources they use most often.
Collaboration â€” People are now able to collaborate their work with each other. Example would be multiple people working on one document.
Communication Channels â€” Allows corporations to promote corporate culture and present information in a more interactive way than before.
Automation â€” Things like workflows and templates can automate specific document creation. 
Alerts can be created to help learn of changes and new additions to the intranet.
Applications â€” Links to applications for associates to perform duties.
User Friendly â€” Application must be easy to use and understand due to a wide range of technical abilities.
Remote Access â€” Ability for users to access content away from the office.
Document Repository â€” Ability to store and retrieve document information while maintaining regular backups to prevent data loss.
Blog â€” Used as a method to provide more timely information to employees, customers, and business partners.
People Search â€” Search enterprise wide for employee information such as contact information, specialty areas, group membership, personal interest, etc.
Enterprise Search â€” search enterprise content using enterprise search
Targeted Content â€” Business portal administrators can target content by business group area, e.g., HR, Marketing, Legal, Corporate Executives, etc.
Intranet portal helps employees make better and more informed decisions, which result from increased knowledge. 
It also helps reduce costs, saves time, increases collaboration, increases productivity and effectiveness.
Intranet portal can help employees find information more easily and perform their jobs better, though few portal designs are optimal just out-of-the-box. 
In fact, especially in smaller companies, designers can realize some features found in off-the-shelf portal software through simpler (do-it-yourself) means. 
Most intranets have become completely unwieldy and present a highly fragmented and confusing user experience, with no consistency and little navigational support. 
Portals aim to correct this problem by presenting a single gateway to all corporate information and services. 
One benefit of creating this consistent look and feel is users need less time to learn how to use the environment. 
They also more easily recognize where they are in the portal and where they can goâ€”no small feat when navigating a large information space. 
By integrating services and presenting personalized snippets on the initial screen, intranet portals also reduce the need for users to browse far and wide to obtain needed information, thus making it easier for them to perform their jobs.
Intranet portal is a Web-based tool that allows users to create a customized site that dynamically pulls in Internet activities and desired content into a single page. 
By providing a contextual framework for information, portals can bring S&T (Science and Technology) and organizational "knowledge" to the desktop.
Intranet Portals can be a large business cost. 
The maintenance and management can be time consuming and expensive. 
Not only is it a cost to keep the portal running but a cost when the system goes offline. Most intranets are established to put all an organization's resources into one place and having that offline can force operations to be put on hold.
Security issues can become an ongoing problem.
 Unauthorized access is a concern and can result in users gaining access to sensitive information. 
Denial of access can cause issues for users needing access for their work.
Having everything in one place is only good if it's organized. 
Information overload can make finding information very difficult - lowering productivity. 
Tools & Resources â€” Area for employees to link to or download necessary applications to perform work functions. Information also provided to find internal and external resources.
Business Operations â€” To give users access to important business policies and manuals.
Company Calendar â€” To give user access to important company event dates and times.
Access Point for Employees â€” Location for employees default main company webpage to obtain all information regarding the company.
Wiki â€” can be used in the business environment for knowledge management
Workflow Management â€” Establish work flows for common business tasks such as submitting expense reports, submitting corporate HR paperwork and document approval processes.
Bulletin Board â€” Manage corporate announcements.
Task Management â€” Create and update shared task lists throughout the corporation.
In business, an intranet strategy is the use of an intranet and associated hardware and software to obtain one or more organizational objectives. 
An intranet is an access-restricted network used internally in an organization. 
An intranet uses the same concepts and technologies as the World Wide Web and Internet.
 This includes web browsers andservers running on the internet protocol suite and using Internet protocols such as FTP, TCP/IP, HTML, and Simple Mail Transfer Protocol (SMTP).
centrally administer all network functions including servers, clients, security, directories, and traffic.
give users access to a variety of internal and external business tools/applications.
integrate different technologies.
conduct regular user research to identify and confirm strategy (random sample surveys, usability testing, focus groups, in-depth interviews with wireframes, etc.)
reduces printing, distribution, and paper costs - particularly on policy manuals, company newsletters, product catalogs, technical drawings, training material, and telephone directories
easy to use - no specialized training required.
inexpensive to use (once it is set up).
moderate initial setup costs (hardware and software).
standardized network protocol (TCP/IP), document protocol (HTML), and file transfer protocol (ftp) already well established and suitable for all platforms.
can be used throughout the enterprise.
reduces employee training costs.
reduces sales and marketing costs.
reduces office administration and accounting costs.
ease of access results in a more integrated company with employees communicating and collaborating more freely and more productively.
it is an evolving technology that requires upgrades and could have software incompatibility problems.
security features can be inadequate.
inadequate system performance management and poor user support.
may not scale up adequately.
maintaining content can be time consuming.
some employees may not have PCs at their desks.
The aims of the organisation in developing an intranet may not align with user needs .
Intranet is system in which multiple PCs are connected to each other.
PCs in intranet are not available to the world outside the intranet.
Usually each company or organization has their own Intranet network and members/employees of that company can access the computers in their intranet.
Each computer in Intranet is also identified by an IP Address which is unique among the computers in that Intranet.
Intranet uses the internet protocols such as TCP/IP and FTP.
Intranet sites are accessible via web browser in similar way as websites in internet. 
But only members of Intranet network can access intranet hosted sites.
In Intranet, own instant messengers can be used as similar to yahoo messenger/ gtalk over the internet.
Internet is general to PCs all over the world whereas Intranet is specific to few PCs.
Internet has wider access and provides a better access to websites to large population whereas Intranet is restricted.
Internet is not as safe as Intranet as Intranet can be safely privatized as per the need.
Definition: Intranet is the generic term for a collection of private computer networks within an organization. 
An intranet uses network technologies as a tool to facilitate communication between people or work groups to improve the data sharing capability and overall knowledge base of an organization's employees.
Intranets utilize standard network hardware and software technologies like Ethernet, Wi-Fi, TCP/IP, Web browsers and Web servers.
An organization's intranet typically includes Internet access but is firewalled so that its computers cannot be reached directly from the outside.
A common extension to intranets, called extranets, opens this firewall to provide controlled access to outsiders.
Many schools and non-profit groups have deployed them, but an intranet is still seen primarily as a corporate productivity tool. 
A simple intranet consists of an internal email system and perhaps a message board service.
 More sophisticated intranets include Web sites and databases containing company news, forms, and personnel information. 
Besides company email and Internet accesss, an intranet generally incorporates internal Web sites, documents, and/or databases.
The server hosting the portal may only be a "pass through" for the user.
By use of portlets, application functionality can be presented in any number of portal pages. 
For the most part, this architecture is transparent to the user.
In such a design, security and concurrent user capacity can be important issues, and security designers need to ensure that only authenticated and authorized users can generate requests to the application server. 
If the security design and administration does not ensure adequate authentication and authorization, then the portal may inadvertently present vulnerabilities to various types of attacks.
An intranet is a network based on TCP/IP protocols (an internet) belonging to an organization, usually a corporation, accessible only by the organization's members, employees, or others withauthorization. 
An intranet's Web sites look and act just like any other Web sites, but the firewall surrounding an intranet fends off unauthorized access.
Like the Internet itself, intranets are used to share information. 
Secure intranets are now the fastest-growing segment of the Internet because they are much less expensive to build and manage than private networks based on proprietary protocols.
This is a network that is not available to the world outside of the Intranet.
If the Intranet network is connected to the Internet, the Intranet will reside behind a firewall and, if it allows access from the Internet, will be an Extranet.
The firewall helps to control access between the Intranet and Internet to permit access to the Intranet only to people who are members of the same company or organisation. 
In its simplest form, an Intranet can be set up on a networked PC without any PC on the network having access via the Intranet network to the Internet. 
A web portal is most often one specially designed Web page that brings information together from diverse sources in a uniform way. 
Usually, each information source gets its dedicated area on the page for displaying information (a portlet); often, the user can configure which ones to display. 
Variants of portals include mashups and intranet "dashboards" for executives and managers. 
The extent to which content is displayed in a "uniform way" may depend on the intended user and the intended purpose, as well as the diversity of the content. 
Very often design emphasis is on a certain "metaphor" for configuring and customizing the presentation of the content and the chosen implementation framework and/or code libraries.
 In addition, the role of the user in an organization may determine which content can be added to the portal or deleted from the portal configuration.
A portal may use a search engine API to permit users to search intranet content as opposed to extranet content by restricting which domains may be searched. 
Apart from this common search engines feature, web portals may offer other services such as e-mail, news, stock quotes, information from databases and even entertainment content.
 Portals provide a way for enterprises and organizations to provide a consistent look and feel with access control and procedures for multiple applications and databases, which otherwise would have been different web entities at various URLs. 
The features available may be restricted by whether access is by an authorized and authenticated user (employee,member) or an anonymous site visitor.
Examples of early public web portals were AOL, Excite, Netvibes, iGoogle, MSN, Naver, Lycos, Indiatimes, Rediff, and Yahoo!. 
See for example, the "My Yahoo!" feature of Yahoo! which may have inspired such features as the later Google "iGoogle" (discontinued as of November 1, 2013.) 
The configurable side-panels of, for example, the modern Opera browser and the option of "Speed Dial" pages by most browsers continue to reflect the earlier "portal" metaphor.In the late 1990s the web portal was a web IT buzzword. 
After the proliferation of web browsers in the late-1990s many companies tried to build or acquire a portal to attempt to obtain a share of an Internet market. 
A search engine is really a general kind of software, however, the term is often used to specifically describe systems like Google, Bing and Yahoo!
The web portal gained special attention because it was, for many users, the starting point of their web browsing if it was set as their home page. 
The content and branding of a portal could change as internet companies merged or were acquired. Netscape became a part of America Online, the Walt Disney Companylaunched Go.com, IBM and others launched Prodigy, and Excite and @Home became a part of AT&T Corporation during the late 1990s. Lycos was said to be a good target for other media companies, such as CBS.
Portals which relied on HTML IFrames gave rise to a need for web access points which either required frames or sites that had to offer non-frames alternatives.
The interest in portals saw some old media companies racing to outbid each other for Internet properties but died down with the dot-com bust in 2000 and 2001.
 Disney pulled the plug on Go.com, Excite went bankrupt, and its remains were sold to iWon.com. 
Some portal sites such as Yahoo! and those others first listed in this article remain active and portals feature widely outside the English-speaking web (Chinese, Japanese, Indian, Russian and other very popular sites not frequented by English-only users.) Portal metaphors are widely used by public library sites for borrowers using a login as users and by university intranets for students and for faculty. 
Vertical markets remain for ISV's offering management and executive intranet "dashboards" for corporations and government agencies in areas such as GRC and risk management.
Web portals are sometimes classified as horizontal or vertical. 
A horizontal portal is used as a platform to several companies in the same economic sector or to the same type of manufacturers or distributors.
A vertical portal (also known as a "vortal") is a specialized entry point to a specific market or industry niche, subject area, or interest.
Some vertical portals are known as "vertical information portals" (VIPs). 
VIPs provide news, editorial content, digital publications, and e-commerce capabilities. In contrast to traditional vertical portals, VIPs also provide dynamic multimedia applications including social networking, video posting, and blogging.
A personal portal is a web page at a web site on the World Wide Web or a local HTML home page including JavaScript and perhaps running in a modified web browser. 
A personal portal typically provides personalized capabilities to its visitors or its local user, providing a pathway to other content.
 It may be designed to use distributed applications, different numbers and types of middleware and hardware to provide services from a number of different sources and may run on a non-standard local web server. 
In addition, business portals can be designed for sharing and collaboration in workplaces.
 A further business-driven requirement of portals is that the content be presented on multiple platforms such as personal computers, personal digital assistants (PDAs), and cell phones/mobile phone/mobile phones. 
Information, news, and updates are examples of content that would be delivered through such a portal. 
Personal portals can be related to any specific topic such as providing friend information on a social network or providing links to outside content that may help others beyond your reach of services.
 Portals are not limited to simply providing links.
 Outside of business intracet user, very often simpler portals become replaced with richer mashup designs. 
Within enterprises, early portals were often replaced by much more powerful "dashboard" designs. 
Some also have relied on newer protocols such as some version of RSS aggregation and may or may not involve some degree of web harvesting.
 Examples of personal portals include:
home.psafe.com â€“ A personal portal based on adaptive neural network technology provides customizable content according to each user's navigation, and provide full security against viruses, malware, phishing and bank fraud.
 The portal is developed by Brazilian online security company PSafe.[3]
At the end of the dot-com boom in the 1990s, many governments had already committed to creating portal sites for their citizens. 
These included primary portals to the governments as well as portals developed for specific audiences. Examples of government web portals include:
australia.gov.au for Australia.
USA.gov for the United States (in English) & GobiernoUSA.gov (in Spanish).
Disability.gov for citizens with disabilities in the United States.
Europa (web portal) links to all EU agencies and institutions in addition to press releases and audiovisual content from press conferences.
gov.uk for citizens & businesslink.gov.uk for businesses in the United Kingdom.
Health-EU portal gathers all relevant health topics from across Europe.
National Resource Directory links to resources for United States Service Members, Veterans and their families.
Cultural portal aggregate digitised cultural collections of galleries, libraries (see: library portal), archives and museums. 
This type of portals provides a point of access to invisible web cultural content that may not be indexed by standard search engines. 
Digitised collections can include books, artworks, photography, journals, newspapers, music, sound recordings, film, maps, diaries and letters, and archived websites as well as the descriptive metadata associated with each type of cultural work. 
These portals are usually based around a specific national or regional groupings of institutions. Examples of cultural portals include:
DigitalNZ â€“ A cultural portal led by the National Library of New Zealand focused on New Zealand digital content.
Europeana â€“ A cultural portal for the European Union based in the National Library of the Netherlands and overseen by the Europeana Foundation.
Trove â€“ A cultural portal led by the National Library of Australia focused on Australian content.
Corporate intranets became common during the 1990s.
 As intranets grew in size and complexity, webmasters were faced with increasing content and user management challenges. 
A consolidated view of company information was judged insufficient; users wanted personalization and customization.
 Webmasters, if skilled enough, were able to offer some capabilities, but for the most part ended up driving users away from using the intranet.
Many companies began to offer tools to help webmasters manage their data, applications and information more easily, and through personalized views. 
Portal solutions can also include workflow management, collaboration between work groups, and policy-managed content publication. 
Most can allow internal and external access to specific corporate information using secure authentication or single sign-on.
JSR168 Standards emerged around 2001. 
Java Specification Request (JSR) 168 standards allow the interoperability of portlets across different portal platforms. 
These standards allow portal developers, administrators and consumers to integrate standards-based portals and portlets across a variety of vendor solutions.
The concept of content aggregation seems to still gain momentum and portal solution will likely continue to evolve significantly over the next few years. 
The Gartner Grouppredicts generation 8 portals to expand on the Business Mashups concept of delivering a variety of information, tools, applications and access points through a single mechanism.
With the increase in user generated content, disparate data silos, and file formats, information architects and taxonomist will be required to allow users the ability to tag (classify) the data. 
This will ultimately cause a ripple effect where users will also be generating ad hoc navigation and information flows.
Corporate Portals also offer customers & employees self-service opportunities.
Also known as stock-share portals, stock market portals or stock exchange portals are Web-based applications that facilitates the process of informing the share-holders with substantial online data such as the latest price, ask/bids, the latest News, reports and announcements. 
Some stock portals use online gateways through a central depository system (CDS) for the visitors (ram) to buy or sell their shares or manage their portfolio.
Search portals aggregate results from several search engines into one page. You can find search portals specialized in a product, for example property search portals likeNestoria or Nuroa.
A tender portal is a gateway for government suppliers to bid on providing goods and services. 
Tender portals allow users to search, modify, submit, review and archive data in order to provide a complete online tendering process.
Using online tendering, bidders can do any of the following:
Receive notification of the tenders.
Receive tender documents online.
Fill out the forms online.
Submit proposals and documents.
Submit bids online.
Hosted web portals gained popularity and a number of companies began offering them as a hosted service. 
The hosted portal market fundamentally changed the composition of portals. 
In many ways they served simply as a tool for publishing information instead of the loftier goals of integrating legacy applications or presenting correlated data from distributed databases. 
The early hosted portal companies such as Hyperoffice.com or the now defunct InternetPortal.com focused on collaboration and scheduling in addition to the distribution of corporate data. 
As hosted web portals have risen in popularity their feature set has grown to include hosted databases, document management, email, discussion forums and more. 
Hosted portals automatically personalize the content generated from their modules to provide a personalized experience to their users. 
In this regard they have remained true to the original goals of the earlier corporate web portals. 
Emerging new classes of internet portals called Cloud Portals are showcasing the power of API (Application Programming Interface) rich software systems leveraging SOA (service-oriented architecture, web services, and custom data exchange) to accommodate machine to machine interaction creating a more fluid user experience for connecting users spanning multiple domains during a given "session".
 Leading cloud portals like Nubifer Cloud Portal showcase what is possible using Enterprise Mashup and Web Service integration approaches to building cloud portals.
A number of portals have come about which are specific to the particular domain, offering access to related companies and services; a prime example of this trend would be the growth in property portals that give access to services such as estate agents, removal firm, and solicitors that offer conveyancing. 
Along the same lines, industry-specific news and information portals have appeared, such as the clinical trials-specific portal.
The main concept is to present the user with a single web page that brings together or aggregates content from a number of other systems or servers.
The application server or architecture performs most of the crucial functions of the application. 
This application server is in turn connected to database servers, and may be part of a clustered server environment. 
High-capacity portal configurations may include load balancing strategies.
For portals that present application functionality to the user, the portal server is in reality the front piece of a server configuration that includes some connectivity to the application server. 
For early web browsers permitting HTML frameset and IFrame elements, diverse information could be presented without violating the browser same-source security policy (relied upon to prevent a variety of cross-site security breaches.) 
More recent client-side technologies rely on JavaScript frameworks and libraries that rely on more recent web functionality such as WebSockets and async callbacks using XMLHttpRequests.
The intranet is essentially a small-scale version of the internet, operating with similar functionality, but existing solely within the firm.
Like the internet, the intranet uses network technologies such as Transmission Control Protocol/Internet Protocol (TCP/IP).
It allows for the creation of internal networks with common internet applications that can allow them to communicate with different operating systems (Newell et al 2000).
Although it need not be, the intranet is usually linked to the internet, where broader searches are implemented. 
However, outsiders are excluded through security measures such as firewalls.
The intranet can be a very useful tool in the knowledge management process.
It allows for the integration of multimedia communication and can act as a platform for groupware applications and publishing. 
It is intended to enhance collaboration, productivity, and socialization, but also to influence organizational culture and to act as a repository for embedded knowledge.
The focus is to provide a useful site that enhances work practices, communicates key information, provides the right navigation tools, and helps define organizational culture. 
Many factors have to be balanced to create the right homepage, including quality of content, site design, site navigation, site & content maintenance and updates, and the application of tools that are directly useful to the business processes and networks.
The objectives of the intranet will also vary depending on the individual business, and may focus more on certain aspects than others.
Perhaps the most important function of the intranet is knowledge sharing and collaboration. 
The main functions supporting this are (Damsgaard & Scheepers 1998 in Newt et al 2000):
Publishing: E,g, homepages, newsletters, documents, employee directories.
Searching: The intranet can integrate different search functions, e.g. through a search engine or using a system of categorization.
Transacting: Allows user to make transactions with other web/intranet homepages.
Interacting: Collaborative applications and other groupware, expert finders, directories, etc.
Recording: It can be used as a storage medium for such elements as procedures, best practices, and FAQs (embedded and explicit knowledge).
Naturally, the implementation of the intranet must be done in line with organizational needs, processes, and objectives, as outlined in the section on implementation of knowledge management systems.
One specific and key concern is the selection of the search engine. Google offers an option for on-site search, which you can read more about here.
In his article, "The Ten Best Intranets of 2011", Jakob Nielsen (2011) indicates that the best intranets implemented solutions in the following areas:
Knowledge sharing: This aspect is very similar to what I have discussed so far on this site and includes the sharing of all manner of explicit knowledge, but also connecting people that require assistance to experts that can help them.
Innovation management: By incorporating tools that support the recording and management of new ideas.
Comments: This is an easy way to allow users to contribute with their insight. 
This type of loose, unstructured communication can provide some limited tacit knowledge transfer and can encourage participation.
Ratings: An even quicker, albeit shallower, way for people to point to good sources of knowledge.
Participation rewards: Point systems, badges, and other symbolic rewards actually increase participation. Sometimes non-symbolic rewards (i.e. actual prizes) were used.
Customized collections: By allowing users to customize content collections, one can bypass the shortcoming of never being fully able to predict a user's knowledge and information needs.
The extranet is an extension of the intranet to the firm's external network, including partners, suppliers and so on. 
The term is sometimes used to refer to a supplementary system working alongside the intranet or to a part of the intranet that is made available to certain external users.
The extranet provides a shared network with limited, controlled access to organizational information and knowledge resources, and uses security protocols, such as authentification, to limit access. 
An extranet can enhance collaboration and information transfer with partners in the external network.
Security is a key concern, and a firm must protect its crucial knowledge and information resources. 
This can be done using firewalls, use of encryption, and simple or strong authentification. 
Simple authentification involves usernames and passwords, while strong authentification makes use of digital certificates.
The content of both intranets and extranets is usually managed with a content management system.
Organizations rely upon intranets and extranets to share information, communicate with employees, suppliers and customers, and enable collaboration on projects. 
This lesson explains how intranets and extranets work.
 It introduces the topics of virtual private networks, tunneling and firewalls.
John is an engineer for an automobile manufacturer. 
He works with a team of engineers to redesign current models of vehicles as well as create new prototype vehicles. 
The design and development of a vehicle is a collaborative process that relies heavily on technology to connect engineers, project managers, suppliers and vendors together.
This collaborative process is made possible through the use of Internet technologies that link different systems and networks within an organization. 
John and his co-workers rely on two commonly used business networks that use Internet technologies: the intranet and extranet.
John uses the intranet, or a private, internal, corporate network that utilizes the Internet, on a daily basis. 
The intranet provides John and his co-workers with access to data across the entire organization. 
Company personnel can access this private network but someone like you or me, who are considered the public, cannot access the automaker's intranet.
It is private and protected by firewalls (or hardware or software designed to keep threats and unintended visitors from accessing a private network).
A firewall is like a security officer standing guard at a gate. 
The security officer can either allow or deny access.
It is common for companies to provide external access to intranets. 
John also uses the company's extranet, or private networks that are extended to users outside of the organization. 
An organization can use an extranet to allow vendors and customers limited access to its intranet. 
That means you and me could gain access to certain portions of the automaker's intranet through the extranet. 
Once again, firewalls are used to secure and limit access to internal data while also authenticating users.
To gain access to the extranet, authentication will usually take the form of a user ID and password.
Are you wondering what technologies enable the use of intranets and extranets? Well, intranets and extranets make use of similar technologies to the World Wide Web. 
They use the Transmission Control Protocol/Internet Protocol (TCP/IP) , Web pages, and Web browsers for access. 
They also use Web programming languages like Hypertext Markup Language (HTML). 
Intranets and extranets operate similarly to the Internet, making a transition to these networks very easy.
So why did John's automobile company decide to incorporate intranets and extranets into their business processes? Well, the intranet is a critical system that provides business value by enhancing communication and collaboration between John, his co-workers and management. 
It can reduce business costs by streamlining processes and creating more operational efficiency.
John relies on the intranet for a variety of tasks. 
When he needs to call his project manager but just can't remember his extension, he can use the intranet to find pertinent information such as telephone numbers and extensions of employees.
If John wishes to move up from engineer to lead engineer, he can browse the intranet for job postings that may enable him to make the move. 
John can use the intranet to view his pay stubs, make changes to his tax deductions, review the company calendar for holidays and practically anything else he needs to know related to his job. 
The intranet is also a valuable tool for collaboration. John's team of engineers and designers can use the intranet to disseminate information and keep all team members informed. 
They can monitor the progress of the new vehicle project and make changes as needed.
Extranets provide business value by facilitating communication with customers, partners, suppliers and vendors. 
John's employer can leverage their partnerships to become more competitive. 
Extranets enable the organization to build and strengthen strategic relationships with customers and suppliers.
Collaboration is enhanced for better product design, development and marketing.
Auto dealers can use the extranet to check on production and delivery dates.
They can place orders to meet customer demand.
Suppliers can access information pertaining to new features or vehicle colors to match their own products. 
John, our engineer, could access the company intranet through the extranet to work on projects from home or while on business trips.
Customers could use the automaker's extranet to find specific information on their vehicle.
They could look up maintenance records, recalls, warranty information or even the current value.
For example, consider an office with a few PCs and a few printers all networked together. 
The network would not be connected to the outside world. On one of the drives of one of the PCs there would be a directory of web pages that comprise the Intranet. 
For those interested in the background of the term â€œintranetâ€, itâ€™s helpful to splice up the word and look at how it relates to the larger internet.
The â€œinternetâ€ is a web between many networks.
An â€œintranetâ€ is a web within a network.
The internet connects many people to many websites and many networks. 
An intranet connects people within a network. 
So your intranet is simply a website within your companyâ€™s network that (mostly) only employees can access.
James Robertson, perhaps the worldâ€™s foremost authority on intranets, says that intranets have 5 purposes:
In 2007, when James first blogged about the purposes of intranets, he only listed three: Content, Communication and Activity. In 2008 he updated his list with Collaboration as intranets started showing serious evolution from static websites towards the place where collaborative work gets done. 
More recently, he added Culture as social intranets have become central to the cultural glue within companies.
Over the past couple of years, as intranets have become more collaborative, folks have argued over the term â€œintranet.â€ 
Should we keep it? Is it a dated term? Do we need a new word to describe this evolved â€œthingâ€ the intranet has become?
The answer (my answer?): No, we donâ€™t.
When â€œweb 2.0â€³ came onto the scene we didnâ€™t stop calling the internet â€œthe internet.â€ Why? Because itâ€™s still accurate. 
The internet is still a web between many networks.
Itâ€™s the stuff we do on websites thatâ€™s evolved. Similarly, the stuff employees can do on intranets has evolved. 
But intranets are still internal websites that help employees get stuff done.
While all us geeks have labored over what exactly â€œweb 2.0â€³ means, average users have simply adapted to the new types of websites available. 
Our moms are using Facebook and theyâ€™re using the internet to do it.
Itâ€™s not important to them whether a particular site is web 2.0 or not.
For them itâ€™s just the stuff you do on the internet, some of which is newer than other stuff.
Instead of coming up with new terms for intranet nerds to use to talk about intranets, letâ€™s keep it simple and straightforward. An intranet is an internal website that helps employees get stuff done.
Some intranet people are interested in rebranding intranets as the â€œDigital Workplaceâ€. 
But thereâ€™s no reason to do that â€” it takes a term that is understood (â€œintranetâ€) and replaces it with something so broad and generic as to render it meaningless. 
Isnâ€™t a phone digital? And an office thermostat? And a watch? Itâ€™s a fine term for a consultant to use to describe their general services, but not a replacement for the term â€œintranetâ€, which has been in use for about 16 years.
You can go there and when you get there you know youâ€™ve arrived. 
An intranet can be social or not and can include lots of different integrated applications. 
But itâ€™s still a place (as much as something on the web can be a place). 
When you show new employees the intranet they know itâ€™s the intranet and they know how to get there.
But â€œdigital workplaceâ€ is a concept. Itâ€™s an idea, a catch-all description. 
The term refers to the set of applications (mostly web-based) that you use to do your work. 
You canâ€™t get to the digital workplace. 
â€œDigital workplaceâ€ is like the atmosphere while â€œintranetâ€ is like a farm.
Youâ€™re always in the atmosphere and itâ€™s around you. 
But when you get to a farm you know youâ€™re there.
And when you leave you know youâ€™re going somewhere else.
An intranet is an internal website that helps employees get stuff done. 
It helps people who donâ€™t know about intranets learn what they are. 
And when one of us intranet geeks uses the word â€œintranet,â€ the rest of us know what it means.
Intranets contain a tremendous amount of information that needs to be easily accessible. 
Implementing the right information architecture (IA) and using appropriate navigation elements are essential to a successful intranet and a good user experience.
On top of that, intranet IA designers face a tremendous challenge: they have limited sources of inspiration. 
Because of the very nature of intranets, designers often have few opportunities to see how other intranets structure their information.
To provide intranet designers with a set of ideas and solutions, we analyzed 77 intranet information architectures and presented their detailed navigation structure, screenshots, and our analysis in the 2nd edition of our Intranet IA Design Methods and Case Studies report.
The first edition was published 7 years ago, so comparing the two sets of case studies allows us to identify new trends in intranet IA. The biggest change lies in the navigational design elements used, not in the topics presented in the main menu. 
We havenâ€™t seen nearly the same amount of change at the IA level over this 7-year-long period, as we have seen at the page level. Page-level intranet design has been transformed, as documented in the Intranet Design Annual series.
Many intranets suffer from confusing menu labels. 
Such labels make it difficult for users to find what theyâ€™re looking for.
The most common reasons for confusion are:
Terms are too broad and end up acting as catch-all (e.g., For Employees).
Sections such as How do Iâ€¦ or Tools grow too big.
Different section names on the site lack distinction (e.g., Human Resources and Administration & Management may appear in the same menu).
Language is polluted with jargon or branded terms.
Unclear naming is one of the biggest and most important projects to tackle when it comes to the intranet IA. 
Each navigation category must be descriptive, specific, and mutually exclusive so that users can pick where to navigate without hesitation.
User-research methods such as card sorting, tree testing, and usability testing can help identify ambiguous labels and unclear information groupings.
In our analysis, task-based structures often better withstood organizational change than intranets organized by department. 
In many cases, intranets are initially structured to mimic the organizational configuration of the company, because this type of IA makes maintenance of the intranet easy: each department or area gets a section of the intranet. 
However, the down side is that every time thereâ€™s a re-org in the institution, the navigation has to change too. 
We have also found, in our user testing of intranets, that task-based navigation tends to facilitate learning.
Luckily, most intranets in our analysis adopted at least a partial task-based organization scheme. 
By the time of the second edition of this report, 86% of the new intranet IAs were task based, or at least mostly organized by task or topic. This means that the content organization was looked at independently of who owned or created the content.
A common pitfall with task-based IAs is difficult-to-scan category names. 
Organizations think that category names need to start with verbs or follow an â€œI need to...â€ pattern in order to be task based. 
Sometimes trying to fit link or menu labels to a specified format makes them long and more difficult to scan, because the most meaningful words don't appear until the end of the label. 
Task-based IA doesn't require any particular grammatical structure for labels; it just means grouping information according to how employees use it, rather than by who creates and maintains it.
that did change between editions was the maximum number of navigation categories. 
In the first edition of the report we saw more organizations with very broad menus. 
Eleven percent of companies in our first edition had more than 12 categories, while none did in the second edition.
This shift towards fewer total categories is good. 
While a broad structure shows a wide range of the available content, too many categories can make it difficult for users to choose a site area, because of potential overlap in the meaning of the many menu items.
Information about departments or divisions was a top-level category in 45% of intranets and support services(common needs across organization) appeared at the top in 40% of intranets. 
There was a very long tail of additional categories found in a smaller proportion of intranets.
Besides these most common topics, companies in the same industry tended to propagate the same categories at the top of their IA. 
For example, manufacturing companies often included a product-related category in their top-level navigation, whereas companies with a focus on intellectual property often presented a top-level knowledge-management (KM) category.
The labels used to represent each of these topics varied widely across intranets.
The intranet is the most logical central location for HR documents. 
When changes happen, itâ€™s easy to replace them with the most current version, thus reducing confusion.
 Hereâ€™s a sampling of documents that can be uploaded into the intranet:
HR-related news are also easily disseminated on the intranet. 
Examples include announcements about new hires, changes in policies and procedures, upcoming holidays, and job vacancies.
An HR calendar provides at-a-glance information on training schedules, deadlines, and other important events.
The intranet is also the perfect place to gather and respond to employeesâ€™ HR-related questions. Answers are archived for the benefit of other staff members. 
Proper tagging and use of meta-data make answers easily searchable.
By using the intranet for information dissemination, the number of inquiries to HR can be greatly reduced, freeing up HR staff for other tasks.
HR can also use the intranet for faster information collection.
 For example, the employee database can be housed in the intranet, and employees can update their personal data as changes happen.
HR can also create an intranet e-form to perform a needs assessment of staff. 
This information can be used for planning many other HR activities.
E-forms can be created for virtually any HR transaction.
For example, vacation applications, benefit enrollments, and resource booking can all be done on the intranet. 
Because the intranet can be accessed on any web browser â€” including mobile devices â€” approvals can be done even when approving officers are out of the office. 
Everything from planning, scheduling and delivering training can be done on the intranet. 
Text, photos, slideshows, and videos can be embedded in a wiki on the intranet. 
Because the training materials are no longer in a physical format, they are available simultaneously to users (whereas previously training DVDs, for example, were limited in number and availability).
The intranet also makes it easy for HR staff to monitor completion of training courses, respond to questions, and assess learning.
The intranet also makes it easy for HR to gather feedback.
Aside from collecting employee questions, HR can easily get staff membersâ€™ comments on documents and other content. 
Polls and surveys help HR collect employee opinions and input. 
By analyzing the content analysis of staff membersâ€™ status updates, blog posts, and intranet comments, HR can get a sense of the general mood and emotions of staff.
HR can track birthdays and anniversaries, organize special events, and help employees know each other better through the intranet.
Posting photos and videos of office events also helps build community and camaraderie.
Performance management processes can be completed in the intranet. 
For example, an e-form can be used to document key result areas for each employee. Performance tracking and appraisals can also be done on the intranet.
Recruitment also becomes easier with the intranet. 
Internal recruitment is served by announcing vacancies and accepting applications via the intranet. 
HR can also use the intranet to find internal talent to fill vacancies.
For external recruitment, applications can be routed and scored through the intranet.
The virtual meeting room can be used by recruiting panelists, especially if theyâ€™re in different locations, to discuss candidates and create a shortlist.
Internal or private network of an organization based on internet technology (such as hypertext and TCP/IP protocols) and accessed over the internet. 
An intranet is meant for the exclusive use of the organization and its associates (customers, employees, members, suppliers, etc.) and is protected from unauthorized access with security systems such as firewalls. 
Intranets provide services such as email, data storage, and search and retrieval functions, and areemployed in disseminating policy manuals and internal directories for the employees, price and product information for the customers, and requirements and specifications for the suppliers. 
Some intranets are confined to a building whereas others span continents.
Intranets typically start by publishing web pages about company events, health and safety policies, and staff newsletters. 
Popular applications follow, such as forms to reclaim expenses or request holidays. 
All these help eliminate paperwork and speed up workflows.
As more features are added, an intranet can become essential to the operation of an organisation.
 It becomes a portal that provides access to all the things workers need.
The intranet is protected from the global internet by firewalls and by the need to log on with a secure password. 
Staff working outside the organisation may be able to access the intranet by using a VPN (virtual private network). 
This means all communications between the intranet and the userâ€™s personal computer are encrypted.
Multimedia is the field concerned with the computer used to control the integration of text, graphics, drawings, still and moving images (Video), animation, audio, and any other media where every type of information can be represented, stored, transmitted and processed digitally.
Multimedia refers to content that uses a combination of different content forms.
This contrasts with media that use only rudimentary computer displays such as text-only or traditional forms of printed or hand-produced material.
Multimedia includes a combination of text, audio, still images, animation, video, or interactivity content forms.
Multimedia is usually recorded and played, displayed, or accessed by information content processing devices, such as computerized and electronic devices, but can also be part of a live performance. 
Multimedia devices are electronic media devices used to store and experience multimedia content. 
Multimedia is distinguished from mixed media in fine art; by including audio, for example, it has a broader scope. 
The term "rich media" is synonymous for interactive multimedia. 
Hypermedia can be considered one particular multimedia application.
The term multimedia was coined by singer and artist Bob Goldstein (later 'Bobb Goldsteinn') to promote the July 1966 opening of his "LightWorks at L'Oursin" show at Southampton, Long Island. 
Goldstein was perhaps aware of an American artist named Dick Higgins, who had two years previously discussed a new approach to art-making he called "intermedia."
On August 10, 1966, Richard Albarino of Variety borrowed the terminology, reporting: "Brainchild of songscribe-comic Bob ('Washington Square') Goldstein, the 'Lightworks' is the latest multi-media music-cum-visuals to debut as discothèque fare."
Two years later, in 1968, the term "multimedia" was re-appropriated to describe the work of a political consultant, David Sawyer, the husband of Iris Sawyer—one of Goldstein’s producers at L’Oursin.
In the intervening forty years, the word has taken on different meanings. 
In the late 1970s, the term referred to presentations consisting of multi-projector slide shows timed to an audio track.
In the 1993 first edition of McGraw-Hill’s Multimedia: Making It Work, Tay Vaughan declared “Multimedia is any combination of text, graphic art, sound, animation, and video that is delivered by computer.
When you allow the user – the viewer of the project – to control what and when these elements are delivered, it is interactive multimedia. 
When you provide a structure of linked elements through which the user can navigate, interactive multimedia becomes hypermedia.” 
The German language society, Gesellschaft für deutsche Sprache, decided to recognize the word's significance and ubiquitousness in the 1990s by awarding it the title of 'Word of the Year' in 1995. 
The institute summed up its rationale by stating "[Multimedia] has become a central word in the wonderful new media world"
In common usage, multimedia refers to an electronically delivered combination of media including video, still images, audio, text in such a way that can be accessed interactively. 
Much of the content on the web today falls within this definition as understood by millions. 
Some computers which were marketed in the 1990s were called "multimedia" computers because they incorporated a CD-ROM drive, which allowed for the delivery of several hundred megabytes of video, picture, and audio data. 
That era saw also a boost in the production of educational multimedia CD-ROMs.
In Education, multimedia is used to produce computer-based training courses (popularly called CBTs) and reference books like encyclopedia and almanacs. 
A CBT lets the user go through a series of presentations, text about a particular topic, and associated illustrations in various information formats. 
Edutainment is the combination of education with entertainment, especially multimedia entertainment.
Newspaper companies all over are also trying to embrace the new phenomenon by implementing its practices in their work. 
While some have been slow to come around, other major newspapers like The New York Times, USA Today and The Washington Post are setting the precedent for the positioning of the newspaper industry in a globalized world.
Software engineers may use multimedia in Computer Simulations for anything from entertainment to training such as military or industrial training. 
Multimedia for software interfaces are often done as a collaboration between creative professionals and software engineers.
Multimedia represents the convergence of text, pictures, video and sound into a single form. 
The power of multimedia and the Internet lies in the way in which information is linked.
Multimedia and the Internet require a completely new approach to writing. 
The style of writing that is appropriate for the 'on-line world' is highly optimized and designed to be able to be quickly scanned by readers.
A good site must be made with a specific purpose in mind and a site with good interactivity and new technology can also be useful for attracting visitors. 
The site must be attractive and innovative in its design, function in terms of its purpose, easy to navigate, frequently updated and fast to download.
When users view a page, they can only view one page at a time. As a result, multimedia users must create a "mental model" of information structure.
Multimedia presentations may be viewed by person on stage, projected, transmitted, or played locally with a media player. 
A broadcast may be a live or recorded multimedia presentation. Broadcasts and recordings can be either analog or digital electronic media technology. 
Digital online multimedia may be downloaded or streamed. 
Streaming multimedia may be live or on-demand.
Multimedia games and simulations may be used in a physical environment with special effects, with multiple users in an online network, or locally with an offline computer, game system, or simulator.
The various formats of technological or digital multimedia may be intended to enhance the users' experience, for example to make it easier and faster to convey information. 
Or in entertainment or art, to transcend everyday experience.
Enhanced levels of interactivity are made possible by combining multiple forms of media content. 
Online multimedia is increasingly becoming object-oriented and data-driven, enabling applications with collaborative end-user innovation and personalization on multiple forms of content over time. 
Examples of these range from multiple forms of content on Web sites like photo galleries with both images (pictures) and title (text) user-updated, to simulations whose co-efficients, events, illustrations, animations or videos are modifiable, allowing the multimedia "experience" to be altered without reprogramming. 
In addition to seeing and hearing, Haptic technology enables virtual objects to be felt. 
Emerging technology involving illusions of taste and smell may also enhance the multimedia experience.
Digital video is a representation of moving visual images in the form of encoded digital data. 
This is in contrast to analog video, which represents moving visual images with analog signals.
They operated by taking a standard analog composite video input and digitizing it internally. 
This made it easier to either correct or enhance the video signal, as in the case of a TBC, or to manipulate and add effects to the video, in the case of a DVE unit. 
The digitized and processed video information that was output from these units would then be converted back to standard analog video.
Later on in the 1970s, manufacturers of professional video broadcast equipment, such as Bosch (through their Fernseh division), RCA, and Ampex developed prototype digital videotape recorders (VTR) in their research and development labs. 
Bosch's machine used a modified 1" Type B transport, and recorded an early form of CCIR 601 digital video. 
Ampex's prototype digital video recorder used a modified 2" Quadruplex VTR (an Ampex AVR-3), but fitted with custom digital video electronics, and a special "octaplex" 8-head headwheel (regular analog 2" Quad machines only used 4 heads). 
The audio on Ampex's prototype digital machine, nicknamed by its developers as "Annie", still recorded the audio in analog as linear tracks on the tape, like 2" Quad. None of these machines from these manufacturers were ever marketed commercially, however.
Digital video was first introduced commercially in 1986 with the Sony D1 format, which recorded an uncompressed standard definition component video signal in digital form instead of the high-band analog forms that had been commonplace until then. 
Due to its expense, and the requirement of component video connections using 3 cables (such as YPbPr or RGB component video) to and from a D1 VTR that most television facilities were not wired for (composite NTSC or PAL video using one cable was the norm for most of them at that time), D1 was used primarily by large television networks and other component-video capable video studios.
In 1988, Sony and Ampex co-developed and released the D2 digital videocassette format, which recorded video digitally without compression in ITU-601 format, much like D1. 
But D2 had the major difference of encoding the video in composite form to the NTSC standard, thereby only requiring single-cable composite video connections to and from a D2 VCR, making it a perfect fit for the majority of television facilities at the time. 
This made D2 quite a successful format in the television broadcast industry throughout the late '80s and the '90s. D2 was also widely used in that era as the master tape format for mastering laserdiscs (prior to D2, most laserdiscs were mastered using analog 1" Type C videotape).
D1 & D2 would eventually be replaced by cheaper systems using video compression, most notably Sony's Digital Betacam (still heavily used as an electronic field production (EFP) recording format by professional television producers) that were introduced into the network's television studios. 
Other examples of digital video formats utilizing compression were Ampex's DCT (the first to employ such when introduced in 1992), the industry-standard DV and MiniDV (and its professional variations, Sony's DVCAM and Panasonic's DVCPRO), and Betacam SX, a lower-cost variant of Digital Betacam using MPEG-2 compression.
One of the first digital video products to run on personal computers was PACo: The PICS Animation Compiler from The Company of Science & Art in Providence, RI, which was developed starting in 1990 and first shipped in May 1991.[1] PACo could stream unlimited-length video with synchronized sound from a single file (with the ".CAV" file extension) on CD-ROM. 
Creation required a Mac; playback was possible on Macs, PCs, and Sun Sparcstations. In 1992, Bernard Luskin, Philips Interactive Media, and Eric Doctorow, Paramount Worldwide Video, successfully put the first fifty videos in digital MPEG 1 on CD, developed the packaging and launched movies on CD, leading to advancing versions of MPEG, and to DVD.
QuickTime, Apple Computer's architecture for time-based and streaming data formats appeared in June, 1991. 
Initial consumer-level content creation tools were crude, requiring an analog video source to be digitized to a computer-readable format. 
The widespread adoption of digital video has also drastically reduced the bandwidth needed for a high-definition video signal (with HDV and AVCHD, as well as several commercial variants such as DVCPRO-HD, all using less bandwidth than a standard definition analog signal) and tapeless camcorders based on flash memory and often a variant of MPEG-4.
Standard film stocks such as 16 mm and 35 mm record at 24 frames per second. 
For video, there are two frame rate standards: NTSC, which shoot at 30/1.001 (about 29.97) frames per second or 59.94 fields per second, and PAL, 25 frames per second or 50 fields per second.
Digital video cameras come in two different image capture formats: interlaced and deinterlaced / progressive scan.
Interlaced cameras record the image in alternating sets of lines: the odd-numbered lines are scanned, and then the even-numbered lines are scanned, then the odd-numbered lines are scanned again, and so on. One set of odd or even lines is referred to as a "field", and a consecutive pairing of two fields of opposite parity is called a frame. 
Deinterlaced cameras records each frame as distinct, with all scan lines being captured at the same moment in time. 
Thus, interlaced video captures samples the scene motion twice as often as progressive video does, for the same number of frames per second. 
Progressive-scan camcorders generally produce a slightly sharper image. 
However, motion may not be as smooth as interlaced video which uses 50 or 59.94 fields per second, particularly if they employ the 24 frames per second standard of film.
Digital video can be copied with no degradation in quality. 
No matter how many generations of a digital source is copied, it will still be as clear as the original first generation of digital footage. 
However a change in parameters like frame size as well as a change of the digital format can decrease the quality of the video due to new calculations that have to be made. 
Digital video can be manipulated and edited to follow an order or sequence on an NLE, or non-linear editing workstation, a computer-based device intended to edit video and audio. 
More and more, videos are edited on readily available, increasingly affordable consumer-grade computer hardware and software. 
However, such editing systems require ample disk space for video footage. 
The many video formats and parameters to be set make it quite impossible to come up with a specific number for how many minutes need how much time.
Digital video has a significantly lower cost than 35 mm film. 
The tape stock itself is very inexpensive. 
Digital video also allows footage to be viewed on location without the expensive chemical processing required by film. 
Also physical deliveries of tapes and broadcasts do not apply anymore. 
Digital television (including higher quality HDTV) started to spread in most developed countries in early 2000s. 
Digital video is also used in modern mobile phones and video conferencing systems. 
Digital video is also used for Internet distribution of media, including streaming video and peer-to-peer movie distribution. 
However even within Europe are lots of TV-Stations not broadcasting in HD, due to restricted budgets for new equipment for processing HD.
Many types of video compression exist for serving digital video over the internet and on optical disks.
Digital video comprises a series of orthogonal bitmap digital images displayed in rapid succession at a constant rate.
In the context of video these images are called frames.[2] We measure the rate at which frames are displayed in frames per second (FPS).
Since every frame is an orthogonal bitmap digital image it comprises a raster of pixels. If it has a width of W pixels and a height of H pixels we say that the frame size is WxH.
Pixels have only one property, their color. 
The color of a pixel is represented by a fixed number of bits. 
The more bits the more subtle variations of colors can be reproduced. 
This is called the color depth (CD) of the video.
Interactive media normally refers to products and services on digital computer-based systems which respond to the user’s actions by presenting content such as text, graphics, animation, video, audio, games, etc.
Interactive media is a method of communication in which the output from the media comes from the input of the users. 
The interactive media lets the user go back with the media.
Interactive media works with the user's participation. 
The media still has the same purpose but the user's input adds the interaction and brings interesting features to the system for a better enjoyment.
The analogue videodisc developed by NV Philips was the pioneering technology for interactive media. 
Additionally, there are several elements that encouraged the development of interactive media including the following:
The laser disc technology was first invented in 1958. 
It enabled the user to access high-quality analogue images on the computer screen. 
This increased the ability of interactive video systems.
The concept of the graphical user interface (GUI), which was developed in the 1970s, popularized by Apple Computer, Inc. was essentially about visual metaphors, intuitive feel and sharing information on the virtual desktop. 
Additional power was the only thing needed to move into multimedia.
The sharp fall in hardware costs and the unprecedented rise in the computer speed and memory transformed the personal computer into an affordable machine capable of combining audio and color video in advanced ways.
Another element is the release of Windows 3.0 in 1990 by Microsoft into the mainstream IBM clone world. 
It accelerated the acceptance of GUI as the standard mechanism for communicating with small computer systems.
The development by NV Philips of optical digital technologies built around the compact disk (CD) in 1979 is also another leading element in the interactive media development as it raised the issue of developing interactive media.
All of the prior elements contributed in the development of the main hardware and software systems used in interactive media.
Though the word media is plural, the term is often used as a singular noun.
Interactive media is related to the concepts interaction design, new media, interactivity, human computer interaction, cyberculture, digital culture, and includes augmented reality.
An essential feature of interactivity is that it is mutual: user and machine each take an active role .
Most interactive computing systems are for some human purpose and interact with humans in human contexts.
Manovich complains that ‘In relation to computer-based media, the concept of interactivity is a tautology. .... 
Therefore, to call computer media “interactive” is meaningless – it simply means stating the most basic fact about computers.’ 
Nevertheless [the term is useful to denote an identifiable body of practices and technologies.
Interactive media are an instance of a computational method influenced by the sciences of cybernetics, autopoiesis and system theories, and challenging notions of reason and cognition, perception and memory, emotions and affection.
Any form of interface between the end user/audience and the medium may be considered interactive. 
Interactive media is not limited to electronic media or digital media. Board games, pop-up books, gamebooks, flip books and constellation wheels are all examples of printed interactive media. 
Books with a simple table of contents or index may be considered interactive due to the non-linear control mechanism in the medium, but are usually considered non-interactive since the majority of the user experience is non-interactive reading.
Interactive media is helpful in these four development dimensions in which young children learn: social and emotional, language development, cognitive and general knowledge, and approaches toward learning. 
Using computers and educational computer software in a learning environment helps children increase communication skills and their attitudes about learning. 
Children who use educational computer software are often found using more complex speech patterns and higher levels of verbal communication. 
A study found that basic interactive books that simply read a story aloud and highlighted words and phrases as they were spoken were beneficial for children with lower reading abilities. 
Children have different styles of learning, and interactive media helps children with visual, verbal, auditory, and tactile learning styles.
Interactive media can be implemented in a wide variety of platforms and applications encompassing virtually all areas of technology. 
Some examples include mobile platforms such as touch screen smartphones and tablets, was well as other interactive mediums that are created exclusively to solve a unique problem or set of problems. 
Interactive media is not limited to a certain field of IT, it instead encompasses any technology that supplies for movie parts or feedback based on the users actions. 
This can include javascript and AJAX utilization in web pages, but can further be extended to any programming languages that share the same or similar functionality. 
One of the most recent innovations to supply for interactivity to solve a problem the plagues individuals on a daily bases is Delta Airlines “Photon Shower.” 
This device was developed from Delta’s collaboration with Professor Russell Foster of Cambridge University. 
The device is designed to reduce the effect of jet lag on customers that often take long flights across time zones. 
The systems interactivity is evident because of the way in which it solves this commonplace problem..
By observing what time zones a person has crossed and matching those to the basic known sleep cycles of the individual, the machine is able to predict when a persons body is expecting light, and when it is expecting darkness. 
It then bombards the individual with the appropriate light source variations for the time, as well as an instructional card to inform them of what times their body expects light and what times it expects darkness.
Growth of interactive media continues to advance today, with the advent of more and more powerful machines the limit to what can be input and manipulated on a display in real time is become virtually non-existent.
The media which allows several geographically remote users to interact synchronously with the media application/system is known as Distributed Interactive Media. 
Some common examples of this type of Media include Online Gaming, Distributed Virtual Environment, Whiteboards which are used for interactive conferences and many more.
Interactive media makes technology more intuitive to use. 
Interactive products such as smartphones, iPad's/iPod's, interactive whiteboards and websites are all easy to use. 
The easy usage of these products encourages consumers to experiment with their products rather than reading instruction manuals.
Interactive media promotes dialogic communication. 
This form of communication allows senders and receivers to build long term trust and cooperation. 
This plays a critical role in building relationships. 
Organizations also use interactive media to go further than basic marketing and develop more positive behavioral relationships.
Virtual Reality (VR), which can be referred to as immersive multimedia or computer-simulated life, replicates an environment that simulates physical presence in places in the real world or imagined worlds. 
Virtual reality can recreate sensory experiences, which include virtual taste, sight, smell, sound, and touch.
Most up to date virtual reality environments are displayed either on a computer screen or with special stereoscopic displays, and some simulations include additional sensory information and emphasise real sound through speakers or headphones targeted towards VR users. 
Some advanced, haptic, systems now include tactile information, generally known as force feedback in medical, gaming and military applications. 
Furthermore, virtual reality covers remote communication environments which provide virtual presence of users with the concepts of telepresence and telexistence or a virtual artifact (VA) either through the use of standard input devices such as a keyboard and mouse, or through multimodal devices such as a wired glove or omnidirectional treadmills. 
The simulated environment can be similar to the real world in order to create a lifelike experience—for example, in simulations for pilot or combat training—or it differs significantly from reality, such as in VR games. 
In practice, it is currently very difficult to create a high-fidelity virtual reality experience, because of technical limitations on processing power, image resolution, and communication bandwidth. 
However, VR's proponents hope that virtual reality's enabling technologies become more powerful and cost effective over time.
Virtual reality is often used to describe a wide variety of applications commonly associated with immersive, highly visual, 3D environments. 
The development of CAD software, graphics hardware acceleration, head-mounted displays, datagloves, and miniaturization have helped popularize the notion. 
In the book The Metaphysics of Virtual Reality by Michael R. Heim, seven different concepts of virtual reality are identified: simulation, interaction, artificiality, immersion, telepresence, full-body immersion, and network communication. People often identify VR with head mounted displays and data suits.
The term "artificial reality", coined by Myron Krueger, has been in use since the 1970s; however, the origin of the term "virtual reality" can be traced back to the French playwright, poet, actor, and director Antonin Artaud. 
In his seminal book The Theatre and Its Double (1938), Artaud described theatre as "la réalité virtuelle", a virtual reality in which, in Erik Davis's words, "characters, objects, and images take on the phantasmagoric force of alchemy's visionary internal dramas".
Artaud claimed that the "perpetual allusion to the materials and the principle of the theater found in almost all alchemical books should be understood as the expression of an identity existing between the world in which the characters, images, and in a general way all that constitutes the virtual reality of the theater develops, and the purely fictitious and illusory world in which the symbols of alchemy are evolved".
The term was also used in The Judas Mandala, a 1982 science-fiction novel by Damien Broderick, where the context of use is somewhat different from that defined above. The earliest use cited by the Oxford English Dictionary is in a 1987 article titled "Virtual reality",[3] but the article is not about VR technology. 
The concept of virtual reality was popularized in mass media by movies such as Brainstorm and The Lawnmower Man. 
The VR research boom of the 1990s was accompanied by the non-fiction book Virtual Reality (1991) by Howard Rheingold.
The book served to demystify the subject, making it more accessible to less technical researchers and enthusiasts.
Multimedia: from Wagner to Virtual Reality, edited by Randall Packer and Ken Jordan and first published in 2001, explores the term and its history from an avant-garde perspective. 
Philosophical implications of the concept of VR are discussed in books including Philip Zhai's Get Real: A Philosophical Adventure in Virtual Reality (1998) and Digital Sensations: Space, Identity and Embodiment in Virtual Reality (1999), written by Ken Hillis.
There has been an increase in interest in the potential social impact of new technologies, such as virtual reality. 
In the book Infinite Reality: Avatars, Eternal Life, New Worlds, and the Dawn of the Virtual Revolution, Blascovich and Bailenson review the literature on the psychology and sociology behind life in virtual reality.
In addition, Mychilo S. Cline, in his book Power, Madness, and Immortality: The Future of Virtual Reality, argues that virtual reality will lead to a number of important changes in human life and activity.
He argues that virtual reality will be integrated into daily life and activity, and will be used in various human ways. 
Another such speculation has been written up on how to reach ultimate happiness via virtual reality.
He also argues that techniques will be developed to influence human behavior, interpersonal communication, and cognition.
As we spend more and more time in virtual space, there would be a gradual "migration to virtual space", resulting in important changes in economics, worldview, and culture.
The first use of a VR presentation in a heritage application was in 1994, when a museum visitor interpretation provided an interactive "walk-through" of a 3D reconstruction of Dudley Castle in England as it was in 1550. This consisted of a computer controlled laserdisc-based system designed by British-based engineer Colin Johnson. 
The system was featured in a conference held by the British Museum in November 1994, and in the subsequent technical paper, Imaging the Past - Electronic Imaging and Computer Graphics in Museums and Archaeology.
Virtual reality enables heritage sites to be recreated extremely accurately, so that the recreations can be published in various media.
The original sites are often inaccessible to the public, or may even no longer exist.
This technology can be used to develop virtual replicas of caves, natural environment, old towns, monuments, sculptures and archaeological elements.
Strides are being made in the realm of education, although much needs to be done. 
The possibilities of VR and education are endless and bring many advantages to pupils of all ages.
Few are creating content that may be used for educational [24] purposes, with most advances being done in the entertainment industry, but many understand and realize the future and the importance of education and VR.
Many science fiction books and films have imagined characters being "trapped in virtual reality".
A comprehensive and specific fictional model for virtual reality was published in 1935 in the short story Pygmalion's Spectacles  by Stanley G. Weinbaum. 
A more modern work to use this idea was Daniel F. Galouye's novel Simulacron-3, which was made into a German teleplay titled Welt am Draht ("World on a Wire") in 1973. 
Other science fiction books have promoted the idea of virtual reality as a partial, but not total, substitution for the misery of reality, or have touted it as a method for creating virtual worlds in which one may escape from Earth.
Stanislaw Lem's 1961 story "I (Profesor Corcoran)", translated in English as "Further Reminiscences of Ijon Tichy I",dealt with a scientist who created a number of computer-simulated people living in a virtual world. Lem further explored the implications of what he termed "phantomatics" in his nonfictional 1964 treatise Summa Technologiae. 
The Piers Anthony novel Killobyte follows the story of a paralyzed cop trapped in a virtual reality game by a hacker, whom he must stop to save a fellow trapped player slowly succumbing to insulin shock.
Other popular fictional works that use the concept of virtual reality include William Gibson's Neuromancer which defined the concept of cyberspace, Neal Stephenson's Snow Crash, in which he made extensive reference to the term avatar to describe one's representation in a virtual world, and Rudy Rucker's The Hacker and the Ants, in which programmer Jerzy Rugby uses VR for robot design and testing. 
The Otherland series of 4 novels by Tad Williams, published from 1996 to 2001 and set in the 2070s, shows a world where the Internet has become accessible via virtual reality.
The Doctor Who serial "The Deadly Assassin", first broadcast in 1976, introduced a dream-like computer-generated reality, known as the Matrix. 
British BBC2 sci-fi series Red Dwarf featured a virtual reality game titled "Better Than Life", in which the main characters had spent many years connected. Saban's syndicated superhero television series VR Troopers also made use of the concept.
The popular .hack multimedia franchise is based on a virtual reality MMORPG dubbed "The World" The French animated series Code Lyoko is based on the virtual world of Lyoko and the Internet.
An anime called Sword Art Online involves the concept of virtual reality, and the possibility of dying in real life when a player dies in the game. 
Also, in Sword Art Online II, they pose the idea of bringing a virtual character into the real world via mobile cameras. 
They use this concept to allow a bedridden individual to attend public school for the first time.
Artmedia, Seminar and Laboratory of the Aesthetics of Media and Communication, was one of the first scientific projects concerning the relationship between art, technology, philosophy and aesthetics. 
It was founded in 1985 at the University of Salerno.
For over two decades, until 2009, dozens of projects, studies, exhibitions and conferences on new technologies made Artmedia a reference point for many internationally renowned scholars and artists,[2] and contributed to the growing cultural interest in the aesthetics of media, the aesthetics of networks, and their ethical and anthropological implications. 
Since the late 1970s, a permanent Seminar of the Aesthetics of Media and Communication has been directed by its founder Mario Costa at the University of Salerno. 
The basic principles of the aesthetics of technological communication were identified and conceptualized in 1983.[3] A conference on "Technological Imaginary", held in 1984 at the Museo del Sannio in Benevento, discussed the issue of the new relationship between art and technology and the consequent need to re-evaluate aesthetics, warning that "all our future existence will be played at the crossroads between technology and imaginary".
The comprehensive relationship between art and technoscience, technology, and philosophy has also been the theoretical subject of the ten international "Artmedia" conferences which were held in Salerno and Paris between 1985 and 2008. 
Particularly relevant were conferences held in Paris between 2002 and 2008, which took place at the Ecole Normale Supérieure, the Bibliotheque Nationale de France (BNF) and the Institut National d'Histoire de l 'Art (INHA), with the partnership of the Société Française d'Esthétique, the Université du Québec à Montréal, the University of Toronto, the Universidade de São Paulo, the Université de Paris 1 Sorbonne, and the U.S. magazine Leonardo.
Artmedia wanted to gather theorists and artists from all over the world and encourage both joint and complementary work, beginning with the need to give attention to theoretical and artistic practices and developing both together.
These also contributed towards spreading the spirit of the project, both in a number of festivals and shows, and through their own artworks and research.
Artists included Fred Forest, Roy Ascott, Takahiko Iimura, Maurizio Bolognini, Tom Klinkowstein, Tom Sherman, Eduardo Kac, Enzo Minarelli, James Dashow, Peter D'Agostino, Mit Mitropoulos, Shawn Brixey, Bruno Di Bello, Antoni Muntadas, Orlan, Kit Galloway, David Rokeby, Miguel Chevalier, Norman White, Richard Kriesche, Olivier Auber, Caterina Davinio and Casey Reas.
The future of meaning (2008).
All Artmedia symposiums have been followed by many publications.
For the two held in Paris, a complete video recording is also available, and can be viewed at the Institut National de l'Audiovisuel.
An assessment of 25 years of Artmedia activity was made into a seminar on The aesthetic object of the future, held at the University of Salerno in 2009. 
The Artmedia project produced a large number of publications and documents that are being catalogued, with a view to their proper placement and use.
Multi-image is the now largely obsolete practice and business of using 35 mm slides (diapositives) projected by single or multiple slide projectors onto one or more screens in synchronization with an audio voice-over or music track. 
Multi-image productions are also known as multi-image slide presentations, slide shows and diaporamas and are a specific form of multimedia or audio-visual production.
One of the hallmarks of multi-image was the use of the wide screen panorama. 
Precisely overlapping slides were placed in slide mounts with soft-edge density masks; when the resulting images were projected, the images would blend seamlessly on the screen to create the panorama. 
By cutting and dissolving between images in the projectors, animation effects were created in the panorama format.
The term multi-image is sometimes used to describe digital photo image computer programs that combine or change images on-screen, for photo montages, and image stitching.
Multi-image presentations were a unique form of communication to audiences of various sizes, to meet a variety of communication and entertainment needs. 
The use of projected photographic images such as lantern slides for entertainment and instruction dates to the early 1800s.[2] Others, such as L. Frank Baum had a traveling show (1908) that included slides, film, and actors describing the land of Oz. 
Throughout the years improvements in technology took place and applications for multi-image continued to expand. 
During the 1960s, automated synchronized audio and slides modules became more common and found use in instructional environments.
Based largely on analog production tools and technologies including art and audio production and film-based photography. 
35mm slide film has high resolution and color range and are based on grain and dye clouds rather than on a fixed raster pattern, which when projected often is perceived[3] as being more realistic, uniform, and detailed than digital images.
Multi-disciplinary in terms of the types of skills required to create and stage multi-image presentations.
Venue driven; multi-image presentations had specific requirements for the equipment and spaces they were intended for and presented in.
Multi-image as a business thrived during the 1970s and 1980s. Multi-image presentations ranged from single projector shows run by projector-viewer[4][5] to large events for business meetings and conventions where multiple shows would be presented and often were rear-projected by 24 or more projectors.
Creating and presenting multi-image productions involved a relatively large number of specialized skills, equipment, and facilities to produce. 
During the height of multi-image, a number of types of businesses were directly engaged in the industry which employed thousands of specialists that ranged from producers and designers, writers, artists, typesetters, photographers, photo lab technicians, audio technicians, programmers, staging specialists as well as others associated with these disciplines.
Promotional slide for the AMI Gold Tour, 1983.
AMI Gold Tour promotion.
A professional organization, the Association for Multi-image International (AMI),[7] was created and had numerous active chapters around the world. 
The AMI held an annual convention and multi-image competition. Local chapters of AMI in various cities also held regional competitions.
An entire industry grew around supplying the tools and equipment needed to supply and support multi-image production.
Corporations who manufactured and sold basic equipment and materials used in the production of multi-image, such as 35mm slide projectors, film, slide mounts, soft-edge masks and other items. 
The list of suppliers for the multi-image industry was extensive with a number of companies attaining international importance for the products they produced.
Manufacturers of highly specialized multi-image equipment such as optical slide cameras and slide projector programming hardware. 
Precise camera systems were developed for the multi-image market featuring pin-registered film movements capable of making multiple exposures, controlled backlit color light sources, motorized multi-axis compounds for precise positioning of artwork, and long-roll film loads. 
Slide projector programming computers and dissolve equipment and software was developed to synchronize the slides with the audio by providing precise time control over slide tray positioning and lamp fade rates needed to create multi-image animation. Programmer and camera manufacturers included Audio Visual Labs.
Driven by changes in technology and by economic considerations, multi-image has almost entirely been replaced by video presentations and by readily available computer based technologies such as laptop computers running PowerPoint and projecting through digital projectors. 
Visual presentation and photo and graphics editing software programs have allowed a wider range of communicators quick, flexible, and easy access to the tools and technologies needed to create presentations. 
Digital photography has reduced the need for laboratory services and complex equipment. 
The expansion and ease of use of desktop computing brought a close to the multi-image industry.
The audio track of a multi-image show provided a framework for the timing of the presentation and for the sequencing and animations of the slides. 
These were produced generally on 1/4-inch audio tape on multi-track tape recorders such as models by Tascam, TEAC, Sony, Fostex and Crown, which allowed for having two tracks or channels for stereo sound and one for the synchronization or click track which was used to encode and playback the signals for the dissolve units. 
The audio and synchronization tracks were normally separated by a blank track to prevent any carryover of the synchronization cues into the audio playback. 
Audio editing of the music or voice-over was done manually to create a scratch track, usually with a cutting block and tape.[20] Once the audio edits were completed, the final version would be copied onto another tape; either to 1/4 inch, cassette or other format so that there tape used to run the presentation would be a fresh uncut tape.
As productions became more sophisticated, 16 and 24-track recording processes were used to create elaborate soundtracks and 4-channel surround sound for large business theatre environments. 
In productions such as the Maritz-produced car announcement shows for GM and Ford Motor Company, 16-track recorders were used for playback onsite.
These 2-inch showtapes would contain extra tracks to support the vocals for a live cast onstage, as well as additional string-section support for the live orchestra and a click track to cue the conductor, in order to maintain synchronization between the cast, orchestra and on-screen visuals. 
Completed slides were mounted into pin-registered slide mounts. 
Three or more pieces of film could be mounted into a slide mount which allowed slides to contain the image film chip and masks to allow for inserting and over-projecting.
Slides were edited and arranged for programming on light tables.
Programming of multi-projector multi-image shows was generally done on one of several systems such as Arion, Audio-Visual Laboratories (AVL).
Slides were placed in projector trays, projectors were set up on a "grid" so that alignment was made of the projectors onto the screen area. The programming itself was done on a system that allowed for input by the programmer and dissolve units which were attached to the projectors and controlled the functions of the projectors based on the programming instructions. 
Programming could also be used to control room lights, rewind of the audio tape and resetting of the projectors, and to trigger other effects that might be used in the presentation, such as a strobe light.
There were two basic slide projector programming controls: a set of instructions to position the slide in the projector and a set of instructions for the slide projector lamp. These controls would be used to define the cues. 
The cues would often designate an action for more than one projector such as with a dissolve between two slides which would require simultaneously fading up on one lamp and fading down another lamp. 
Similar commands were used to control motion-picture projectors, as well as auxiliary controls for lighting and effects, etc.
As multi-image programming devices progressed to digital computers and became more sophisticated in the late 1970s, more programming features were added. 
Complex looping effects, independent cycling allowing background animation over foreground effects, comprehensive control of motion picture projectors, control of (and by) video devices and other peripheral devices, and the use of SMPTE timecode for synchronization became commonplace. 
Multiply-exposed optical effects and the use of computer-generated imagery allowed the medium to emerge, briefly, as an art form. 
The use of multitrack audio playback enhanced the experience and provided for surround sound.
Multimedia Messaging Service (MMS) is a standard way to send messages that include multimedia content to and from mobile phones. 
It extends the core SMS (Short Message Service) capability that allowed exchange of text messages only up to 160 characters in length.
The most popular use is to send photographs from camera-equipped handsets. 
It is also used on a commercial basis by media companies as a method of delivering news and entertainment content and by retail brands as a tool for delivering scannable coupon codes, product images, videos and other information. 
Unlike text only SMS, commercial MMS can deliver a variety of media including up to forty seconds of video, one image, multiple images via slideshow or audio plus unlimited characters.
The standard is developed by the Open Mobile Alliance (OMA), although during development it was part of the 3GPP and WAP groups.
Multimedia messaging services were first developed as a captive technology that would enable service providers to "collect a fee every time anyone snaps a photo."
Early MMS deployments were plagued by technical issues and frequent consumer disappointments, but in recent years MMS deployment by major technology companies have solved many of the early challenges through handset detection, content optimization, increased throughput, etc.
China was one of the early markets to make MMS a major commercial success partly as the penetration rate of personal computers was modest but MMS-capable cameraphones spread rapidly. 
The chairman and CEO of China Mobile said at the GSM Association Mobile Asia Congress in 2009 that MMS in China is now a mature service on par with SMS text messaging.
Europe's most advanced MMS market has been Norway and in 2008 the Norwegian MMS usage level had passed 84% of all mobile phone subscribers.
 Norwegian mobile subscribers average one MMS sent per week.
Between 2010 and 2013, MMS traffic in the U.S. increased by 70% from 57 Billion to 96 Billion messages sent. 
One of the main reason behind increase in MMS traffic is decrease of usage of proprietary Mobile Operating Systems, as they had different implementations for enconding and MMS message handling. 
As most of the smartphones are running Android, MMS encoding and implementation gets generalized hence usage of MMS has been increased substantially.
MMS messages are delivered in a completely different way from SMS. 
The first step is for the sending device to encode the multimedia content in a fashion similar to sending a MIME e-mail (MIME content formats are defined in the MMS Message Encapsulation specification). 
The message is then forwarded to the carrier's MMS store and forward server, known as the MMSC (Multimedia Messaging Service Centre). 
If the receiver is on another carrier, then the MMSC [acts as a relay, and] forwards the message to the MMSC of the recipient's carrier using the Internet.[3]
Once the recipient's MMSC has received a message, it first determines whether the receiver's handset is "MMS capable", that it supports the standards for receiving MMS. If so, the content is extracted and sent to a temporary storage server with an HTTP front-end. 
An SMS "control message" containing the URL of the content is then sent to the recipient's handset to trigger the receiver's WAP browser to open and receive the content from the embedded URL. Several other messages are exchanged to indicate status of the delivery attempt.
Before delivering content, some MMSCs also include a conversion service that will attempt to modify the multimedia content into a format suitable for the receiver. 
This is known as "content adaptation".
If the receiver's handset is not MMS capable, the message is usually delivered to a web based service from where the content can be viewed from a normal internet browser. The URL for the content is usually sent to the receiver's phone in a normal text message. 
This behaviour is usually known as the "legacy experience" since content can still be received by a phone number, even if the phone itself does not support MMS.
The method for determining whether a handset is MMS capable is not specified by the standards. 
A database is usually maintained by the operator, and in it each mobile phone number is marked as being associated with a legacy handset or not. 
This method is unreliable, however, because customers can change their handset at will, and many of these databases are not updated dynamically.
MMS does not utilize one's own operator maintained data plan to distribute multimedia content. 
Operator maintained data plans are only used when message included links (if any) are explicitly clicked.
E-mail and web-based gateways to the MMS (and SMS) system are common. 
On the reception side, the content servers can typically receive service requests both from WAP and normal HTTP browsers, so delivery via the web is simple. 
For sending from external sources to handsets, most carriers allow MIME encoded message to be sent to the receiver's phone number with a special domain. 
An example of this would be PTN@messaging.carrier.com, where PTN is the public telephone number. 
Typically the special domain name is carrier specific.
Content adaptation-Multimedia content created by one brand of MMS phone may not be entirely compatible with the capabilities of the recipient's MMS phone. 
In the MMS architecture, the recipient MMSC is responsible for providing for content adaptation (e.g., image resizing, audio codec transcoding, etc.), if this feature is enabled by the mobile network operator. 
When content adaptation is supported by a network operator, its MMS subscribers enjoy compatibility with a larger network of MMS users than would otherwise be available.
The flow of peer-to-peer MMS messaging involves several over-the-air transactions that become inefficient when MMS is used to send messages to large numbers of subscribers, as is typically the case for VASPs. 
For example, when one MMS message is submitted to a very large number of recipients, it is possible to receive a delivery report and read-reply report for each and every recipient. 
Future MMS specification work is likely to optimize and reduce the transactional overhead for the bulk-messaging case.
Handset Configuration: Unlike SMS, MMS requires a number of handset parameters to be set. 
Poor handset configuration is often blamed as the first point of failure for many users. 
Service settings are sometimes preconfigured on the handset, but mobile operators are now looking at new device management technologies as a means of delivering the necessary settings for data services (MMS, WAP, etc.) via over-the-air programming (OTA).
Few mobile network operators offer direct connectivity to their MMSCs for content providers.
This has resulted in many content providers using WAP push as the only method available to deliver 'rich content' to mobile handsets. 
WAP push enables 'rich content' to be delivered to a handset by specifying the URL (via binary SMS) of a pre-compiled MMS, hosted on a content provider's web server. 
A consequence is that the receiver who pays WAP per kb or minute (as opposed to a flat monthly fee) pays for receiving the MMS, as opposed to only paying for sending one, and also paying a different rate.
Multimedia applications can include many types of media. 
The primary characteristic of a multimedia system is the use of more than one kind of media to deliver content and functionality. 
Web and desktop computing programs can both involve multimedia components. 
As well as different media items, a multimedia application will normally involve programming code and enhanced user interaction. Multimedia items generally fall into one of five main categories and use varied techniques for digital formatting.
It may be an easy content type to forget when considering multimedia systems, but text content is by far the most common media type in computing applications. 
Most multimedia systems use a combination of text and other media to deliver functionality. 
Text in multimedia systems can express specific information, or it can act as reinforcement for information contained in other media items. 
This is a common practice in applications with accessibility requirements. 
For example, when Web pages include image elements, they can also include a short amount of text for the user's browser to include as an alternative, in case the digital image item is not available
Digital image files appear in many multimedia applications. 
Digital photographs can display application content or can alternatively form part of a user interface. 
Interactive elements, such as buttons, often use custom images created by the designers and developers involved in an application. 
Digital image files use a variety of formats and file extensions. 
Among the most common are JPEGs and PNGs. 
Both of these often appear on websites, as the formats allow developers to minimize on file size while maximizing on picture quality. Graphic design software programs such as Photoshop and .NET allow developers to create complex visual effects with digital images.
Audio files and streams play a major role in some multimedia systems.
Audio files appear as part of application content and also to aid interaction. 
When they appear within Web applications and sites, audio files sometimes need to be deployed using plug-in media players. 
Audio formats include MP3, WMA, Wave, MIDI and RealAudio. When developers include audio within a website, they will generally use a compressed format to minimize on download times. 
Web services can also stream audio, so that users can begin playback before the entire file is downloaded.
Digital video appears in many multimedia applications, particularly on the Web. 
As with audio, websites can stream digital video to increase the speed and availability of playback. 
Common digital video formats include Flash, MPEG, AVI, WMV and QuickTime. 
Most digital video requires use of browser plug-ins to play within Web pages.
But in many cases the user's browser will already have the required resources installed.
Animated components are common within both Web and desktop multimedia applications. 
Animations can also include interactive effects, allowing users to engage with the animation action using their mouse and keyboard. 
The most common tool for creating animations on the Web is Adobe Flash, which also facilitates desktop applications. 
Using Flash, developers can author FLV files, exporting them as SWF movies for deployment to users. 
Flash also uses ActionScript code to achieve animated and interactive effects.
Multimedia is the encompass of all media used in electronics, particularly with computers. 
The use of computers to present text, graphics, video, animation, and sound in an integrated way. 
Long touted as the future revolution in computing, multimedia applications were, until the mid-90s, uncommon due to the expensive hardware required. 
With increases in performance and decreases in price, however, multimedia is now commonplace.
Nearly all Personal Computers are capable of displaying video, though the resolution available depends on the power of the computerâ€™s video adapter and microprocessor.
Interactive Multimedia is the means to interface with these media typically with a computer keyboard, mouse, touch screen, on screen buttons, and text entry allowing a user to make decisions as to what takes place next.
Animation is the process of creating motion and shape change illusion by means of the rapid display of a sequence of static images that minimally differ from each other. 
The illusionâ€”as in motion pictures in generalâ€”is thought to rely on the phi phenomenon. 
Animators are artists who specialize in the creation of animation.
Animations can be recorded on either analogue media, such as a flip book, motion picture film, video tape, or on digital media, including formats such as animated GIF, Flash animation or digital video. To display animation, a digital camera, computer, or projector are used along with new technologies that are produced.
Animation creation methods include the traditional animation creation method and those involving stop motion animation of two and three-dimensional objects, such as paper cutouts, puppets and clay figures. 
Images are displayed in a rapid succession, usually 24, 25, 30, or 60 frames per second.
Early examples of attempts to capture the phenomenon of motion into a still drawing can be found in paleolithic cave paintings, where animals are often depicted with multiple legs in superimposed positions, clearly attempting to convey the perception of motion.
An earthen goblet discovered at the site of the 5,200-year-old Burnt City in southeastern Iran, depicts what could possibly be the worldâ€™s oldest example of animation. 
The artifact bears five sequential images depicting a Persian Desert Ibex jumping up to eat the leaves of a tree.
Ancient Chinese records contain several mentions of devices that were said to "give an impression of movement" to human or animal figures.
But these accounts are unclear and may only refer to the actual movement of the figures through space.
In the 19th century, the phenakistoscope (1832), zoetrope (1834) and praxinoscope (1877), as well as the common flip book, were early animation devices that produced an illusion of movement from a series of sequential drawings.
But animation did not develop further until the advent of motion picture film and cinematography in the 1890s.
The cinÃ©matographe was a projector, printer, and camera in one machine that allowed moving pictures to be shown successfully on a screen which was invented by history's earliest film makers, Auguste and Louis LumiÃ¨re, in 1894.
The first animated projection (screening) was created in France, by Charles-Ã‰mile Reynaud, who was a French science teacher. 
Reynaud created the Praxinoscope in 1877 and the ThÃ©Ã¢tre Optique in December 1888. 
On 28 October 1892, he projected the first animation in public, Pauvre Pierrot, at the MusÃ©e GrÃ©vin in Paris. 
This film is also notable as the first known instance of film perforations L used. 
His films were not photographed, but drawn directly onto the transparent strip. 
In 1900, more than 500,000 people had attended these screenings.
The first film that was recorded on standard picture film and included animated sequences was the 1900 Enchanted Drawing,which was followed by the first entirely animated film - the 1906 Humorous Phases of Funny Faces by J. Stuart Blackton, who, because of that, is considered the father of American animation.
In Europe, the French artist, Ã‰mile Cohl, created the first animated film using what came to be known as traditional animation creation methods - the 1908 Fantasmagorie.
The film largely consisted of a stick figure moving about and encountering all manner of morphing objects, such as a wine bottle that transforms into a flower. 
There were also sections of live action in which the animatorâ€™s hands would enter the scene. 
The film was created by drawing each frame on paper and then shooting each frame onto negative film, which gave the picture a blackboard look.
The author of the first puppet-animated film (The Beautiful Lukanida (1912)) was the Russian-born (ethnically Polish) director Wladyslaw Starewicz, known as Ladislas Starevich.
The more detailed hand-drawn animations, requiring a team of animators drawing each frame manually with detailed backgrounds and characters, were those directed by Winsor McCay, a successful newspaper cartoonist, including the 1911 Little Nemo, the 1914 Gertie the Dinosaur, and the 1918 The Sinking of the Lusitania
Video is an electronic medium for the recording, copying, playback, broadcasting, and display of moving visual media.
Video technology was first[citation needed] developed for cathode ray tube (CRT) television systems, but several new technologies for video display devices have since been invented. 
Charles Ginsburg led an Ampex research team developing one of the first practical video tape recorder (VTR).
In 1951 the first video tape recorder captured live images from television cameras by converting the camera's electrical impulses and saving the information onto magnetic video tape.
Video recorders were sold for $50,000 in 1956, and videotapes cost $300 per one-hour reel.
However, prices gradually dropped over the years; in 1971, Sony began selling videocassette recorder (VCR) decks and tapes to the public.
The use of digital techniques in video created digital video, which allowed higher quality and, eventually, much lower cost than earlier analog technology. 
After the invention of the DVD in 1997 and Blu-ray Disc in 2006, sales of videotape and recording equipment plummeted. 
Advances in computer technology allowed even inexpensive personal computers to capture, store, edit and transmit digital video, further reducing the cost of video production, allowing program-makers and broadcasters to move to tapeless production. 
The advent of digital broadcasting and the subsequent process of digital television transition is in the process of relegating analog video to the status of a legacy technology in most parts of the world. 
As of 2015, with the increasing use of high-resolution video cameras with improved dynamic range and color gamuts, and high-dynamic-range digital intermediate data formats with improved color depth, modern digital video technology is slowly converging with digital film technology.
Frame rate, the number of still pictures per unit of time of video, ranges from six or eight frames per second (frame/s) for old mechanical cameras to 120 or more frames per second for new professional cameras. PAL standards  and SECAM  specify 25 frame/s, while NTSC standards (USA, Canada, Japan, etc.) specify 29.97 frames.
 Film is shot at the slower frame rate of 24 frames per second, which slightly complicates the process of transferring a cinematic motion picture to video. 
The minimum frame rate to achieve a comfortable illusion of a moving image is about sixteen frames per second.
Video can be interlaced or progressive. 
Interlacing was invented as a way to reduce flicker in early mechanical and CRT video displays without increasing the number of complete frames per second, which would have sacrificed image detail to remain within the limitations of a narrow bandwidth.
The horizontal scan lines of each complete frame are treated as if numbered consecutively, and captured as two fields: an odd field (upper field) consisting of the odd-numbered lines and an even field (lower field) consisting of the even-numbered lines.
Analog display devices reproduce each frame in the same way, effectively doubling the frame rate as far as perceptible overall flicker is concerned. 
When the image capture device acquires the fields one at a time, rather than dividing up a complete frame after it is captured, the frame rate for motion is effectively doubled as well, resulting in smoother, more lifelike reproduction (although with halved detail) of rapidly moving parts of the image when viewed on an interlaced CRT display.
But the display of such a signal on a progressive scan device is problematic.
NTSC, PAL and SECAM are interlaced formats. Abbreviated video resolution specifications often include an i to indicate interlacing.
For example, PAL video format is often specified as 576i50, where 576 indicates the total number of horizontal scan lines, i indicates interlacing, and 50 indicates 50 fields (half-frames) per second.
In progressive scan systems, each refresh period updates all scan lines in each frame in sequence. When displaying a natively progressive broadcast or recorded signal, the result is optimum spatial resolution of both the stationary and moving parts of the image. 
When displaying a natively interlaced signal, however, overall spatial resolution is degraded by simple line doublingâ€”artifacts such as flickering or "comb" effects in moving parts of the image appear unless special signal processing eliminates them.
A procedure known as deinterlacing can optimize the display of an interlaced video signal from an analog, DVD or satellite source on a progressive scan device such as an LCD Television, digital video projector or plasma panel. 
Deinterlacing cannot, however, produce video quality that is equivalent to true progressive scan source material.
Aspect ratio describes the dimensions of video screens and video picture elements. 
All popular video formats are rectilinear, and so can be described by a ratio between width and height. The screen aspect ratio of a traditional television screen is 4:3, or about 1.33:1. 
High definition televisions use an aspect ratio of 16:9, or about 1.78:1. The aspect ratio of a full 35 mm film frame with soundtrack (also known as the Academy ratio) is 1.375:1.
Ratios where height is taller than width are uncommon in general everyday use, but are used in computer systems where some applications are better suited for a vertical layout.
The most common tall aspect ratio of 3:4 is referred to as portrait mode and is created by physically rotating the display device 90 degrees from the normal position. 
Other tall aspect ratios such as 9:16 are technically possible but rarely used.
Pixels on computer monitors are usually square, but pixels used in digital video often have non-square aspect ratios, such as those used in the PAL and NTSC variants of the CCIR 601 digital video standard, and the corresponding anamorphic widescreen formats. 
Therefore, a 720 by 480 pixel NTSC DV image displayes with the 4:3 aspect ratio (the traditional television standard) if the pixels are thin, and displays at the 16:9 aspect ratio (the anamorphic widescreen format) if the pixels are fat.
In computer science, particularly in operating systems, a semaphore is a variable or abstract data type that is used for controlling access, by multiple processes, to a common resource in a parallel programming or a multi user environment.
A useful way to think of a semaphore is as a record of how many units of a particular resource are available, coupled with operations to safely (i.e., without race conditions) adjust that record as units are required or become free, and, if necessary, wait until a unit of the resource becomes available. 
Semaphores are a useful tool in the prevention of race conditions.
However, their use is by no means a guarantee that a program is free from these problems. 
Semaphores which allow an arbitrary resource count are called counting semaphores, while semaphores which are restricted to the values 0 and 1 (or locked/unlocked, unavailable/available) are called binary semaphores.
The semaphore concept was invented by Dutch computer scientist Edsger Dijkstra in 1962 or 1963, and has found widespread use in a variety of operating systems.
Suppose a library has 10 identical study rooms, to be used by one student at a time. 
To prevent disputes, students must request a room from the front desk if they wish to make use of a study room. 
If no rooms are free, students wait at the desk until someone relinquishes a room. 
When a student has finished using a room, the student must return to the desk and indicate that one room has become free.
In the simplest implementation, the clerk at the front desk does not need to keep track of which rooms are occupied or who is using them, nor does she know if any given room is actually being used. 
Only the number of free rooms available, which she only knows correctly if all of the students actually use their room while they've signed up for them and return them when they're done. 
When a student requests a room, the clerk decreases this number. 
When a student releases a room, the clerk increases this number. 
Once access to a room is granted, the room can be used for as long as desired, and so it is not possible to book rooms ahead of time.
In this scenario the front desk count-holder represents a counting semaphore, the rooms are the resources, and the students represent processes. 
The value of the semaphore in this scenario is initially 10. 
When a student requests a room she is granted access and the value of the semaphore is changed to 9.
 After the next student comes, it drops to 8, then 7 and so on. 
If someone requests a room and the resulting value of the semaphore would be negative, they are forced to wait until a room is freed (when the count is increased from 0).
When used for a pool of resources, a semaphore tracks only how many resources are free.
It does not keep track of which of the resources are free. 
Some other mechanism (possibly involving more semaphores) may be required to select a particular free resource.
The paradigm is especially powerful because the semaphore count may serve as a useful trigger for a number of different actions. 
The librarian above may turn the lights off in the study hall when there are no students remaining, or may place a sign that says the rooms are very busy when most of the rooms are occupied.
The success of the protocol requires applications follow it correctly. 
Fairness and safety are likely to be compromised (which practically means a program may behave slowly, act erratically, hang or crash) if even a single process acts incorrectly. 
This includes: requesting a resource and forgetting to release it, releasing a resource that was never requested, holding a resource for a long time without needing it, using a resource without requesting it first (or after releasing it).
Even if all processes follow these rules, multi-resource deadlock may still occur when there are different resources managed by different semaphores and when processes need to use more than one resource at a time, as illustrated by the dining philosophers problem.
Counting semaphores are equipped with two operations, historically denoted as V (also known as signal) and P (or wait). 
Operation V increments the semaphore S, and operation P decrements it.
The value of the semaphore S is the number of units of the resource that are currently available. 
The P operation wastes time or sleeps until a resource protected by the semaphore becomes available, at which time the resource is immediately claimed.
 The V operation is the inverse: it makes a resource available again after the process has finished using it. 
 One important property of semaphore S is that its value cannot be changed except by using the V and P operations.
A simple way to understand wait and signal operations is.
wait: If the value of semaphore variable is not negative, decrements it by 1. 
If the semaphore variable is now negative, the process executing wait is blocked (i.e., added to the semaphore's queue) until the value is greater or equal to 1. 
Otherwise, the process continues execution, having used a unit of the resource.
signal: Increments the value of semaphore variable by 1. 
After the increment, if the pre-increment value was negative (meaning there are processes waiting for a resource), it transfers a blocked process from the semaphore's waiting queue to the ready queue.
Many operating systems provide efficient semaphore primitives that unblock a waiting process when the semaphore is incremented. 
This means that processes do not waste time checking the semaphore value unnecessarily.
The counting semaphore concept can be extended with the ability to claim or return more than one "unit" from the semaphore, a technique implemented in Unix.
 The modified V and P operations are as follows, using square brackets to indicate atomic operations, i.e., operations which appear indivisible from the perspective of other processes.
To avoid starvation, a semaphore has an associated queue of processes (usually with first-in, first out semantics). 
If a process performs a P operation on a semaphore that has the value zero, the process is added to the semaphore's queue and its execution is suspended. 
When another process increments the semaphore by performing a V operation, and there are processes on the queue, one of them is removed from the queue and resumes execution.
 When processes have different priorities the queue may be ordered by priority.
So that the highest priority process is taken from the queue first.
If the implementation does not ensure atomicity of the increment, decrement and comparison operations, then there is a risk of increments or decrements being forgotten, or of the semaphore value becoming negative. 
Atomicity may be achieved by using a machine instruction that is able to read, modify and write the semaphore in a single operation. 
In the absence of such a hardware instruction, an atomic operation may be synthesized through the use of a software mutual exclusion algorithm.
 On uniprocessor systems, atomic operations can be ensured by temporarily suspending preemption or disabling hardware interrupts. 
This approach does not work on multiprocessor systems where it is possible for two programs sharing a semaphore to run on different processors at the same time. 
To solve this problem in a multiprocessor system a locking variable can be used to control access to the semaphore. 
The locking variable is manipulated using a test-and-set-lock command.
In ALGOL 68, the Linux kernel,and in some English textbooks, the V and P operations are called, respectively, up and down.
 In computing, virtual memory is a memory management technique that is implemented using both hardware and software.
 It maps memory addresses used by a program, called virtual addresses, into physical addresses in computer memory. 
Main storage as seen by a process or task appears as a contiguous address space or collection of contiguous segments. 
The operating system manages virtual address spaces and the assignment of real memory to virtual memory. 
Address translation hardware in the CPU, often referred to as a memory management unit or MMU, automatically translates virtual addresses to physical addresses. 
Software within the operating system may extend these capabilities to provide a virtual address space that can exceed the capacity of real memory and thus reference more memory than is physically present in the computer.
The primary benefits of virtual memory include freeing applications from having to manage a shared memory space, increased security due to memory isolation, and being able to conceptually use more memory than might be physically available, using the technique of paging.
Virtual memory makes application programming easier by hiding fragmentation of physical memory. 
By delegating to the kernel the burden of managing the memory hierarchy (eliminating the need for the program to handle overlays explicitly); and, when each process is run in its own dedicated address space, by obviating the need to relocate program code or to access memory with relative addressing.
Memory virtualization can be considered a generalization of the concept of virtual memory.
Virtual memory is an integral part of a modern computer architecture; implementations require hardware support, typically in the form of a memory management unit built into the CPU. 
While not necessary, emulators and virtual machines can employ hardware support to increase performance of their virtual memory implementations.
Consequently, older operating systems, such as those for the mainframes of the 1960s, and those for personal computers of the early to mid-1980s (e.g. DOS), generally have no virtual memory functionality, though notable exceptions for mainframes of the 1960s. 
They  include the Atlas Supervisor for the Atlas MCP for the Burroughs B5000 MTS, TSS/360 and CP/CMS for the IBM System/360 Model 67 Multics for the GE 645 the Time Sharing Operating System for the RCA Spectra 70/46.
The Apple Lisa is an example of a personal computer of the 1980s that features virtual memory.
Most modern operating systems that support virtual memory also run each process in its own dedicated address space. 
Each program thus appears to have sole access to the virtual memory. 
However, some older operating systems (such as OS/VS1 and OS/VS2 SVS) and even modern ones (such as IBM i) are single address space operating systems that run all processes in a single address space composed of virtualized memory.
Embedded systems and other special-purpose computer systems that require very fast and/or very consistent response times may opt not to use virtual memory due to decreased determinism.
Virtual memory systems trigger unpredictable traps that may produce unwanted "jitter" during I/O operations. 
This is because embedded hardware costs are often kept low by implementing all such operations with software (a technique called bit-banging) rather than with dedicated hardware.
Nearly all implementations of virtual memory divide a virtual address space into pages, blocks of contiguous virtual memory addresses. 
Pages on contemporary systems are usually at least 4 kilobytes in size; systems with large virtual address ranges or amounts of real memory generally use larger page sizes.
Page tables are used to translate the virtual addresses seen by the application into physical addresses used by the hardware to process instructions.
Such hardware that handles this specific translation is often known as the memory management unit. 
Each entry in the page table holds a flag indicating whether the corresponding page is in real memory or not. 
If it is in real memory, the page table entry will contain the real memory address at which the page is stored. 
When a reference is made to a page by the hardware, if the page table entry for the page indicates that it is not currently in real memory, the hardware raises a page fault exception, invoking the paging supervisor component of the operating system.
Systems can have one page table for the whole system, separate page tables for each application and segment, a tree of page tables for large segments or some combination of these. 
If there is only one page table, different applications running at the same time use different parts of a single range of virtual addresses. 
If there are multiple page or segment tables, there are multiple virtual address spaces and concurrent applications with separate page tables redirect to different real addresses.
This part of the operating system creates and manages page tables. 
If the hardware raises a page fault exception, the paging supervisor accesses secondary storage, returns the page that has the virtual address that resulted in the page fault, updates the page tables to reflect the physical location of the virtual address and tells the translation mechanism to restart the request.
When all physical memory is already in use, the paging supervisor must free a page in primary storage to hold the swapped-in page. 
The supervisor uses one of a variety of page replacement algorithms such as least recently used to determine which page to free.
Some systems, such as the Burroughs B5500, use segmentation instead of paging, dividing virtual address spaces into variable-length segments. 
A virtual address here consists of a segment number and an offset within the segment. 
The Intel 80286 supports a similar segmentation scheme as an option, but it is rarely used. 
Segmentation and paging can be used together by dividing each segment into pages; systems with this memory structure, such as Multics and IBM System/38, are usually paging-predominant, segmentation providing memory protection.
In the Intel 80386 and later IA-32 processors, the segments reside in a 32-bit linear, paged address space. 
Segments can be moved in and out of that space; pages there can "page" in and out of main memory, providing two levels of virtual memory; few if any operating systems do so, instead using only paging. 
Early non-hardware-assisted x86 virtualization solutions combined paging and segmentation because x86 paging offers only two protection domains whereas a VMM / guest OS / guest applications stack needs three.
The difference between paging and segmentation systems is not only about memory division; segmentation is visible to user processes, as part of memory model semantics. 
Hence, instead of memory that looks like a single large space, it is structured into multiple spaces.
This difference has important consequences; a segment is not a page with variable length or a simple way to lengthen the address space. 
Segmentation that can provide a single-level memory model in which there is no differentiation between process memory and file system consists of only a list of segments (files) mapped into the process's potential address space.
This is not the same as the mechanisms provided by calls such as mmap and Win32's MapViewOfFile, because inter-file pointers do not work when mapping files into semi-arbitrary places. 
In Multics, a file (or a segment from a multi-segment file) is mapped into a segment in the address space, so files are always mapped at a segment boundary. 
A file's linkage section can contain pointers for which an attempt to load the pointer into a register or make an indirect reference through it causes a trap. 
This eliminates the need for a linker completely and works when different processes map the same file into different places in their private address spaces.
Some operating systems provide for swapping entire address spaces, in addition to whatever facilities they have for paging and segmentation. 
When this occurs, the OS writes those pages and segments currently in real memory to swap files. In a swap-in, the OS reads back the data from the swap files but does not automatically read back pages that had been paged out at the time of the swap out operation.
IBM's MVS, from OS/VS2 Release 2 through z/OS, provides for marking an address space as unswappable; doing so does not pin any pages in the address space. 
This can be done for the duration of a job by entering the name of an eligible main program in the Program Properties Table with an unswappable flag.
 In addition, privileged code can temporarily make an address space unswappable With a SYSEVENT Supervisor Call instruction (SVC); certain changes in the address space properties require that the OS swap it out and then swap it back in, using SYSEVENT TRANSWAP,.
In computing, scheduling is the method by which threads, processes or data flows are given access to system resources (e.g. processor time, communications bandwidth). 
This is usually done to load balance and share system resources effectively or achieve a target quality of service.
 The need for a scheduling algorithm arises from the requirement for most modern systems to perform multitasking (executing more than one process at a time) and multiplexing (transmit multiple data streams simultaneously across a single physical channel).
In practice, these goals often conflict (e.g. throughput versus latency).
Thus a scheduler will implement a suitable compromise.
 Preference is given to any one of the concerns mentioned above, depending upon the user's needs and objectives.
In real-time environments, such as embedded systems for automatic control in industry (for example robotics), the scheduler also must ensure that processes can meet deadlines.
This is crucial for keeping the system stable. 
Scheduled tasks can also be distributed to remote devices across a network and managed through an administrative back end.
The scheduler is an operating system module that selects the next jobs to be admitted into the system and the next process to run.
 Operating systems may feature up to three distinct scheduler types.
 A long-term scheduler (also known as an admission scheduler or high-level scheduler), a mid-term or medium-term scheduler, and a short-term scheduler.
 The names suggest the relative frequency with which their functions are performed.
The process scheduler is a part of the operating system that decides which process runs at a certain point in time.
 It usually has the ability to pause a running process, move it to the back of the running queue and start a new process.
Such a scheduler is known as preemptive scheduler, otherwise it is a cooperative scheduler.
The long-term scheduler, or admission scheduler, decides which jobs or processes are to be admitted to the ready queue (in main memory).
That is, when an attempt is made to execute a program, its admission to the set of currently executing processes is either authorized or delayed by the long-term scheduler. 
Thus, this scheduler dictates what processes are to run on a system, and the degree of concurrency to be supported at any one time â€“ whether many or few processes are to be executed concurrently, and how the split between I/O-intensive and CPU-intensive processes is to be handled. 
The long term scheduler is responsible for controlling the degree of multiprogramming.
In general, most processes can be described as either I/O-bound[3] or CPU-bound. 
An I/O-bound process is one that spends more of its time doing I/O than it spends doing computations. 
A CPU-bound process, in contrast, generates I/O requests infrequently, using more of its time doing computations. 
It is important that a long-term scheduler selects a good process mix of I/O-bound and CPU-bound processes. 
If all processes are I/O-bound, the ready queue will almost always be empty, and the short-term scheduler will have little to do. 
On the other hand, if all processes are CPU-bound, the I/O waiting queue will almost always be empty, devices will go unused, and again the system will be unbalanced. 
The system with the best performance will thus have a combination of CPU-bound and I/O-bound processes. 
In modern operating systems, this is used to make sure that real-time processes get enough CPU time to finish their tasks.
Long-term scheduling is also important in large-scale systems such as batch processing systems, computer clusters, supercomputers and render farms. 
In these cases, special purpose job scheduler software is typically used to assist these functions.
In addition to any underlying admission scheduling support in the operating system.
The medium-term scheduler temporarily removes processes from main memory and places them in secondary memory (such as a hard disk drive) or vice-versa, which is commonly referred to as "swapping out" or "swapping in" (also incorrectly as "paging out" or "paging in"). 
The medium-term scheduler may decide to swap out a process which has not been active for some time, or a process which has a low priority, or a process which is page faulting frequently, or a process which is taking up a large amount of memory in order to free up main memory for other processes, swapping the process back in later when more memory is available, or when the process has been unblocked and is no longer waiting for a resource. 
In many systems today (those that support mapping virtual address space to secondary storage other than the swap file), the medium-term scheduler may actually perform the role of the long-term scheduler, by treating binaries as "swapped out processes" upon their execution.
 In this way, when a segment of the binary is required it can be swapped in on demand, or "lazy loaded". 
The short-term scheduler (also known as the CPU scheduler) decides which of the ready, in-memory processes is to be executed (allocated a CPU) after a clock interrupt, an I/O interrupt, an operating system call or another form of signal. 
Thus the short-term scheduler makes scheduling decisions much more frequently than the long-term or mid-term schedulers - a scheduling decision will at a minimum have to be made after every time slice, and these are very short. 
This scheduler can be preemptive, implying that it is capable of forcibly removing processes from a CPU when it decides to allocate that CPU to another process, or non-preemptive (also known as "voluntary" or "co-operative"), in which case the scheduler is unable to "force" processes off the CPU.
A preemptive scheduler relies upon a programmable interval timer which invokes an interrupt handler that runs in kernel mode and implements the scheduling function.
Another component that is involved in the CPU-scheduling function is the dispatcher, which is the module that gives control of the CPU to the process selected by the short-term scheduler.
 It receives control in kernel mode as the result of an interrupt or system call. 
The functions of a dispatcher involve the following.
Context switches, in which the dispatcher saves the state (also known as context) of the process or thread that was previously running; the dispatcher then loads the initial or previously saved state of the new process.
Switching to user mode.
Jumping to the proper location in the user program to restart that program indicated by its new state.
The dispatcher should be as fast as possible, since it is invoked during every process switch. 
During the context switches, the processor is virtually idle for a fraction of time, thus unnecessary context switches should be avoided. 
The time it takes for the dispatcher to stop one process and start another is known as the dispatch latency.
Each I/O device driver can provide a driver-specific set of I/O application programming interfaces to the applications. 
This arrangement requires each application to be aware of the nature of the underlying I/O device, including the restrictions imposed by the device.
 The API set is driver and implementation specific.
 Which makes the applications using this API set difficult to port. 
To reduce this implementation-dependence, embedded systems often include an I/O subsystem.
The I/O subsystem defines a standard set of functions for I/O operations in order to hide device peculiarities from applications. 
All I/O device drivers conform to and support this function set.
Because the goal is to provide uniform I/O to applications across a wide spectrum of I/O devices of varying types.
The following steps must take place to accomplish uniform I/O operations at the application-level. 
The I/O subsystem defines the API set. 
The device driver implements each function in the set. 
The device driver exports the set of functions to the I/O subsystem. 
The device driver does the work necessary to prepare the device for use. 
In addition, the driver sets up an association between the I/O subsystem API set and the corresponding device-specific I/O calls. 
The device driver loads the device and makes this driver and device association known to the I/O subsystem.
This action enables the I/O subsystem to present the illusion of an abstract or virtual instance of the device to applications. 
This section discusses one approach to uniform I/O. 
This approach is general, and the goal is to offer insight into the I/O subsystem layer and its interaction with the application layer above and the device driver layer below. 
Another goal is to give the reader an opportunity to observe how the pieces are put together to provide uniform I/O capability in an embedded environment. 
I/O devices are classified as either character-mode devices or block-mode devices. 
The classification refers to how the device handles data transfer with the system. 
Character-mode devices allow for unstructured data transfers. 
The data transfers typically take place in serial fashion, one byte at a time. 
Character-mode devices are usually simple devices.
Such as the serial interface or the keypad. 
The driver buffers the data in cases where the transfer rate from system to the device is faster than what the device can handle. 
Block-mode devices transfer data one block at time.
For example, 1,024 bytes per data transfer. 
The underlying hardware imposes the block size. 
Some structure must be imposed on the data or some transfer protocol enforced. 
Otherwise an error is likely to occur. 
Therefore, sometimes it is necessary for the block-mode device driver to perform additional work for each read or write operation. 
When servicing a write operation with large amounts of data, the device driver must first divide the input data into multiple blocks, each with a device-specific block size. 
In this example, the input data is divided into four blocks, of which all but the last block is of the required block size. In practice, the last partition often is smaller than the normal device block size.
Each block is transferred to the device in separate write requests. 
The first three are straightforward write operations. 
The device driver must handle the last block differently from the first three.
Because the last block has a different size.
 The method used to process this last block is device specific. 
In some cases, the driver pads the block to the required size. 
The example is based on a hard-disk drive. 
In this case, the device driver first performs a read operation of the affected block and replaces the affected region of the block with the new data. 
The modified block is then written back. 
Another strategy used by block-mode device drivers for small write operations is to accumulate the data in the driver cache and to perform the actual write after enough data has accumulated for a required block size. 
This technique also minimizes the number of device accesses. 
Some disadvantages occur with this approach.
 First, the device driver is more complex. 
For example, the block-mode device driver for a hard disk must know if the cached data can satisfy a read operation. 
The delayed write associated with caching can also cause data loss if a failure occurs and if the driver is shut down and unloaded ungracefully. 
Data caching in this case implies data copying that can result in lower I/O performance.
The simplest method of implementing a directory is to use a linear list of file names with pointers to the data blocks. 
A linear list of directory entries requires a linear search to find a particular entry. 
This method is simple to program but time-consuming to execute. 
To create a new file, We must first search the directory to be sure that no existing file has the same name. 
Then, we add a new entry at the end of the directory. 
To delete a file, We search the directory for the named file, then release the space allocated to it. 
To reuse the directory entry, we can do one of several things. 
We can mark the entry as unused (by assigning it a special 	name, such as an all-blank name, or with a used-unused bit in 	each entry), or  We can attach it to a list of free directory entries. 
A third alternative is to copy the last entry in the directory 	into the freed location, and to decrease the length of the directory. 
A linked list can also be used to decrease the time to delete a file.
 The real disadvantage of a linear list of directory entries is the linear search to find a file. 
Directory information is used frequently, and users would notice a slow implementation of access to it. 
In fact, many operating systems implement a software cache to store the most recently used directory information. 
A cache hit 	avoids constantly rereading the information from disk. 
A sorted list allows a binary search and decreases the average search time. 
However, 	the requirement that the list must be kept sorted may complicate the process. 
A more sophisticated tree data structure, such as a B-tree, might help here. An 	advantage of the sorted list is that a sorted directory listing can be produced without a 	separate sort step.
Another data structure that has been used for a file directory is a hash table. 
In this method,  a linear list stores the directory entries, but a hash data structure is 	also used. 
The hash table takes a value computed from the file name and returns a pointer to 	the file name in the linear list. Therefore, it can greatly decrease the directory 	search time.
Insertion and deletion are also fairly straightforward, although some provision must be made for collisions-situations where two file names hash to the same location. 
The major difficulties with a hash table are its generally fixed size and the dependence of the hash function on that size.
Lookups may be somewhat slowed, because searching for a name might require stepping through a linked list of colliding table entries, but this is likely to be much faster than a linear search through the entire directory.
The direct-access nature of disks allows us flexibility in the implementation of files. 
In almost every case, many files will be stored on the same disk. 
The main problem is how to allocate space to these files so that disk space is utilized effectively and files can be accessed quickly. 
Three major methods of allocating disk space are in wide use, 	Contiguous, Linked, and Indexed.
Each method has advantages and disadvantages. More commonly, a system will use one particular method for all files.
Disk addresses define a linear ordering on the disk.With this ordering,     accessing 	block
b + 1 after block b normally requires no head movement. 
When head movement is needed (from the last sector of one cylinder to the first sector of 	the next cylinder), it is only one track. 
Thus, the number of disk seeks required for accessing contiguously allocated files is minimal. 
If the Contiguous allocation of a file is n blocks long and starts at location b, then it occupies blocks b, b + 1, b + 2, ..., b + n - 1. 
The directory entry for each file is , ( Name, Starting block and the length of the file in terms of no. of blocks ). 
Accessing a file that has been allocated contiguously is easy. 
For sequential access, the file system remembers the disk address of the last block referenced and,  when necessary, reads the next block. 
For direct access to block i of a file that starts at block b, we can immediately access block   b + i. 
Thus, both sequential and direct access can be supported by contiguous allocation. 
One difficulty is finding space for a new file. 
Any free-space management system can be used, but some are slower than others. 
First fit and best fit are the most common strategies used to select a free hole from the 	set of available holes. 
These algorithms suffer from the problem of external 	fragmentation. 
Depending on the total amount of disk storage and the average file size, external fragmentation may be a minor or a major problem. 
 To prevent loss of significant amounts of disk space to external fragmentation, the user had to run a repacking routine that copied the entire file system onto another floppy disk or onto a tape. 
The original floppy disk was then freed completely, creating one large contiguous free space. 
The routine then copied the files back onto the floppy disk by allocating contiguous space from this one large hole. 
This scheme effectively compacts all free space into one contiguous space, solving the fragmentation problem. The cost of this compaction is time.
The time cost is particularly severe for large hard disks that use contiguous allocation, where compacting all the space may take hours and may be necessary on a weekly basis.
 During this down time, normal system operation generally cannot be permitted, so such compaction is avoided at all costs on .	production machines. 
Second problem of Contiguous allocation is Determining how much space is needed for a file. 
When the file is created, the total amount of space it will need must be found and allocated. 
In some cases, this determination may be fairly simple (copying an existing file, for example); 
In general, If we allocate too little space to a file .
Especially with a best-fit allocation strategy, the space on both sides of the file may be in use. 
Hence, we cannot make the file larger in place.
The operating system is responsible for several other aspects of disk management, too. 
Here we discuss disk initialization, booting from disk, and bad-block recovery.
A new magnetic disk is a blank slate.
It is just platters of a magnetic recording material. 
Before a disk can store data, it must be divided into sectors that the disk controller can read and write. 
This process is called low-level formatting (or physical formatting). 
Low-level formatting fills the disk with a special data structure for each sector. 
The data structure for a sector typically consists of a header, a data area (usually 512 bytes in size), and a trailer. 
The header and trailer contain information used by the disk 	controller, such as a sector number and an Error-Correcting 	Code (ECC). 
When the controller writes a sector of data during normal I/O, the ECC is updated with a value calculated from all the bytes in the data area. 
When the sector is read, the ECC is recalculated and is compared with the stored value. 
If the stored and calculated numbers are different, this mismatch indicates that the data area of the sector has become corrupted and that the disk sector may be bad .
The ECC is an error-correcting code because it contains enough information that, if only a few bits of data have been corrupted, the controller can identify which bits have changed and can calculate what their correct values should be. 
The controller automatically does the ECC processing whenever a sector is read or written.
Most hard disks are low-level formatted at the factory as a part of the manufacturing process. 
This formatting enables the manufacturer to test the disk and to initialize the mapping from logical block numbers to defect-free sectors on the disk. 
For many hard disks, when the disk controller is instructed to low-level format the disk, it can also be told how many bytes of data space to leave between the header and trailer of all sectors.
 It is usually possible to choose among a few sizes, such as 256, 512, and 1,024 bytes. 
Formatting a disk with a larger sector size means that fewer sectors can fit on each track, but that also means fewer headers and trailers are written on each track, and thus increases the space available for user data. Some operating systems can handle only a sector size of 512 bytes.
To use a disk to hold files, the operating system still needs to record its own data structures on the disk. It does so in two steps.
The first step is to partition the disk into one or more groups of 	cylinders.
 The operating system can treat each partition as though it 	were a separate disk.
For instance, one partition can hold a copy of the operating system's executable code, while another holds user files. 
The second step is logical formatting (or creation of a file system). In this step, the operating system stores the initial file-system data structures onto the disk. 
These data structures may include maps of free and allocated space (a FAT) and an initial empty directory.
Some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks, without any file-system data structures. 
This array is sometimes called the raw disk, and I/O to this array is termed raw I/O.
For example, some database systems prefer raw I/O because it enables them to control the exact disk location where each database record is stored. Raw I/O bypasses all the file-system services, such as the buffer cache, file locking, prefetching, space allocation, file names, and directories.
We can make certain applications more efficient by implementing their own special-purpose storage services on a raw partition, but most applications perform better when they use the regular file-system services.
For a computer to start running-for instance, when it is powered up or rebooted-it needs to have an initial program to run. 
This initial bootstrap program tends to be simple.
It initializes all aspects of the system, from CPU registers to device controllers and the contents of main memory, and then 	starts the operating system. 
To do its job, the bootstrap program finds the operating system kernel on disk, loads that kernel into memory, and jumps to an 	initial address to begin the operating-system execution.
For most computers, the bootstrap is stored in read-only memory (ROM). 
This location is convenient, because ROM needs no initialization and is at a fixed location that the processor can start executing when powered up or reset. 
And, since ROM is read only, it cannot be infected by a computer virus. 
The problem is that changing this bootstrap code requires changing the ROM hardware chips.
For this reason, most systems store a tiny bootstrap loader program in the boot ROM, whose only job is to bring in a full bootstrap program from disk. 
The full bootstrap program can be changed easily.
A new version is simply written onto the disk. 
The full bootstrap program is stored in a partition called the boot blocks, at a fixed location on the disk. 
A disk that has a boot partition is called a boot disk or system disk.
The code in the boot ROM(a tiny bootstrap loader program) instructs the disk controller to read the boot 
Because disks have moving parts and small tolerances (recall that the disk head flies just above the disk surface), they are prone to failure.
 Sometimes the failure is complete, and the disk needs to be replaced, and its contents restored from backup media to the new disk. 
More frequently, one or more sectors become defective. 
Most disks even come from the factory with bad blocks. Depending on the disk and controller in use, these blocks are handled in a variety of ways.
On simple disks, such as some disks with IDE controllers, bad blocks are handled manually.
For instance, the MS-DOS format command does a logical format and, as a part of the process, scans the disk to find bad blocks. 
If format finds a bad block, it writes a special value into the corresponding FAT entry to tell the allocation routines not to use that block.
If blocks go bad during normal operation, a special program (such as chkdsk) must be run manually to search for the bad blocks and to lock them away as before.
Data that resided on the bad blocks usually are lost.
The controller maintains a list of bad blocks on the disk. 
The list is initialized during the low-level format at the factory, and is updated over the life of the disk.
An operating system (OS) is software that manages computer hardware and software resources and provides common services for computer programs. 
The operating system is an essential component of the system software in a computer system. 
Application programs usually require an operating system to function.
Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.
For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware,although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or be interrupted by it. 
Operating systems are found on many devices that contain a computer from cellular phones and video game consoles to web servers and supercomputers.
Examples of popular modern operating systems include Android, BlackBerry 10, BSD, Chrome OS, iOS, Linux, OS X, QNX, Microsoft Windows,[3] Windows Phone, and z/OS. 
The first eight of these examples share roots in UNIX.
A single-tasking system can only run one program at a time, while a multi-tasking operating system allows more than one program to be running in concurrency.
 This is achieved by time-sharing, dividing the available processor time between multiple processes which are each interrupted repeatedly in time slices by a task scheduling subsystem of the operating system. 
 Multi-tasking may be characterized in pre-emptive and co-operative types. 
 In pre-emptive multitasking, the operating system slices the CPU time and dedicates a slot to each of the programs. 
 Unix-like operating systems, e.g., Solaris, Linux, as well as AmigaOS support pre-emptive multitasking. 
 Cooperative multitasking is achieved by relying on each process to provide time to the other processes in a defined manner. 
 16-bit versions of Microsoft Windows used cooperative multi-tasking. 
 32-bit versions of both Windows NT and Win9x, used pre-emptive multi-tasking. 
 Mac OS prior to OS X also used to support cooperative multitasking.
 Single-user operating systems have no facilities to distinguish users, but may allow multiple programs to run at the same time A multi-user operating system extends the basic concept of multi-tasking with facilities that identify processes and resources, such as disk space, belonging to multiple users, and the system permits multiple users to interact with the system at the same time. 
 Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources to multiple users.
 A distributed operating system manages a group of distinct computers and makes them appear to be a single computer. 
 The development of networked computers that could be linked and communicate with each other gave rise to distributed computing.
 Distributed computations are carried out on more than one machine. 
 When computers in a group work in cooperation, they form a distributed system.
 In an OS, distributed and cloud computing context, templating refers to creating a single virtual machine image as a guest operating system, then saving it as a tool for multiple running virtual machines. 
 The technique is used both in virtualization and cloud computing management, and is common in large server warehouses.
 Embedded operating systems are designed to be used in embedded computer systems. 
 They are designed to operate on small machines like PDAs with less autonomy. 
 They are able to operate with a limited number of resources. 
 They are very compact and extremely efficient by design. 
 Windows CE and Minix 3 are some examples of embedded operating systems.
 A real-time operating system is an operating system that guaranties to process events or data within a certain short amount of time. 
 A real-time operating system may be single- or multi-tasking, but when multitasking, it uses specialized scheduling algorithms so that a deterministic nature of behavior is achieved. 
  An event-driven system switches between tasks based on their priorities or external events while time-sharing operating systems switch tasks based on clock interrupts.
  Unix was originally written in assembly language.
  Ken Thompson wrote B, mainly based on BCPL, based on his experience in the MULTICS project.
  B was replaced by C, and Unix, rewritten in C, developed into a large, complex family of inter-related operating systems which have been influential in every modern operating system.
The Unix-like family is a diverse group of operating systems, with several major sub-categories including System V, BSD, and Linux. 
The name "UNIX" is a trademark of The Open Group which licenses it for use with any operating system that has been shown to conform to their definitions. 
"UNIX-like" is commonly used to refer to the large set of operating systems which resemble the original UNIX.
Unix-like systems run on a wide variety of computer architectures. 
They are used heavily for servers in business, as well as workstations in academic and engineering environments. 
Free UNIX variants, such as Linux and BSD, are popular in these areas.
Four operating systems are certified by The Open Group (holder of the Unix trademark) as Unix.
HP's HP-UX and IBM's AIX are both descendants of the original System V Unix and are designed to run only on their respective vendor's hardware.
 In contrast, Sun Microsystems's Solaris Operating System can run on multiple types of hardware, including x86 and Sparc servers, and PCs. 
 Apple's OS X, a replacement for Apple's earlier (non-Unix) Mac OS, is a hybrid kernel-based BSD variant derived from NeXTSTEP, Mach, and FreeBSD.
Unix interoperability was sought by establishing the POSIX standard. 
The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants.
Microsoft Windows is a family of proprietary operating systems designed by Microsoft Corporation and primarily targeted to Intel architecture based computers, with an estimated 88.9 percent total usage share on Web connected computers.
The newest version is Windows 8.1 for workstations and Windows Server 2012 R2 for servers. 
Windows 7 recently overtook Windows XP as most used OS.
Microsoft Windows originated in 1985 as an operating environment running on top of MS-DOS, which was the standard operating system shipped on most Intel architecture personal computers at the time. 
In 1995, Windows 95 was released which only used MS-DOS as a bootstrap. 
For backwards compatibility, Win9x could run real-mode MS-DOS and 16 bits Windows 3.x[26] drivers. 
Windows ME, released in 2000, was the last version in the Win9x family. 
Later versions have all been based on the Windows NT kernel. 
Current client versions of Windows run on IA-32, x86-64 and 32-bit ARM microprocessors.
In addition Itanium is still supported in older server version Windows Server 2008 R2. 
In the past, Windows NT supported additional architectures.
Server editions of Windows are widely used. 
In recent years, Microsoft has expended significant capital in an effort to promote the use of Windows as a server operating system. 
However, Windows' usage on servers is not as widespread as on personal computers, as Windows competes against Linux and BSD for server market share.
The first PC that used windows operating system was the IBM Personal System/2.
Memory management is the act of managing computer memory at the system level. 
The essential requirement of memory management is to provide ways to dynamically allocate portions of memory to programs at their request, and free it for reuse when no longer needed. 
This is critical to any advanced computer system where more than a single process might be underway at any time.
Several methods have been devised that increase the effectiveness of memory management. 
Virtual memory systems separate the memory addresses used by a process from actual physical addresses, allowing separation of processes and increasing the effectively available amount of RAM using paging or swapping to secondary storage. 
The quality of the virtual memory manager can have an extensive effect on overall system performance.
The task of fulfilling an allocation request consists of locating a block of unused memory of sufficient size. 
Memory requests are satisfied by allocating portions from a large pool of memory called the heap or free store. 
At any given time, some parts of the heap are in use, while some are "free" (unused) and thus available for future allocations.
Several issues complicate the implementation, such as external fragmentation, which arises when there are many small gaps between allocated memory blocks, which invalidates their use for an allocation request. 
The allocator's metadata can also inflate the size of (individually) small allocations. This is often managed by chunking. 
The memory management system must track outstanding allocations to ensure that they do not overlap and that no memory is ever "lost" as a memory leak.
The specific dynamic memory allocation algorithm implemented can impact performance significantly. 
A study conducted in 1994 by Digital Equipment Corporation illustrates the overheads involved for a variety of allocators. 
The lowest average instruction path length required to allocate a single memory slot was 52 (as measured with an instruction level profiler on a variety of software).
Since the precise location of the allocation is not known in advance, the memory is accessed indirectly, usually through a pointer reference. 
The specific algorithm used to organize the memory area and allocate and deallocate chunks is interlinked with the kernel, and may use any of the following methods:
Fixed-size blocks allocation, also called memory pool allocation, uses a free list of fixed-size blocks of memory (often all of the same size).
 This works well for simple embedded systems where no large objects need to be allocated, but suffers from fragmentation, especially with long memory addresses. 
 However, due to the significantly reduced overhead this method can substantially improve performance for objects that need frequent allocation / de-allocation and is often used in video games.
In this system, memory is allocated into several pools of memory instead of just one, where each pool represents blocks of memory of a certain power of two in size.
 All blocks of a particular size are kept in a sorted linked list or tree and all new blocks that are formed during allocation are added to their respective memory pools for later use. 
 If a smaller size is requested than is available, the smallest available size is selected and halved. 
One of the resulting halves is selected, and the process repeats until the request is complete. 
When a block is allocated, the allocator will start with the smallest sufficiently large block to avoid needlessly breaking blocks. 
When a block is freed, it is compared to its buddy. 
If they are both free, they are combined and placed in the next-largest size buddy-block list.
Virtual memory is a method of decoupling the memory organization from the physical hardware. 
The applications operate memory via virtual addresses. 
Each time an attempt to access stored data is made, virtual memory data orders translate the virtual address to a physical address. 
In this way addition of virtual memory enables granular control over memory systems and methods of access.
In virtual memory systems the operating system limits how a process can access the memory. 
This feature, called memory protection, can be used to disallow a process to read or write to memory that is not allocated to it, preventing malicious or malfunctioning code in one program from interfering with the operation of another.
Even though the memory allocated for specific processes is normally isolated, processes sometimes need to be able to share information.
Shared memory is one of the fastest techniques for inter-process communication.
Memory is usually classified by access rate into primary storage and secondary storage. 
Memory management systems, among other operations, also handle the moving of information between these two levels of memory.
In computer programming, an automatic variable is a local variable which is allocated and deallocated automatically when program flow enters and leaves the variable's scope. 
The scope is the lexical context, particularly the function or block in which a variable is defined. 
Local data is typically (in most languages) invisible outside the function or lexical context where it is defined. 
Local data is also invisible and inaccessible to a called function, but is not deallocated, coming back in scope as the execution thread returns to the caller.
Automatic local variables primarily applies to recursive lexically-scoped languages.
Automatic local variables are normally allocated in the stack frame of the procedure in which they are declared.
This was originally done to achieve re-entrancy and allowing recursion, considerations that still applies today. 
The concept of automatic variables in recursive (and nested) functions in a lexically scoped language was introduced to the wider audience with ALGOL in the late 1950s, and further popularized by its many descendants.
The term local variable is usually synonymous with automatic variable, since these are the same thing in many programming languages, but local is more general â€“ most local variables are automatic local variables, but static local variables also exist, notably in C. 
For a static local variable, the allocation is static (the lifetime is the entire program execution), not automatic, but it is only in scope during the execution of the function.
In computer science, garbage collection (GC) is a form of automatic memory management.
 The garbage collector, or just collector, attempts to reclaim garbage, or memory occupied by objects that are no longer in use by the program. 
Garbage collection was invented by John McCarthy around 1959 to solve problems in Lisp.
Garbage collection is often portrayed as the opposite of manual memory management, which requires the programmer to specify which objects to deallocate and return to the memory system. 
However, many systems use a combination of approaches, including other techniques such as stack allocation and region inference.
 Like other memory management techniques, garbage collection may take a significant proportion of total processing time in a program and can thus have significant influence on performance.
In computer operating systems, paging is one of the memory management schemes by which a computer stores and retrieves data from the secondary storage for use in main memory.
In the paging memory-management scheme, the operating system retrieves data from secondary storage in same-size blocks called pages.
 The main advantage of paging over memory segmentation is that it allows the physical address space of a process to be noncontiguous. 
Before paging came into use, systems had to fit whole programs into storage contiguously, which caused various storage and fragmentation problems.
Paging is an important part of virtual memory implementation in most contemporary general-purpose operating systems, allowing them to use secondary storage for data that does not fit into physical random-access memory (RAM).
The main functions of paging are performed when a program tries to access pages that are not currently mapped to physical memory (RAM). 
This situation is known as a page fault. The operating system must then take control and handle the page fault, in a manner invisible to the program.
Therefore, the operating system must, Determine the location of the data in secondary storage.
Obtain an empty page frame in RAM to use as a container for the data.
Load the requested data into the available page frame.
Update the page table to refer to the new page frame.
Return control to the program, transparently retrying the instruction that caused the page fault.
If there is not enough available RAM when obtaining an empty page frame, a page replacement algorithm is used to choose an existing page frame for eviction. 
If the evicted page frame has been dynamically allocated during execution of a program, or if it is part of a program's data segment and has been modified since it was read into RAM (in other words, if it has become "dirty"), it must be written out to a location in secondary storage before being freed. 
Otherwise, the contents of the page's frame in RAM are the same as the contents of the page in its secondary storage, so it does not need to be written out to secondary storage. 
If, at a later stage, a reference is made to that memory page, another page fault will occur and another empty page frame must be obtained so that the contents of the page in secondary storage can be again read into RAM.
Efficient paging systems must determine the page frame to empty by choosing one that is least likely to be needed within a short time. 
There are various page replacement algorithms that try to do this. 
Most operating systems use some approximation of the least recently used (LRU) page replacement algorithm (the LRU itself cannot be implemented on the current hardware) or a working set-based algorithm.
To further increase responsiveness, paging systems may employ various strategies to predict which pages will be needed soon. 
Such systems will attempt to load pages into main memory preemptively, before a program references them.
When pure demand paging is used, page loading only occurs at the time of the data request, and not before. 
In particular, when demand paging is used, a program usually begins execution with none of its pages pre-loaded in RAM. 
Pages are copied from the executable file into RAM the first time the executing code references them, usually in response to page faults. 
As a consequence, pages of the executable file containing code not executed during a particular run will never be loaded into memory.
This technique, sometimes also called "swap prefetch", preloads a process's non-resident pages that are likely to be referenced in the near future (taking advantage of locality of reference).
 Such strategies attempt to reduce the number of page faults a process experiences. 
Some of those strategies are "if a program references one virtual address which causes a page fault, perhaps the next few pages' worth of virtual address space will soon be used" and "if one big program just finished execution, leaving lots of free RAM, perhaps the user will return to using some of the programs that were recently paged out".
The free page queue is a list of page frames that are available for assignment after a page fault. 
Some operating systems support page reclamation; if a page fault occurs for a page that had been stolen and the page frame was never reassigned, then the operating system avoids the necessity of reading the page back in by assigning the unmodified page frame.
Some operating systems periodically look for pages that have not been recently referenced and add them to the Free page queue, after paging them out if they have been modified.
Unix operating systems periodically use sync to pre-clean all dirty pages, that is, to save all modified pages to hard disk. Windows operating systems do the same thing via "modified page writer" threads.
Pre-cleaning makes starting a new program or opening a new data file much faster. 
The hard drive can immediately seek to that file and consecutively read the whole file into pre-cleaned page frames. 
Without pre-cleaning, the hard drive is forced to seek back and forth between writing a dirty page frame to disk, and then reading the next page of the file into that frame.
Most programs reach a steady state in their demand for memory locality both in terms of instructions fetched and data being accessed. 
This steady state is usually much less than the total memory required by the program. 
This steady state is sometimes referred to as the working set: the set of memory pages that are most frequently accessed.
Virtual memory systems work most efficiently when the ratio of the working set to the total number of pages that can be stored in RAM is low enough that the time spent resolving page faults is not a dominant factor in the workload's performance. 
A program that works with huge data structures will sometimes require a working set that is too large to be efficiently managed by the page system resulting in constant page faults that drastically slow down the system. 
This condition is referred to as thrashing: pages are swapped out and then accessed causing frequent faults.
An interesting characteristic of thrashing is that as the working set grows, there is very little increase in the number of faults until the critical point (when faults go up dramatically and the majority of the system's processing power is spent on handling them).
An extreme example of this sort of situation occurred on the IBM System/360 Model 67, and IBM System/370 through z/Architecture, mainframe computers.
 An execute instruction that crosses a page boundary could point to a move instruction that also crosses a page boundary, and the move instruction could move data from a source that crosses a page boundary to a target of data that also crosses a page boundary. 
The total number of pages thus being used by this particular instruction is eight, and all eight pages must be present in memory at the same time. 
If the operating system will allocate less than eight pages of actual memory in this example, when it attempts to swap out some part of the instruction or data to bring in the remainder, the instruction will again page fault, and it will thrash on every attempt to restart the failing instruction.
To decrease excessive paging, and thus possibly resolve thrashing problem, a user can do any of the following: Increase the amount of RAM in the computer (generally the best long-term solution).
Decrease the number of programs being concurrently run on the computer.
The term thrashing is also used in contexts other than virtual memory systems, for example to describe cache issues in computing or silly window syndrome in networking.
In multi-programming or in multi-user environment it is common for many users to be executing the same program. 
If individual copies of these programs were given to each user, much of the primary storage would be wasted. The solution is to share those pages that can be shared.
Sharing must be carefully controlled to prevent one process from modifying data that another process is accessing. 
In most systems the shared programs are divided into separate pages i.e. coding and data are kept separate. 
This is achieved by having page map table entries of different processes point to the same page frame, that page frame is shared among those processes.
Process management is an integral part of any modern-day operating system (OS). 
The OS must allocate resources to processes, enable processes to share and exchange information, protect the resources of each process from other processes and enable synchronisation among processes. 
To meet these requirements, the OS must maintain a data structure for each process, which describes the state and resource ownership of that process, and which enables the OS to exert control over each process.
In many modern operating systems, there can be more than one instance of a program loaded in memory at the same time. 
For example, more than one user could be executing the same program, each user having separate copies of the program loaded into memory. 
With some programs, it is possible to have one copy loaded into memory, while several users have shared access to it so that they each can execute the same program-code. 
Such a program is said to be re-entrant. 
The processor at any instant can only be executing one instruction from one program but several processes can be sustained over a period of time by assigning each process to the processor at intervals while the remainder become temporarily inactive. 
A number of processes being executed over a period of time instead of at the same time is called concurrent execution.
A multiprogramming or multitasking OS is a system executing many processes concurrently. 
Multiprogramming requires that the processor be allocated to each process for a period of time and de-allocated at an appropriate moment. 
If the processor is de-allocated during the execution of a process, it must be done in such a way that it can be restarted later as easily as possible.
There are two possible ways for an OS to regain control of the processor during a programâ€™s execution in order for the OS to perform de-allocation or allocation.
The process issues a system call (sometimes called a software interrupt); for example, an I/O request occurs requesting to access a file on hard disk.
A hardware interrupt occurs; for example, a key was pressed on the keyboard, or a timer runs out (used in pre-emptive multitasking).
The stopping of one process and starting (or restarting) of another process is called a context switch or context change. 
In many modern operating systems, processes can consist of many sub-processes. This introduces the concept of a thread. 
A thread may be viewed as a sub-process; that is, a separate, independent sequence of execution within the code of one process. 
Threads are becoming increasingly important in the design of distributed and clientâ€“server systems and in software run on multi-processor systems.
Operating systems need some ways to create processes. 
In a very simple system designed for running only a single application (e.g., the controller in a microwave oven), it may be possible to have all the processes that will ever be needed be present when the system comes up. 
In general-purpose systems, however, some way is needed to create and terminate processes as needed during operation.

There are four principal events that cause a process to be created: System initialization, Execution of process creation system call by a running process, A user request to create a new process, Initiation of a batch job.
When an operating system is booted, typically several processes are created. 
Some of these are foreground processes, that interacts with a (human) user and perform work for them. Other are background processes, which are not associated with particular users, but instead have some specific function. 
For example, one background process may be designed to accept incoming e-mails, sleeping most of the day but suddenly springing to life when an incoming e-mail arrives. 
Another background process may be designed to accept an incoming request for web pages hosted on the machine, waking up when a request arrives to service that request.
Process creation in UNIX and Linux are done through fork() or clone() system calls. There are several steps involved in process creation. 
The first step is the validation of whether the parent process has sufficient authorization to create a process. 
Upon successful validation, the parent process is copied almost entirely, with changes only to the unique process id, parent process, and user-space. 
Each new process gets its own user space.
Contemporary processors incorporate a mode bit to define the execution capability of a program in the processor. 
This bit can be set to kernel mode or user mode. 
Kernel mode is also commonly referred to as supervisor mode, monitor mode or ring 0. 
In kernel mode, the processor can execute every instruction in its hardware repertoire, whereas in user mode, it can only execute a subset of the instructions. 
Instructions that can be executed only in kernel mode are called kernel, privileged or protected instructions to distinguish them from the user mode instructions. 
For example, I/O instructions are privileged. 
So, if an application program executes in user mode, it cannot perform its own I/O. Instead, it must request the OS to perform I/O on its behalf.
 The system may logically extend the mode bit to define areas of memory to be used when the processor is in kernel mode versus user mode. 
If the mode bit is set to kernel mode, the process executing in the processor can access either the kernel or user partition of the memory. 
However, if user mode is set, the process can reference only the user memory space. 
We frequently refer to two classes of memory user space and system space (or kernel, supervisor or protected space). 
In general, the mode bit extends the operating system's protection rights. 
The mode bit is set by the user mode trap instruction, also called a supervisor call instruction. 
This instruction sets the mode bit, and branches to a fixed location in the system space. 
Since only system code is loaded in the system space, only system code can be invoked via a trap. 
When the OS has completed the supervisor call, it resets the mode bit to user mode prior to the return.
There are two techniques by which a program executing in user mode can request the kernel's services: System call, Message passing.
Operating systems are designed with one or the other of these two facilities, but not both. 
First, assume that a user process wishes to invoke a particular target system function. 
For the system call approach, the user process uses the trap instruction.
The idea is that the system call should appear to be an ordinary procedure call to the application program; the OS provides a library of user functions with names corresponding to each actual system call. 
Each of these stub functions contains a trap to the OS function. 
When the application program calls the stub, it executes the trap instruction, which switches the CPU to kernel mode, and then branches (indirectly through an OS table), to the entry point of the function which is to be invoked. 
When the function completes, it switches the processor to user mode and then returns control to the user process; thus simulating a normal procedure return.
For many people it is a machine that imitates a humanâ€”like the androids in Star Wars, Terminator and Star Trek: The Next Generation. 
However much these robots capture our imagination, such robots still only inhabit Science Fiction. 
People still haven't been able to give a robot enough 'common sense' to reliably interact with a dynamic world. 
However, Rodney Brooks and his team at MIT Artificial Intelligence Lab are working on creating such humanoid robots.
The type of robots that you will encounter most frequently are robots that do work that is too dangerous, boring, onerous, or just plain nasty. 
Most of the robots in the world are of this type. 
They can be found in auto, medical, manufacturing and space industries. 
In fact, there are over a million of these type of robots working for us today.
Some robots like the Mars Rover Sojourner and the upcoming Mars Exploration Rover, or the underwater robot Caribou help us learn about places that are too dangerous for us to go. 
While other types of robots are just plain fun for kids of all ages. 
Popular toys such as Teckno, Polly or AIBO ERS-220 seem to hit the store shelves every year around Christmas time.
And as much fun as robots are to play with, robots are even much more fun to build. 
In Being Digital, Nicholas Negroponte tells a wonderful story about an eight year old, pressed during a televised premier of MITMedia Lab's LEGO/Logo work at Hennigan School. 
A zealous anchor, looking for a cute sound bite, kept asking the child if he was having fun playing with LEGO/Logo. 
Clearly exasperated, but not wishing to offend, the child first tried to put her off. 
After her third attempt to get him to talk about fun, the child, sweating under the hot television lights, plaintively looked into the camera and answered, "Yes it is fun, but it's hard fun."
As strange as it might seem, there really is no standard definition for a robot. 
However, there are some essential characteristics that a robot must have and this might help you to decide what is and what is not a robot. 
It will also help you to decide what features you will need to build into a machine before it can count as a robot.
Sensing First of all your robot would have to be able to sense its surroundings. 
It would do this in ways that are not unsimilar to the way that you sense your surroundings. 
Giving your robot sensors: light sensors (eyes), touch and pressure sensors (hands), chemical sensors (nose), hearing and sonar sensors (ears), and taste sensors (tongue) will give your robot awareness of its environment.
Movement A robot needs to be able to move around its environment. Whether rolling on wheels, walking on legs or propelling by thrusters a robot needs to be able to move. 
To count as a robot either the whole robot moves, like the Sojourner or just parts of the robot moves, like the Canada Arm.
Energy A robot needs to be able to power itself. 
A robot might be solar powered, electrically powered, battery powered. 
The way your robot gets its energy will depend on what your robot needs to do.
Intelligence A robot needs some kind of "smarts." 
This is where programming enters the pictures.
A programmer is the person who gives the robot its 'smarts.' 
The robot will have to have some way to receive the program so that it knows what it is to do.
Well it is a system that contains sensors, control systems, manipulators, power supplies and software all working together to perform a task. 
Designing, building, programming and testing a robots is a combination of physics, mechanical engineering, electrical engineering, structural engineering, mathematics and computing. 
In some cases biology, medicine, chemistry might also be involved. 
A study of robotics means that students are actively engaged with all of these disciplines in a deeply problem-posing problem-solving environment.
Jim Fuller provides numerous suggestions for teachers working with robotics. 
He has developed a Robotics Kit for teachers and students who are looking for cheaper alternatives to the commercially available kits. 
Although he does not make these plans available through his website, you can email him to obtain it.
"What is the Robotics Kit? -The Robotics Kit is the culmination of years of work and experimentation. 
In the past, schools have been offered site rights to the information needed to construct Real World interfaces for IBM Compatibles and all of the associated reference texts and software, but not the actual hardware. 
By building your own interfaces you could save THOUSANDS over commercial alternatives. 
The problem was that while most Computing teachers were happy enough to get involved with the programming and application side, many were hesitant to do the required 'hands-on' soldering. 
If you are one of those people who would love to get your students involved with inexpensive interfacing (eg for Robotics and the new Year 12 Industry Information Technology), but want a fully assembled and tested interface, this is for you ...
Robotics was designed to introduce the science behind the design and operation of robots. 
Define a robot as a machine that gathers information about its environment (senses) and uses that information (thinks) to follow instructions to do work (acts).
Recognize the advantages and limitations of robots by comparing how robots and humans sense, think, and act by exploring uses of robots in manufacturing, research and everyday settings.
Understand your connection with technology and create an excitement about science and math that will prepare you for a workplace in which computer, robotics, and automation are common and essential.
Robotics brings together several very different engineering areas and skills. 
There is metalworking for the body. 
There is mechanics for mounting the wheels on the axles, connecting them to the motors and keeping the body in balance. 
You need electronics to power the motors and connect the sensors to the controllers. 
At last you need the software to understand the sensors and drive the robot around.
This book tries to cover all the key areas of robotics as a hobby. 
When possible examples from industrial robots will be addressed too.
You'll notice very few "exact" values in these texts. 
Instead, vague terms like "small", "heavy" and "light" will be used. 
This is because most of the time you'll have a lot of freedom in picking these values, and all robot projects are unique in available materials.
Most robots have an electronic "brain" (part that uses sensor input to determine actuator output). 
This can be either a simple electronic circuit, a Âµcontroller or a PC. 
This is not the only way to build a robot "brain".
One of the first robots was designed by Da Vinci, this was a small vehicle powered by wind-up springs and it was "programed" by changing gears. 
Although possible this kind of robot brains are very hard to design and build.
If you need to automate some part of a plant where explosive gases are used it might be too dangerous to use electronic circuits. 
Instead pneumatic circuits are used. It's possible to construct logical circuits, timers, sequences,... using pneumatic valves and cylinders.
Robotics is the science or technology of designing, building and using robots. 
Robotics is the use of computer-controlled robots to perform manual tasks. 
Robots are commonly used by the military and businesses to complete tasks that are dangerous for people, such as defusing bombs, exploring shipwrecks, and mines. 
They are also used to perform monotonous jobs, such as on an assembly line. 
There are personal or service robots to assist with personal tasks. 
There are robots for the use of entertainment. 
The appearance of robots depends on their function or purpose. 
Today, robotics is a rapidly growing field. 
Robotics research is continuing to make smarter and more capable robots. 
NASA researchers have developed a way to make a crew of robots work together to grasp, lift, and more heavy loads across rough, varied terrain. 
The software allows the robots to "share a brain" so that each robot knows what the rest are doing. This enables the robots to work together the develop plans, such as how to maneuver around a rock or other obstacle the crew may encounter.
The Defence Advance Research Project Agency, which was known to be the agency which created the ARPANET which is the precursor to the internet that we enjoy today, creates and manages projects that cover a wide variety of aspects usually within the military industry. 
Recently, in a partnership with a company called Boston Dynamics, they have created a four legged robot that is able to sprint up to speeds of 28.3 miles per hour on a treadmill in their laboratory. 
This robot is also able to regain it's balance if it is facing a dynamic terrain or is pushed from the side. The reason they created this project is for emergency response, assistance, and other missions such as carrying cargo.
Our service men and women often carry heavy combat loads which increases the potential for injuries. Lockheed Martin has come up with this new technology called HULC exoskeleton back in 2010. 
This design was much heavier and the battery power would die down after about an hour. 
When carrying such heavy loads this weight is transferred to the ground through powered titanium legs without loss of mobility. 
The HULC is a completely un-tethered, hydraulic-powered anthropomorphic exoskeleton. 
This can provide users with the ability to carry loads of up to 200 pounds for longer periods of time and over all types of terrain. 
This newer design can go up to 8 hours and lasts for days on a single charge if you are just standing guard. It is flexible enough for deep squats, crawls and upper-body lifting. 
There is a micro-computer attached within the suit that moves with the individual. 
It can operate on battery power for an extended amount of time and features a power-saving design, which means if the power is low it will continue to support the carrier and their loads. It can also support loads with or without power. 
Lockheed Martin is also exploring exoskeleton designs for industrial use and a wider variety of military mission specific applications. 
The HULC is now being revamped to be smaller, lighter, and more energy-efficient, including an unloaded machine gun on a pivoting mechanical arm. 
HULC adds an artificial external spine, hips, and legs to a soldierâ€™s flesh and bones. 
They are also working on a fuel cell type which would last about 72 hours in the harshest conditions. It is being called cyborg combat.
Googleâ€™s new project is working on cars that use artificial intelligence to drive themselves without the need of any human intervention. 
These vehicles can sense anything near the vehicle, they mimic the decision a driver makes, and they are programmed with road maps and speed limit information.
 Not only can the AI do everything that a human can, it can do it better. 
Robot drivers react faster than humans, have 360-degree perception, and do not get distracted, sleepy or intoxicated. 
These new vehicles could make driving safer and also be better for the environment. 
They can optimize the amount of fuel used and if accidents are no longer a concern, they could be built lighter thus requiring even less fuel. 
Also, if the cars do not require a driver, they could be summoned to different locations only when needed and allow for people to share vehicles, reducing the amount of vehicles on the road and the need for parking spaces. 
If this sounds too farfetched, consider the fact that Google has already drove the AI vehicles on the road and through city traffic. 
The vehicles performed as expected and the only accident was when one of the AI cars was rear ended by a human driven vehicle while waiting at a stop light.
Personal robots are designed for individual use to assist with personal tasks. 
In addition, some personal robots are also designed for entertainment. 
â€œThey typically use sensors, cameras, microphones, and other technologies to input data about their current surroundings, and then interact with people (P. 483).â€ 
Toy companies design robotic toys for children, and others are designed to help individuals with house work, such as vacuum cleaners and lawn mowers. 
One of the common robots used for vacuuming is the iRobot RoombaÂ®.
 â€œHousehold robots that can assist individuals with more complex tasks, such as putting away the dishes or picking up toys before vacuuming the living room, are a little further in the future, when robot technology improves to allow for better navigation and improved physical manipulation , and as prices come down (P. 483).â€ 
In the future, robots will become more human like (humanoid). 
Robot technology has also made its way into businesses. 
Robots can be used to search for hazardous materials or gas leaks, all of which could potentially be dangerous for humans.
Industries that use robots for these dangerous tasks include the coal mining industry. 
These robots can be used to mine coal or even to search for people in collapsed mines. 
They can also be used in assembly lines to speed up production and ensure that the product is consistently up to standards.
 Another area where robots are frequently used is in the medical field. 
There are robots that help facilitate videoconferencing. 
A doctor can control the robot to make his rounds, called "virtual rounds", without ever leaving his office. The medical field also uses robots in surgeries. 
In these robot-assisted surgeries, surgeons control the actions of the robot, allowing for more accurate results, and a steadier hand.
Robots are often used by the military as well. 
They can be used to help navigate through caves, trails, buildings and other places to see if they are safe enough for the soldiers to enter into. 
They are also useful in locating and disposing of bombs, landmines, and other kinds of explosive devices. 
Water-based robots can perform underwater surveillance.
Right now the military robots are controlled by the soldiers but researchers are currently working on autonomous robots that will be able to navigate by themselves. 
They will be able work on their own without any directions from a human operator, this will allow them to join soldiers into combat.
Although there are many benefits in creating robots to benefit society, some people may have concerns with the idea of having robots around. 
Society has many concerns with the implications robots may cause, some may think that robots may come to close to realistic that may potentially harm humanity. 
Other concerns that may arise with the issue of having robots take over human jobs.
 Another disadvantage of having robots is that they may be expensive to build and maintain. 
Robots also have limited duties, so they are only able to perform specific task and are not able to think for themselves. 
Timing might also be a problem; at times of emergencies some robots may not respond properly.
Robotics can be described as the current pinnacle of technical development. 
Robotics is a confluence science using the continuing advancements of mechanical engineering, material science, sensor fabrication, manufacturing techniques, and advanced algorithms. 
The study and practice of robotics will expose a dabbler or professional to hundreds of different avenues of study. For some, the romanticism of robotics brings forth an almost magical curiosity of the world leading to creation of amazing machines. 
A journey of a lifetime awaits in robotics.
Robotics can be defined as the science or study of the technology primarily associated with the design, fabrication, theory, and application of robots.
 While other fields contribute the mathematics, the techniques, and the components, robotics creates the magical end product. 
The practical applications of robots drive development of robotics and drive advancements in other sciences in turn. Crafters and researchers in robotics study more than just robotics.
The promise of robotics is easy to describe but hard for the mind to grasp. 
Robots hold the promise of moving and transforming materials with the same elan and ease as a computer program transforms data. 
Today, robots mine minerals, assemble semi-processed materials into automobile components, and assemble those components into automobiles. 
On the immediate horizon are self-driving cars, robotics to handle household chores, and assemble specialized machines on demand. 
It is not unreasonable to imagine robots that are given some task, such as reclaim desert into photovoltaic cells and arable land, and left to make their own way. 
Then the promise of robotics exceeds the minds grasp.
In summary, robotics is the field related to science and technology primarily related to robotics. 
It stands tall by standing the accomplishments of many other fields of study.
Robot used in English describes any construct that automates some behavior. 
For example, a garage door opener automates the behavior of opening a door. 
A garage door opener has a sensor to detect the signal from the remote control, actuators to open the door, and a control system to stop turn off the motors and lights when the garage is fully closed. 
In practice, this type of a machine is better described as a Mechatronic device, and is a subset of the more interesting robots that include autonomy or resourcefulness. 
This book will consider mechatronic devices to be degenerate robots.
A Mechatronic Device is a degenerate robot with these components:
Sensors, which detect the state of the environment.
Actuators, which modify the state of the environment.
A Control System, which controls the actuators based on the environment as depicted by the sensors.
A Robot is a mechatronic device which also includes resourcefulness or autonomy. 
A device with autonomy does its thing "on its own" without a human directly guiding it moment-by-moment. 
Some authors would contend that all mechatronic devices are robots, and that this book's restriction on robot entails only specialized software.
Various types of robots are usually classified by their capabilities. 
Two examples will be used to capture most of what we see as a "robot".
Machine Pet: A machine, capable of moving in some way, that can sense its surroundings and can act on what it senses autonomously. 
Most of these robots have no real useful purpose, other than to entertain and challenge. 
These are also commonly used for experimenting with sensors, artificial intelligence, actuators and more. 
Most of this book covers this type of robot.
Autonomous Machine: A machine with sensors and actuators that can do some sort of work "on its own".
This includes things like robotic lawnmowers and vacuum cleaners, and also self-operating construction machines such as CNC cutters. 
Most industrial and commercial robots fall in this category.
What isn't considered a "robot" in this book? Pretty much everything you see on RobotWars; those are remote-controlled vehicles without any form of autonomy, no sensors, and just enough of a control system to drive the actuators. 
These devices use many of the same mechanical technologies described in this book, but not the advanced controls.
In short: If it has autonomy it's a robot (in this book). If it's remote controlled, it isn't.
Another example is the TU Delft Flame.
Perhaps the most promising approach utilizes passive dynamics where the momentum of swinging limbs is used for greater efficiency.
It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. 
Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. 
This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.
A modern passenger airliner is essentially a flying robot, with two humans to manage it.
The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing.
Other flying robots are uninhabited, and are known as unmanned aerial vehicles (UAVs). 
They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. 
Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. 
Other flying robots include cruise missiles, the Entomopter, and theEpson micro helicopter robot. 
Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, propelled by paddles, and guided by sonar.
Robotics is the branch of mechanical engineering, electrical engineering and computer science that deals with the design, construction, operation, and application of robots, as well as computer systems for their control, sensory feedback, and information processing.
These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, and/or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics.
The concept of creating machines that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century.
Throughout history, robotics has been often seen to mimic human behavior, and often manage tasks in a similar fashion. 
Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes, whether domestically, commercially, ormilitarily. 
Many robots do jobs that are hazardous to people such as defusing bombs, mines and exploring shipwrecks.
The word robotics was derived from the word robot, which was introduced to the public by Czech writer Karel ÄŒapek in his play R.U.R. (Rossum's Universal Robots), which was published in 1920.
The word robot comes from the Slavic word robota, which means labour. 
The play begins in a factory that makes artificial people called robots, creatures who can be mistaken for humans â€“ similar to the modern ideas of androids. Karel ÄŒapek himself did not coin the word. 
He wrote a short letter in reference to an etymology in theOxford English Dictionary in which he named his brother Josef ÄŒapek as its actual originator.
According to the Oxford English Dictionary, the word robotics was first used in print by Isaac Asimov, in his science fiction short story "Liar!", published in May 1941 in Astounding Science Fiction. 
Asimov was unaware that he was coining the term; since the science and technology of electrical devices is electronics, he assumed robotics already referred to the science and technology of robots. 
In some of Asimov's other works, he states that the first use of the word robotics was in his short story Runaround (Astounding Science Fiction, March 1942).
However, the original publication of "Liar!" predates that of "Runaround" by ten months, so the former is generally cited as the word's origin.
In 1927 the Maschinenmensch ("machine-human") gynoid humanoid robot (also called "Parody", "Futura", "Robotrix", or the "Maria impersonator"), the first depiction of a robot ever to appear on film, was played by German actress Brigitte Helm in Fritz Lang's film Metropolis.
In 1942 the science fiction writer Isaac Asimov formulated his Three Laws of Robotics.
In 1948 Norbert Wiener formulated the principles of cybernetics, the basis of practical robotics.
Fully autonomous robots only appeared in the second half of the 20th century.
 The first digitally operated and programmable robot, the Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them. 
Commercial and industrial robots are widespread today and used to perform jobs more cheaply, or more accurately and reliably, than humans. 
They are also employed in jobs which are too dirty, dangerous, or dull to be suitable for humans. 
Robots are widely used in manufacturing, assembly, packing and packaging, transport, earth and space exploration, surgery, weaponry, laboratory research, safety, and the mass production of consumer and industrial goods.[6]
There are many types of robots; they are used in many different environments and for many different uses, although being very diverse in application and form they all share three basic similarities when it comes to their construction:
Robots all have some kind of mechanical construction, a frame, form or shape designed to achieve a particular task.
 For example, a robot designed to travel across heavy dirt or mud, might use caterpillar tracks. 
The mechanical aspect is mostly the creator's solution to completing the assigned task and dealing with the physics of the environment around it.
Robots have electrical components which power and control the machinery. 
For example, the robot with caterpillar tracks would need some kind of power to move the tracker treads. 
That power comes in the form of electricity, which will have to travel through a wire and originate from a battery, a basic electrical circuit. 
Even gas powered machines that get their power mainly from gas still require an electrical current to start the gas using process which is why most gas powered machines like cars, have batteries. 
The electrical aspect of robots is used for movement (through motors), sensing (where electrical signals are used to measure things like heat, sound, position, and energy status) and operation (robots need some level of electrical energy supplied to their motors and sensors in order to activate and perform basic operations)
All robots contain some level of computer programming code. 
A program is how a robot decides when or how to do something. 
In the caterpillar track example, a robot that needs to move across a muddy road may have the correct mechanical construction, and receive the correct amount of power from its battery, but would not go anywhere without a program telling it to move. 
Programs are the core essence of a robot, it could have excellent mechanical and electrical construction, but if its program is poorly constructed its performance will be very poor or it may not perform at all. 
There are three different types of robotic programs: remote control, artificial intelligence and hybrid.
 A robot with remote control programing has a preexisting set of commands that it will only perform if and when it receives a signal from a control source, typically a human being with a remote control. 
It is perhaps more appropriate to view devices controlled primarily by human commands as falling in the discipline of automation rather than robotics.
 Robots that use artificial intelligence interact with their environment on their own without a control source, and can determine reactions to objects and problems they encounter using their preexisting programming. 
Hybrid is a form of programming that incorporates both AI and RC functions.
At present mostly (lead-acid) batteries are used as a power source. 
Many different types of batteries can be used as a power source for robots. 
They range from lead acid batteries which are safe and have relatively long shelf lives but are rather heavy to silver cadmium batteries that are much smaller in volume and are currently much more expensive. 
Designing a battery powered robot needs to take into account factors such as safety, cycle lifetime and weight. Generators, often some type of internal combustion engine, can also be used. 
However, such designs are often mechanically complex and need fuel, require heat dissipation and are relatively heavy. 
A tether connecting the robot to a power supply would remove the power supply from the robot entirely. 
This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. 
However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.
faeces (human, animal); may be interesting in a military context as faeces of small combat groups may be reused for the energy requirements of the robot assistant (see DEKA's project Slingshot Stirling engine on how the system would operate)
Actuators are like the "muscles" of a robot, the parts which convert stored energy into movement. 
By far the most popular actuators are electric motors that spin a wheel or gear, and linear actuators that control industrial robots in factories. 
But there are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.
The vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. 
These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.
Various types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. 
They are typically powered by compressed air (pneumatic actuator) or an oil (hydraulic actuator).
A spring can be designed as part of the motor actuator, to allow improved force control. It has been used in various robots, particularly walking humanoid robots.
Pneumatic artificial muscles, also known as air muscles, are special tubes that contract (typically up to 40%) when air is forced inside them. 
They have been used for some robot applications.
Muscle wire, also known as shape memory alloy, NitinolÂ® or FlexinolÂ® wire, is a material that contracts slightly (typically under 5%) when electricity runs through it. 
They have been used for some small robot applications.
EAPs or EPAMs are a new plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,and to allow new robots to float, fly, swim or walk.
Recent alternatives to DC motors are piezo motors or ultrasonic motors. 
These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. 
There are different mechanisms of operation; one type uses the vibration of the piezo elements to walk the motor in a circle or a straight line.
 Another type uses the piezo elements to cause a nut to vibrate and drive a screw. 
The advantages of these motors are nanometer resolution, speed, and available force for their size. 
These motors are already available commercially, and being used on some robots.
Elastic nanotubes are a promising artificial muscle technology in early-stage experimental development. 
The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm3 for metal nanotubes. 
Human biceps could be replaced with an 8 mm diameter wire of this material. 
Such compact "muscle" might allow future robots to outrun and outjump humans.
Sensors allow robots to receive information about a certain measurement of the environment, or internal components. 
This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. 
They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real time information of the task it is performing.
Current robotic and prosthetic hands receive far less tactile information than the human hand. 
Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips.
The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. 
When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. 
The researchers expect that an important function of such artificial fingertips will be adjusting robotic grip on held objects.
Scientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real oneâ€”allowing patients to write with it, type on a keyboard, play piano and perform other fine movements. 
The prosthesis has sensors which enable the patient to sense real feeling in its fingertips.
Computer vision is the science and technology of machines that see. 
As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. 
The image data can take many forms, such as video sequences and views from cameras.
In most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.
Computer vision systems rely on image sensors which detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. 
The sensors are designed using solid-state physics. 
The process by which light propagates and reflects off surfaces is explained using optics. 
Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. 
Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. 
Like human eyes, robots' "eyes" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.
There is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. 
Also, some of the learning-based methods developed within computer vision have their background in biology.
Other common forms of sensing in robotics use lidar, radar and sonar.
Robots need to manipulate objects; pick up, modify, destroy, or otherwise have an effect. 
Thus the "hands" of a robot are often referred to as end effectors,while the "arm" is referred to as a manipulator.
Most robot arms have replaceable effectors, each allowing them to perform some small range of tasks. 
Some have a fixed manipulator which cannot be replaced, while a few have one very general purpose manipulator, for example a humanoid hand.
One of the most common effectors is the gripper.
 In its simplest manifestation it consists of just two fingers which can open and close to pick up and let go of a range of small objects. 
Fingers can for example be made of a chain with a metal wire run through it.
Hands that resemble and work more like a human hand include the Shadow Hand, the Robonaut hand,ands that are of a mid-level complexity include the Delft hand.
Mechanical grippers can come in various types, including friction and encompassing jaws. 
Friction jaws use all the force of the gripper to hold the object in place using friction. 
Encompassing jaws cradle the object in place, using less friction.
Vacuum grippers are very simple astrictive devices, but can hold very large loads provided the prehension surface is smooth enough to ensure suction.
Pick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum grippers.
Some advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS, and the Schunk hand.
These are highly dexterous manipulators, with as many as 20 degrees of freedom and hundreds of tactile sensors.
For simplicity most mobile robots have four wheels or a number of continuous tracks. 
Some researchers have tried to create more complex wheeled robots with only one or two wheels. 
These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four wheeled robot would not be able to.
Balancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum.
Many different balancing robots have been designed.
While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). 
An example of this use has been as NASA'sRobonaut that has been mounted on a Segway.
A one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel.
Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University's "Ballbot" that is the approximate height and width of a person, and Tohoku Gakuin University's "BallIP".
Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.
Several attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball,or by rotating the outer shells of the sphere.
These have also been referred to as an orb bot or a ball bot.
Using six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.
Tank tracks provide even more traction than a six-wheeled robot.
Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor and military robots, where the robot must drive on very rough terrain. 
However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot "Urbie".
Walking is a difficult and dynamic problem to solve. 
Several robots have been made which can walk reliably on two legs, however none have yet been made which are as robust as a human. 
There has been much study on human inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University. 
Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct.
Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. 
Hybrids too have been proposed in movies such as I, Robot, where they walk on 2 legs and switch to 4 (arms+legs) when going to a sprint. 
Typically, robots on 2 legs can walk well on flat floors and can occasionally walk up stairs. 
None can walk over rocky, uneven terrain. Some of the methods which have been tried are:
The Zero Moment Point (ZMP) is the algorithm used by robots such as Honda's ASIMO. 
The robot's onboard computer tries to keep the total inertial forces (the combination ofEarth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). 
In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over).
However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory.
ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below).
However, it still requires a smooth surface to walk on.
Several robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. 
Initially, a robot with only one leg, and a very small foot, could stay upright simply by hopping.
The movement is the same as that of a person on a pogo stick.
As the robot falls to one side, it would jump slightly in that direction, in order to catch itself.
Soon, the algorithm was generalised to two and four legs. 
A bipedal robot was demonstrated running and even performing somersaults.
A quadrupedwas also demonstrated which could trot, run, pace, and bound.
For a full list of these robots, see the MIT Leg Lab Robots page.
A more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability.
This technique was recently demonstrated by Anybots' Dexter Robot,which is so stable, it can even jump.
Several snake robots have been successfully developed. 
Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings.
The Japanese ACM-R5 snake robot[73] can even navigate both on land and in water.
A small number of skating robots have been developed, one of which is a multi-mode walking and skating device.
It has four legs, with unpowered wheels, which can either step or roll.
Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.
Several different approaches have been used to develop robots that have the ability to climb vertical surfaces. 
One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage.
 An example of this is Capuchin, built by Dr. Ruixiang Zhang at Stanford University, California. 
Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass.
 Examples of this approach include Wallbot and Stickybot.
 China's "Technology Daily" November 15, 2008 reported New Concept Aircraft (ZHUHAI) Co., Ltd.
 Dr. Li Hiu Yeung and his research group have recently successfully developed the bionic gecko robot "Speedy Freelander". 
According to Dr. Li introduction, this gecko robot can rapidly climbing up and down in a variety of building walls, ground and vertical wall fissure or walking upside down on the ceiling, it is able to adapt on smooth glass, rough or sticky dust walls as well as the various surface of metallic materials and also can automatically identify obstacles, circumvent the bypass and flexible and realistic movements. 
Its flexibility and speed are comparable to the natural gecko. A third approach is to mimic the motion of a snake climbing a pole.
It is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%.
Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. 
Therefore, many researchers studying underwater robots would like to copy this type of locomotion.
Notable examples are the Essex University Computer Science Robotic Fish,and the Robot Tuna built by the Institute of Field Robotics, to analyze and mathematically model thunniform motion.
The Aqua Penguin,designed and built by Festo of Germany, copies the streamlined shape and propulsion by front "flippers" of penguins.
 Festo have also built the Aqua Ray and Aqua Jelly, which emulate the locomotion of manta ray, and jellyfish, respectively.
Sailboat robots have also been developed in order to make measurements at the surface of the ocean.
 A typical sailboat robot is Vaimos[85] built by IFREMER and ENSTA-Bretagne. 
Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail).
 If the robot is equipped with solar panels, the robot could theoretically navigate forever. 
The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot.
Though a significant percentage of robots in commission today are either human controlled, or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. 
These robots require some combination of navigation hardware and software in order to traverse their environment. 
In particular unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. 
Some highly advanced robots such as ASIMO, and MeinÃ¼ robot have particularly good robot navigation hardware and software. 
Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information. 
Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints.
The state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. 
If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. 
The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive.
 Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. 
Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. 
It will probably be a long time before robots interact as naturally as the fictional C-3PO.
Interpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability ofspeech.
The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. 
It becomes even harder when the speaker has a different accent.
Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first "voice input system" which recognized "ten digits spoken by a single user with 100% accuracy" in 1952.
Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.
Other hurdles exist when allowing the robot to use voice for interacting with humans. 
For social reasons, synthetic voice proves suboptimal as a communication medium,making it necessary to develop the emotional component of robotic voice through various techniques.
One can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. 
In both of these cases, making hand gestureswould aid the verbal descriptions.
 In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation.
 In the second case, the robot police officer would gesture to indicate "down the road, then turn right". 
It is likely that gestures will make up a part of the interaction between humans and robots.
 A great many systems have been developed to recognize human hand gestures.
Facial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. 
Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos).
 The coating and servos are built on a metal skull. 
A robot should know how to approach a human, judging by their facial expression and body language. 
Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. 
Likewise, robots like Kismet and the more recent addition, Nexi can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.
Artificial emotions can also be generated, composed of a sequence of facial expressions and/or gestures. 
As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. 
To simplify this programming in the movie, presets were created together with a special software program. 
This decreased the amount of time needed to make the film. 
These presets could possibly be transferred for use in real-life robots.
Many of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future.
Nevertheless, researchers are trying to create robots which appear to have a personality: i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. 
One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.
The Socially Intelligent Machines Lab of the Georgia Institute of Technology researches new concepts of guided teaching interaction with robots. 
Aim of the projects is a social robot learns task goals from human demonstrations without prior knowledge of high-level concepts. 
These new concepts are grounded from low-level continuous sensor data through unsupervised learning, and task goals are subsequently learned using a Bayesian approach. 
These concepts can be used to transfer knowledge to future tasks, resulting in faster learning of those tasks. 
The results re demonstrated by the robot Curi who can easily cook pasta.
The mechanical structure of a robot must be controlled to perform tasks.
 The control of a robot involves three distinct phases â€“ perception, processing, and action (robotic paradigms). 
Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). 
This information is then processed to be stored or transmitted, and to calculate the appropriate signals to the actuators (motors) which move the mechanical.
The processing phase can range in complexity.
 At a reactive level, it may translate raw sensor information directly into actuator commands.
 Sensor fusion may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. 
An immediate task (such as moving the gripper in a certain direction) is inferred from these estimates. Techniques from control theory convert the task into commands that drive the actuators.
At longer time scales or with more sophisticated tasks, the robot may need to build and reason with a "cognitive" model. 
Cognitive models try to represent the robot, the world, and how they interact. 
Pattern recognition and computer vision can be used to track objects.
 Mappingtechniques can be used to build maps of the world. 
Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. 
For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.
Control systems may also have varying levels of autonomy.
Direct interaction is used for haptic or tele-operated devices, and the human has nearly complete control over the robot's motion.
Operator-assist modes have the operator commanding medium-to-high-level tasks, with the robot automatically figuring out how to achieve them.
An autonomous robot may go for extended periods of time without human interaction. 
Higher levels of autonomy do not necessarily require more complex cognitive capabilities. 
For example, robots in assembly plants are completely autonomous, but operate in a fixed pattern.
Another classification takes into account the interaction between human control and the machine motions.
Teleoperation. A human controls each movement, each machine actuator change is specified by the operator.
Supervisory. A human specifies general moves or position changes and the machine decides specific movements of its actuators.
Task-level autonomy. The operator specifies only the task and the robot manages itself to complete it.
Full autonomy. The machine will create and complete all its tasks without human interaction.
Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them but other investigations, such as MIT's cyberflora project, are almost wholly academic.
A first particular new innovation in robot design is the opensourcing of robot-projects.
 To describe the level of advancement of a robot, the term "Generation Robots" can be used. 
This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. 
First generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. 
Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse.
 The third generation robot should have the intelligence comparable to that of a monkey. 
Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.
The second is Evolutionary Robots. 
This is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. 
In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. 
Those that perform worst are removed from the population, and replaced by a new set, which have new behaviors based on those of the winners.
 Over time the population improves, and eventually a satisfactory robot may appear. 
This happens without any direct programming of the robots by the researchers. 
Researchers use this method both to create better robots,and to explore the nature of evolution.
Because the process often requires many generations of robots to be simulated, this technique may be run entirely or mostly in simulation, then tested on real robots once the evolved algorithms are good enough.
Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.
The study of motion can be divided into kinematics and dynamics.
Direct kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known.
 Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. 
Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, andsingularity avoidance. 
Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. 
Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known.
 Direct dynamics is used incomputer simulations of the robot. 
Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end effector acceleration. 
This information can be used to improve the control algorithms of a robot.
In each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. 
To do this, criteria for "optimal" performance and ways to optimize design, structure, and control of robots must be developed and implemented.
Bionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. 
For example, the design of BionicKangaroo was based on the way kangaroos jump.
Robotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics.
 Robots have become a popular educational tool in some middle and high schools, as well as in numerous youth summer camps, raising interest in programming, artificial intelligence and robotics among students.
 First-year computer science courses at several universities now include programming of a robot in addition to traditional software engineering-based coursework.
 On the TechnionI&M faculty an educational laboratory was established in 1994 by Dr. Jacob Rubinovitz.
Universities offer bachelors, masters, and doctoral degrees in the field of robotics.
 Vocational schools offer robotics training aimed at careers in robotics.
The Robotics Certification Standards Alliance (RCSA) is an international robotics certification authority that confers various industry- and educational-related robotics certifications.
Several national summer camp programs include robotics as part of their core curriculum, including Digital Media Academy, RoboTech, and Cybercamps. 
In addition, youth summer robotics programs are frequently offered by celebrated museums such as the American Museum of Natural History and The Tech Museum of Innovation in Silicon Valley, CA, just to name a few. 
An educational robotics lab also exists at the IE & mgmnt Faculty of the Technion. 
It was created by Dr. Jacob Rubinovitz.
Many schools across the country are beginning to add robotics programs to their after school curriculum. 
Two main programs for afterschool robotics are FIRST Robotics Competition and Botball.
The Lego company began a program for children to learn and get excited about robotics at a young age.
Robotics is an essential component in many modern manufacturing environments. 
As factories increase their use of robots, the number of roboticsâ€“related jobs grow and have been observed to be steadily rising.
Virtual Reality (VR), which can be referred to as immersive multimedia or computer-simulated life, replicates an environment that simulates physical presence in places in the real world or imagined worlds. Virtual reality can recreate sensory experiences, which includevirtual taste, sight, smell, sound, and touch.
Most up to date virtual reality environments are displayed either on a computer screen or with special stereoscopic displays, and some simulations include additional sensory information and emphasise real sound through speakers or headphones targeted towards VR users. 
Some advanced, haptic, systems now include tactile information, generally known as force feedback in medical, gaming and military applications. 
Furthermore, virtual reality covers remote communication environments which provide virtual presence of users with the concepts of telepresence and telexistence or a virtual artifact (VA) either through the use of standard input devices such as a keyboard and mouse, or through multimodal devices such as a wired glove or omnidirectional treadmills. 
The simulated environment can be similar to the real world in order to create a lifelike experienceâ€”for example, in simulations for pilot or combat trainingâ€”or it differs significantly from reality, such as in VR games. 
In practice, it is currently very difficult to create a high-fidelity virtual reality experience, because of technical limitations on processing power, image resolution, and communication bandwidth. 
However, VR's proponents hope that virtual reality's enabling technologies become more powerful and cost effective over time.
Virtual reality is often used to describe a wide variety of applications commonly associated with immersive, highly visual, 3D environments. 
The development of CAD software,graphics hardware acceleration, head-mounted displays, datagloves, and miniaturization have helped popularize the notion.
 In the book The Metaphysics of Virtual Reality byMichael R. Heim, seven different concepts of virtual reality are identified: simulation, interaction, artificiality, immersion, telepresence, full-body immersion, and network communication.
 People often identify VR with head mounted displays and data suits.
The term "artificial reality", coined by Myron Krueger, has been in use since the 1970s; however, the origin of the term "virtual reality" can be traced back to the French playwright, poet, actor, and director Antonin Artaud.
 In his seminal book The Theatre and Its Double (1938), Artaud described theatre as "la rÃ©alitÃ© virtuelle", a virtual reality in which, in Erik Davis's words, "characters, objects, and images take on the phantasmagoric force of alchemy's visionary internal dramas".
Artaud claimed that the "perpetual allusion to the materials and the principle of the theater found in almost all alchemical books should be understood as the expression of an identity  existing between the world in which the characters, images, and in a general way all that constitutes the virtual reality of the theater develops, and the purely fictitious and illusory world in which the symbols of alchemy are evolved".
The term was also used in The Judas Mandala, a 1982 science-fiction novel by Damien Broderick, where the context of use is somewhat different from that defined above. 
The earliest use cited by the Oxford English Dictionary is in a 1987 article titled "Virtual reality,but the article is not about VR technology. 
The concept of virtual reality was popularized in mass media by movies such as Brainstorm and The Lawnmower Man. 
The VR research boom of the 1990s was accompanied by the non-fiction book Virtual Reality (1991) by Howard Rheingold. 
 The book served to demystify the subject, making it more accessible to less technical researchers and enthusiasts.
Multimedia: from Wagner to Virtual Reality, edited by Randall Packer and Ken Jordan and first published in 2001, explores the term and its history from an avant-garde perspective.
 Philosophical implications of the concept of VR are discussed in books including Philip Zhai's Get Real: A Philosophical Adventure in Virtual Reality (1998) and Digital Sensations: Space, Identity and Embodiment in Virtual Reality (1999), written by Ken Hillis.
1860s : Virtual reality can trace its roots to the 1860s, when 360-degree art through panoramic murals began to appear. 
An example of this would be Baldassare Peruzzi's piece titled, Sala delle Prospettive.
1930s : "Pygmalion's Spectacles" by Stanley G. Weinbaum describes a goggle-based virtual reality system with holographic recording of fictional experiences including smell and touch.
1950s : Morton Heilig wrote in the 1950s of an "Experience Theatre" that could encompass all the senses in an effective manner, thus drawing the viewer into the onscreen activity. 
He built a prototype of his vision dubbed the Sensorama in 1962, along with five short films to be displayed in it while engaging multiple senses (sight, sound, smell, and touch). 
Predating digital computing, the Sensorama was a mechanical device, which reportedly still functions today. 
Around this time, Douglas Englebart uses computer screens as both input and output devices.
1966 : Thomas A. Furness III introduces a visual flight simulator for the Air Force.
1968 : Ivan Sutherland, with the help of his student Bob Sproull, created what is widely considered to be the first virtual reality and augmented reality (AR) head-mounted display (HMD) system.
 It was primitive both in terms of user interface and realism, and the HMD to be worn by the user was so heavy it had to be suspended from the ceiling. 
The graphics comprising the virtual environment were simple wire-frame model rooms. 
The formidable appearance of the device inspired its name, The Sword of Damocles.
1977 : Also notable among the earlier hypermedia and virtual reality systems was the Aspen Movie Map, which was created at MIT in 1977. 
The program was a crude virtual simulation of Aspen, Colorado in which users could wander the streets in one of three modes: summer, winter, and polygons. 
The first two were based on photographsâ€”the researchers actually photographed every possible movement through the city's street grid in both seasonsâ€”and the third was a basic 3-D model of the city.
1980s : The term "virtual reality" was popularized by Jaron Lanier, one of the modern pioneers of the field.
 Lanier had founded the company VPL Research in 1985, which developed and built some of the seminal "goggles and gloves" systems of that decade.
1990 : Jonathan Waldern, a VR Ph.D, demonstrates "Virtuality" at the Computer Graphics 90 exhibition staged at London's Alexandra Palace.
1991 : Sega announces the Sega VR headset for arcade games and the Mega Drive console.
 It used LCD screens in the visor, stereo headphones, and inertial sensors that allowed the system to track and react to the movements of the user's head.
 1991 : Virtuality launches and goes on to become the first mass-produced, networked, multiplayer VR location-based entertainment system. 
It would be released in many countries, including a dedicated VR arcade at Embarcadero Center in San Francisco. 
Costing up to $73,000 per multi-pod Virtuality system, they featured headsets and exoskeleton gloves that gave one of the first "immersive" VR experiences.[7]
1991 : Antonio Medina, a MIT graduate and NASA scientist, designed a virtual reality system to "drive" Mars rovers from Earth in apparent real time despite the substantial delay of Mars-Earth-Mars signals. The system, termed "Computer-Simulated Teleoperation" as published by Rand, is an extension of virtual reality.
 1991 : Carolina Cruz-Neira, Daniel J. Sandin and Thomas A. DeFanti from Electronic Visualisation Lab create the first cubic immersive room, replacing goggles by a multi-projected environment where people can see their body and other people around.
1992 : Computer Gaming World predicts "Affordable VR by 1994".
1994 : Sega releases the Sega VR-1 motion simulator arcade attraction, in SegaWorld amusement arcades. It was able to track head movement and featured 3D polygon graphics in stereoscopic 3D, powered by the Sega Model 1 arcade system board.
1995 : The artist Maurice Benayoun creates the first VR artwork connecting in real time 2 continents: the "Tunnel under the Atlantic" between the Pompidou Centre in Paris and the Museum of Contemporary Art in Montreal. 
The installation included dynamic real time 3d modeling, video chat, spatialized sound and AI content management.
1995 : Virtual Boy was created by Nintendo and was released in Japan on July 21 and in North America on August 15.[13]
2001 : SAS3 or SAS Cube has been the first PC based cubic room, developed by Z-A Production (Maurice Benayoun, David Nahon), Barco, ClartÃ©, installed in Laval France in April 2001. 
The SAS library gave birth to Virtools VRPack.
2007 : Google introduces Street View, a service that shows panoramic views of an increasing number of worldwide positions such as roads, indoor buildings and rural areas. 
It also features a stereoscopic 3D mode, introduced in 2010. 
2013 : Nintendo files a patent for the concept of using VR technology to produce a more realistic 3D effect on a 2D television. 
A camera on the TV tracks the viewer's location relative to the TV, and if the viewer moves, everything on the screen reorients itself appropriately. 
"For example, if you were looking at a forest, you could shift your head to the right to discover someone standing behind a tree." 
2014 : Facebook purchases a company that makes virtual reality headsets, Oculus VR, for $2 billion.[16] Sony announces Project Morpheus, a virtual reality headset for thePS4.
 Google announces Cardboard, a do-it-yourself stereoscopic viewer for smartphones.
2014 : Google and others invest more than $500m into Magic Leap, a startup company that is working on head-mounted devices which superimpose 3D computer-generated imagery over real world objects, by projecting a digital light field into the user's eye.
There has been an increase in interest in the potential social impact of new technologies, such as virtual reality. 
In the book Infinite Reality: Avatars, Eternal Life, New Worlds, and the Dawn of the Virtual Revolution, Blascovich and Bailenson review the literature on the psychology and sociology behind life in virtual reality.
In addition, Mychilo S. Cline, in his book Power, Madness, and Immortality: The Future of Virtual Reality, argues that virtual reality will lead to a number of important changes in human life and activity.
 He argues that virtual reality will be integrated into daily life and activity, and will be used in various human ways. 
Another such speculation has been written up on how to reach ultimate happiness via virtual reality.
 He also argues that techniques will be developed to influence human behavior, interpersonal communication, and cognition.
As we spend more and more time in virtual space, there would be a gradual "migration to virtual space", resulting in important changes in economics, worldview, and culture.
The first use of a VR presentation in a heritage application was in 1994, when a museum visitor interpretation provided an interactive "walk-through" of a 3D reconstruction ofDudley Castle in England as it was in 1550. 
This consisted of a computer controlled laserdisc-based system designed by British-based engineer Colin Johnson.
 The system was featured in a conference held by the British Museum in November 1994, and in the subsequent technical paper, Imaging the Past - Electronic Imaging and Computer Graphics in Museums and Archaeology.[citation needed]
Virtual reality enables heritage sites to be recreated extremely accurately, so that the recreations can be published in various media.
The original sites are often inaccessible to the public, or may even no longer exist.
 This technology can be used to develop virtual replicas of caves, natural environment, old towns, monuments, sculptures and archaeological elements. 
Strides are being made in the realm of education, although much needs to be done. 
The possibilities of VR and education are endless and bring many advantages to pupils of all ages.
Few are creating content that may be used for educational purposes, with most advances being done in the entertainment industry, but many understand and realize the future and the importance of education and VR.
Many science fiction books and films have imagined characters being "trapped in virtual reality".
A comprehensive and specific fictional model for virtual reality was published in 1935 in the short story Pygmalion's Spectacles [5] by Stanley G. Weinbaum. 
A more modern work to use this idea was Daniel F. Galouye's novel Simulacron-3, which was made into a German teleplay titled Welt am Draht ("World on a Wire") in 1973. 
Other science fiction books have promoted the idea of virtual reality as a partial, but not total, substitution for the misery of reality, or have touted it as a method for creating virtual worlds in which one may escape from Earth.
StanisÅ‚aw Lem's 1961 story "I (Profesor Corcoran)", translated in English as "Further Reminiscences of Ijon Tichy I",dealt with a scientist who created a number of computer-simulated people living in a virtual world. Lem further explored the implications of what he termed "phantomatics" in his nonfictional 1964 treatise Summa Technologiae. 
The Piers Anthony novel Killobyte follows the story of a paralyzed cop trapped in a virtual reality game by a hacker, whom he must stop to save a fellow trapped player slowly succumbing to insulin shock.
Other popular fictional works that use the concept of virtual reality include William Gibson's Neuromancer which defined the concept of cyberspace, Neal Stephenson's Snow Crash, in which he made extensive reference to the term avatar to describe one's representation in a virtual world, and Rudy Rucker's The Hacker and the Ants, in which programmer Jerzy Rugby uses VR for robot design and testing. 
The Otherland series of 4 novels by Tad Williams, published from 1996 to 2001 and set in the 2070s, shows a world where the Internet has become accessible via virtual reality.
The Doctor Who serial "The Deadly Assassin", first broadcast in 1976, introduced a dream-like computer-generated reality, known as the Matrix. 
British BBC2 sci-fi series Red Dwarf featured a virtual reality game titled "Better Than Life", in which the main characters had spent many years connected. Saban's syndicated superhero television series VR Troopers also made use of the concept.
The popular .hack multimedia franchise is based on a virtual reality MMORPG dubbed "The World" The French animated series Code Lyoko is based on the virtual world ofLyoko and the Internet.
An anime called Sword Art Online involves the concept of virtual reality, and the possibility of dying in real life when a player dies in the game. 
Also, in Sword Art Online II, they pose the idea of bringing a virtual character into the real world via mobile cameras.
 They use this concept to allow a bedridden individual to attend public school for the first time.
Rainer Werner Fassbinder's 1973 film Welt am Draht is based on a virtual reality simulation inside a virtual reality simulation
One year later in 1983, the Natalie Wood / Christopher Walken film Brainstorm revolved around the production, use, and misuse of a VR device.
Total Recall (1990 film), directed by Paul Verhoeven and based on the Philip K. Dick story "We Can Remember It for You Wholesale"
A VR-like system, used to record and play back dreams, figures centrally in Wim Wenders' 1991 film Until the End of the World.
The 1992 film The Lawnmower Man tells the tale of a research scientist who uses a VR system to jumpstart the mental and physical development of his mentally handicapped gardener.
The 1993 film Arcade is centered around a new virtual reality game (from which the film gets its name) that actively traps those who play it inside its world.
The 1995 film Johnny Mnemonic has the main character Johnny (played by Keanu Reeves) use virtual reality goggles and brainâ€“computer interfaces to access the Internet and extract encrypted information in his own brain.
The 1995 film Virtuosity has Russell Crowe as a virtual reality serial killer name SID 6.7 (Sadistic, Intelligent and Dangerous) who is used a simulation to train real-world police officer, but manages to escape into the real world.
Plot of The Thirteenth Floor (1999) is based on two virtual reality simulations, one in another.
In 1999, The Matrix and later sequels explored the possibility that our world is actually a vast Virtual Reality (or more precisely, simulated reality) created by artificially intelligent machines.
eXistenZ (1999), by David Cronenberg, in which level switches occur so seamlessly and numerously that at the end of the movie it is difficult to tell whether the main characters are back in "reality"
In the film Avatar (2009) the humans are hooked up to experience what their avatars perform remotely.
Surrogates (2009) is based on a brainâ€“computer interface that allows people to control realistic humanoid robots, giving them full sensory feedback.
In 2009, British digital radio station BBC Radio 7 broadcast Planet B, a science-fiction drama set in a virtual world. Planet B was the largest ever commission for an original drama programme.
David Em was the first fine artist to create navigable virtual worlds in the 1970s. 
His early work was done on mainframes at Information International, Inc., Jet Propulsion Laboratory, and California Institute of Technology. 
Jeffrey Shaw explored the potential of VR in fine arts with early works like Legible City (1989), Virtual Museum (1991), andGolden Calf (1994).
 Canadian artist Char Davies created immersive VR art pieces Osmose (1995) and EphÃ©mÃ¨re (1998). 
Maurice Benayoun's work introduced metaphorical, philosophical or political content, combining VR, network, generation and intelligent agents, in works like Is God Flat? (1994), 
"Is the Devil Curved? (1995)"The Tunnel under the Atlantic (1995), and World Skin, a Photo Safari in the Land of War (1997).
 Other pioneering artists working in VR have include Luc Courchesne, Rita Addison, Knowbotic Research, Rebecca Allen, Perry Hoberman, Jacki Morie, Margaret Dolinsky and Brenda Laurel. 
All mentioned artists are documented in the Database of Virtual Art.
The use of graphics, sound and input technology in video games can be incorporated into VR. 
Several Virtual Reality head mounted displays (HMD) were released for gaming during the early-mid 1990s.
 These included the Virtual Boy developed by Nintendo, the iGlasses developed by Virtual I-O, the Cybermaxx developed by Victormaxx and the VFX-1 developed by Forte Technologies. 
Other modern examples of narrow VR for gaming include the Wii Remote, the Kinect, and the PlayStation Move/PlayStation Eye, all of which track and send motion input of the players to the game console somewhat accurately. 
There is also a new high field of view VR headset system in development designed specifically for gaming called the Oculus Rift. 
There has also been recent development in consumer-orientedomnidirectional treadmills because of Oculus Rift such as Virtuix Omni and Cyberith Virtualizer, which can simulate the motion of walking in a stationary environment. 
Sony announced their rival to the Oculus Rift technology as the prototype Project Morpheus at the Game Developers Conference during March 2014. 
Immersive virtual musical instruments build on the trend in electronic musical instruments to develop new ways to control sound and perform music such as evidenced by conferences like NIME and aim to represent musical events and sound parameters in a virtual reality in such a way that they can be perceived not only through auditory feedback, but also visually in 3D and possibly through tactile as well as haptic feedback, allowing the development of novel interaction metaphors beyond manipulation such as prehension.
The second music video of Take On Me, a song by the Norwegian synthpop band A-ha used a pencil-sketch animation / live-action combination called rotoscoping, in which the live-action footage is traced-over frame by frame to give the characters realistic movements.
 Approximately 3,000 frames were rotoscoped, which took 16 weeks to complete. 
The primary use of VR in a therapeutic role is its application to various forms of exposure therapy, ranging from phobia treatments to newer approaches to treating PTSD. 
A very basic VR simulation with simple sight and sound models has been shown to be invaluable in phobia treatment, like zoophobia, and acrophobia, as a step between basic exposure therapy such as the use of simulacra and true exposure. 
A much more recent application is being piloted by the U.S. Navy to use a much more complex simulation to immerse veterans suffering from PTSD in simulations of urban combat settings. 
Much as in phobia treatment, exposure to the subject of the trauma or fear leads to desensitization, and a significant reduction in symptoms. 
Other research fields in which the use of virtual reality is being explored are physical medicine, rehabilitation, physical therapy, and occupational therapy.
 In adult rehabilitation, a variety of virtual reality applications are currently being evaluated within upper and lower limb motor rehabilitation for individuals recovering from stroke or spinal cord injury. 
In pediatrics, the use of virtual reality is being evaluated to promote movement abilities, navigational abilities, or social skills in children with cerebral palsy, acquired brain injury, or other disabilities.
Research evidence is emerging rapidly in the field of virtual reality for therapeutic uses.
 A number of recent reviews published in peer-reviewed journals have summarized the current evidence for the use of Virtual Reality within pediatric and adult rehabilitation.
 One such review concluded that the field is potentially promising.
 The new field of Virtual rehabilitation has emerged recently.
There has also been talks of letting physical therapist use VR to work with patients who are in another location.
 They could use multiple 3-D cameras to project a 3-D avatar of the therapist who can then guide the patient throughout the patient's exercise. 
Haptic devices can also be used for the doctor to feel the conditions of the patient's muscle. 
However, to transfer the required information to support real time interactions, is far too slow at the moment. 
Computer-generated graphics in the 1980s made possible a new paradigm for therapy planning introducing the ability to recreate human anatomy in a virtual space, in what could be termed surgical simulation.
 The objective was to make medical therapy (and education[) more precise, effective, and affordable. 
This type of virtual reality consists of a visually realistic environment with which the user has some degree of sensory interaction. 
The interactions range from a standard computer-user interface with cursor and keyboard, to the use of positionally sensitive controllers to manipulate the virtual image.
The human anatomy initially used standardised patient data to create a digital representation of anatomic space.
 Later, with the advent of Computer Tomography andMagnetic Resonance Imaging , the anatomy became patient-specific and this gave impulse to surgical planning in several surgical specialties, notably neurosurgery and oral and maxillofacial surgery.
This ultimately gave rise to the tools now available for intra-operative navigation and stereotactic surgery, like for example the Dextroscope, which was the "first widely marketed simplified virtual reality surgical simulation environment in neurosurgery.â€
The usage of VR in a training perspective is to allow professionals to conduct training in a virtual environment where they can improve upon their skills without the consequence of failing the operation.
VR plays an important role in combat training for the military. 
It allows the recruits to train under a controlled environment where they are to respond to different types of combat situations. 
A fully immersive virtual reality that uses Head-mounted display (HMD), data suits, data glove, and VR weapon are used to train for combat. 
This setup allows the training's reset time to be cut down, and allows more repetition in a shorter amount of time. 
The fully immersive training environment allows the soldiers to train through a wide variety of terrains, situations and scenarios. 
VR is also used in flight simulation for the Air Force where people are trained to be pilots.
 The simulator would sit on top of a hydraulic lift system that reacts to the user inputs and events.
 When the pilot steer the aircraft, the module would turn and tilt accordingly to providehaptic feedback.
 The flight simulator can range from a fully enclosed module to a series of computer monitors providing the pilot's point of view.
 The most important reasons on using simulators over learning with a real aircraft are the reduction of transference time between land training and real flight, the safety, economy and absence of pollution.
By the same token, virtual driving simulations are used to train tank drivers on the basics before allowing them to operate the real vehicle.
Finally, the same goes for truck driving simulators, in which Belgian firemen are for example trained to drive in a way that prevents as much damage as possible. 
As these drivers often have less experience than other truck drivers, virtual reality training allows them to compensate this. 
In the near future, similar projects are expected for all drivers of priority vehicles, including the police.
Medical personnel are able to train through VR to deal with a wider variety of injuries.
An experiment was performed by sixteen surgical residents where eight of them went through laparoscopic cholecystectomy through VR training. 
They then came out 29% faster at gallbladder dissection than the controlled group.[47]
Virtual reality can serve to new product design, helping as an ancillary tool for engineering in manufacturing processes, new product prototypes, and simulation. 
Among other examples, electronic design automation, CAD, Finite Element Analysis, and computer-aided manufacturing are widely utilized programs.
The use ofStereolithography and 3D printing shows how computer graphic modeling can be applied to create physical parts of real objects used in naval, aerospace, and automotiveindustries, which can be seen, for example, in the VR laboratory of VW in MladÃ¡ Boleslav. 
Beyond modeling assembly parts, 3D computer graphics techniques are currently used in the research and development of medical devices for therapies, treatments, patient monitoring, and early diagnoses of complex diseases.
In 2010, 3D virtual reality was becoming widely used for urban regeneration and planning and transport projects. 
In 2007 development began on a virtual reality software which took design coordinate geometry used by land surveyors and civil engineers and incorporated precision spatial information created automatically by the lines and curves typically shown on subdivision plats and land surveying plans. 
These precise spatial areas cross referenced color and texture to an item list. 
The item list contained a set of controls for 3D rendering such as water reflective surface or building height.
 The land surface in software to create a contour map uses a digital terrain model (DTM). By 2010, prototype software was developed for the core technology to automate the process leading from design to virtualization. 
The first beta users in 2011 were able to press a single function and automatically drape the design or survey data over the digital terrain to create data structures that are passed into a video gaming engine to create a virtual interactive world showing massing of buildings in relation to man made improvements.
It was the first application where virtual reality was made effortless for Urban Planning principals using technology. 
The software was improved to implement massing or 3D models from other free or commercially sold software to create more realistic virtual reality with very little time and effort (see the below image). 
The software is marketed as LandMentor and is the first precision design technology to make Urban Planning widely available with a short learning curve.
Virtual reality technology faces a number of challenges, most of which involve motion sickness and technical matters.
 Users might become disoriented in a purely virtual environment, causing balance issues; computer latency might affect the simulation, providing a less-than-satisfactory end-user experience.
The complicated nature of head-mounted displays and input systems such as specialized gloves and boots may require specialized training to operate, and navigating the non-virtual environment (if the user is not confined to a limited area) might prove dangerous without external sensory information.
In January 2014, Michael Abrash gave a talk on VR at Steam Dev Days.
 He listed all the requirements necessary to establish presenceand concluded that a great VR system will be available in 2015 or soon after. 
While the visual aspect of VR is close to being solved, he stated that there are other areas of VR that need solutions, such as 3D audio, haptics, body tracking, and input. 
However, 3D audio effects exist in games and simulate the head-related transfer function of the listener (especially using headphones). 
Examples includeEnvironmental Audio Extensions (EAX), DirectSound and OpenAL.
VR audio developer Varun Nair points out that from a design perspective, sound for VR is still very much an open book. Many of the game audio design principles, especially those related to FPS games, crumble in virtual reality.
 He encourages more sound designers to get involved in virtual reality audio to experiment and push VR audio forward. 
The companies working in the virtual reality sector fall broadly into three categories of involvement: hardware (that is, making headsets and input devices specific to VR), software (that is, producing software for interfacing with the hardware or for delivering content to users) and content creation (that is, producing content, whether interactive or passive, for consumption with VR hardware.
The virtuality continuum is a continuous scale ranging between the completely virtual, a virtuality, and the completely real, reality. 
The reality-virtuality continuum therefore encompasses all possible variations and compositions of real and virtual objects. 
It has been described as a concept in new media and computer science, but in fact it could be considered a matter of anthropology.
 The concept was first introduced by Paul Milgram.
The area between the two extremes, where both the real and the virtual are mixed, is the so-called mixed reality. 
This in turn is said to consist of both augmented reality, where the virtual augments the real, and augmented virtuality, where the real augments the virtual.
This continuum has been extended into a two-dimensional plane of virtuality and mediality. Taxonomy of reality, virtuality, mediality. 
The origin R denotes unmodified reality. 
A continuum across the virtuality axis, V, includes reality augmented with graphics (augmented reality), as well as graphics augmented by reality (augmented virtuality). 
However, the taxonomy also includes modification of reality or virtuality or any combination of these. 
The modification is denoted by moving up the mediality axis. 
Further up this axis, for example, we can find mediated reality, mediated virtuality, or any combination of these. 
Further up and to the right we have virtual worlds that are responsive to a severely modified version of reality. (at right) Mediated reality generalizes the concepts of mixed reality, etc. 
It includes the virtuality reality continuum (mixing) but also, in addition to additive effects, also includes multiplicative effects (modulation) of (sometimes deliberately) diminished reality. Moreover, it considers, more generally, that reality may be modified in various ways. 
The mediated reality framework describes devices that deliberately modify reality, as well as devices that accidentally modify it.
While the term augmented virtuality is rarely used nowadays, augmented reality and mixed reality are now sometimes used as synonyms[citation needed].
The virtuality continuum has grown and progressed past labels such as computer science and new media. 
As the concept has much to do with the way in which humans continue to change how they communicate; the way in which form identities and the way in which they interact to and within the world; it is more accurately described as a subject withinanthropology.
Changes in attitudes towards and the increase in availability of technology and media have changed and progressed the way it is used. 
One to one (SMS), one to many (email), and many to many (chat rooms), have become ingrained in society. 
The use of such items have made once clear distinctions like online and offline obsolete, and the distinctions between reality and virtuality have become blurred as people are incorporating and relying heavily upon virtuality within their everyday personal realities.
Daniel Miller and Don Slater are prominent researchers pursuing the concept of the virtuality continuum and the media and its effect on communities, especially in the Caribbean, most notably Trinidad and Jamaica.
Steve Woolgar is another researcher who has established four rules of virtuality. 
The way in which media and technology affect people relies on their non-information communication technology (ICT) related background which may include gender, age, social status, income amongst others.
Risks and fears in regards to new media and technology are unevenly socially distributed.
Advancements in media and technology supplement rather than replace existing activities in reality.
New media and technology tends to create new kinds of localism rather than furthering globalisation.
The definition of virtual reality comes, naturally, from the definitions for both â€˜virtualâ€™ and â€˜realityâ€™. 
The definition of â€˜virtualâ€™ is near and reality is what we experience as human beings. So the term â€˜virtual realityâ€™ basically means â€˜near-realityâ€™. 
This could, of course, mean anything but it usually refers to a specific type of reality emulation.
Answering "what is virtual reality" in technical terms is straight-forward.
 Virtual reality is the term used to describe a three-dimensional, computer generated environment which can be explored and interacted with by a person. 
That person becomes part of this virtual world or is immersed within this environment and whilst there, is able to manipulate objects or perform a series of actions.
The person wears a head-mounted display (HMD) or glasses which displays three-dimensional images as part of their experience. 
Some systems enable the person to experience additional sensory input, e.g. sound or video which contributes to their overall experience.
Find out more in the virtual reality gear article.
They are aided by various sensory stimuli such as sound, video and images which form part of most virtual reality environments. 
But many newer environments include touch or force feedback through a haptic device such as a â€˜data gloveâ€™ which further enhances the experience.
Many people who work with virtual reality prefer to use the term â€˜virtual environmentsâ€™ instead. 
This is a response to a perceived negativity to this technology which has often turned out to be true.
 There are people who view virtual reality with little enthusiasm and dismiss it as â€˜science fictionâ€™, seeing it as having no practical application in the real world.
Virtual reality can lead to new and exciting discoveries in these areas which impact upon our day to day lives.
 One example of this is the use of virtual reality in medicine, such as surgical simulations, which helps with training the next generation of surgeons.
There are many different types of virtual reality systems but they all share the same characteristics such as the ability to allow the person to view three-dimensional images. 
These images appear life-sized to the person.
Plus they change as the person moves around their environment which corresponds with the change in their field of vision. 
The aim is for a seamless join between the personâ€™s head and eye movements and the appropriate response, e.g. change in perception. 
This ensures that the virtual environment is both realistic and enjoyable.
A virtual environment should provide the appropriate responses â€“ in real time- as the person explores their surroundings.
 The problems arise when there is a delay between the personâ€™s actions and system response or latency which then disrupts their experience. 
The person becomes aware that they are in an artificial environment and adjusts their behaviour accordingly which results in a stilted, mechanical form of interaction.
The aim is for a natural, free-flowing form of interaction which will result in a memorable experience.
NASA, the Department of Defense and the National Science Foundation funded much of the research and development for virtual reality projects. 
$80,000 was contributed by the CIA for research purpose to Sutherland.
After that in 1984, Michael McGreevy, a computer scientist started to experiment with VR technology as a path to advance human-computer interface (HCI)designs.
 HCI still is a domination factor in VR research. It further led to the media picking up on the idea of VR within a couple of years.
Jaron Lanier coined the term Virtual Reality in 1987. Today Virtual Reality plays a big part in the every day lives of the world's population.
Virtual reality development has really slowed in recent years and recent progress is not exactly recent. 
This is more than likely due to public disappointment, further due to the high expectations created by media sensationalisation.
Thankfully, virtual reality lives on in popular culture as a heavy influence on many consumer products that are in use today. 
These range from games to revolutionary input devices.
Virtual worlds combine the power of 3D graphics and the internet, giving users the ability to create new versions of themselves literally within a virtual world.
Second Life, arguably the most popular of these games, has seen massive successes, which includes creating millionaires out of some of their long-time and most dedicated players.
 This is made possible by their own currency and exchange rates.
Virtual worlds have become so popular, laws have been extended to include property acquired on them.
In a similar thread of thought, modern input devices have been massively influenced by virtual reality and may become the corner stone of further virtual reality developments.
Microsoft's Kinect â€“ This device uses a camera to track a player's movements, which are then reflected in-game.
Wii Controls â€“ The Wii, mostly due to its controls, took the world by storm. Using a controller, which can be latched to the hand, movement becomes a form of input.
In mid 1950s visionary cinematographer Morton H Eilig built a single user console called Sensorama that included a stereoscopic display, fans, or emitters, stereo speakers and a moving chair. 
This enabled the user watch television in three dimensional ways. 
In 1961, Philco Corporation engineers developed the first HMD known as the Headsight. 
The helmet consisted of a video screen along with a tracking system. 
Then they linked to a closed circuit camera system. 
Then somewhat similar HMD was used for helicopter pilots. 
While flying in the dark these were of great help.
Many people are familiar with the term â€˜virtual realityâ€™ but are unsure about the uses of this technology. 
Gaming is an obvious virtual reality application as are virtual worlds but there are a whole host of uses for virtual reality â€“ some of which are more challenging or unusual than others.
In 1965, a computer scientist named Ivan Sutherland envisioned what he called the â€œUltimate Display.
â€ After using this display a person imagines the virtual world very similar to the real world.
During 1966, an HMD was built by Sutherland, which was tethered to a computer system.
Some of these will be more familiar than others but visit any of these to find out more about a particular use of virtual reality.
There are many more uses of VR than first realised which range from academic research through to engineering, design, business, the arts and entertainment.
But irrespective of the use, virtual reality produces a set of data which is then used to develop new models, training methods, communication and interaction. In many ways the possibilities are endless.
The only stumbling blocks are time, costs and technological limitations.
 Virtual reality systems such as aCAVE system are expensive and time consuming to develop. 
Plus there are issues of ergonomics, specifically the need to design systems which are â€˜user friendlyâ€™ and not likely to cause problems such as motion sickness.
But if these problems are solved then there is an exciting future for virtual reality.
Virtual reality has been adopted by the military â€“ this includes all three services (army, navy and air force) â€“ where it is used for training purposes.
 This is particularly useful for training soldiers for combat situations or other dangerous settings where they have to learn how to react in an appropriate manner.
A virtual reality simulation enables them to do so but without the risk of death or a serious injury. 
They can re-enact a particular scenario, for example engagement with an enemy in an environment in which they experience this but without the real world risks. This has proven to be safer and less costly than traditional training methods.
Virtual reality is also used to treat post-traumatic stress disorder. 
Soldiers suffering from battlefield trauma and other psychological conditions can learn how to deal with their symptoms in a â€˜safeâ€™ environment.
 The idea is for them to be exposed to the triggers for their condition which they gradually adjust to.
 This has the effect of decreasing their symptoms and enabling them to cope to new or unexpected situations.
This is discussed further in the virtual reality treatment for PTSD (post traumatic stress disorder) article.
Virtual reality training is conducted using head mounted displays (HMD) with an inbuilt tracking system and data gloves to enable interaction within the virtual environment.
Another use is combat visualisation in which soldiers and other related personnel are given virtual reality glasses to wear which create a 3D depth of illusion. 
The results of this can be shared amongst large numbers of personnel.
Find out more about individual uses of virtual reality by the different services, e.g. virtual reality navy training in the separate virtual reality and the military section.
This section discusses the various military applications of virtual reality and the ramifications from using this form of technology. 
The military may not be an obvious candidate for virtual reality but it has been adopted by all branches â€“ army, navy and air force.
What the military stress is that virtual reality is designed to be used as an additional aid and will not replace real life training.
What is apparent is that virtual environments are ideal set ups for military training in that they enable the participants, i.e. soldiers, to experience a particular situation within a controlled area.
 For example, a battlefield scenario in which they can interact with events but without any personal danger to themselves.
The main advantages of this are time and cost: military training is prohibitively expensive especially airborne training so it is more cost-effective to use flight simulators than actual aircraft. 
Plus it is possible to introduce an element of danger into these scenarios but without causing actual physical harm to the trainees.
Flight simulators are a popular theme in military VR training but there are others which include: medical training (battlefield), combat training, vehicle training and â€˜boot campâ€™.
But another use and one which is not immediately thought of is virtual reality and post traumatic stress disorder (PTSD). 
PTSD or â€˜combat stressâ€™ has only recently been acknowledged as a medical condition but it causes very real damage to the person concerned and their family. 
They are constantly pushing boundaries as regards what their bodies can do which drives the sports clothing and equipment industry. 
This industry has to keep pace with this constant drive for sporting perfection and uses the very latest technology to do so.
Virtual reality has also been used to improve the audienceâ€™s experience of a sporting event. 
Some systems allow the audience to walkthrough a stadium or other sporting location, which helps them when purchasing a ticket to an event.
And then there are virtual reality games with a sports theme which allow the player to become part of the competition. 
One example is an interactive football game which projects this match onto a real world surface.
Virtual reality is used to help the sufferer adjust to their symptoms and develop coping strategies whenever they are placed in a new situation.
This is discussed at greater length in our virtual reality treatment for PTSD article.
Generally, virtual reality training involves the use of head mounted displays (HMD) and data gloves to enable military personnel to interact with objects within a virtual environment. 
Alternately, they may be given virtual reality glasses to wear which display a 3D image.
Education is another area which has adopted virtual realityfor teaching and learning situations. 
The advantage of this is that it enables large groups of students to interact with each other as well as within a three dimensional environment.
It is able to present complex data in an accessible way to students which is both fun and easy to learn. 
Plus these students can interact with the objects in that environment in order to discover more about them.
For example, astronomy students can learn about the solar system and how it works by physical engagement with the objects within. 
They can move planets, see around stars and track the progress of a comet.
 This also enables them to see how abstract concepts work in a three dimensional environment which makes them easier to understand and retain.
This is useful for students who have a particular learning style, e.g. creative or those who find it easier to learn using symbols, colours and textures.
One ideal learning scenario is medicine: virtual reality can be used to develop surgery simulations or three dimensional images of the human body which the students can explore. 
This has been used in medical schools both in the UK and abroad.
The use of virtual reality in medicine is discussed in a series of separate articles in the virtual reality and healthcaresection.
Then there is the fact that children today are familiar with all forms of technology and use these at school as well as at home. 
They have grown up with technology from a very early age and unlike adults, do not have any fear or hesitation in using it.
Plus we live in a technological society. So it makes sense to implement virtual reality as one of several forms of technology in order to educate tomorrowâ€˜s technological elite.
 Education has moved on from books, pencils and pens to the use of interactive technologies to help impart knowledge and understanding.
Healthcare is one of the biggest adopters of virtual reality which encompasses surgery simulation, phobia treatment, robotic surgery and skills training.
One of the advantages of this technology is that it allows healthcare professionals to learn new skills as well as refreshing existing ones in a safe environment. 
Plus it allows this without causing any danger to the patients.
One example of this is the HumanSim system which enables doctors, nurses and other medical personnel to interact with others in an interactive environment. 
They engage in training scenarios in which they have to interact with a patient but within a 3D environment only. 
This is an immersive experience which measures the participantâ€™s emotions via a series of sensors.
Virtual reality is often used as a diagnostic tool in that it enables doctors to arrive at a diagnosis in conjunction with other methods such as MRI scans. 
This removes the need for invasive procedures or surgery.
A popular use of this technology is in robotic surgery. 
This is where surgery is performed by means of a robotic device â€“ controlled by a human surgeon, which reduces time and risk of complications. 
Virtual reality has been also been used for training purposes and, in the field of remote telesurgery in which surgery is performed by the surgeon at a separate location to the patient.
The main feature of this system is force feedback as the surgeon needs to be able to gauge the amount of pressure to use when performing a delicate procedure.
But there is an issue of time delay or latency which is a serious concern as any delay â€“ even a fraction of a second â€“ can feel abnormal to the surgeon and interrupt the procedure. 
So there needs to be precise force feedback in place to prevent this.
The entertainment industry is one of the most enthusiastic advocates of virtual reality, most noticeably in games and virtual worlds. 
These environments enable members of the public to engage with the exhibits in ways which were previously forbidden or unknown. 
They wear virtual reality glasses with stereoscopic lenses which allow them to see 3D objects and at different angles. 
And in some cases they can interact with the exhibits by means of an input device such as a data glove.
An example of this is a historical building which the member of the public can view at different angles. 
Plus they are able to walk through this building, visiting different rooms to find out more about how people lived at that particular time in history.
They are able to do this by means of a tracking system (built into the glasses) which tracks their movements and feeds this information back to a computer. 
The computer responds by changing the images in front of the person to match their change in perception and maintain a sense of realism.
There are a range of virtual reality systems available for audience entertainment which includes CAVE systems, augmented reality systems, simulators and 3D display platforms.
Virtual reality gaming is a very popular form of entertainment which is discussed in more detail in a separate section. 
Fashion is not something that immediately springs to mind when thinking about virtual reality but nevertheless, it is used by the fashion industry in a variety of ways.
Second Life is discussed further as a separate article. Please see the Virtual Reality Games section.
There are situations in which the boundaries are blurred between fashion and technology, for example, the use of virtual reality as part of a live fashion show. 
In that scenario, a 3D image was projected into a real world setting, i.e. a catwalk as part of the show.
Another example is where fashion is influenced by technology, in particular virtual worlds such as Second Life. 
The models and clothes are based upon avatars in that environment.
The meeting of fashion and virtual reality can be seen in games which are designed for girls. 
These games combine fashion, music and technology in a way designed to appeal to the interests of teenage girls.
The issue of girls and virtual reality gaming is discussed in more detail within the virtual reality games section.
This refers to the use of virtual reality in museum and historical settings, e.g. visitor centres.
 These settings employ interaction as a means of communicating information to the general public in new and exciting ways.
There has been a move away from the traditional type of experience associated with museums, galleries and visitor centres. 
The old model was that of passive engagement in which people viewed the exhibit/s but did not get involved to an experience in which interaction is the main feature.
Interactive displays form a large part of many exhibitions and particularly appeal to children. 
Children are often difficult to attract to a museum or gallery as they tend to see this as a boring experience. 
But the use of interactive technologies such as virtual reality has changed that perception and opened up these spaces to a new audience.
Virtual reality has been used to construct virtual walkthroughs of these sites which enhances the visitorâ€™s experience.
Plus this is a useful way of challenging attitudes towards heritage sites such as museums and to encourage more people to visit and take part.
Virtual reality is being used in a number of ways by the business community which include:
Many businesses have embraced virtual reality as a cost effective way of developing a product or service. 
For example it enables them to test a prototype without having to develop several versions of this which can be time consuming and expensive.
Plus it is a good way of detecting design problems at an early stage which can then be dealt with sooner rather than later.
For some businesses, fully immersive virtual reality a la CAVE system is the way forward. 
They like the fact that they can use this to test drive a product in the early stages of development but without any additional costs (or risks) to themselves.
This is particularly useful for companies who produce dangerous or potentially harmful products which need to be evaluated before use. 
They can test their product within a virtual environment but at no risk to themselves or their employees. 
And virtual reality technology has advanced to the stage where it has a high degree of realism and efficiency.
Some companies use virtual reality to help with data analysis and forecasting trends in order to gain an edge over their competitors. 
One example of this is a system developed by researchers at the University of Warwick which is designed to help businesses gain a greater understanding of their data.
Another use is virtual worlds: there are companies who use virtual worlds as a means of holding meetings with people who are based in various locations.
 This is often a low cost solution to the problem of communication with large numbers of employees in remote locations.
Virtual environments are also used for training courses and role playing scenarios.
Virtual reality engineering includes the use of 3D modelling tools and visualisation techniques as part of the design process. 
This technology enables engineers to view their project in 3D and gain a greater understanding of how it works.
 Plus they can spot any flaws or potential risks before implementation.
This also allows the design team to observe their project within a safe environment and make changes as and where necessary. 
This saves both time and money.
What is important is the ability of virtual reality to depict fine grained details of an engineering product to maintain the illusion.
 This means high end graphics, video with a fast refresh rate and realistic sound and movement.
In some cases, virtual reality can be used from the start of the design lifecycle, e.g. the initial concept through to the build and implementation stages. 
This is reviewed at stages to check for faults, structural weaknesses and other design issues.
Virtual reality engineering is employed by Balfour Beatty Rail, a rail infrastructure contractor who includes this as part of their design process. 
It is used for planning, prototyping and construction purposes, and helps with project realisation.
Car manufacturers use virtual reality for prototyping purposes during the design process.
 This enables them to produce several versions which are then tested and changed as per the results.
 This removes the need to build a physical prototype and speeds up the development stage. 
The result is a cost effective streamlined process.
An example of this can be seen at the JLR Virtual Reality Centre in the UK. 
This is state of the art virtual reality - both semi-immersive and CAVE systems - with advanced tracking and projection facilities which is used to help design the next generation of Land Rovers.
Virtual reality is used as a training aid in many sports such as golf, athletics, skiing, cycling etc.
 It is used as an aid to measuring athletic performance as well as analysing technique and is designed to help with both of these.
 It also used in clothing/equipment design and as part of the drive to improve the audienceâ€™s experience.
The athlete uses this technology to fine tune certain aspects of their performance, for example, a golfer looking to improve their swing or a track cyclist wanting to go faster in the individual pursuit. 
Three dimensional systems can pinpoint aspects of an athleteâ€™s performance which require changing, for example, their biomechanics or technique.
Another popular use is sports manufacture: virtual reality is used in the design of sporting clothes and equipment, e.g. running shoe design. 
Innovation is a key factor in this industry as the bar is raised higher and higher in terms of sporting achievement.
Sportspeople are constantly looking at ways of gaining them that edge which can mean being faster, stronger, better endurance etc. 
This article discusses the various ways virtual reality is used in the media. This includes radio, television, music and film as well as books and the arts.
Virtual reality has featured in several film and television programmes. 
It is often used to illustrate the concept of being trapped within the machine (or in this case, cyberspace), or as a form of advanced technology.
And then there are television programmes such as selected episodes of Doctor Who, Red Dwarf and Star Trek: 
The Next Generation which utilise virtual reality technology.
 One example is the holodeck as seen in Star Trek which enables the person to experience any situation they so wish.
This technology has formed part of experimental sound displays and sound installations.
 Another use is virtual reality musical instruments which the person can interact with these instruments as a new type of performance or to create new compositions.
Virtual reality has been a staple theme of many fictional stories such as William Gibsonâ€™s Neuromancer and Mona Lisa Overdrive as well as Orson Scott Cardâ€™s Enders Game.
There are artists who use virtual reality to explore certain ideas or concepts. They create a three dimensional environment as a form of communication with the audience. 
One example is the work of Kenneth Rinaldo who uses robotics and augmented reality to explore ideas related to the human-technology boundary.
Virtual reality is being increasingly used in the field of scientific visualisation. 
This field is based upon using computer graphics to express complex ideas and scientific concepts, for example molecular models or statistical results.
Scientific visualisation is used as a means of communicating abstract concepts to an audience which also aids with understanding. 
The audience can interact with these images, for example, viewing a molecular structure at different angles or as a means of problem solving.
Virtual reality enables scientists to demonstrate a method or convey complex ideas in a visual format. 
This includessemi-immersive and full immersive environments in which they visualise research theories or discuss large data sets.
This technology raises possibilities for collaboration between different disciplines or new forms of research and development.
Virtual reality is considered alongside other forms of visualisation technology such as computer simulation, animation and information visualisation. 
All of these are designed to show a visual model of a live system, e.g. human body, complex data set or a large collection of numerical information.
Telecommunications is another field which has utilised virtual reality technology, in particular mobile communications which enables easy access to a variety of VR based projects.
The main challenge is that of dealing with a medium which mainly relies upon tone of voice, intonation, gesture and body language as compared to spoken words. 
In fact, spoken words only account for a very small percentage of the overall communication.
But traditional forms of communication such as the telephone are being superseded by video conferencing, Skype and live chat. 
These communication mediums can be used on the internet and other similar systems and are seen as cheaper and more flexible.
Telecommunications can be used to help virtual reality systems such as surgery simulation or telemedicine. 
An example of this is remote surgery in which images from that surgery can be transmitted to various locations around the world. 
It also enables surgery to be performed in remote locations using robotic technology and virtual reality.
Virtual reality can be extremely useful in the construction industry, which is often known as having a very high amount of inefficiency and low profit margins. 
Using a virtual environment, an organisation can not only render the resulting structure in 3D but also experience them as they would in the real world.
Building a construction project in a virtual environmentoffers many key benefits. 
One of the most obvious of these is having the ability to test a number of factors without the time and cost of building the structure, reducing the number of errors present in the completed building.
One important factor that needs to be thoroughly tested is the viability of an architectural design.
 For many years, human judgement and scale models were the only methods to determine whether a structure was viable or not. 
As we know, human judgement can be highly, and sometimes intentionally, erroneous and scale models cannot fully simulate the environment the structure must withstand.
Not only can the viability of a building be tested before itâ€™s built, construction workers and employees can actually explore it. 
Feedback about a design from this is phenomenal, being able to pick up even small details such as whether a worker can fit in within a space.
Furthermore, the construction of a building can be simulated in virtual reality as it would in its normal environment. 
This allows an organisation to fine-tune construction processes for maximum efficiency and a minimum amount of change.
Although itâ€™s impossible to tell when exactly virtual reality in construction will become the norm, itâ€™s only a matter of time before it does. 
Virtual reality will allow us to make grander and more robust buildings in a shorter space of time - a very desirable property indeed.
Virtual reality is a very common theme in science fiction movies, where it is often used a way to turn the fantastical into something that seems totally real.
TRON, for instance, was one of the first movies to use virtual reality as a plot element. 
The main characters were taken from reality and transported into a virtual world inside a computer.
 This is not 100% like the virtual reality we know today but the concept of another reality inside of a computer reminds the same.
Some of the most popular movies of our time use concepts of virtual reality. 
Some of these movies, which you've probably heard of, include:
The list continues indefinitely. We can certainly expect such a list to continue growing in the future as the ideas behind virtual reality are fully explored in film.
Second Life, at one time, partnered with the 48 Hour Film Project to produce the first film festival to take place in a virtual world.
The participants of the festival had to create a film set totally within the world of Second Life. 
In true 48HFP style, they were given a genre, a character, a proper and a piece of dialogue, which must be incorporated into their films, and went away to write and edit a Second Life film within 48 hours.
The winners of the 'virtual film festival' were given the opportunity to have their films shown at the real-life 48 Hour Film Project event.
For virtual reality to be truly effective, it must have a good sense of realism.
 Just on its own this is a technical challenge and, as such, virtual reality is highly demanding on many resources. 
From hardware performance to the intellectual ability of the implementor of the system, how these are managed are a massive issue.
As mentioned above, the realism involved in virtual reality requires a large amount of hardware resources. 
The most obvious requirement is processing speed, which will become more of a problem as Moore's law becomes less effective. 
A convincing virtual environment must have extremely realistic visuals so good usage of the GPU, for graphics rendering, is also a definite requirement. 
Sound is also another factor and the quality of audio output must be extremely high, which requires good use of the sound card. 
This barely scratches the surface of the technical requirements involved.
Not only is the use of hardware an issue, the person using these resources must also be highly skilled. 
A good knowledge of in-depth computer science topics is a must have, usually requiring post-graduate education.
Much like software development today, as time progresses and the field expands, these requirements must be dealt with effectively and the barrier of entry much be lowered. 
One such method is to employ a domain-specific programming language geared especially towards virtual reality.
A DSL (domain-specific language) can be carefully tailored to a problem domain in many critical ways. 
Carefully crafting the correct language(s) to virtual reality will allow developers to write less code which is optimised especially for the creation of a virtual environment.
This programming language was invented by Jaron Lanier of VPL Research to aid the building of virtual reality experiences. 
VPL is described as a â€œpost-symbolicâ€ programming language, which is taken to mean programs are 'written' by means other than letters, numbers and other written characters. 
For reference, most computer programs are written using the letters of the alphabet, numbers and punctuation characters.
Microsoft have developed a language with the same name which fits a â€œpost-symbolicâ€ description using drawing as a method of writing programs.
The concepts behind virtual reality are based upon theories about a long held human desire to escape the boundaries of the â€˜real worldâ€™ by embracing cyberspace. 
Once there we can interact with this virtual environment in a more naturalistic manner which will generate new forms of human-machine interaction(HMI).
The aim is to move beyond standard forms of interaction such as the keyboard and mouse which most people work with on a daily basis. 
This is seen as an unnatural way of working which forces people to adapt to the demands of the technology rather than the other way around.
But a virtual environment does the opposite. It allows someone to fully immerse themselves in a highly visual world which they explore by means of their senses.
 This natural form of interaction within this world often results in new forms of communication and understanding.
The experience of a virtual world mimics that of a real world scenario but often without many of its constraints. Virtual reality enables allows someone to do the following:
Plus the fact that they can do this in a 3D environment means that they replicate an experience similar to that in the real world but without many of the dangers.
This is preferable to trying to simulate these experiences in a two-dimensional setting, e.g. a computer desktop.
Find out more about these and other applications in the how virtual reality is used article.
Virtual reality also acts as a problem solving device in that it enables us to explore various options as a means of finding an answer to a problem.
For example, an engineering company will use virtual reality to produce a prototype which is then tested and the results fed back to the design team. 
The advantage of this is that it enables the designers to make alterations to their design but at far less time and cost.
This is a preferred option to building a physical prototype which is expensive to build and make changes to: especially if it undergoes several alterations as part of the design process.
These provide a framework for virtual reality which spans a period of more than 70 years. 
In this time we saw the development of the first simulation device, the first form of interactive multimedia theatre, and the first head mounted display (HMD) and the use of virtual reality in the field of human-computer interaction (HCI).
Virtual reality came to the publicâ€™s attention in the late 1980â€™s and 1990â€™s. 
This can be attributed to pioneering computer scientist Jaron Lanier who introduced the world back in 1987 to the term â€˜virtual realityâ€™.
Research into virtual reality continued into the 1990â€™s and that combined with the appearance of films such as The Lawnmower Man helped to raise its profile.
More information about these can be found in any of the three articles listed above as bullet points.
But the growth in popularity of virtual reality was matched by a growth in public expectations which then over hyped the abilities of this technology.
 This led to dissatisfaction with virtual reality in general and a scaling back of many research activities in this area.
However, virtual reality has not disappeared and has instead been repackaged as virtual environments instead.
 This is done to prevent any unrealistic expectations as regards this technology which keeps it in the domain of science fact and not science fiction.
Virtual reality has re-invented itself into something which fits our vision and expectations of what this technology can do. 
One example of this is the development of virtual reality games for the Playstation 2 and 3, Xbox, Mac and PC.
These games have become popular with children and adults and continue to push the boundaries of gaming technology.
It looks like virtual reality is here to stay for the time being.
Virtual reality is possible thanks to developments in interactive technologies by people such as Jaron Lanier, Douglas Engelbart, Ivan Sutherland and Morton Heilig
These people were pushing the boundaries of technological research and experimented with new forms of input devices, user interfaces, multimedia and 360 degrees user experience.
A cryptographic hash function provides message integrity and authentication.
A function is used to compute a short `fingerprint’ of the data; if the data is modified, the fingerprint will not be valid.
h is the hash function, and x is the data.
The fingerprint is defined as y=h(x).
The fingerprint y is called as `message digest’ ; it is fairly short, e.g., 160 bits.
This is a survival guide covering the mind-numbing topics of Cryptography, Encryption, Authorization and Authentication. 
For the mathematically challenged (and who is not) the maths involved in cryptography are gruesome in the extreme and are covered, if at all, at the level of 'stuff happens'. 
The guide concentrates on system functionality and consists almost exclusively of descriptions and explanations rather than specific commands or implementations. 
Much of this stuff is background for SSL/TLS and X.509 certificates in which there are commands and implementation descriptions to give you a hearty buzz for the rest of your life.
MD5, technically called MD5 Message-Digest Algorithm, is a cryptographic hash function.
The MD5 cryptographic hash function is most often used to verify that a file has been unaltered by comparing the checksums created after running the algorithm on two seemingly identical files.
MD5 has certain flaws and so it isn't useful for advanced encryption applications but it's perfectly acceptable to use for standard file verifications.
SHA-1 is another commonly used cryptographic hash function.
This MessageDigest class provides applications the functionality of a message digest algorithm, such as SHA-1 or SHA-256. 
Message digests are secure one-way hash functions that take arbitrary-sized data and output a fixed-length hash value.
A MessageDigest object starts out initialized. 
The data is processed through it using the update methods. 
At any point reset can be called to reset the digest. 
Once all the data to be updated has been updated, one of the digest methods should be called to complete the hash computation.
The digest method can be called once for a given number of updates. 
After digest has been called, the MessageDigest object is reset to its initialized state.
Implementations are free to implement the Cloneable interface. 
Client applications can test cloneability by attempting cloning and catching the CloneNotSupportedException:
encryption takes a plain text and converts it to an encrypted text using a key and an encryption algorithm. 
The resulting encrypted text can later be decrypted (by using the key and the algorithm)
digest takes a plain text and generates a hashcode which can be used to verify if the plain text is unmodified but ich CANNOT be used to decrypt the original text form the hash value.
Message digest functions also called hash functions , are used to produce digital summaries of information called message digests. 
Message digests (also called hashes ) are commonly 128 bits to 160 bits in length and provide a digital identifier for each digital file or document. 
Message digest functions are mathematical functions that process information to produce a different message digest for each unique document. 
Identical documents have the same message digest; but if even one of the bits for the document changes, the message digest changes. 
A message digest or hash function is used to turn input of arbitrary length into an output of fixed length, which is called the digest or hash of the input. 
This output can then be used in place of the original input. 
This has many advantages. 
The output always has the same length, so this can be taken into account when processing or storing the message digest. 
Also, the output is much shorter than the input, so that processing and storing can be done much quicker.
The most common cryptographic hash function is MD5. 
MD5 was designed by well-known cryptographer Ronald Rivest in 1991. 
In 2004, some serious flaws were found in MD5. 
The complete implications of these flaws has yet to be determined. Another popular hash function is SHA-1.
To make hash functions work, they should have two properties:
Given a particular message digest, it should be very difficult to find an input that has the same message digest.
It should be very difficult to find two inputs that have the same message digest.
Figure 14.3 shows the basic message digest process.
A message digest is a cryptographic hash function containing a string of digits created by a one-way hashing formula. 
Message digests are designed to protect the integrity of a piece of data or media to detect changes and alterations to any part of a message. 
They are a type of cryptography utilizing hash values that can warn the copyright owner of any modifications applied to their work. 
Message digest hash numbers represent specific files containing the protected works.
 One message digest is assigned to particular data content.
 It can reference a change made deliberately or accidentally, but it prompts the owner to identify the modification as well as the individual(s) making the change. 
Message digests are algorithmic numbers.
This term is also known as a hash value and sometimes as a checksum.
The MD5 message-digest algorithm is a widely used cryptographic hash function producing a 128-bit (16-byte) hash value, typically expressed in text format as a 32 digit hexadecimal number. MD5 has been utilized in a wide variety of cryptographic applications, and is also commonly used to verify data integrity.
MD5 was designed by Ron Rivest in 1991 to replace an earlier hash function, MD4.[3] The source code in RFC 1321 contains a "by attribution" RSA license.
In 1996 a flaw was found in the design of MD5. 
While it was not deemed a fatal weakness at the time, cryptographers began recommending the use of other algorithms, such as SHA-1—which has since been found to be vulnerable as well.
 In 2004 it was shown that MD5 is not collision resistant.
 As such, MD5 is not suitable for applications like SSL certificates or digital signatures that rely on this property for digital security. 
Also in 2004 more serious flaws were discovered in MD5, making further use of the algorithm for security purposes questionable; specifically, a group of researchers described how to create a pair of files that share the same MD5 checksum.
Further advances were made in breaking MD5 in 2005, 2006, and 2007.
 In December 2008, a group of researchers used this technique to fake SSL certificate validity, and CMU Software Engineering Institute now says that MD5 "should be considered cryptographically broken and unsuitable for further use", and most U.S. government applications now require the SHA-2 family of hash functions.
In 2012, the Flame malware exploited the weaknesses in MD5 to fake a Microsoft digital signature.
A cryptographic hash function is a hash function that takes an arbitrary block of data and returns a fixed-size bit string, the cryptographic hash value, such that any (accidental or intentional) change to the data will (with very high probability) change the hash value. 
The data to be encoded are often called the message, and the hash value is sometimes called the message digest or simply digest.
The ideal cryptographic hash function has four main properties.
it is easy to compute the hash value for any given message.
it is infeasible to generate a message that has a given hash.
it is infeasible to modify a message without changing the hash.
it is infeasible to find two different messages with the same hash.
Cryptographic hash functions have many information security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. 
They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. 
Indeed, in information security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.
Traditional cryptography is based on the sender and receiver of a message knowing and using the same secret key: the sender uses the secret key to encrypt the message, and the receiver uses the same secret key to decrypt the message. 
This method is known as secret-key or symmetric cryptography. The main problem is getting the sender and receiver to agree on the secret key without anyone else finding out. 
If they are in separate physical locations, they must trust a courier, or a phone system, or some other transmission medium to prevent the disclosure of the secret key being communicated. 
Anyone who overhears or intercepts the key in transit can later read, modify, and forge all messages encrypted or authenticated using that key. The generation, transmission and storage of keys is called key management; all cryptosystems must deal with key management issues. 
Because all keys in a secret-key cryptosystem must remain secret, secret-key cryptography often has difficulty providing secure key management, especially in open systems with a large number of users.
The concept of public-key cryptography was introduced in 1976 by Whitfield Diffie and Martin Hellman [DH76] in order to solve the key management problem. 
In their concept, each person gets a pair of keys, one called the public key and the other called the private key. 
Each person's public key is published while the private key is kept secret. 
The need for the sender and receiver to share secret information is eliminated: all communications involve only public keys, and no private key is ever transmitted or shared. 
No longer is it necessary to trust some communications channel to be secure against eavesdropping or betrayal. 
The only requirement is that public keys are associated with their users in a trusted (authenticated) manner (for instance, in a trusted directory). 
Anyone can send a confidential message by just using public information, but the message can only be decrypted with a private key, which is in the sole possession of the intended recipient. 
Furthermore, public-key cryptography can be used not only for privacy (encryption), but also for authentication (digital signatures).
An encryption method that uses a two-part key: a public key and a private key. 
To send an encrypted message to someone, you use the recipient's public key, which can be sent to you via regular e-mail or made available on any public Web site or venue. 
To decrypt the message, the recipient uses the private key, which he or she keeps secret. Contrast with "secret key cryptography," which uses the same key to encrypt and decrypt. 
To create a digital signature that ensures the integrity of a message, document or other file, the keys are used in reverse. 
The private key is used to sign the file (encrypt the digest), and the public key is used to verify it (decrypt the digest).
Symmetric encryption is the oldest and best-known technique. A secret key, which can be a number, a word, or just a string of random letters, is applied to the text of a message to change the content in a particular way. 
This might be as simple as shifting each letter by a number of places in the alphabet.
 As long as both sender and recipient know the secret key, they can encrypt and decrypt all messages that use this key.
Asymmetric Encryption.
The problem with secret keys is exchanging them over the Internet or a large network while preventing them from falling into the wrong hands. 
Anyone who knows the secret key can decrypt the message. 
One answer is asymmetric encryption, in which there are two related keys--a key pair. 
A public key is made freely available to anyone who might want to send you a message. 
A second, private key is kept secret, so that only you know it. 
Any message (text, binary files, or documents) that are encrypted by using the public key can only be decrypted by applying the same algorithm, but by using the matching private key. 
Any message that is encrypted by using the private key can only be decrypted by using the matching public key. 
This means that you do not have to worry about passing public keys over the Internet (the keys are supposed to be public). 
A problem with asymmetric encryption, however, is that it is slower than symmetric encryption. 
It requires far more processing power to both encrypt and decrypt the content of the message.
That’s symmetric cryptography: you have one key, and you use it to encrypt (“lock”) and decrypt (“unlock”) your data.
As I’m working on a product that will make heavy use of encryption, I’ve found myself trying to explain public-key cryptography to friends more than once lately.
To my surprise, anything related I’ve come across online makes it look more complicated than it should. 
But it’s not.
First of all, let’s see how “symmetric” cryptography works.
John has a box with a lock. As usual, the lock has a key that can lock and unlock the box. 
So, if John wants to protect something, he puts it in the box and locks it. 
Obviously, only he or someone else with a copy of his key can open the box.
Public-key cryptography and related standards and techniques underlie security features of many Red Hat products, including signed and encrypted email, form signing, object signing, single sign-on, and the Secure Sockets Layer (SSL) protocol. 
This document introduces the basic concepts of public-key cryptography.
A cryptographic system that uses two keys -- a public key known to everyone and a private or secret key known only to the recipient of the message. 
When John wants to send a secure message to Jane, he uses Jane's public key to encrypt the message. 
Jane then uses her private key to decrypt it.
An important element to the public key system is that the public and private keys are related in such a way that only the public key can be used to encrypt messages and only the corresponding private key can be used to decrypt them. 
Moreover, it is virtually impossible to deduce the private key if you know the public key.
Public-key systems, such as Pretty Good Privacy (PGP), are becoming popular for transmitting information via the Internet. 
They are extremely secure and relatively simple to use. 
The only difficulty with public-key systems is that you need to know the recipient's public key to encrypt a message for him or her. 
What's needed, therefore, is a global registry of public keys, which is one of the promises of the new LDAP technology.
One of the weaknesses some point out about symmetric key encryption is that two users attempting to communicate with each other need a secure way to do so; otherwise, an attacker can easily pluck the necessary data from the stream. 
In November 1976, a paper published in the journal IEEE Transactions on Information Theory, titled "New Directions in Cryptography," addressed this problem and offered up a solution: public-key encryption.
Also known as asymmetric-key encryption, public-key encryption uses two different keys at once -- a combination of a private key and a public key. 
The private key is known only to your computer, while the public key is given by your computer to any computer that wants to communicate securely with it. 
To decode an encrypted message, a computer must use the public key, provided by the originating computer, and its own private key. 
Although a message sent from one computer to another won't be secure since the public key used for encryption is published and available to anyone, anyone who picks it up can't read it without the private key. 
The key pair is based on prime numbers (numbers that only have divisors of itself and one, such as 2, 3, 5, 7, 11 and so on) of long length. 
This makes the system extremely secure, because there is essentially an infinite number of prime numbers available, meaning there are nearly infinite possibilities for keys. 
One very popular public-key encryption program is Pretty Good Privacy (PGP), which allows you to encrypt almost anything.
The Public and Private key pair comprise of two uniquely related cryptographic keys (basically long random numbers). Below is an example of a Public Key.
The Public Key is what its name suggests - Public. It is made available to everyone via a publicly accessible repository or directory. 
On the other hand, the Private Key must remain confidential to its respective owner.
Asymmetric cryptography or public-key cryptography is cryptography in which a pair of keys is used to encrypt and decrypt a message so that it arrives securely. 
Initially, a network user receives a public and private key pair from a certificate authority. 
Any other user who wants to send an encrypted message can get the intended recipient's public key from a public directory.
 They use this key to encrypt the message, and they send it to the recipient.
 When the recipient gets the message, they decrypt it with their private key, which no one else should have access to.
Public-key cryptography, also known as asymmetric cryptography which requires public key and secret key.  
Although different, the two parts of this key pair are mathematically linked. 
The public key is used to encrypt plaintext or to verify a digital signature; whereas the private key is used to decrypt ciphertext or to create a digital signature. 
The term "asymmetric" stems from the use of different keys to perform these opposite functions, each the inverse of the other – as contrasted with conventional ("symmetric") cryptography which relies on the same key to perform both.
Public-key algorithms are based on mathematical problems which currently admit no efficient solution that are inherent in certain integer factorization, discrete logarithm, and elliptic curve relationships. 
It is computationally easy for a user to generate their own public and private key-pair and to use them for encryption and decryption. 
The strength lies in the fact that it is "impossible" (computationally unfeasible) for a properly generated private key to be determined from its corresponding public key. 
Thus the public key may be published without compromising security, whereas the private key must not be revealed to anyone not authorized to read messages or perform digital signatures. 
Public key algorithms, unlike symmetric key algorithms, do not require a secure initial exchange of one (or more) secret keys between the parties.
Message authentication involves processing a message with a private key to produce a digital signature. 
Thereafter anyone can verify this signature by processing the signature value with the signer's corresponding public key and comparing that result with the message. 
Success confirms the message is unmodified since it was signed, and – presuming the signer's private key has remained secret to the signer – that the signer, and no one else, intentionally performed the signature operation. 
In practice, typically only a hash or digest of the message, and not the message itself, is encrypted as the signature.
I was reading about block ciphers and most articles state they are being used in symmetric key cryptography. Are they also being used in public key cryptography?
block ciphers are used in public key cryptography, though typically as auxiliary building blocks rather than as the heart of the public key scheme by themselves.
One example is in digital signatures: many use a hash such as SHA-1, to digest the message being signed into a short cryptogram, and/or as part of a Mask Generating Function (loosely speaking: used as a randomizer before applying a public-key primitive). 
And in turn many hashes, including SHA-1, use a compression function built from a block cipher.
 Sometime, the function of a Mask Generating Functions is directly performed by a block cipher, as in this proposal.
In cryptography, a cipher (or cypher) is an algorithm for performing encryption or decryption—a series of well-defined steps that can be followed as a procedure. 
An alternative, less common term is encipherment.
 To encipher or encode is to convert information from plain text into cipher or code. 
In non-technical usage, a 'cipher' is the same thing as a 'code'; however, the concepts are distinct in cryptography. 
In classical cryptography, ciphers were distinguished from codes.
Codes generally substitute different length strings of characters in the output, whilst ciphers generally substitute the same number of characters as are input. 
There are exceptions and some cipher systems may use slightly more, or fewer, characters when output versus the number that were input.
Codes operated by substituting according to a large codebook which linked a random string of characters or numbers to a word or phrase. 
For example, "UQJHSE" could be the code for "Proceed to the following coordinates". 
When using a cipher the original information is known as plaintext, and the encrypted form as ciphertext. The ciphertext message contains all the information of the plaintext message, but is not in a format readable by a human or computer without the proper mechanism to decrypt it.
The operation of a cipher usually depends on a piece of auxiliary information, called a key (or, in traditional NSA parlance, a cryptovariable). 
The encrypting procedure is varied depending on the key, which changes the detailed operation of the algorithm. A key must be selected before using a cipher to encrypt a message. 
Without knowledge of the key, it should be extremely difficult, if not impossible, to decrypt the resulting ciphertext into readable plaintext.
Most modern ciphers can be categorized in several ways
By whether they work on blocks of symbols usually of a fixed size (block ciphers), or on a continuous stream of symbols (stream ciphers).
By whether the same key is used for both encryption and decryption (symmetric key algorithms), or if a different key is used for each (asymmetric key algorithms). 
If the algorithm is symmetric, the key must be known to the recipient and sender and to no one else.
 If the algorithm is an asymmetric one, the enciphering key is different from, but closely related to, the deciphering key. 
If one key cannot be deduced from the other, the asymmetric key algorithm has the public/private key property and one of the keys may be made public without loss of confidentiality.
Cryptography, the use of codes and ciphers to protect secrets, began thousands of years ago. Until recent decades, it has been the story of what might be called classic cryptography — that is, of methods of encryption that use pen and paper, or perhaps simple mechanical aids. 
In the early 20th century, the invention of complex mechanical and electromechanical machines, such as the Enigma rotor machine, provided more sophisticated and efficient means of encryption; and the subsequent introduction of electronics and computing has allowed elaborate schemes of still greater complexity, most of which are entirely unsuited to pen and paper.
The development of cryptography has been paralleled by the development of cryptanalysis — the "breaking" of codes and ciphers. 
The discovery and application, early on, of frequency analysis to the reading of encrypted communications has, on occasion, altered the course of history. 
Thus the Zimmermann Telegram triggered the United States' entry into World War I; and Allied reading of Nazi Germany's ciphers shortened World War II, in some evaluations by as much as two years.
Until the 1970s, secure cryptography was largely the preserve of governments. 
Two events have since brought it squarely into the public domain: the creation of a public encryption standard (DES), and the invention of public-key cryptography.
Symmetric-key algorithms are a class of algorithms for cryptography that use the same cryptographic keys for both encryption of plaintext and decryption of ciphertext.
 The keys may be identical or there may be a simple transformation to go between the two keys. 
The keys, in practice, represent a shared secret between two or more parties that can be used to maintain a private information link.
[2] This requirement that both parties have access to the secret key is one of the main drawbacks of symmetric key encryption, in comparison to public-key encryption.
Symmetric ciphers are commonly used to achieve other cryptographic primitives than just encryption.
Encrypting a message does not guarantee that this message is not changed while encrypted.
 Hence often a message authentication code is added to a ciphertext to ensure that changes to the ciphertext will be noted by the receiver. 
Message authentication codes can be constructed from symmetric ciphers.
However, symmetric ciphers cannot be used for non-repudiation purposes. 
See the ISO 13888-2 standard.
Another application is to build hash functions from block ciphers. 
See one-way compression function for descriptions of several such methods.
Cryptography prior to the modern age was effectively synonymous with encryption, the conversion of information from a readable state to apparent nonsense. 
The originator of an encrypted message shared the decoding technique needed to recover the original information only with intended recipients, thereby precluding unwanted persons to do the same. 
Since World War I and the advent of the computer, the methods used to carry out cryptology have become increasingly complex and its application more widespread.
Cryptography is the discipline of using codes and ciphers to encrypt a message and make it unreadable unless the recipient knows the secret to decrypt it. 
Encryption has been used for many thousands of years. 
The following codes and ciphers can be learned and used to encrypt and decrypt messages by hand.
By World War II mechanical and electromechanical cryptographic cipher machines were in wide use, although where these were impractical manual systems continued to be used. 
Great advances were made in both practical and mathematical cryptography in this period, all in secrecy. 
Information about this period has begun to be declassified in recent years as the official 50-year (British) secrecy period has come to an end, as the relevant US archives have slowly opened, and as assorted memoirs and articles have been published.
The Germans made heavy use (in several variants) of an electromechanical rotor based cipher system known as Enigma.
 Marian Rejewski, in Poland, attacked and 'broke' the early German Army Enigma system (an electromechanical rotor cipher machine) using theoretical mathematics in 1932. 
It was the greatest breakthrough in cryptanalysis in a thousand years and more. 
The Polish break into Enigma traffic continued up to '39, when changes in the way the German Army used Enigma required more resources to continue the break than the Poles had available. 
They passed their knowledge, some sample machines, on to the British and French that summer. 
Even Rejewski and his fellow mathematicians and cryptographers from the Biuro Szyfrow ended up with the British and French after the German blitzkrieg. 
The work was extended by Alan Turing, Gordon Welchman, and others at Bletchley Park leading to sustained breaks into several other of the Enigma variants and into the message traffic on the assorted networks for which they were used.
Ciphers and cryptography.
Substitution cipher.
The substitution cipher involves one piece of data being replaced with another. 
The data could be a letter. 
For example, in a message the letter a is replaced by the letter d, the letter b replaced by the letter e, the letter c replaced by the letter f, and so on. 
Each letter is replaced by the one three to the right of it in the alphabet. When letter x is reached it is transposed to the letter a. 
In effect the alphabet has been 'wrapped around'. 
The table below shows the arrangement of plaintext and ciphertext characters using a shift of three.
As mentioned before, most of the ciphers we have examined are not really cryptographically secure, although they may have been adequate prior to the widespread availability of computers. 
A cryptosystem is called secure if a good cryptanalyst, armed with knowledge the details of the cipher (but not the key used), would require a prohibitively large amount of computation to decipher a plaintext. 
This idea (that the potential cryptanalyst knows everything but the key) is called Kerckhoff's Law or sometimes Shannon's Maxim.
Kerckhoff's Law at first seems unreasonably strong rule; you may be thinking that, given a mass of encrypted data, how is the cryptanalyst to know by what means it was encrypted?
 If you are encrypting your own files for personal use and writing your own software which you keep under lock and key, this assumption is not unreasonable.
 But today, most encryption is done by software or hardware that the user did not produce himself.
 One can reverse engineer a piece of software (or hardware) and make the cryptographic algorithm apparent, so we cannot rely on the secrecy of the method alone as a good measure of security. 
For example, when you make a purchase over the internet, the encryption method that is used between your browser and the seller's web server is public knowledge. 
EAVESDROPPING, be it simply sticking an ear against a door or listening to and analysing the noises made by tapping different keys on a keyboard, is a stock-in-trade of spying. 
Listening to a computer itself, though, as it hums away doing its calculations, is a new idea. 
But it is one whose time has come, according to Adi Shamir, of the Weizmann Institute, in Israel, and his colleagues. 
And Dr Shamir should know. 
He donated the initial letter of his surname to the acronym “RSA”, one of the most commonly used forms of encryption. 
Acoustic cryptanalysis, as the new method is known, threatens RSA’s security.
Acoustic cryptanalysis works by listening to a computer’s sonic signature—the noise its capacitors and coils make as they vibrate in response to the amount of power being drawn by its processor. 
Dr Shamir and his collaborator Eran Tromer, of Tel Aviv University, showed in 2004 that processing different RSA keys (the huge numbers needed to unlock the hidden message) produces different sonic signatures. 
At the time, they were unable to extract from these signatures the individual binary digits (bits) of a key, but in collaboration with Daniel Genkin of the Technion-Israel Institute of Technology they have overcome this obstacle, by tricking machines into decrypting known pieces of text.
Indeed, it must be public: if not, only those privy to the secret could create web browsers or web servers. 
The security of the transaction depends on the security of the keys.
Quantum cryptography describes the use of quantum mechanical effects (in particular quantum communication and quantum computation) to perform cryptographic tasks or to break cryptographic systems.
Well-known examples of quantum cryptography are the use of quantum communication to exchange a key securely (quantum key distribution) and the hypothetical use of quantum computers that would allow the breaking of various popular public-key encryption and signature schemes (e.g., RSA and ElGamal).
The advantage of quantum cryptography lies in the fact that it allows the completion of various cryptographic tasks that are proven or conjectured to be impossible using only classical (i.e. non-quantum) communication (see below for examples).
 For example, quantum mechanics guarantees that measuring quantum data disturbs that data; this can be used to detect eavesdropping in quantum key distribution.
Quantum key distribution (QKD) uses quantum mechanics to guarantee secure communication. 
It enables two parties to produce a shared random secret key known only to them, which can then be used to encrypt and decrypt messages. 
It is often incorrectly called quantum cryptography, as it is the most well known example of the group of quantum cryptographic tasks.
An important and unique property of quantum distribution is the ability of the two communicating users to detect the presence of any third party trying to gain knowledge of the key. 
This results from a fundamental aspect of quantum mechanics: the process of measuring a quantum system in general disturbs the system. 
A third party trying to eavesdrop on the key must in some way measure it, thus introducing detectable anomalies. 
By using quantum superpositions or quantum entanglement and transmitting information in quantum states, a communication system can be implemented which detects eavesdropping. 
If the level of eavesdropping is below a certain threshold, a key can be produced that is guaranteed to be secure (i.e. the eavesdropper has no information about it), otherwise no secure key is possible and communication is aborted.
The security of quantum key distribution relies on the foundations of quantum mechanics, in contrast to traditional key distribution protocol which relies on the computational difficulty of certain mathematical functions, and cannot provide any indication of eavesdropping or guarantee of key security.
Quantum key distribution is only used to produce and distribute a key, not to transmit any message data. 
This key can then be used with any chosen encryption algorithm to encrypt (and decrypt) a message, which can then be transmitted over a standard communication channel. 
The algorithm most commonly associated with QKD is the one-time pad, as it is provably secure when used with a secret, random key.[1]
The idea that a vote cast by a person remains the same after he submitted it is taken very seriously in any democracy. 
Voting is the right of the citizen, and it's how we choose the people who make important decisions on our behalf. 
When the security of the ballot is compromised, so, too, is the individual's right to choose his leaders.
There are plentiful examples of vote tampering throughout history in the United States and in other countries. 
Votes get lost, the dead manage to sho­w up on the poll results, and sometimes votes are even changed when they're tallied.
But, hopefully, the days when paper ballots get lost on the back roads of Florida en route to be counted will soon be gone, and the hanging chad will become an obscure joke on sitcom reruns from the early 21st century. 
In other words, it's possible that the votes we cast will soon become much more secure.
One of the ways to safeguard votes is to limit access to them when they're being transferred from precincts to central polling stations where they're tallied. 
And this is just what the Swiss are looking into. 
The nation best known for its neutrality is on the cutting edge of research into quantum cryptography. 
But unlike traditional cryptology methods -- encoding and decoding information or messages -- quantum cryptology depends on physics, not mathematics.
U­sing a machine developed by Swiss manufacturer Id Quantique, votes cast in the Swiss canton of Geneva during the October 2007 parliamentary elections were transmitted using a secure encryption encoded by a key generated using photons -- tiny, massless packets of light. 
Since this method uses physics instead of math to create the key used to encrypt the data, there's little chance it can be cracked using mathematics. 
In other words, the votes cast by citizens in Geneva are more protected than ever.
Id Quantiques' quantum encryption is the first public use of such a technique. 
What's more, it has catapulted the little-known world of quantum cryptology onto the world stage. 
So how does it work? Since it's based on quantum physics -- the smallest level of matter science has been able to detect -- it can seem a little confusing. 
But don't worry, even quantum physicists find quantum physics incredibly perplexing.
In this article, we'll get to the bottom of how quantum encryption works, and how it differs from modern cryptology. 
But first, we'll look at the uses and the limitations of traditional cryptology methods.
Every time you perform an online transaction, such as a purchase or bank transfer, you entrust your personal data to a secure encryption system. 
Such encryption is based on mathematical problems too difficult for present-day computers to crack, which is why your information is relatively safe. 
But future computers — quantum computers in particular — will be able to decrypt many such coded messages. 
We need new cryptographic tools that are secure in a quantum world. 
Fortunately, the rules of quantum mechanics enable codes that cannot be broken with any amount of computing power.
The rules of quantum mechanics dictate that a quantum system cannot be observed without being disrupted. 
This means that “ key” material exchanged via quantum communication will bear the indelible fingerprint of any attempted eavesdropping. 
Eavesdropped keys can be abandoned, and only truly private keys are kept to be used in unbreakable encryption protocols.
Cryptography offers — among other things — confidentiality of data transmissions. 
Before being transmitted, data is encrypted using an encryption algorithm (or process) and a secret key. 
After transmission, data is decrypted by reversing the encryption algorithm using the same secret key. 
The security of this scheme is based on the premise that the key is distributed only to the legitimate parties. 
This implies that the key transmission is a central problem (more information under Key Distribution).
Conventional cryptographic techniques rely on mathematical approaches to secure key transmission. 
However the security they offer is based on unproven assumptions and depends on the technology available to an eavesdropper.
Quantum Key Distribution is a technology that allows transmission of a sequence of random bits across an optical network and also verifies if this sequence was intercepted or not. 
This verification is based on the laws of quantum physics.
In practice, QKD is combined with conventional key distribution techniques (dual key agreement) to produce a key that is as secure as the strongest of the two original keys. 
With this approach, one can be sure to get the best of the classical and quantum world.
In summary, QKD provides long-term data transmission secrecy, which is not vulnerable to technological progress. 
On the contrary, classical cryptography provides secrecy only for a limited period of time.
Quantum cryptography uses our current knowledge of physics to develop a cryptosystem that is not able to be defeated - that is, one that is completely secure against being compromised without knowledge of the sender or the receiver of the messages. 
The word quantum itself refers to the most fundamental behavior of the smallest particles of matter and energy: quantum theory explains everything that exists and nothing can be in violation of it.
Quantum cryptography is different from traditional cryptographic systems in that it relies more on physics, rather than mathematics, as a key aspect of its security model.
Essentially, quantum cryptography is based on the usage of individual particles/waves of light (photon) and their intrinsic quantum properties to develop an unbreakable cryptosystem - essentially because it is impossible to measure the quantum state of any system without disturbing that system. 
It is theoretically possible that other particles could be used, but photons offer all the necessary qualities needed, their behavior is comparatively well-understood, and they are the information carriers in optical fiber cables, the most promising medium for extremely high-bandwidth communications.
The aim of this workshop will be to set out some short and medium term objectives towards the long-term goal of a quantum-safe cryptographic infrastructure. 
Such goals could include, for example, a standard for combining a battle-tested conventional key-establishment algorithm with a quantum-safe key establishment protocol, other study items or pre-standards, etc.
In order to achieve these goals, all stakeholders are encouraged to start new standardization work within ETSI (either new groups and/or expand the role of some existing groups).
The targeted audience consists of the key players and decision makers in deploying a global quantum-safe cryptographic infrastructure. 
In room-size metal boxes ­secure against electromagnetic leaks, the National Security Agency is racing to build a computer that could break nearly every kind of encryption used to protect banking, medical, business and government records around the world.
According to documents provided by former NSA contractor Edward Snowden, the effort to build “a cryptologically useful quantum computer” — a machine exponentially faster than classical computers — is part of a $79.7 million research program titled “Penetrating Hard Targets.” 
Much of the work is hosted under classified contracts at a laboratory in College Park, Md.
The development of a quantum computer has long been a goal of many in the scientific community, with revolutionary implications for fields such as medicine as well as for the NSA’s code-breaking mission. 
With such technology, all current forms of public key encryption would be broken, including those used on many secure Web sites as well as the type used to protect state secrets.
Physicists and computer scientists have long speculated about whether the NSA’s efforts are more advanced than those of the best civilian labs. 
Although the full extent of the agency’s research remains unknown, the documents provided by Snowden suggest that the NSA is no closer to success than others in the scientific community.
“It seems improbable that the NSA could be that far ahead of the open world without anybody knowing it,” said Scott Aaronson, an associate professor of electrical engineering and computer science at the Massachusetts Institute of Technology.
FORTUNE -- As revelations about the depth and breadth of the NSA's digital eavesdropping program continue to come to light, Ohio-based Battelle Memorial Institute is rolling out a new kind of network encryption designed to be virtually un-hackable -- not only now, but in the future. The non-profit research and development contractor has installed the first quantum key distribution (QKD) protected network in the U.S. linking its headquarters in Columbus to those in its manufacturing facilities in Dublin, Ohio, some 20 miles away.
Transmission of secure data typically relies on encryption and decryption "keys" generated by sophisticated algorithms and swapped between sender and receiver so encrypted data can be deciphered. 
These keys are generally considered secure, but their degree of security is highly dependent on how much computing power a third party has at its disposal. 
High-powered supercomputers can crack many of today's standard encryptions, and those encryption schemes that aren't breakable now will become so in the future as the speed and power of supercomputers continue their ever-accelerating uptick.
In other words, even the best standard encryption that's considered unbreakable today will be vulnerable at some point in the future -- likely the near future. 
That's one reason agencies like the NSA are building massive server farms in the Utah desert on which to bank encrypted data that they can't yet decipher.
 And it's why Battelle and its partners at Swiss quantum technology outfit ID Quantique are investing heavily in a new encryption scheme that they see as the future of high-value data security.
With the revelations that the National Security Agency is snooping in everyone’s data, it’s no surprise that some institutions would want to protect it with something that is spy-proof, and with unbreakable encryption as well. 
Banks, hospitals and government agencies all deal in sensitive data, which often by law they are obligated to protect.
10 Good Techs Gone Bad
The answer is quantum cryptography. 
Once it was the province of government agencies only, but lately the technology has moved to where smaller institutions can use it too.
Several companies have entered the field to build the technology, among them MagiQ Technologies, Quintessence Labs and ID Quantique. 
Nokia and the University of Bristol in the UK have even developed a version of quantum encryption for mobile phones.
It’s a measure of how seriously people take this that investors are putting serious funding into companies that were previously like start-ups — ID Quantique just got $5.6 million from a venture fund, QWave, that specializes in cutting-edge technology.
Quantum encryption is so powerful because it’s impossible to listen in on the data without corrupting it. 
Quantum bits can’t be copied, so any attempt to listen in changes them and alerts the people communicating that something is up.
A document management system (DMS) is a system (based on computer programs in the case of the management of digital documents) used to track, manage and store documents and reduce paper. 
Most are capable of keeping a record of the various versions created and modified by different users (history tracking). 
The term has some overlap with the concepts of content management systems. 
It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems.
Beginning in the 1980s, a number of vendors began developing software systems to manage paper-based documents. 
These systems dealt with paper documents, which included not only printed and published documents, but also photographs, prints, etc..
Later developers began to write a second type of system which could manage electronic documents, i.e., all those documents, or files, created on computers, and often stored on users' local file-systems. 
The earliest electronic document management (EDM) systems managed either proprietary file types, or a limited number of file formats. 
Many of these systems later[when?] became known as document imaging systems, because they focused on the capture, storage, indexing and retrieval of image file formats. 
EDM systems evolved to a point where systems could manage any type of file format that could be stored on the network. 
The applications grew to encompass electronic documents, collaboration tools, security, workflow, and auditing capabilities.
These systems enabled an organization to capture faxes and forms, to save copies of the documents as images, and to store the image files in the repository for security and quick retrieval (retrieval made possible because the system handled the extraction of the text from the document in the process of capture, and the text-indexer function provided text-retrieval capabilities).
While many EDM systems store documents in their native file format (Microsoft Word or Excel, PDF), some web-based document management systems are beginning to store content in the form of html. 
These policy management systems[1] require content to be imported into the system. 
The html format allows for better application of search capabilities such as full-text searching and stemming.
In computing, an image scanner—often abbreviated to just scanner, although the term is ambiguous out of context (barcode scanner, CAT scanner, etc.)—is a device that optically scans images, printed text, handwriting, or an object, and converts it to a digital image. 
Commonly used in offices are variations of the desktop flatbed scanner where the document is placed on a glass window for scanning. 
Hand-held scanners, where the device is moved by hand, have evolved from text scanning "wands" to 3D scanners used for industrial design, reverse engineering, test and measurement, orthotics, gaming and other applications. 
Mechanically driven scanners that move the document are typically used for large-format documents, where a flatbed design would be impractical.
Modern scanners typically use a charge-coupled device (CCD) or a contact image sensor (CIS) as the image sensor, whereas drum scanners, developed earlier and still used for the highest possible image quality, use a photomultiplier tube (PMT) as the image sensor.
 A rotary scanner, used for high-speed document scanning, is a type of drum scanner that uses a CCD array instead of a photomultiplier.
 Non-contact planetary scanners essentially photograph delicate books and documents. 
All these scanners produce two-dimensional images of subjects that are usually flat, but sometimes solid; 3D scanners produce information on the three-dimensional structure of solid objects.
Digital cameras can be used for the same purposes as dedicated scanners. 
when compared to a true scanner, a camera image is subject to a degree of distortion, reflections, shadows, low contrast, and blur due to camera shake (reduced in cameras with image stabilization). 
Resolution is sufficient for less demanding applications. 
Digital cameras offer advantages of speed, portability and non-contact digitizing of thick documents without damaging the book spine. As of 2010 scanning technologies were combining 3D scanners with digital cameras to create full-color, photo-realistic 3D models of objects.[1]
In the biomedical research area, detection devices for DNA microarrays are called scanners as well. 
These scanners are high-resolution systems (up to 1 µm/ pixel), similar to microscopes. The detection is done via CCD or a photomultiplier tube.
Modern scanners are considered the successors of early telephotography and fax input devices.
The pantelegraph (Italian: pantelegrafo; French: pantélégraphe) was an early form of facsimile machine transmitting over normal telegraph lines developed by Giovanni Caselli, used commercially in the 1860s, that was the first such device to enter practical service. 
It used electromagnets to drive and synchronize movement of pendulums at the source and the distant location, to scan and reproduce images. 
It could transmit handwriting, signatures, or drawings within an area of up to 150 x 100mm.
Édouard Belin's Belinograph of 1913, scanned using a photocell and transmitted over ordinary phone lines, formed the basis for the AT&T Wirephoto service. 
In Europe, services similar to a wirephoto were called a Belino.
 It was used by news agencies from the 1920s to the mid-1990s, and consisted of a rotating drum with a single photodetector at a standard speed of 60 or 120 rpm (later models up to 240 rpm). 
They send a linear analog AM signal through standard telephone voice lines to receptors, which synchronously print the proportional intensity on special paper. 
Color photos were sent as three separated RGB filtered images consecutively, but only for special events due to transmission costs.
Drum scanners capture image information mostly with photomultiplier tubes (PMT), rather than the charge-coupled device (CCD) arrays found in flatbed scanners and inexpensive film scanners. 
"Reflective and transmissive originals are mounted on an acrylic cylinder, the scanner drum, which rotates at high speed while it passes the object being scanned in front of precision optics that deliver image information to the PMTs. 
Most modern color drum scanners use three matched PMTs, which read red, blue, and green light, respectively.
 Light from the original artwork is split into separate red, blue, and green beams in the optical bench of the scanner."
Photomultipliers offer superior dynamic range and for this reason drum scanners using photomultiplier tubes can extract more detail from very dark shadow areas of a transparency than flatbed scanners using CCD sensors. 
The smaller dynamic range of the CCD sensors, versus photomultiplier tubes, can lead to loss of shadow detail, especially when scanning very dense transparency film.
 Some DeskTop Publishing drum scanners use cheaper illumination/sensor systems than PMT such as halogen lamps and photodiodes, which have their dynamic range less than that of photomultipliers but higher than that of charge coupled devices.[5]
The drum scanner gets its name from the clear acrylic cylinder, the drum, on which the original artwork is mounted for scanning. 
Depending on size, it is possible to mount originals up to 20"x28", but maximum size varies by manufacturer. 
"One of the unique features of drum scanners is the ability to control sample area and aperture size independently. 
The sample size is the area that the scanner encoder reads to create an individual pixel. The aperture is the actual opening that allows light into the optical bench of the scanner. 
The ability to control aperture and sample size separately is particularly useful for smoothing film grain when scanning black-and-white and color negative originals."[3]
While drum scanners are capable of scanning both reflective and transmissive artwork, a good-quality flatbed scanner can produce good scans from reflective artwork. 
As a result, drum scanners are rarely used to scan prints now that high-quality, inexpensive flatbed scanners are readily available. 
Film, however, is where drum scanners continue to be the tool of choice for high-end applications. 
Because film can be wet-mounted to the scanner drum and because of the exceptional sensitivity of the PMTs, drum scanners are capable of capturing very subtle details in film originals.
The situation as of 2014 was that only a few companies continued to manufacture drum scanners.
While prices of both new and used units dropped from the start of the 21st century, they were still much more costly than CCD flatbed and film scanners. 
Image quality produced by flatbed scanners had improved to the degree that the best ones were suitable for most graphic-arts operations, and they replaced drum scanners in many cases as they were less expensive and faster. 
However, drum scanners with their superior resolution (up to 24,000 PPI), color gradation, and value structure continued to be used for scanning images to be much enlarged, and for museum-quality archiving of photographs and print production of high-quality books and magazine advertisements. 
As second-hand drum scanners became more plentiful and less costly, many fine-art photographers acquired them.
This type of scanner is sometimes called reflective scanner because it works by shining white light onto the object to be scanned and reading the intensity and color of light that is reflected from it, usually a line at a time. 
They are designed for scanning prints or other flat, opaque materials but some have available transparency adapters, which for a number of reasons, in most cases, are not very well suited to scanning film.[6]
"A flatbed scanner is usually composed of a glass pane (or platen), under which there is a bright light (often xenon, LED or cold cathode fluorescent) which illuminates the pane, and a moving optical array in CCD scanning. 
CCD-type scanners typically contain three rows (arrays) of sensors with red, green, and blue filters
Contact image sensor (CIS) scanning consists of a moving set of red, green and blue LEDs strobed for illumination and a connected monochromatic photodiode array under a rod lens array for light collection. 
"Images to be scanned are placed face down on the glass, an opaque cover is lowered over it to exclude ambient light, and the sensor array and light source move across the pane, reading the entire area. 
An image is therefore visible to the detector only because of the light it reflects. 
Transparent images do not work in this way, and require special accessories that illuminate them from the upper side. Many scanners offer this as an option."[7]
Main article: Film scanner
DSLR camera and slide scanner
This type of scanner is sometimes called a slide or transparency scanner and it works by passing a narrowly focused beam of light through the film and reading the intensity and color of the light that emerges.
 "Usually, uncut film strips of up to six frames, or four mounted slides, are inserted in a carrier, which is moved by a stepper motor across a lens and CCD sensor inside the scanner. 
Some models are mainly used for same-size scans. Film scanners vary a great deal in price and quality."
The lowest-cost dedicated film scanners can be had for less than $50 and they might be sufficient for modest needs. 
From there they inch up in staggered levels of quality and advanced features upward of five figures. 
"The specifics vary by brand and model and the end results are greatly determined by the level of sophistication of the scanner's optical system and, equally important, the sophistication of the scanning software."[9]
Scanners are available that pull a flat sheet over the scanning element between rotating rollers.
 They can only handle single sheets up to a specified width (typically about 210 mm, the width of many printed letters and documents), but can be very compact, just requiring a pair of narrow rollers between which the document is passed. 
Some are portable, powered by batteries and with their own storage, eventually transferring stored scans to a computer over a USB or other interface.
"Hand held document scanners are manual devices that are dragged across the surface of the image to be scanned by hand. 
Scanning documents in this manner requires a steady hand, as an uneven scanning rate produces distorted images; an indicator light on the scanner indicates if motion is too fast.
 They typically have a "start" button, which is held by the user for the duration of the scan; some switches to set the optical resolution; and a roller, which generates a clock pulse for synchronization with the computer. 
Older hand scanners were monochrome, and produced light from an array of green LEDs to illuminate the image";[8] later ones scan in monochrome or color, as desired. 
A hand scanner may have a small window through which the document being scanned could be viewed. 
In the early 1990s many hand scanners had a proprietary interface module specific to a particular type of computer, such as an Atari ST or Commodore Amiga. 
Since the introduction of the USB standard, it is the interface most commonly used. 
As hand scanners are much narrower than most normal document or book sizes, software (or the end user) needed to combine several narrow "strips" of scanned document to produce the finished article.
Inexpensive portable battery-powered "glide-over" hand scanners, typically capable of scanning an area as wide as a normal letter and much longer, remain, available as of 2014.
Handheld 3D scanners are used in industrial design, reverse engineering, inspection and analysis, digital manufacturing and medical applications. 
"To compensate for the uneven motion of the human hand, most 3D scanning systems rely on the placement of reference markers, typically adhesive reflective tabs that the scanner uses to align elements and mark positions in space.
Image scanners are usually used in conjunction with a computer which controls the scanner and stores scans. 
Small portable scanners, either roller-fed or "glide-over" hand-operated, operated by batteries and with storage capability, are available for use away from a computer; stored scans can be transferred later. 
Many can scan both small documents such as business cards and till receipts, and letter-sized documents.
Many such apps can scan multiple-page documents with successive camera exposures and output them either as a single file or multiple page files. 
Some smartphone scanning apps can save documents directly to online storage locations, such as Dropbox and Evernote, send via email or fax documents via email-to-fax gateways.[10]
Color scanners typically read RGB (red-green-blue color) data from the array.
 This data is then processed with some proprietary algorithm to correct for different exposure conditions, and sent to the computer via the device's input/output interface (usually USB, previous to which was SCSI or bidirectional parallel port in older units).
Color depth varies depending on the scanning array characteristics, but is usually at least 24 bits. 
High quality models have 36-48 bits of color depth.
Another qualifying parameter for a scanner is its resolution, measured in pixels per inch (ppi), sometimes more accurately referred to as Samples per inch (spi). 
Instead of using the scanner's true optical resolution, the only meaningful parameter, manufacturers like to refer to the interpolated resolution, which is much higher thanks to software interpolation. 
As of 2009, a high-end flatbed scanner can scan up to 5400 ppi and drum scanners have an optical resolution of between 3,000 and 24,000 ppi.
"Effective resolution" is the true resolution of a scanner, and is determined by using a resolution test chart. 
The effective resolution of most all consumer flatbed scanners is considerably lower than the manufactures' given optical resolution. 
Example is the Epson V750 Pro with an optical resolution given by manufacturer as being 4800dpi and 6400dpi (dual lens),[13] but tested "According to this we get a resolution of only about 2300 dpi - that's just 40% of the claimed resolution!"
 Dynamic range is claimed to be 4.0 Dmax, but "Regarding the density range of the Epson Perfection V750 Pro, which is indicated as 4.0, one must say that here it doesn't reach the high-quality <of> film scanners either."[14]
Manufacturers often claim interpolated resolutions as high as 19,200 ppi; but such numbers carry little meaningful value, because the number of possible interpolated pixels is unlimited and doing so does not increase the level of captured detail.
The size of the file created increases with the square of the resolution; doubling the resolution quadruples the file size. 
A resolution must be chosen that is within the capabilities of the equipment, preserves sufficient detail, and does not produce a file of excessive size. 
The file size can be reduced for a given resolution by using "lossy" compression methods such as JPEG, at some cost in quality. 
If the best possible quality is required lossless compression should be used; reduced-quality files of smaller size can be produced from such an image when required (e.g., image designed to be printed on a full page, and a much smaller file to be displayed as part of a fast-loading web page).
Purity can be diminished by scanner noise, optical flare, poor analog to digital conversion, scratches, dust, Newton's rings, out of focus sensors, improper scanner operation, and poor software. 
Drum scanners are said to produce the purest digital representations of the film, followed by high end film scanners that use the larger Kodak Tri-Linear sensors.
The third important parameter for a scanner is its density range (Dynamic Range) or Drange (see Densitometry). 
A high density range means that the scanner is able to record shadow details and brightness details in one scan. 
Density of film is measured on a base 10 log scale and varies between 0.0 (transparent) and 5.0, about 16 stops.
Density range is the space taken up in the 0 to 5 scale, and Dmin and Dmax denote where the least dense and most dense measurements on a negative or positive film. 
The density range of negative film is up to 3.6d,[15] while slide film dynamic range is 2.4d.
 Color negative density range after processing is 2.0d thanks to compression of the 12 stops into a small density range. 
Dmax will be the densest on slide film for shadows, and densest on negative film for highlights. 
Some slide films can have a Dmax close to 4.0d with proper exposure, and so can black and white negative film.
Consumer level flatbed photo scanners have a dynamic range in the 2.0-3.0 range, which can be inadequate for scanning all types of photographic film, as Dmax can and often is between 3.0d and 4.0d with traditional black and white film. 
Color film compresses its 12 stops of a possible 16 stops (film latitude) into just 2.0d of space via the process of dye coupling and removal of all silver from the emulsion. 
Kodak Vision 3 has 18 stops. 
So, color negative film scans the easiest of all film types on the widest range of scanners. 
Because traditional black and white film retains the image creating silver after processing, density range can be almost twice that of color film. 
 Office document scanners can have a dynamic range less than 2.0d.
 Drum scanners have a dynamic range of 3.6-4.5.
By combining full-color imagery with 3D models, modern hand-held scanners are able to completely reproduce objects electronically. 
The addition of 3D color printers enables accurate miniaturization of these objects, with applications across many industries and professions.
Scans must virtually always be transferred from the scanner to a computer or information storage system for further processing or storage.
 There are two basic issues: (1) how the scanner is physically connected to the computer and (2) how the application retrieves the information from the scanner.
Direct physical connection to a computer
The file size of a scan can be up to about 100 megabytes for a 600 DPI 23 x 28 cm (9"x11") (slightly larger than A4 paper) uncompressed 24-bit image. 
Scanned files must be transferred and stored. 
Scanners can generate this volume of data in a matter of seconds, making a fast connection desirable.
Scanners communicate to their host computer using one of the following physical interfaces, listing roughly from slow to fast:
Parallel port - Connecting through a parallel port is the slowest common transfer method. 
Early scanners had parallel port connections that could not transfer data faster than 70 kilobytes/second. 
The primary advantage of the parallel port connection was economic and user skill level: it avoided adding an interface card to the computer.
GPIB - General Purpose Interface Bus. 
Certain drumscanners like the Howtek D4000 featured both a SCSI and GPIB interface. 
The latter conforms to the IEEE-488 standard, introduced in the mid ’70's.
 The GPIB-interface has only been used by a few scanner manufactures, mostly serving the DOS/Windows environment. 
For Apple Macintosh systems, National Instruments provided a NuBus GPIB interface card.
Small Computer System Interface (SCSI), rarely used since the early twentyfirst century, supported only by computers with a SCSI interface on a card or built in. 
During the evolution of the SCSI standard speeds increased. 
Widely available and easily set up USB and Firewire largely supplanted SCSI.
Universal Serial Bus (USB) scanners can transfer data quickly. 
The early USB 1.1 standard could transfer data at 1.5 megabytes per second (slower than SCSI), but the later USB 2.0/3.0 standards can transfer at more than 20/60 megabytes per second in practice.
FireWire, or IEEE-1394, is an interface of comparable speed to USB 2.0. 
Possible FireWire speeds are 25, 50, and 100, 400 and 800 megabits per second, but devices may not support all speeds.
Proprietary interfaces were used on some early scanners that used a proprietary interface card rather than a standard interface.
Indirect (network) connection to a computer
During the early 1990s professional flatbed scanners were available over a local computer network. 
This proved useful to publishers, print shops, etc.
This functionality largely fell out of use as the cost of flatbed scanners reduced enough to make sharing unnecessary.
From 2000 all-in-one multi-purpose devices became available which were suitable for both small offices and consumers, with printing, scanning, copying, and fax capability in a single apparatus which can be made available to all members of a workgroup.
Battery-powered portable scanners store scans on internal memory; they can later be transferred to a computer either by direct connection, typically USB, or in some cases a memory card may be removed from the scanner and plugged into the computer.
Applications Programming Interface
A paint application such as GIMP or Adobe Photoshop must communicate with the scanner. 
There are many different scanners, and many of those scanners use different protocols. 
In order to simplify applications programming, some Applications Programming Interfaces ("API") were developed. 
The API presents a uniform interface to the scanner. 
This means that the application does not need to know the specific details of the scanner in order to access it directly. 
For example, Adobe Photoshop supports the TWAIN standard; therefore in theory Photoshop can acquire an image from any scanner that has a TWAIN driver.
In practice, there are often problems with an application communicating with a scanner.
Either the application or the scanner manufacturer (or both) may have faults in their implementation of the API.
Typically, the API is implemented as a dynamically linked library. 
Each scanner manufacturer provides software that translates the API procedure calls into primitive commands that are issued to a hardware controller (such as the SCSI, USB, or FireWire controller). 
The manufacturer's part of the API is commonly called a device driver, but that designation is not strictly accurate: the API does not run in kernel mode and does not directly access the device. 
Rather the scanner API library translates application requests into hardware requests.
Common scanner software API interfaces:
SANE (Scanner Access Now Easy) is a free/open source API for accessing scanners. 
Originally developed for Unix and Linux operating systems, it has been ported to OS/2, Mac OS X, and Microsoft Windows. 
Unlike TWAIN, SANE does not handle the user interface. 
This allows batch scans and transparent network access without any special support from the device driver.
TWAIN is used by most scanners. 
Originally used for low-end and home-use equipment, it is now widely used for large-volume scanning.
ISIS (Image and Scanner Interface Specification) created by Pixel Translations, which still uses SCSI-II for performance reasons, is used by large, departmental-scale, machines.
WIA (Windows Image Acquisition) is an API provided by Microsoft for use on Microsoft Windows.
Bundled applications
Although no software beyond a scanning utility is a feature of any scanner, many scanners come bundled with software. 
Typically, in addition to the scanning utility, some type of image-editing application (such as Photoshop), and optical character recognition (OCR) software are supplied. 
OCR software converts graphical images of text into standard text that can be edited using common word-processing and text-editing software; accuracy is rarely perfect.
Some scanners, especially those designed for scanning printed documents, only work in black and white but most modern scanners work in color.
For the latter, the scanned result is a non-compressed RGB image, which can be transferred to a computer's memory. 
The color output of different scanners is not the same due to the spectral response of their sensing elements, the nature of their light source and the correction applied by the scanning software. 
While most image sensors have a linear response, the output values are usually gamma compressed. 
Some scanners compress and clean up the image using embedded firmware. 
Once on the computer, the image can be processed with a raster graphics program (such as Photoshop or the GIMP) and saved on a storage device (such as a hard disk).
Images are usually stored on a hard disk. 
Pictures are normally stored in image formats such as uncompressed Bitmap, "non-lossy" (lossless) compressed TIFF and PNG, and "lossy" compressed JPEG. 
Documents are best stored in TIFF or PDF format; JPEG is particularly unsuitable for text. 
Optical character recognition (OCR) software allows a scanned image of text to be converted into editable text with reasonable accuracy, so long as the text is cleanly printed and in a typeface and size that can be read by the software. 
OCR capability may be integrated into the scanning software, or the scanned image file can be processed with a separate OCR program.
The scanning or digitization of paper documents for storage makes different requirements of the scanning equipment used than scanning of pictures for reproduction. 
While documents can be scanned on general-purpose scanners, it is more efficiently performed on dedicated document scanners.
When scanning large quantities of documents, speed and paper-handling is very important, but the resolution of the scan will normally be much lower than for good reproduction of pictures.
Document scanners have document feeders, usually larger than those sometimes found on copiers or all-purpose scanners. 
Scans are made at high speed, perhaps 20 to 150 pages per minute, often in grayscale, although many scanners support color. 
Many scanners can scan both sides of double-sided originals (duplex operation). 
Sophisticated document scanners have firmware or software that cleans up scans of text as they are produced, eliminating accidental marks and sharpening type; this would be unacceptable for photographic work, where marks cannot reliably be distinguished from desired fine detail. 
Files created are compressed as they are made.
The resolution used is usually from 150 to 300 dpi, although the hardware may be capable of somewhat higher resolution; this produces images of text good enough to read and for optical character recognition (OCR), without the higher demands on storage space required by higher-resolution images.
Document scans are often processed using OCR technology to create editable and searchable files.
 Most scanners use ISIS or TWAIN device drivers to scan documents into TIFF format so that the scanned pages can be fed into a document management system that will handle the archiving and retrieval of the scanned pages. 
Lossy JPEG compression, which is very efficient for pictures, is undesirable for text documents, as slanted straight edges take on a jagged appearance, and solid black (or other color) text on a light background compresses well with lossless compression formats.
While paper feeding and scanning can be done automatically and quickly, preparation and indexing are necessary and require much work by humans. 
Preparation involves manually inspecting the papers to be scanned and making sure that they are in order, unfolded, without staples or anything else that might jam the scanner. 
Additionally, some industries such as legal and medical may require documents to have Bates Numbering or some other mark giving a document identification number and date/time of the document scan.
Indexing involves associating relevant keywords to files so that they can be retrieved by content. 
This process can sometimes be automated to some extent, but it often requires manual labour performed by data-entry clerks. 
One common practice is the use of barcode-recognition technology: during preparation, barcode sheets with folder names or index information are inserted into the document files, folders, and document groups. 
Using automatic batch scanning, the documents are saved into appropriate folders, and an index is created for integration into document-management systems.
A specialized form of document scanning is book scanning. 
Technical difficulties arise from the books usually being bound and sometimes fragile and irreplaceable, but some manufacturers have developed specialized machinery to deal with this. 
Often special robotic mechanisms are used to automate the page turning and scanning process.
Another category of document scanner is the document camera. 
Capturing images on document cameras differs from that of flatbed and Automatic document feeder (ADF) scanners in that there are no moving parts required to scan the object. 
Conventionally either the illumination/reflector rod inside the scanner must be moved over the document (such as for a flatbed scanner), or the document must be passed over the rod (such as for feeder scanners) in order to produce a scan of a whole image. 
Document cameras capture the whole document or object in one step, usually instantly.
 Typically, documents are placed on a flat surface, usually the office desk, underneath the capture area of the document camera. 
The process of whole-surface-at-once capturing has the benefit of increasing reaction time for the work flow of scanning. 
After being captured, the images are usually processed through software which may enhance the image and perform such tasks like automatically rotating, cropping and straightening them.
It is not required that the documents or objects being scanned make contact with the document camera, therefore increasing flexibility of the types of documents which are able to be scanned. 
Objects which have previously been difficult to scan on conventional scanners are now able to be done so with one device. 
This includes in particular documents which are of varying sizes and shapes, stapled, in folders or bent/crumpled which may get jammed in a feed scanner. 
Other objects include books, magazines, receipts, letters, tickets etc.
 No moving parts can also remove the need for maintenance, a consideration in the Total cost of ownership, which includes the continuing operational costs of scanners.
Increased reaction time whilst scanning also has benefits in the realm of context-scanning. 
ADF scanners, whilst very fast and very good at batch scanning, also require pre- and post- processing of the documents. 
Document cameras are able to be integrated directly into a Workflow or process, for example a teller at a bank. 
The document is scanned directly in the context of the customer, in which it is to be placed or used. 
Reaction time is an advantage in these situations. 
Document cameras usually also require a small amount of space and are often portable.
Whilst scanning with document cameras may have a quick reaction time, large amounts of batch scanning of even, unstapled documents is more efficient with an ADF scanner. 
There are challenges which face this kind of technology regarding external factors (such as lighting) which may have influence on the scan results. 
The way in which these issues are resolved strongly depends on the sophistication of the product and how it deals with these issues.
Infrared cleaning is a technique used to remove the effects of dust and scratches on images scanned from film; many modern scanners incorporate this feature. 
Scanner manufacturers usually have their own name attached to this technique. 
For example, Epson, Minolta, Nikon, Konica Minolta, Microtek, and others use Digital ICE, while Canon uses its own system FARE (Film Automatic Retouching and Enhancement system).
 Plustek uses LaserSoft Imaging iSRD. 
Some independent software developers design infrared cleaning tools.
Flatbed scanners have been used as digital backs for large-format cameras to create high-resolution digital images of static subjects.
 A modified flatbed scanner has been used for documentation and quantification of thin layer chromatograms detected by fluorescence quenching on silica gel layers containing an ultraviolet (UV) indicator.
'ChromImage' is allegedly the first commercial flatbed scanner densitometer. 
It enables acquisition of TLC plate images and quantification of chromatograms by use of Galaxie-TLC software.
 Other than being turned into densitometers, flatbed scanners were also turned into colorimeters using different methods.
 Trichromatic Color Analyser is allegedly the first distributable system using a flatbed scanner as a tristimulus colorimetric device.
Pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data, although it is in some cases considered to be nearly synonymous with machine learning.
Pattern recognition systems are in many cases trained from labeled "training" data (supervised learning), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning).
The terms pattern recognition, machine learning, data mining and knowledge discovery in databases (KDD) are hard to separate, as they largely overlap in their scope. 
Machine learning is the common term for supervised learning methods[dubious – discuss] and originates from artificial intelligence, whereas KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. 
Pattern recognition has its origins in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. 
In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern, while machine learning traditionally focuses on maximizing the recognition rates. 
Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics, and they've become increasingly similar by integrating developments and ideas from each other.
In machine learning, pattern recognition is the assignment of a label to a given input value. 
In statistics, discriminant analysis was introduced for this same purpose in 1936. 
An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is "spam" or "non-spam"). 
However, pattern recognition is a more general problem that encompasses other types of output as well. 
Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.
Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform "most likely" matching of the inputs, taking into account their statistical variation. 
This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. 
A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors. 
In contrast to pattern recognition, pattern matching is generally not considered a type of machine learning, although pattern-matching algorithms (especially with fairly general, carefully tailored patterns) can sometimes succeed in providing similar-quality output of the sort provided by pattern-recognition algorithms.
Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. 
Supervised learning assumes that a set of training data (the training set) has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output.
 A learning procedure then generates a model that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of "simple", in accordance with Occam's Razor, discussed below). 
Unsupervised learning, on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances.
 A combination of the two that has recently been explored is semi-supervised learning, which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). 
Note that in cases of unsupervised learning, there may be no training data at all to speak of; in other words, the data to be labeled is the training data.
Note that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. 
Note also that in some fields, the terminology is different: For example, in community ecology, the term "classification" is used to refer to what is commonly known as "clustering".
The piece of input data for which an output value is generated is formally termed an instance. 
The instance is formally described by a vector of features, which together constitute a description of all known characteristics of the instance. 
(These feature vectors can be seen as defining points in an appropriate multidimensional space, and methods for manipulating vectors in vector spaces can be correspondingly applied to them, such as computing the dot product or the angle between two vectors.)
 Often, categorical and ordinal data are grouped together; likewise for integer-valued and real-valued data. 
Furthermore, many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be discretized into groups (e.g., less than 5, between 5 and 10, or greater than 10).
Many common pattern recognition algorithms are probabilistic in nature, in that they use statistical inference to find the best label for a given instance. 
Unlike other algorithms, which simply output a "best" label, often probabilistic algorithms also output a probability of the instance being described by the given label. 
In addition, many probabilistic algorithms output a list of the N-best labels with associated probabilities, for some value of N, instead of simply a single best label. 
When the number of possible labels is fairly small (e.g., in the case of classification), N may be set so that the probability of all possible labels is output. 
Probabilistic algorithms have many advantages over non-probabilistic algorithms:
They output a confidence value associated with their choice. 
(Note that some other algorithms may also output confidence values, but in general, only for probabilistic algorithms is this value mathematically grounded in probability theory. 
Non-probabilistic confidence values can in general not be given any specific meaning, and only used to compare against other confidence values output by the same algorithm.)
Correspondingly, they can abstain when the confidence of choosing any particular output is too low.
Because of the probabilities output, probabilistic pattern-recognition algorithms can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation.
Feature selection algorithms attempt to directly prune out redundant or irrelevant features. 
A general introduction to feature selection which summarizes approaches and challenges, has been given.
 The complexity of feature-selection is, because of its non-monotonous character, an optimization problem where given a total of n features the powerset consisting of all 2^n-1 subsets of features need to be explored. 
The Branch-and-Bound algorithm [4] does reduce this complexity but is intractable for medium to large values of the number of available features n. 
For a large-scale comparison of feature-selection algorithms see .[5]
Techniques to transform the raw feature vectors (feature extraction) are sometimes used prior to application of the pattern-matching algorithm. 
For example, feature extraction algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as principal components analysis (PCA). 
The distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features.
Within medical science, pattern recognition is the basis for computer-aided diagnosis (CAD) systems. 
CAD describes a procedure that supports the doctor's interpretations and findings.
Pattern & Shape Recognition Technology (SRT) in a people counter system
Other typical applications of pattern recognition techniques are automatic speech recognition, classification of text into several categories (e.g., spam/non-spam email messages), the automatic recognition of handwritten postal codes on postal envelopes, automatic recognition of images of human faces, or handwriting image extraction from medical forms.
The last two examples form the subtopic image analysis of pattern recognition that deals with digital images as input to pattern recognition systems.[8][9]
Optical character recognition is a classic example of the application of a pattern classifier, see OCR-example. 
The method of signing one's name was captured with stylus and overlay starting in 1990.
The strokes, speed, relative min, relative max, acceleration and pressure is used to uniquely identify and confirm identity. 
Banks were first offered this technology, but were content to collect from the FDIC for any bank fraud and did not want to inconvenience customers.
Artificial neural networks (neural net classifiers) and Deep Learning have many real-world applications in image processing, a few examples:
identification and authentication: e.g., license plate recognition,[10] fingerprint analysis and face detection/verification;[11]
medical diagnosis: e.g., screening for cervical cancer (Papnet)[12] or breast tumors;
defence: various navigation and guidance systems, target recognition systems, shape recognition technology etc.
For a discussion of the aforementioned applications of neural networks in image processing, see e.g.[13]
In psychology, pattern recognition, making sense of and identifying the objects we see is closely related to perception, which explains how the sensory inputs we receive are made meaningful. 
Pattern recognition can be thought of in two different ways: the first being template matching and the second being feature detection. 
A template is a pattern used to produce items of the same proportions. 
The template-matching hypothesis suggests that incoming stimuli are compared with templates in the long term memory. 
If there is a match, the stimulus is identified. 
Feature detection models, such as the Pandemonium system for classifying letters (Selfridge, 1959), suggest that the stimuli are broken down into their component parts for identification. 
For example, a capital E has three horizontal lines and one vertical line.
Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.
In 1959, Arthur Samuel defined machine learning as a "Field of study that gives computers the ability to learn without being explicitly programmed".
 Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.
Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[4]:2 rather than following strictly static program instructions.
Machine learning is closely related to and often overlaps with computational statistics; a discipline which also focuses in prediction-making through the use of computers. 
It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. 
Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms is infeasible. 
Example applications include spam filtering, optical character recognition (OCR),[5] search engines and computer vision. 
Machine learning is sometimes conflated with data mining,[6] where the latter sub-field focuses more on exploratory data analysis and is known as unsupervised learning
Tom M. Mitchell provided a widely quoted, more formal definition: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E".
Machine learning tasks are typically classified into three broad categories, depending on the nature of the learning "signal" or "feedback" available to a learning system. 
Supervised learning: The computer is presented with example inputs and their desired outputs, given by a "teacher", and the goal is to learn a general rule that maps inputs to outputs.
Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. 
Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).
Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle), without a teacher explicitly telling it whether it has come close to its goal. 
Another example is learning to play a game by playing against an opponent.
Between supervised and unsupervised learning is semi-supervised learning, where the teacher gives an incomplete training signal: a training set with some (often many) of the target outputs missing. 
Transduction is a special case of this principle where the entire set of problem instances is known at learning time, except that part of the targets are missing.
A support vector machine is a classifier that divides its input space into two regions, separated by a linear boundary. 
Here, it has learned to distinguish black and white circles.
Among other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. 
Another categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:[4]:3
In classification, inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one (or multi-label classification) or more of these classes. 
This is typically tackled in a supervised way. 
Spam filtering is an example of classification, where the inputs are email (or other) messages and the classes are "spam" and "not spam".
In regression, also a supervised problem, the outputs are continuous rather than discrete.
In clustering, a set of inputs is to be divided into groups. 
Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task.
Density estimation finds the distribution of inputs in some space.
Dimensionality reduction simplifies inputs by mapping them into a lower-dimensional space. 
Topic modeling is a related problem, where a program is given a list of human language documents and is tasked to find out which documents cover similar topics.
As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. 
Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. 
They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. 
Probabilistic reasoning was also employed, especially in automated medical diagnosis.
However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. 
Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.
 By 1980, expert systems had come to dominate AI, and statistics was out of favor.
 Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.
 755 Neural networks research had been abandoned by AI and computer science around the same time. 
This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. 
Their main success came in the mid-1980s with the reinvention of backpropagation.[10]:25
Machine learning, reorganized as a separate field, started to flourish in the 1990s. 
The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. 
It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.
 It also benefited from the increasing availability of digitized information, and the possibility to distribute that via the internet.
Machine learning and data mining often employ the same methods and overlap significantly. 
They can be roughly distinguished as follows:
Machine learning focuses on prediction, based on known properties learned from the training data.
Data mining focuses on the discovery of (previously) unknown properties in the data. 
This is the analysis step of Knowledge Discovery in Databases.
The two areas overlap in many ways: data mining uses many machine learning methods, but often with a slightly different goal in mind. 
On the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. 
Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.
Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. 
Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set examples). 
The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.
Machine learning and statistics are closely related fields. 
According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.
 He also suggested the term data science as a placeholder to call the overall field.
Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[14] wherein 'algorithmic model' means more or less the machine learning algorithms like Random forest.
Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.
A core objective of a learner is to generalize from its experience.
Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. 
The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.
The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. 
Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. 
Instead, probabilistic bounds on the performance are quite common. 
The bias–variance decomposition is one way to quantify generalization error.
In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. 
In computational learning theory, a computation is considered feasible if it can be done in polynomial time. 
There are two kinds of time complexity results. 
Positive results show that a certain class of functions can be learned in polynomial time. 
Negative results show that certain classes cannot be learned in polynomial time.
There are many similarities between machine learning theory and statistical inference, although they use different terms.
Data mining is an interdisciplinary subfield of computer science.
 It is the computational process of discovering patterns in large data sets ("big data") involving methods at the intersection of artificial intelligence, machine learning, statistics, and database systems.
 The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use.
Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.
Data mining is the analysis step of the "knowledge discovery in databases" process, or KDD.
The term is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.
It also is a buzzword[6] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence, machine learning, and business intelligence. 
The book Data mining: Practical machine learning tools and techniques with Java[7] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.
 Often the more general terms (large scale) data analysis and analytics – or, when referring to actual methods, artificial intelligence and machine learning – are more appropriate.
The actual data mining task is the automatic or semi-automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining). 
This usually involves using database techniques such as spatial indices. 
These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. 
For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. 
Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. 
These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In the 1960s, statisticians used terms like "Data Fishing" or "Data Dredging" to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. 
The term "Data Mining" appeared around 1990 in the database community. 
For a short time in 1980s, a phrase "database mining"™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[9] researchers consequently turned to "data mining". 
Other terms used include Data Archaeology, Information Harvesting, Information Discovery, Knowledge Extraction, etc. 
Gregory Piatetsky-Shapiro coined the term "Knowledge Discovery in Databases" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and Machine Learning Community. 
However, the term data mining became more popular in the business and press communities.
 Currently, Data Mining and Knowledge Discovery are used interchangeably. 
Since about 2007, "Predictive Analytics" and since 2011, "Data Science" terms were also used to describe this field.
In the Academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. 
It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. 
A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding Editor-in-Chief. 
Later he started the SIGKDDD Newsletter SIGKDD Explorations.
 The KDD International conference became the primary highest quality conference in Data Mining with an acceptance rate of research paper submissions below 18%. 
The Journal Data Mining and Knowledge Discovery is the primary research journal of the field.
The manual extraction of patterns from data has occurred for centuries. 
Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). 
The proliferation, ubiquity and increasing power of computer technology has dramatically increased data collection, storage, and manipulation ability. 
Data mining is the process of applying these methods with the intention of uncovering hidden patterns[12] in large data sets. 
Data mining involves six common classes of tasks:[4]
Anomaly detection (Outlier/change/deviation detection) – The identification of unusual data records, that might be interesting or data errors that require further investigation.
Association rule learning (Dependency modelling) – Searches for relationships between variables. 
For example, a supermarket might gather data on customer purchasing habits. 
Using association rule learning, the supermarket can determine which products are frequently bought together and use this information for marketing purposes. 
This is sometimes referred to as market basket analysis.
Clustering – is the task of discovering groups and structures in the data that are in some way or another "similar", without using known structures in the data.
Classification – is the task of generalizing known structure to apply to new data. 
For example, an e-mail program might attempt to classify an e-mail as "legitimate" or as "spam".
Regression – attempts to find a function which models the data with the least error.
Summarization – providing a more compact representation of the data set, including visualization and report generation.
This section is missing information about non-classification tasks in data mining. 
It only covers machine learning. 
Please expand the section to include this information. 
Further details may exist on the talk page. (September 2011)
The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. 
Not all patterns found by the data mining algorithms are necessarily valid. 
It is common for the data mining algorithms to find patterns in the training set which are not present in the general data set. 
This is called overfitting. 
To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. 
The learned patterns are applied to this test set, and the resulting output is compared to the desired output. 
For example, a data mining algorithm trying to distinguish "spam" from "legitimate" emails would be trained on a training set of sample e-mails. 
Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. 
The accuracy of the patterns can then be measured from how many e-mails they correctly classify. 
A number of statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. 
If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
Big data is a broad term for data sets so large or complex that traditional data processing applications are inadequate. 
Challenges include analysis, capture, data curation, search, sharing, storage, transfer, visualization, querying and information privacy. 
The term often refers simply to the use of predictive analytics or certain other advanced methods to extract value from data, and seldom to a particular size of data set. 
Accuracy in big data may lead to more confident decision making, and better decisions can result in greater operational efficiency, cost reduction and reduced risk.
Analysis of data sets can find new correlations to "spot business trends, prevent diseases, combat crime and so on.
Scientists, business executives, practitioners of medicine, advertising and governments alike regularly meet difficulties with large data sets in areas including Internet search, finance and business informatics. 
Scientists encounter limitations in e-Science work, including meteorology, genomics,[3] connectomics, complex physics simulations, biology and environmental research.
Data sets are growing rapidly in part because they are increasingly gathered by cheap and numerous information-sensing mobile devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks.
 The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s;[7] as of 2012, every day 2.5 exabytes (2.5×1018) of data are created.
One question for large enterprises is determining who should own big data initiatives that affect the entire organization.
Relational database management systems and desktop statistics and visualization packages often have difficulty handling big data. 
The work instead requires "massively parallel software running on tens, hundreds, or even thousands of servers".
 What is considered "big data" varies depending on the capabilities of the users and their tools, and expanding capabilities make big data a moving target. 
"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. 
For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration
Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time.
 Big data "size" is a constantly moving target, as of 2012 ranging from a few dozen terabytes to many petabytes of data. 
Big data requires a set of techniques and technologies with new forms of integration to reveal insights from datasets that are diverse, complex, and of a massive scale.
In a 2001 research report[14] and related lectures, META Group (now Gartner) analyst Doug Laney defined data growth challenges and opportunities as being three-dimensional, i.e. increasing volume (amount of data), velocity (speed of data in and out), and variety (range of data types and sources). 
Gartner, and now much of the industry, continue to use this "3Vs" model for describing big data.
In 2012, Gartner updated its definition as follows: "Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization.
" Gartner's definition of the 3Vs is still widely used, and in agreement with a consensual definition that states that "Big Data represents the Information assets characterized by such a High Volume, Velocity and Variety to require specific Technology and Analytical Methods for its transformation into Value".
Additionally, a new V "Veracity" is added by some organizations to describe it,[17] revisionism challenged by some industry authorities.
 The 3Vs have been expanded to other complementary characteristics of big data:
Volume: big data doesn't sample; it just observes and tracks what happens
Velocity: big data is often available in real-time
Variety: big data draws from text, images, audio, video; plus it completes missing pieces through data fusion
Machine Learning: big data often doesn't ask why and simply detects patterns[21]
Digital footprint: big data is often a cost-free byproduct of digital interaction[20]
The growing maturity of the concept more starkly delineates the difference between big data and Business Intelligence:[22]
Business Intelligence uses descriptive statistics with data with high information density to measure things, detect trends, etc..
Big data uses inductive statistics and concepts from nonlinear system identification[23] to infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information density[24] to reveal relationships and dependencies, or to perform predictions of outcomes and behaviors.
In a popular tutorial article published in IEEE Access Journal,[26] the authors classified existing definitions of big data into three categories: Attribute Definition, Comparative Definition and Architectural Definition. 
The authors also presented a big-data technology map that illustrates its key technological evolutions.
Big data can be described by the following characteristics:[19][20]
The quantity of generated and stored data. 
The size of the data determines the value and potential insight- and whether it can actually be considered big data or not.
The type and nature of the data. 
This helps people who analyze it to effectively use the resulting insight.
In this context, the speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development.
Inconsistency of the data set can hamper processes to handle and manage it.
The quality of captured data can vary greatly, affecting accurate analysis.
Data must be processed with advanced tools (analytics and algorithms) to reveal meaningful information.
 For example, to manage a factory one must consider both visible and invisible issues with various components. 
Information generation algorithms must detect and address invisible issues such as machine degradation, component wear, etc. on the factory floor.
In 2000, Seisint Inc. (now LexisNexis Group) developed a C++-based distributed file-sharing framework for data storage and query. 
The system stores and distributes structured, semi-structured, and unstructured data across multiple servers. 
Users can build queries in a C++ dialect called ECL. 
ECL uses an "apply schema on read" method to infer the structure of stored data when it is queried, instead of when it is stored. 
In 2004, LexisNexis acquired Seisint Inc.
 and in 2008 acquired ChoicePoint, Inc.
 and their high-speed parallel processing platform. 
The two platforms were merged into HPCC (or High-Performance Computing Cluster) Systems and in 2011, HPCC was open-sourced under the Apache v2.0 License. 
Currently, HPCC and Quantcast File System[33] are the only publicly available platforms capable of analyzing multiple exabytes of data.
In 2004, Google published a paper on a process called MapReduce that uses a similar architecture. 
The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data. 
With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the Map step). 
The results are then gathered and delivered (the Reduce step). The framework was very successful,[34] so others wanted to replicate the algorithm. 
Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named Hadoop.
MIKE2.0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled "Big Data Solution Offering".
The methodology addresses handling big data in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records.[37]
Recent studies show that a multiple-layer architecture is one option to address the issues that big data presents.
 A distributed parallel architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. 
This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. 
This type of framework looks to make the processing power transparent to the end user by using a front-end application server.
Big Data Analytics for Manufacturing Applications can be based on a 5C architecture (connection, conversion, cyber, cognition, and configuration).
The data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. 
This enables quick segregation of data into the data lake, thereby reducing the overhead time
A 2011 McKinsey Global Institute report characterizes the main components and ecosystem of big data as follows:[41]
Techniques for analyzing data, such as A/B testing, machine learning and natural language processing
Big Data technologies, like business intelligence, cloud computing and databases
Visualization, such as charts, graphs and other displays of the data
Multidimensional big data can also be represented as tensors, which can be more efficiently handled by tensor-based computation,[42] such as multilinear subspace learning.
 Additional technologies being applied to big data include massively parallel-processing (MPP) databases, search-based applications, data mining, distributed file systems, distributed databases, cloud-based infrastructure (applications, storage and computing resources) and the Internet.
Some but not all MPP relational databases have the ability to store and manage petabytes of data. 
Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the RDBMS.
DARPA's Topological Data Analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called Ayasdi.
The practitioners of big data analytics processes are generally hostile to slower shared storage,[46] preferring direct-attached storage (DAS) in its various forms from solid state drive (Ssd) to high capacity SATA disk buried inside parallel processing nodes. 
The perception of shared storage architectures—Storage area network (SAN) and Network-attached storage (NAS) —is that they are relatively slow, complex, and expensive. 
These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.
Real or near-real time information delivery is one of the defining characteristics of big data analytics. 
Latency is therefore avoided whenever and wherever possible. 
Data in memory is good—data on spinning disk at the other end of a FC SAN connection is not. 
The cost of a SAN at the scale needed for analytics applications is very much higher than other storage techniques.
There are advantages as well as disadvantages to shared storage in big data analytics, but big data analytics practitioners as of 2011 did not favour it
Big data has increased the demand of information management specialists in that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP and Dell have spent more than $15 billion on software firms specializing in data management and analytics. 
In 2010, this industry was worth more than $100 billion and was growing at almost 10 percent a year: about twice as fast as the software business as a whole.[2]
Developed economies increasingly use data-intensive technologies. 
There are 4.6 billion mobile-phone subscriptions worldwide, and between 1 billion and 2 billion people accessing the internet.
Between 1990 and 2005, more than 1 billion people worldwide entered the middle class, which means more people become more literate, which in turn leads to information growth. 
The world's effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986, 471 petabytes in 1993, 2.2 exabytes in 2000, 65 exabytes in 2007[7] and predictions put the amount of internet traffic at 667 exabytes annually by 2014.
 According to one estimate, one third of the globally stored information is in the form of alphanumeric text and still image data,[48] which is the format most useful for most big data applications. 
This also shows the potential of yet unused data (i.e. in the form of video and audio content).
While many vendors offer off-the-shelf solutions for Big Data, experts recommend the development of in-house solutions custom-tailored to solve the company's problem at hand if the company has sufficient technical capabilities.
The use and adoption of Big Data within governmental processes is beneficial and allows efficiencies in terms of cost, productivity, and innovation. 
That said, this process does not come without its flaws. 
Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. 
Below are the thought leading examples within the Governmental Big Data space.
In 2012, the Obama administration announced the Big Data Research and Development Initiative, to explore how big data could be used to address important problems faced by the government.
 The initiative is composed of 84 different big data programs spread across six departments.
Big data analysis played a large role in Barack Obama's successful 2012 re-election campaign.
The United States Federal Government owns six of the ten most powerful supercomputers in the world.
The Utah Data Center is a data center currently being constructed by the United States National Security Agency. 
When finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet.
The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few exabyte
Big data analysis helped in parts, responsible for the NDA to win Indian General Election 2014.
The Indian Government utilises numerous techniques to ascertain how the Indian electorate is responding to government action, as well as ideas for policy augmentation.
Examples of uses of big data in public services:
Data on prescription drugs: by connecting origin, location and the time of each prescription, a research unit was able to exemplify the considerable delay between the release of any given drug, and a UK-wide adaptation of the National Institute for Health and Care Excellence guidelines. 
This suggests that new/most up-to-date drugs take some time to filter through to the general patient.
Joining up data: a local authority blended data about services, such as road gritting rotas, with services for people at risk, such as 'meals on wheels'. 
The connection of data allowed the local authority to avoid any weather related delay
Research on the effective usage of information and communication technologies for development (also known as ICT4D) suggests that big data technology can make important contributions but also present unique challenges to International development.
 Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, economic productivity, crime, security, and natural disaster and resource management.
 However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues.
Based on TCS 2013 Global Trend Study, improvements in supply planning and product quality provide the greatest benefit of big data for manufacturing.
Big data provides an infrastructure for transparency in manufacturing industry, which is the ability to unravel uncertainties such as inconsistent component performance and availability. 
Predictive manufacturing as an applicable approach toward near-zero downtime and transparency requires vast amount of data and advanced prediction tools for a systematic process of data into useful information.
 A conceptual framework of predictive manufacturing begins with data acquisition where different type of sensory data is available to acquire such as acoustics, vibration, pressure, current, voltage and controller data. 
Vast amount of sensory data in addition to historical data construct the big data in manufacturing. The generated big data acts as the input into predictive tools and preventive strategies such as Prognostics and Health Management (PHM).[
Current PHM implementations mostly use data during the actual usage while analytical algorithms can perform more accurately when more information throughout the machine's lifecycle, such as system configuration, physical knowledge and working principles, are included. 
There is a need to systematically integrate, manage and analyze machinery or process data during different stages of machine life cycle to handle data/information more efficiently and further achieve better transparency of machine health condition for manufacturing industry.
With such motivation a cyber-physical (coupled) model scheme has been developed. 
The coupled model is a digital twin of the real machine that operates in the cloud platform and simulates the health condition with an integrated knowledge from both data driven analytical algorithms as well as other available physical knowledge. 
It can also be described as a 5S systematic approach consisting of sensing, storage, synchronization, synthesis and service. 
The coupled model first constructs a digital image from the early design stage. 
System information and physical knowledge are logged during product design, based on which a simulation model is built as a reference for future analysis. 
Initial parameters may be statistically generalized and they can be tuned using data from testing or the manufacturing process using parameter estimation. 
After that step, the simulation model can be considered a mirrored image of the real machine—able to continuously record and track machine condition during the later utilization stage. 
Finally, with the increased connectivity offered by cloud computing technology, the coupled model also provides better accessibility of machine condition for factory managers in cases where physical access to actual equipment or machine data is limited.
To understand how the media utilises Big Data, it is first necessary to provide some context into the mechanism used for media process. 
It has been suggested by Nick Couldry and Joseph Turow that practitioners in Media and Advertising approach big data as many actionable points of information about millions of individuals. 
The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead tap into consumers with technologies that reach targeted people at optimal times in optimal locations. 
The ultimate aim is to serve, or convey, a message or content that is (statistically speaking) in line with the consumers mindset. 
For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various data-mining activities.
Big Data and the IoT work in conjunction. From a media perspective, data is the key derivative of device inter connectivity and allows accurate targeting. 
The Internet of Things, with the help of big data, therefore transforms the media industry, companies and even governments, opening up a new era of economic growth and competitiveness.
 The intersection of people, data and intelligent algorithms have far-reaching impacts on media efficiency. The wealth of data generated allows an elaborate layer on the present targeting mechanisms of the industry.
eBay.com uses two data warehouses at 7.5 petabytes and 40PB as well as a 40PB Hadoop cluster for search, consumer recommendations, and merchandising.
Amazon.com handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers.
 The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world's three largest Linux databases, with capacities of 7.8 TB, 18.5 TB, and 24.7 TB.
Facebook handles 50 billion photos from its user base.
As of August 2012, Google was handling roughly 100 billion searches per month.
Oracle NoSQL Database has been tested to past the 1M ops/sec mark with 8 shards and proceeded to hit 1.2M ops/sec with 10 shards.
Walmart handles more than 1 million customer transactions every hour, which are imported into databases estimated to contain more than 2.5 petabytes (2560 terabytes) of data—the equivalent of 167 times the information contained in all the books in the US Library of Congress.[2]
FICO Card Detection System protects accounts world-wide.
The volume of business data worldwide, across all companies, doubles every 1.2 years, according to estimates.
Windermere Real Estate uses anonymous GPS signals from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day.
The Large Hadron Collider experiments represent about 150 million sensors delivering data 40 million times per second. 
There are nearly 600 million collisions per second.
 After filtering and refraining from recording more than 99.99995%[76] of these streams, there are 100 collisions of interest per second.
As a result, only working with less than 0.001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012). This becomes nearly 200 petabytes after replication.
If all sensor data were recorded in LHC, the data flow would be extremely hard to work with. 
The data flow would exceed 150 million petabytes annual rate, or nearly 500 exabytes per day, before replication. To put the number in perspective, this is equivalent to 500 quintillion (5×1020) bytes per day, almost 200 times more than all the other sources combined in the world.
The Square Kilometre Array is a radio telescope built of thousands of antennas. 
It is expected to be operational by 2024. 
Collectively, these antennas are expected to gather 14 exabytes and store one petabyte per day.
 It is considered one of the most ambitious scientific projects ever undertaken.
Science and research
When the Sloan Digital Sky Survey (SDSS) began to collect astronomical data in 2000, it amassed more in its first few weeks than all data collected in the history of astronomy previously. 
Continuing at a rate of about 200 GB per night, SDSS has amassed more than 140 terabytes of information. 
When the Large Synoptic Survey Telescope, successor to SDSS, comes online in 2020, its designers expect it to acquire that amount of data every five days.[2]
Decoding the human genome originally took 10 years to process, now it can be achieved in less than a day. 
The DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times cheaper than the reduction in cost predicted by Moore's Law.
The Nasa Center for Climate Simulation (NCCS) stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster.
Google's DNAStack compiles and organizes DNA samples of genetic data from around the world to identify diseases and other medical defects. 
These fast and exact calculations eliminate any 'friction points,' or human errors that could be made by one of the numerous science and biology experts working with the DNA.
 DNAStack, a part of Google Genomics, allows scientists to use the vast sample of resources from Google's search server to scale social experiments that would usually take years, instantly.
Big data can be used to improve training and understanding competitors. 
Besides, it is possible to predict winners in a match using big data analytics.
Future performance of players could be predicted as well. 
Thus, players' value and salary is determined by data collected throughout the season.
The movie MoneyBall demonstrates how big data could be used to scout players and also identify undervalued players.
In Formula One races, race cars with hundreds of sensors generate terabytes of data. 
These sensors collect data points from tire pressure to fuel burn efficiency. 
Then, this data is transferred to team headquarters in United Kingdom through fiber optic cables that could carry data at the speed of light.
Based on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. 
Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.
Encrypted search and cluster formation in big data was demonstrated in March 2014 at the American Society of Engineering Education. 
Gautam Siwach engaged at Tackling the challenges of Big Data by MIT Computer Science and Artificial Intelligence Laboratory and Dr. 
Amir Esmailpour at UNH Research Group investigated the key features of big data as formation of clusters and their interconnections. 
They focused on the security of big data and the actual orientation of the term towards the presence of different type of data in an encrypted form at cloud interface by providing the raw definitions and real time examples within the technology. 
Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.
In March 2012, The White House announced a national "Big Data Initiative" that consisted of six Federal departments and agencies committing more than $200 million to big data research projects.
The initiative included a National Science Foundation "Expeditions in Computing" grant of $10 million over 5 years to the AMPLab[91] at the University of California, Berkeley.
 The AMPLab also received funds from DARPA, and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion[93] to fighting cancer.
The White House Big Data Initiative also included a commitment by the Department of Energy to provide $25 million in funding over 5 years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute,[95] led by the Energy Department’s Lawrence Berkeley National Laboratory. 
The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the Department's supercomputers.
The U.S. state of Massachusetts announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions.
 The Massachusetts Institute of Technology hosts the Intel Science and Technology Center for Big Data in the MIT Computer Science and Artificial Intelligence Laboratory, combining government, corporate, and institutional funding and research efforts.
The European Commission is funding the 2-year-long Big Data Public Private Forum through their Seventh Framework Program to engage companies, academics and other stakeholders in discussing big data issues. 
The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. 
Outcomes of this project will be used as input for Horizon 2020, their next framework program.
The British government announced in March 2014 the founding of the Alan Turing Institute, named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyse large data sets.
At the University of Waterloo Stratford Campus Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.
In May 2013, IMS Center held an industry advisory board meeting focusing on big data where presenters from various industrial companies discussed their concerns, issues and future goals in Big Data environment.
Computational social sciences – Anyone can use Application Programming Interfaces (APIs) provided by Big Data holders, such as Google and Twitter, to do research in the social and behavioral sciences.
 Often these APIs are provided for free.
 Tobias Preis et al. used Google Trends data to demonstrate that Internet users from countries with a higher per capita gross domestic product (GDP) are more likely to search for information about the future than information about the past. 
The findings suggest there may be a link between online behaviour and real-world economic indicators.
the authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year ('2011') to the volume of searches for the previous year ('2009'), which they call the 'future orientation index'.
 They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP. 
The results hint that there may potentially be a relationship between the economic success of a country and the information-seeking behavior of its citizens captured in big data.
Tobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends.
 Their analysis of Google search volume for 98 terms of varying financial relevance, published in Scientific Reports,[108] suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.
Big data sets come with algorithmic challenges that previously did not exist. 
Hence, there is a need to fundamentally change the processing ways.
An important research question that can be asked about big data sets is whether you need to look at the full data to draw certain conclusions about the properties of the data or is a sample good enough. 
The name big data itself contains a term related to size and this is an important characteristic of big data. 
But Sampling (statistics) enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. 
For example, there are about 600 million tweets produced every day. 
Is it necessary to look at all of them to determine the topics that are discussed during the day? Is it necessary to look at all the tweets to determine the sentiment on each of the topics? In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage and controller data are available at short time intervals. 
To predict down-time it may not be necessary to look at all the data but a sample may be sufficient.
There has been some work done in Sampling algorithms for Big Data. 
A theoretical formulation for sampling Twitter data has been developed.
Critiques of the big data paradigm come in two flavors, those that question the implications of the approach itself, and those that question the way it is currently done.
Critiques of the big data paradigm
"A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data".
In their critique, Snijders, Matzat, and Reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes.
 Mark Graham has leveled broad critiques at Chris Anderson's assertion that big data will spell the end of theory: focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts.
 Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. 
To overcome this insight deficit, "big data", no matter how comprehensive or well analyzed, must be complemented by "big judgment," according to an article in the Harvard Business Review.
Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably "informed by the world as it was in the past, or, at best, as it currently is".
 Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. 
If the systems dynamics of the future change, the past can say little about the future. 
For this, it would be necessary to have a thorough understanding of the systems dynamic, which implies theory.
As a response to this critique it has been suggested to combine big data approaches with computer simulations, such as agent-based models[60] and Complex Systems. 
Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms.
 In addition, use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.
In health and biology, conventional scientific approaches are based on experimentation. 
For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.
 A new postulate is accepted now in biosciences: the information provided by the data in huge volumes (omics) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation.
In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor.
The search logic is reversed and the limits of induction ("Glory of Science and Philosophy scandal", C. D. Broad, 1926) are to be considered.
Privacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information; expert panels have released various policy recommendations to conform practice to expectations of privacy.
Critiques of big data execution
Big data has been called a "fad" in scientific research and its use was even made fun of as an absurd practice in a satirical example on "pig data".
 Researcher danah boyd has raised concerns about the use of big data in science neglecting principles such as choosing a representative sample by being too concerned about actually handling the huge amounts of data.
This approach may lead to results bias in one way or another. 
Integration across heterogeneous data resources—some that might be considered "big data" and others not—presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.
In the provocative article "Critical Questions for Big Data",[131] the authors title big data a part of mythology: "large data sets offer a higher form of intelligence and knowledge , with the aura of truth, objectivity, and accuracy". 
Users of big data are often "lost in the sheer volume of numbers", and "working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth".
 Recent developments in BI domain, such as pro-active reporting especially target improvements in usability of Big Data, through automated filtering of non-useful data and correlations.[132]
Big data analysis is often shallow compared to analysis of smaller data sets.
In many big data projects, there is no large data analysis happening, but the challenge is the extract, transform, load part of data preprocessing.
Big data is a buzzword and a "vague term",[134][135] but at the same time an "obsession"[135] with entrepreneurs, consultants, scientists and the media. 
Big data showcases such as Google Flu Trends failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. 
Similarly, Academy awards and election predictions solely based on Twitter were more often off than on target. 
Big data often poses the same challenges as small data; and adding more data does not solve problems of bias, but may emphasize other problems.
 In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. 
Google Translate—which is based on big data statistical analysis of text—does a good job at translating web pages. 
However, results from specialized domains may be dramatically skewed. 
On the other hand, big data may also introduce new problems, such as the multiple comparisons problem: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant. 
Data processing is, broadly, "the collection and manipulation of items of data to produce meaningful information."
In this sense it can be considered a subset of information processing, "the change (processing) of information in any manner detectable by an observer." 
The term is often used more specifically in the context of a business or other organization to refer to the class of commercial data processing applications
Data processing may involve various processes, including:
Validation – Ensuring that supplied data is "clean, correct and useful"
Sorting – "arranging items in some sequence and/or in different sets."
Summarization – reducing detail data to its main points.
Aggregation – combining multiple pieces of data.
Analysis – the "collection, organization, analysis, interpretation and presentation of data.".
Reporting – list detail or summary data or computed information.
Classification – separates data into various categories.
The United States Census Bureau illustrates the evolution of data processing from manual through electronic procedures.
Although widespread use of the term data processing dates only from the nineteen-fifties[3] data processing functions have been performed manually for millennia. 
For example, bookkeeping involves functions such as posting transactions and producing reports like the balance sheet and the cash flow statement. 
Completely manual methods were augmented by the application of mechanical or electronic calculators. 
A person whose job was to perform calculations manually or using a calculator was called a "computer."
The 1850 United States Census schedule was the first to gather data by individual rather than household. 
A number of questions could be answered by making a check in the appropriate box on the form. 
From 1850 through 1880 the Census Bureau employed "a system of tallying, which, by reason of the increasing number of combinations of classifications required, became increasingly complex. 
Only a limited number of combinations could be recorded in one tally, so it was necessary to handle the schedules 5 or 6 times, for as many independent tallies."
 "It took over 7 years to publish the results of the 1880 census"[5] using manual processing methods.
The term automatic data processing was applied to operations performed by means of unit record equipment, such as Herman Hollerith's application of punched card equipment for the 1890 United States Census.
 "Using Hollerith's punchcard equipment, the Census Office was able to complete tabulating most of the 1890 census data in 2 to 3 years, compared with 7 to 8 years for the 1880 census.... 
It is also estimated that using Herman Hollerith's system saved some $5 million in processing costs"[5] (in 1890 dollars) even with twice as many questions as in 1880.
Computerized data processing, or Electronic data processing represents the further evolution, with the computer taking the place of several independent pieces of equipment. 
The Census Bureau first made limited use of electronic computers for the 1950 United States Census, using a UNIVAC I system,[4] delivered in 1952.
"Data processing (DP)" has also previously been used to refer to the department within an organization responsible for the operation of data processing applications.
 The term data processing has mostly been subsumed under the newer and somewhat more general term information technology (IT).
 "Data processing" has acquired a negative connotation, suggesting use of older technologies. 
As an example, in 1996 the Data Processing Management Association (DPMA) changed its name to the Association of Information Technology Professionals. 
Nevertheless, the terms are roughly synonymous.
Commercial data processing involves a large volume of input data, relatively few computational operations, and a large volume of output. 
For example, an insurance company needs to keep records on tens or hundreds of thousands of policies, print and mail bills, and receive and post payments.
In a science or engineering field, the terms data processing and information systems are considered too broad, and the more specialized term data analysis is typically used. 
Data analysis makes use of specialized and highly accurate algorithms and statistical calculations that are less often observed in the typical general business environment.
For data analysis, packages like SPSS or SAS, or their free counterparts such as DAP, gretl or PSPP are often used.
Predictive analytics encompasses a variety of statistical techniques from predictive modeling, machine learning, and data mining that analyze current and historical facts to make predictions about future or otherwise unknown events.
In business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. 
Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision making for candidate transactions.
Predictive analytics is used in actuarial science,[4] marketing,[5] financial services,[6] insurance, telecommunications,[7] retail,[8] travel,[9] healthcare,[10] pharmaceuticals,[11] capacity planning[citation needed] and other fields.
One of the most well known applications is credit scoring,[1] which is used throughout financial services. 
Scoring models process a customer's credit history, loan application, customer data, etc., in order to rank-order individuals by their likelihood of making future credit payments on time.
Predictive analytics is an area of data mining that deals with extracting information from data and using it to predict trends and behavior patterns. 
Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. 
For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs.
 The core of predictive analytics relies on capturing relationships between explanatory variables and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. 
It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.
Predictive analytics is often defined as predicting at a more detailed level of granularity, i.e., generating predictive scores (probabilities) for each individual organizational element. This distinguishes it from forecasting. 
For example, "Predictive analytics—Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions."[13]
Generally, the term predictive analytics is used to mean predictive modeling, "scoring" data with predictive models, and forecasting. 
However, people are increasingly using the term to refer to related analytical disciplines, such as descriptive modeling and decision modeling or optimization. 
These disciplines also involve rigorous data analysis, and are widely used in business for segmentation and decision making, but have different purposes and the statistical techniques underlying them vary.
Predictive models are models of the relation between the specific performance of a unit in a sample and one or more known attributes or features of the unit. 
The objective of the model is to assess the likelihood that a similar unit in a different sample will exhibit the specific performance. 
This category encompasses models in many areas, such as marketing, where they seek out subtle data patterns to answer questions about customer performance, or fraud detection models. 
Predictive models often perform calculations during live transactions, for example, to evaluate the risk or opportunity of a given customer or transaction, in order to guide a decision. 
With advancements in computing speed, individual agent modeling systems have become capable of simulating human behaviour or reactions to given stimuli or scenarios.
The available sample units with known attributes and known performances is referred to as the “training sample.” 
The units in other samples, with known attributes but unknown performances, are referred to as “out of [training] sample” units. 
The out of sample bear no chronological relation to the training sample units.
 For example, the training sample may consists of literary attributes of writings by Victorian authors, with known attribution, and the out-of sample unit may be newly found writing with unknown authorship; a predictive model may aid in attributing a work to a known author. 
Another example is given by analysis of blood splatter in simulated crime scenes in which the out of sample unit is the actual blood splatter pattern from a crime scene. 
The out of sample unit may be from the same time as the training units, from a previous time, or from a future time.
Descriptive models quantify relationships in data in a way that is often used to classify customers or prospects into groups. 
Unlike predictive models that focus on predicting a single customer behavior (such as credit risk), descriptive models identify many different relationships between customers or products. 
Descriptive models do not rank-order customers by their likelihood of taking a particular action the way predictive models do. 
Instead, descriptive models can be used, for example, to categorize customers by their product preferences and life stage. 
Descriptive modeling tools can be utilized to develop further models that can simulate large number of individualized agents and make predictions.
Decision models describe the relationship between all the elements of a decision — the known data (including results of predictive models), the decision, and the forecast results of the decision — in order to predict the results of decisions involving many variables. 
These models can be used in optimization, maximizing certain outcomes while minimizing others. 
Decision models are generally used to develop decision logic or a set of business rules that will produce the desired action for every customer or circumstance.
Although predictive analytics can be put to use in many applications, we outline a few examples where predictive analytics has shown positive impact in recent years.
Analytical Customer Relationship Management is a frequent commercial application of Predictive Analysis. 
Methods of predictive analysis are applied to customer data to pursue CRM objectives, which involve constructing a holistic view of the customer no matter where their information resides in the company or the department involved. 
CRM uses predictive analysis in applications for marketing campaigns, sales, and customer services to name a few. 
These tools are required in order for a company to posture and focus their efforts effectively across the breadth of their customer base. 
They must analyze and understand the products in demand or have the potential for high demand, predict customers' buying habits in order to promote relevant products at multiple touch points, and proactively identify and mitigate issues that have the potential to lose customers or reduce their ability to gain new ones. 
Analytical Customer Relationship Management can be applied throughout the customers lifecycle (acquisition, relationship growth, retention, and win-back). 
Several of the application areas described below (direct marketing, cross-sell, customer retention) are part of Customer Relationship Managements.
Experts use predictive analysis in health care primarily to determine which patients are at risk of developing certain conditions, like diabetes, asthma, heart disease, and other lifetime illnesses. 
Additionally, sophisticated clinical decision support systems incorporate predictive analytics to support medical decision making at the point of care. 
A working definition has been proposed by Robert Hayward of the Centre for Health Evidence: "Clinical Decision Support Systems link health observations with health knowledge to influence health choices by clinicians for improved health care.
Many portfolios have a set of delinquent customers who do not make their payments on time. 
The financial institution has to undertake collection activities on these customers to recover the amounts due. 
A lot of collection resources are wasted on customers who are difficult or impossible to recover. 
Predictive analytics can help optimize the allocation of collection resources by identifying the most effective collection agencies, contact strategies, legal actions and other strategies to each customer, thus significantly increasing recovery at the same time reducing collection costs.
Often corporate organizations collect and maintain abundant data (e.g. customer records, sale transactions) as exploiting hidden relationships in the data can provide a competitive advantage. 
For an organization that offers multiple products, predictive analytics can help analyze customers' spending, usage and other behavior, leading to efficient cross sales, or selling additional products to current customers.
 This directly leads to higher profitability per customer and stronger customer relationships.
With the number of competing services available, businesses need to focus efforts on maintaining continuous consumer satisfaction, rewarding consumer loyalty and minimizing customer attrition. 
In addition, small increases in customer retention have been shown to increase profits disproportionately. 
One study concluded that a 5% increase in customer retention rates will increase profits by 25% to 95%.
 Businesses tend to respond to customer attrition on a reactive basis, acting only after the customer has initiated the process to terminate service. 
At this stage, the chance of changing the customer's decision is almost impossible. 
Proper application of predictive analytics can lead to a more proactive retention strategy.
 By a frequent examination of a customer’s past service usage, service performance, spending and other behavior patterns, predictive models can determine the likelihood of a customer terminating service sometime soon.
 An intervention with lucrative offers can increase the chance of retaining the customer. 
Silent attrition, the behavior of a customer to slowly but steadily reduce usage, is another problem that many companies face. 
Predictive analytics can also predict this behavior, so that the company can take proper actions to increase customer activity.
When marketing consumer products and services, there is the challenge of keeping up with competing products and consumer behavior. 
Apart from identifying prospects, predictive analytics can also help to identify the most effective combination of product versions, marketing material, communication channels and timing that should be used to target a given consumer. 
The goal of predictive analytics is typically to lower the cost per order or cost per action.
Fraud is a big problem for many businesses and can be of various types: inaccurate credit applications, fraudulent transactions (both offline and online), identity thefts and false insurance claims. 
These problems plague firms of all sizes in many industries. 
Some examples of likely victims are credit card issuers, insurance companies,[15] retail merchants, manufacturers, business-to-business suppliers and even services providers. 
A predictive model can help weed out the "bads" and reduce a business's exposure to fraud.
Predictive modeling can also be used to identify high-risk fraud candidates in business or the public sector. 
Mark Nigrini developed a risk-scoring method to identify audit targets. 
He describes the use of this approach to detect fraud in the franchisee sales reports of an international fast-food chain. 
Each location is scored using 10 predictors. 
The 10 scores are then weighted to give one final overall risk score for each location. 
The same scoring approach was also used to identify high-risk check kiting accounts, potentially fraudulent travel agents, and questionable vendors. 
A reasonably complex model was used to identify fraudulent monthly reports submitted by divisional controllers.
The Internal Revenue Service (IRS) of the United States also uses predictive analytics to mine tax returns and identify tax fraud.
Recent[when?] advancements in technology have also introduced predictive behavior analysis for web fraud detection. 
This type of solution utilizes heuristics in order to study normal web user behavior and detect anomalies indicating fraud attempts.
Portfolio, product or economy-level prediction
Often the focus of analysis is not the consumer but the product, portfolio, firm, industry or even the economy. 
For example, a retailer might be interested in predicting store-level demand for inventory management purposes. 
Or the Federal Reserve Board might be interested in predicting the unemployment rate for the next year.
These types of problems can be addressed by predictive analytics using time series techniques (see below). 
They can also be addressed via machine learning approaches which transform the original time series into a feature vector space, where the learning algorithm finds patterns that have predictive power.
When employing risk management techniques, the results are always to predict and benefit from a future scenario. 
The Capital asset pricing model (CAP-M) "predicts" the best portfolio to maximize return, Probabilistic Risk Assessment (PRA)--when combined with mini-Delphi Techniques and statistical approaches yields accurate forecasts and RiskAoA is a stand-alone predictive tool.
 These are three examples of approaches that can extend from project to market, and from near to long term. 
Underwriting (see below) and other business approaches identify risk management as a predictive method.
Many businesses have to account for risk exposure due to their different services and determine the cost needed to cover the risk. 
For example, auto insurance providers need to accurately determine the amount of premium to charge to cover each automobile and driver.
 A financial company needs to assess a borrower's potential and ability to pay before granting a loan. For a health insurance provider, predictive analytics can analyze a few years of past medical claims data, as well as lab, pharmacy and other records where available, to predict how expensive an enrollee is likely to be in the future. 
Predictive analytics can help underwrite these quantities by predicting the chances of illness, default, bankruptcy, etc. 
Predictive analytics can streamline the process of customer acquisition by predicting the future risk behavior of a customer using application level data.
Predictive analytics in the form of credit scores have reduced the amount of time it takes for loan approvals, especially in the mortgage market where lending decisions are now made in a matter of hours rather than days or even weeks. 
Proper predictive analytics can lead to proper pricing decisions, which can help mitigate future risk of default.
Big data is a collection of data sets that are so large and complex that they become awkward to work with using traditional database management tools. 
The volume, variety and velocity of big data have introduced challenges across the board for capture, storage, search, sharing, analysis, and visualization.
 Examples of big data sources include web logs, RFID, sensor data, social networks, Internet search indexing, call detail records, military surveillance, and complex data in astronomic, biogeochemical, genomics, and atmospheric sciences. 
Big Data is the core of most predictive analytic services offered by IT organizations.
Thanks to technological advances in computer hardware — faster CPUs, cheaper memory, and MPP architectures — and new technologies such as Hadoop, MapReduce, and in-database and text analytics for processing big data, it is now feasible to collect, analyze, and mine massive amounts of structured and unstructured data for new insights.
 Today, exploring big data and using predictive analytics is within reach of more organizations than ever before and new methods that are capable for handling such datasets are proposed 
The approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.
Regression models are the mainstay of predictive analytics.
 The focus lies on establishing a mathematical equation as a model to represent the interactions between the different variables in consideration. 
epending on the situation, there are a wide variety of models that can be applied while performing predictive analytics. 
Some of them are briefly discussed below.
The linear regression model analyzes the relationship between the response or dependent variable and a set of independent or predictor variables. 
This relationship is expressed as an equation that predicts the response variable as a linear function of the parameters. 
These parameters are adjusted so that a measure of fit is optimized. 
Much of the effort in model fitting is focused on minimizing the size of the residual, as well as ensuring that it is randomly distributed with respect to the model predictions.
The goal of regression is to select the parameters of the model so as to minimize the sum of the squared residuals. 
This is referred to as ordinary least squares (OLS) estimation and results in best linear unbiased estimates (BLUE) of the parameters if and only if the Gauss-Markov assumptions are satisfied.
Once the model has been estimated we would be interested to know if the predictor variables belong in the model – i.e. is the estimate of each variable's contribution reliable? To do this we can check the statistical significance of the model’s coefficients which can be measured using the t-statistic. 
This amounts to testing whether the coefficient is significantly different from zero. 
How well the model predicts the dependent variable based on the value of the independent variables can be assessed by using the R² statistic. 
It measures predictive power of the model i.e. the proportion of the total variation in the dependent variable that is "explained" (accounted for) by variation in the independent variables.
Multivariate regression (above) is generally used when the response variable is continuous and has an unbounded range. 
Often the response variable may not be continuous but rather discrete. 
While mathematically it is feasible to apply multivariate regression to discrete ordered dependent variables, some of the assumptions behind the theory of multivariate linear regression no longer hold, and there are other techniques such as discrete choice models which are better suited for this type of analysis. 
If the dependent variable is discrete, some of those superior methods are logistic regression, multinomial logit and probit models. Logistic regression and probit models are used when the dependent variable is binary.
For more details on this topic, see logistic regression.
In a classification setting, assigning outcome probabilities to observations can be achieved through the use of a logistic model, which is basically a method which transforms information about the binary dependent variable into an unbounded continuous variable and estimates a regular multivariate model.
The Wald and likelihood-ratio test are used to test the statistical significance of each coefficient b in the model (analogous to the t tests used in OLS regression; see above). 
A test assessing the goodness-of-fit of a classification model is the "percentage correctly predicted".
an extension of the binary logit model to cases where the dependent variable has more than 2 categories is the multinomial logit model. 
In such cases collapsing the data into two categories might not make good sense or may lead to loss in the richness of the data. 
The multinomial logit model is the appropriate technique in these cases, especially when the dependent variable categories are not ordered (for examples colors like red, blue, green). 
Some authors have extended multinomial regression to include feature selection/importance methods such as Random multinomial logit.
Probit models offer an alternative to logistic regression for modeling categorical dependent variables. 
Even though the outcomes tend to be similar, the underlying distributions are different. Probit models are popular in social sciences like economics.
A good way to understand the key difference between probit and logit models is to assume that there is a latent variable z.
We do not observe z but instead observe y which takes the value 0 or 1. 
In the logit model we assume that y follows a logistic distribution. 
In the probit model we assume that y follows a standard normal distribution. 
Note that in social sciences (e.g. economics), probit is often used to model situations where the observed variable y is continuous but takes values between 0 and 1.
The Probit model has been around longer than the logit model. 
They behave similarly, except that the logistic distribution tends to be slightly flatter tailed. 
One of the reasons the logit model was formulated was that the probit model was computationally difficult due to the requirement of numerically calculating integrals. 
Modern computing however has made this computation fairly simple. 
The coefficients obtained from the logit and probit model are fairly close. However, the odds ratio is easier to interpret in the logit model.
Practical reasons for choosing the probit model over the logistic model would be:
There is a strong belief that the underlying distribution is normal
The actual event is not a binary outcome (e.g., bankruptcy status) but a proportion (e.g., proportion of population at different debt levels).
Time series models are used for predicting or forecasting the future behavior of variables. 
These models account for the fact that data points taken over time may have an internal structure (such as autocorrelation, trend or seasonal variation) that should be accounted for. 
As a result, standard regression techniques cannot be applied to time series data and methodology has been developed to decompose the trend, seasonal and cyclical component of the series. 
Modeling the dynamic path of a variable can improve forecasts since the predictable component of the series can be projected into the future.
Time series models estimate difference equations containing stochastic components. 
Two commonly used forms of these models are autoregressive models (AR) and moving-average (MA) models. 
The Box–Jenkins methodology (1976) developed by George Box and G.M. Jenkins combines the AR and MA models to produce the ARMA (autoregressive moving average) model which is the cornerstone of stationary time series analysis. 
ARIMA (autoregressive integrated moving average models) on the other hand are used to describe non-stationary time series.
 Box and Jenkins suggest differencing a non stationary time series to obtain a stationary series to which an ARMA model can be applied. 
Non stationary time series have a pronounced trend and do not have a constant long-run mean or variance.
Box and Jenkins proposed a three-stage methodology which includes: model identification, estimation and validation. 
The identification stage involves identifying if the series is stationary or not and the presence of seasonality by examining plots of the series, autocorrelation and partial autocorrelation functions. 
In the estimation stage, models are estimated using non-linear time series or maximum likelihood estimation procedures. 
Finally the validation stage involves diagnostic checking such as plotting the residuals to detect outliers and evidence of model fit.
In recent years time series models have become more sophisticated and attempt to model conditional heteroskedasticity with models such as ARCH (autoregressive conditional heteroskedasticity) and GARCH (generalized autoregressive conditional heteroskedasticity) models frequently used for financial time series. 
In addition time series models are also used to understand inter-relationships among economic variables represented by systems of equations using VAR (vector autoregression) and structural VAR models.
Survival analysis is another name for time to event analysis. 
These techniques were primarily developed in the medical and biological sciences, but they are also widely used in the social sciences like economics, as well as in engineering (reliability and failure time analysis).
Censoring and non-normality, which are characteristic of survival data, generate difficulty when trying to analyze the data using conventional statistical models such as multiple linear regression.
 The normal distribution, being a symmetric distribution, takes positive as well as negative values, but duration by its very nature cannot be negative and therefore normality cannot be assumed when dealing with duration/survival data. 
Hence the normality assumption of regression models is violated.
The assumption is that if the data were not censored it would be representative of the population of interest. 
In survival analysis, censored observations arise whenever the dependent variable of interest represents the time to a terminal event, and the duration of the study is limited in time.
An important concept in survival analysis is the hazard rate, defined as the probability that the event will occur at time t conditional on surviving until time t.
 Another concept related to the hazard rate is the survival function which can be defined as the probability of surviving to time t.
Most models try to model the hazard rate by choosing the underlying distribution depending on the shape of the hazard function. 
A distribution whose hazard function slopes upward is said to have positive duration dependence, a decreasing hazard shows negative duration dependence whereas constant hazard is a process with no memory usually characterized by the exponential distribution. 
Some of the distributional choices in survival models are: F, gamma, Weibull, log normal, inverse normal, exponential etc. 
All these distributions are for a non-negative random variable.
Duration models can be parametric, non-parametric or semi-parametric. 
Some of the models commonly used are Kaplan-Meier and Cox proportional hazard model (non parametric).
Globally-optimal classification tree analysis (GO-CTA) (also called hierarchical optimal discriminant analysis) is a generalization of optimal discriminant analysis that may be used to identify the statistical model that has maximum accuracy for predicting the value of a categorical dependent variable for a dataset consisting of categorical and continuous variables. 
The output of HODA is a non-orthogonal tree that combines categorical variables and cut points for continuous variables that yields maximum predictive accuracy, an assessment of the exact Type I error rate, and an evaluation of potential cross-generalizability of the statistical model. 
Hierarchical optimal discriminant analysis may be thought of as a generalization of Fisher's linear discriminant analysis. 
Optimal discriminant analysis is an alternative to ANOVA (analysis of variance) and regression analysis, which attempt to express one dependent variable as a linear combination of other features or measurements. 
However, ANOVA and regression analysis give a dependent variable that is a numerical variable, while hierarchical optimal discriminant analysis gives a dependent variable that is a class variable.
Classification and regression trees (CART) are a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively.
Decision trees are formed by a collection of rules based on variables in the modeling data set:
Rules based on variables' values are selected to get the best split to differentiate observations based on the dependent variable
Once a rule is selected and splits a node into two, the same process is applied to each "child" node (i.e. it is a recursive procedure)
Splitting stops when CART detects no further gain can be made, or some pre-set stopping rules are met. (Alternatively, the data are split as much as possible and then the tree is later pruned.)
Each branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules.
A very popular method for predictive analytics is Leo Breiman's Random forests.
Multivariate adaptive regression splines (MARS) is a non-parametric technique that builds flexible models by fitting piecewise linear regressions.
An important concept associated with regression splines is that of a knot. Knot is where one local regression model gives way to another and thus is the point of intersection between two splines.
In multivariate and adaptive regression splines, basis functions are the tool used for generalizing the search for knots.
Basis functions are a set of functions used to represent the information contained in one or more variables. 
Multivariate and Adaptive Regression Splines model almost always creates the basis functions in pairs.
Multivariate and adaptive regression spline approach deliberately overfits the model and then prunes to get to the optimal model. 
The algorithm is computationally very intensive and in practice we are required to specify an upper limit on the number of basis functions.
Machine learning, a branch of artificial intelligence, was originally employed to develop techniques to enable computers to learn. 
In certain applications it is sufficient to directly predict the dependent variable without focusing on the underlying relationships between variables. 
In other cases, the underlying relationships can be very complex and the mathematical form of the dependencies unknown. 
For such cases, machine learning techniques emulate human cognition and learn from training examples to predict future events.
A brief discussion of some of these methods used commonly for predictive analytics is provided below. 
A detailed study of machine learning can be found in Mitchell (1997).
Neural networks are nonlinear sophisticated modeling techniques that are able to model complex functions. 
They can be applied to problems of prediction, classification or control in a wide spectrum of fields such as finance, cognitive psychology/neuroscience, medicine, engineering, and physics.
Neural networks are used when the exact nature of the relationship between inputs and output is not known. 
A key feature of neural networks is that they learn the relationship between inputs and output through training. 
There are three types of training in neural networks used by different networks, supervised and unsupervised training, reinforcement learning, with supervised being the most common one.
Some examples of neural network training techniques are backpropagation, quick propagation, conjugate gradient descent, projection operator, Delta-Bar-Delta etc. 
Some unsupervised network architectures are multilayer perceptrons, Kohonen networks, Hopfield networks, etc.
The Multilayer Perceptron (MLP) consists of an input and an output layer with one or more hidden layers of nonlinearly-activating nodes or sigmoid nodes.
This is determined by the weight vector and it is necessary to adjust the weights of the network. 
The backpropagation employs gradient fall to minimize the squared error between the network output values and desired values for those outputs. 
The weights adjusted by an iterative process of repetitive present of attributes. 
Small changes in the weight to get the desired values are done by the process called training the net and is done by the training set (learning rule).
A radial basis function (RBF) is a function which has built into it a distance criterion with respect to a center. 
Such functions can be used very efficiently for interpolation and for smoothing of data.
 Radial basis functions have been applied in the area of neural networks where they are used as a replacement for the sigmoidal transfer function. 
Such networks have 3 layers, the input layer, the hidden layer with the RBF non-linearity and a linear output layer. The most popular choice for the non-linearity is the Gaussian. 
RBF networks have the advantage of not being locked into local minima as do the feed-forward networks such as the multilayer perceptron.
Support Vector Machines (SVM) are used to detect and exploit complex patterns in data by clustering, classifying and ranking the data. 
They are learning machines that are used to perform binary classifications and regression estimations. 
They commonly use kernel based methods to apply linear classification techniques to non-linear classification problems. 
There are a number of types of SVM such as linear, polynomial, sigmoid etc.
Naïve Bayes based on Bayes conditional probability rule is used for performing classification tasks. 
Naïve Bayes assumes the predictors are statistically independent which makes it an effective classification tool that is easy to interpret. 
It is best employed when faced with the problem of ‘curse of dimensionality’ i.e. when the number of predictors is very high.
The nearest neighbour algorithm (KNN) belongs to the class of pattern recognition statistical methods. 
The method does not impose a priori any assumptions about the distribution from which the modeling sample is drawn. 
It involves a training set with both positive and negative values.
 A new sample is classified by calculating the distance to the nearest neighbouring training case.
 The sign of that point will determine the classification of the sample.
 In the k-nearest neighbour classifier, the k nearest points are considered and the sign of the majority is used to classify the sample. 
The performance of the kNN algorithm is influenced by three main factors: (1) the distance measure used to locate the nearest neighbours; (2) the decision rule used to derive a classification from the k-nearest neighbours; and (3) the number of neighbours used to classify the new sample. 
Conceptually, geospatial predictive modeling is rooted in the principle that the occurrences of events being modeled are limited in distribution. 
Occurrences of events are neither uniform nor random in distribution – there are spatial environment factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. 
Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences. 
Geospatial predictive modeling is a process for analyzing events through a geographic filter in order to make statements of likelihood for event occurrence or emergence.
Historically, using predictive analytics tools—as well as understanding the results they delivered—required advanced skills.
However, modern predictive analytics tools are no longer restricted to IT specialists[citation needed]. 
As more organizations adopt predictive analytics into decision-making processes and integrate it into their operations, they are creating a shift in the market toward business users as the primary consumers of the information.
 Business users want tools they can use on their own.
 Vendors are responding by creating new software that removes the mathematical complexity, provides user-friendly graphic interfaces and/or builds in short cuts that can, for example, recognize the kind of data available and suggest an appropriate predictive model.
 Predictive analytics tools have become sophisticated enough to adequately present and dissect data problems[citation needed], so that any data-savvy information worker can utilize them to analyze data and retrieve meaningful, useful results.
 For example, modern tools present findings using simple charts, graphs, and scores that indicate the likelihood of possible outcomes.
There are numerous tools available in the marketplace that help with the execution of predictive analytics. 
These range from those that need very little user sophistication to those that are designed for the expert practitioner. 
The difference between these tools is often in the level of customization and heavy data lifting allowed.
In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis. 
DWs are central repositories of integrated data from one or more disparate sources.
They store current and historical data and are used for creating analytical reports for knowledge workers throughout the enterprise. Examples of reports could range from annual and quarterly comparisons and trends to detailed daily sales analyses.
The data stored in the warehouse is uploaded from the operational systems (such as marketing, sales, etc., shown in the figure to the right). 
The data may pass through an operational data store for additional operations before it is used in the DW for reporting.
A data mart is a simple form of a data warehouse that is focused on a single subject (or functional area) hence, they draw data from a limited number of sources such as sales, finance or marketing. 
Data marts are often built and controlled by a single department within an organization. 
The sources could be internal operational systems, a central data warehouse, or external data.
Denormalization is the norm for data modeling techniques in this system. 
Given that data marts generally cover only a subset of the data contained in a data warehouse, they are often easier and faster to implement.
Online analytical processing (OLAP)
OLAP is characterized by a relatively low volume of transactions. 
Queries are often very complex and involve aggregations. 
For OLAP systems, response time is an effectiveness measure. 
OLAP applications are widely used by Data Mining techniques. 
OLAP databases store aggregated, historical data in multi-dimensional schemas (usually star schemas).
 OLAP systems typically have data latency of a few hours, as opposed to data marts, where latency is expected to be closer to one day.
The OLAP approach is used to analyze multidimensional data from multiple sources and perspectives. 
The three basic operations in OLAP are : Roll-up (Consolidation), Drill-down and Slicing & Dicing.[6]
Online transaction processing (OLTP)
OLTP is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). 
OLTP systems emphasize very fast query processing and maintaining data integrity in multi-access environments. 
For OLTP systems, effectiveness is measured by the number of transactions per second. OLTP databases contain detailed and current data. 
The schema used to store transactional databases is the entity model (usually 3NF).
 Normalization is the norm for data modeling techniques in this system.
Predictive analysis is about finding and quantifying hidden patterns in the data using complex mathematical models that can be used to predict future outcomes. 
Predictive analysis is different from OLAP in that OLAP focuses on historical data analysis and is reactive in nature, while predictive analysis focuses on the future. 
These systems are also used for CRM (customer relationship management).
The typical extract-transform-load (ETL)-based data warehouse uses staging, data integration, and access layers to house its key functions. 
The staging layer or staging database stores raw data extracted from each of the disparate source data systems. 
The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an operational data store (ODS) database. 
The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups often called dimensions and into facts and aggregate facts. 
The combination of facts and dimensions is sometimes called a star schema. 
The access layer helps users retrieve data.
This definition of the data warehouse focuses on data storage.
 The main source of the data is cleaned, transformed, cataloged and made available for use by managers and other business professionals for data mining, online analytical processing, market research and decision support.
 However, the means to retrieve and analyze data, to extract, transform and load data, and to manage the data dictionary are also considered essential components of a data warehousing system. 
Many references to data warehousing use this broader context. 
Thus, an expanded definition for data warehousing includes business intelligence tools, tools to extract, transform and load data into the repository, and tools to manage and retrieve metadata.
A data warehouse maintains a copy of information from the source transaction systems. 
This architectural complexity provides the opportunity to :
Congregate data from multiple sources into a single database so a single query engine can be used to present data.
Mitigate the problem of database isolation level lock contention in transaction processing systems caused by attempts to run large, long running, analysis queries in transaction processing databases.
Maintain data history, even if the source transaction systems do not.
Integrate data from multiple source systems, enabling a central view across the enterprise. 
This benefit is always valuable, but particularly so when the organization has grown by merger.
Improve data quality, by providing consistent codes and descriptions, flagging or even fixing bad data.
Present the organization's information consistently.
Provide a single common data model for all data of interest regardless of the data's source.
Restructure the data so that it makes sense to the business users.
Restructure the data so that it delivers excellent query performance, even for complex analytic queries, without impacting the operational systems.
Add value to operational business applications, notably customer relationship management (CRM) systems.
Make decision–support queries easier to write.
Generic data warehouse environment
The environment for data warehouses and marts includes the following:
Source systems that provide data to the warehouse or mart;
Data integration technology and processes that are needed to prepare the data for use;
Different architectures for storing data in an organization's data warehouse or data marts;
Different tools and applications for the variety of users;
Metadata, data quality, and governance processes must be in place to ensure that the warehouse or mart meets its purposes.
In regards to source systems listed above, Rainer[clarification needed] states, “A common source for the data in data warehouses is the company’s operational databases, which can be relational databases”.
Rainer discusses storing data in an organization’s data warehouse or data marts.
Metadata are data about data. 
IT personnel need information about data sources; database, table, and column names; refresh schedules; and data usage measures
Today, the most successful companies are those that can respond quickly and flexibly to market changes and opportunities. 
A key to this response is the effective and efficient use of data and information by analysts and managers.
 A “data warehouse” is a repository of historical data that are organized by subject to support decision makers in the organization.
 Once data are stored in a data mart or warehouse, they can be accessed.
The concept of data warehousing dates back to the late 1980s[12] when IBM researchers Barry Devlin and Paul Murphy developed the "business data warehouse".
 In essence, the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to decision support environments. 
The concept attempted to address the various problems associated with this flow, mainly the high costs associated with it. 
In the absence of a data warehousing architecture, an enormous amount of redundancy was required to support multiple decision support environments. 
In larger corporations it was typical for multiple decision support environments to operate independently. 
hough each environment served different users, they often required much of the same stored data.
 The process of gathering, cleaning and integrating data from various sources, usually from long-term existing operational systems (usually referred to as legacy systems), was typically in part replicated for each environment. 
Moreover, the operational systems were frequently reexamined as new decision support requirements emerged. 
Often new requirements necessitated gathering, cleaning and integrating new data from "data marts" that were tailored for ready access by users.
Key developments in early years of data warehousing were:
1960s — General Mills and Dartmouth College, in a joint research project, develop the terms dimensions and facts.
1970s — ACNielsen and IRI provide dimensional data marts for retail sales.
1970s — Bill Inmon begins to define and discuss the term: Data Warehouse.
1975 — Sperry Univac introduces MAPPER (MAintain, Prepare, and Produce Executive Reports) is a database management and reporting system that includes the world's first 4GL. 
First platform designed for building Information Centers (a forerunner of contemporary Enterprise Data Warehousing platforms)
1983 — Teradata introduces a database management system specifically designed for decision support.
1984 — Metaphor Computer Systems, founded by David Liddle and Don Massaro, releases Data Interpretation System (DIS). 
DIS was a hardware/software package and GUI for business users to create a database management and analytic system.
1988 — Barry Devlin and Paul Murphy publish the article An architecture for a business and information system where they introduce the term "business data warehouse".
1990 — Red Brick Systems, founded by Ralph Kimball, introduces Red Brick Warehouse, a database management system specifically for data warehousing.
1991 — Prism Solutions, founded by Bill Inmon, introduces Prism Warehouse Manager, software for developing a data warehouse.
1992 — Bill Inmon publishes the book Building the Data Warehouse.
1995 — The Data Warehousing Institute, a for-profit organization that promotes data warehousing, is founded.
1996 — Ralph Kimball publishes the book The Data Warehouse Toolkit.
2012 — Bill Inmon developed and made public technology known as "textual disambiguation". 
Textual disambiguation applies context to raw text and reformats the raw text and context into a standard data base format. 
Once raw text is passed through textual disambiguation, it can easily and efficiently be accessed and analyzed by standard business intelligence technology. 
Textual disambiguation is accomplished through the execution of textual ETL. 
Textual disambiguation is useful wherever raw text is found, such as in documents, Hadoop, email, and so forth.
A fact is a value or measurement, which represents a fact about the managed entity or system.
Facts as reported by the reporting entity are said to be at raw level. 
E.g. if a BTS (business transformation service) received 1,000 requests for traffic channel allocation, it allocates for 820 and rejects the remaining then it would report 3 facts or measurements to a management system:
Facts at raw level are further aggregated to higher levels in various dimensions to extract more service or business-relevant information out of it. 
These are called aggregates or summaries or aggregated facts.
E.g. if there are 3 BTSs in a city, then facts above can be aggregated from BTS to city level in network dimension. E.g.
There are three or more leading approaches to storing data in a data warehouse — the most important approaches are the dimensional approach and the normalized approach.
The dimensional approach refers to Ralph Kimball’s approach in which it is stated that the data warehouse should be modeled using a Dimensional Model/star schema. 
The normalized approach, also called the 3NF model (Third Normal Form) refers to Bill Inmon's approach in which it is stated that the data warehouse should be modeled using an E-R model/normalized model.
In a dimensional approach, transaction data are partitioned into "facts", which are generally numeric transaction data, and "dimensions", which are the reference information that gives context to the facts. For example, a sales transaction can be broken up into facts such as the number of products ordered and the price paid for the products, and into dimensions such as order date, customer name, product number, order ship-to and bill-to locations, and salesperson responsible for receiving the order.
A key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use. 
Also, the retrieval of data from the data warehouse tends to operate very quickly.
 Dimensional structures are easy to understand for business users, because the structure is divided into measurements/facts and context/dimensions. 
Facts are related to the organization’s business processes and operational system whereas the dimensions surrounding them contain context about the measurement (Kimball, Ralph 2008).
 Another advantage offered by dimensional model is that it does not involve a relational database every time. 
Thus,this type of modeling technique is very useful for end-user queries in data warehouse.[6]
The main disadvantages of the dimensional approach are the following:
In order to maintain the integrity of facts and dimensions, loading the data warehouse with data from different operational systems is complicated.
It is difficult to modify the data warehouse structure if the organization adopting the dimensional approach changes the way in which it does business.
In the normalized approach, the data in the data warehouse are stored following, to a degree, database normalization rules. 
Tables are grouped together by subject areas that reflect general data categories (e.g., data on customers, products, finance, etc.). 
The normalized structure divides data into entities, which creates several tables in a relational database.
 When applied in large enterprises the result is dozens of tables that are linked together by a web of joins. 
Furthermore, each of the created entities is converted into separate physical tables when the database is implemented (Kimball, Ralph 2008)[citation needed]. 
The main advantage of this approach is that it is straightforward to add information into the database. 
Both normalized and dimensional models can be represented in entity-relationship diagrams as both contain joined relational tables. 
The difference between the two models is the degree of normalization (also known as Normal Forms).
 These approaches are not mutually exclusive, and there are other approaches. 
Dimensional approaches can involve normalizing data to a degree (Kimball, Ralph 2008).
In Information-Driven Business,[17] Robert Hillard proposes an approach to comparing the two approaches based on the information needs of the business problem. 
The technique shows that normalized models hold far more information than their dimensional equivalents (even when the same fields are used in both models) but this extra information comes at the cost of usability. 
The technique measures information quantity in terms of information entropy and usability in terms of the Small Worlds data transformation measure.[18]
This section needs additional citations for verification. 
Please help improve this article by adding citations to reliable sources. 
Unsourced material may be challenged and removed. (July 2015)
In the bottom-up approach, data marts are first created to provide reporting and analytical capabilities for specific business processes. 
These data marts can then be integrated to create a comprehensive data warehouse. 
The data warehouse bus architecture is primarily an implementation of "the bus", a collection of conformed dimensions and conformed facts, which are dimensions that are shared (in a specific way) between facts in two or more data marts.
The top-down approach is designed using a normalized enterprise data model. 
"Atomic" data, that is, data at the lowest level of detail, are stored in the data warehouse.
 Dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse.
Data warehouses (DW) often resemble the hub and spokes architecture. 
Legacy systems feeding the warehouse often include customer relationship management and enterprise resource planning, generating large amounts of data. 
To consolidate these various data models, and facilitate the extract transform load process, data warehouses often make use of an operational data store, the information from which is parsed into the actual DW. 
To reduce data redundancy, larger systems often store the data in a normalized way. Data marts for specific reports can then be built on top of the DW.
The DW database in a hybrid solution is kept on third normal form to eliminate data redundancy.
 A normal relational database, however, is not efficient for business intelligence reports where dimensional modelling is prevalent. 
Small data marts can shop for data from the consolidated warehouse and use the filtered, specific data for the fact tables and dimensions required. 
The DW provides a single source of information from which the data marts can read, providing a wide range of business information. 
The hybrid architecture allows a DW to be replaced with a master data management solution where operational, not static information could reside.
The Data Vault Modeling components follow hub and spokes architecture.
 This modeling style is a hybrid design, consisting of the best practices from both third normal form and star schema. 
The Data Vault model is not a true third normal form, and breaks some of its rules, but it is a top-down architecture with a bottom up design.
The Data Vault model is geared to be strictly a data warehouse. 
It is not geared to be end-user accessible, which when built, still requires the use of a data mart or star schema based release area for business purposes.
Operational systems are optimized for preservation of data integrity and speed of recording of business transactions through use of database normalization and an entity-relationship model. 
Operational system designers generally follow the Codd rules of database normalization in order to ensure data integrity. 
Codd defined five increasingly stringent rules of normalization. 
Fully normalized database designs (that is, those satisfying all five Codd rules) often result in information from a business transaction being stored in dozens to hundreds of tables.
 Relational databases are efficient at managing the relationships between these tables. 
The databases have very fast insert/update performance because only a small amount of data in those tables is affected each time a transaction is processed. 
Finally, in order to improve performance, older data are usually periodically purged from operational systems.
Data warehouses are optimized for analytic access patterns. 
Analytic access patterns generally involve selecting specific fields and rarely if ever 'select *' as is more common in operational databases. 
Because of these differences in access patterns, operational databases (loosely, OLTP) benefit from the use of a row-oriented DBMS whereas analytics databases (loosely, OLAP) benefit from the use of a column-oriented DBMS. 
Unlike operational systems which maintain a snapshot of the business, data warehouses generally maintain an infinite history which is implemented through ETL processes that periodically migrate data from the operational systems over to the data warehouse.
Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). 
It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, and bioinformatics.
Cluster analysis itself is not one specific algorithm, but the general task to be solved. 
It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. 
Popular notions of clusters include groups with small distances among the cluster members, dense areas of the data space, intervals or particular statistical distributions. 
Clustering can therefore be formulated as a multi-objective optimization problem. 
The appropriate clustering algorithm and parameter settings (including values such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. 
Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. 
It will often be necessary to modify data preprocessing and model parameters until the result achieves the desired properties.
Besides the term clustering, there are a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek βότρυς "grape") and typological analysis. 
The subtle differences are often in the usage of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest. 
This often leads to misunderstandings between researchers coming from the fields of data mining and machine learning, since they use the same terms and often the same algorithms, but have different goals.
Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Zubin in 1938 and Robert Tryon in 1939[1][2] and famously used by Cattell beginning in 1943[3] for trait theory classification in personality psychology.
The notion of a "cluster" cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms.
 There is a common denominator: a group of data objects. 
However, different researchers employ different cluster models, and for each of these cluster models again different algorithms can be given. 
The notion of a cluster, as found by different algorithms, varies significantly in its properties. 
Understanding these "cluster models" is key to understanding the differences between the various algorithms. Typical cluster models include:
Connectivity models: for example, hierarchical clustering builds models based on distance connectivity.
Centroid models: for example, the k-means algorithm represents each cluster by a single mean vector.
Distribution models: clusters are modeled using statistical distributions, such as multivariate normal distributions used by the Expectation-maximization algorithm.
Density models: for example, DBSCAN and OPTICS defines clusters as connected dense regions in the data space.
Subspace models: in Biclustering (also known as Co-clustering or two-mode-clustering), clusters are modeled with both cluster members and relevant attributes.
Group models: some algorithms do not provide a refined model for their results and just provide the grouping information.
Graph-based models: a clique, that is, a subset of nodes in a graph such that every two nodes in the subset are connected by an edge can be considered as a prototypical form of cluster. 
Relaxations of the complete connectivity requirement (a fraction of the edges can be missing) are known as quasi-cliques, as in the HCS clustering algorithm.
A "clustering" is essentially a set of such clusters, usually containing all objects in the data set. 
Additionally, it may specify the relationship of the clusters to each other, for example, a hierarchy of clusters embedded in each other. Clusterings can be roughly distinguished as:
hard clustering: each object belongs to a cluster or not
soft clustering (also: fuzzy clustering): each object belongs to each cluster to a certain degree (for example, a likelihood of belonging to the cluster)
There are also finer distinctions possible, for example:
strict partitioning clustering: here each object belongs to exactly one cluster
strict partitioning clustering with outliers: objects can also belong to no cluster, and are considered outliers.
overlapping clustering (also: alternative clustering, multi-view clustering): while usually a hard clustering, objects may belong to more than one cluster.
hierarchical clustering: objects that belong to a child cluster also belong to the parent cluster
subspace clustering: while an overlapping clustering, within a uniquely defined subspace, clusters are not expected to overlap.
Clustering algorithms can be categorized based on their cluster model, as listed above.
 The following overview will only list the most prominent examples of clustering algorithms, as there are possibly over 100 published clustering algorithms. 
Not all provide models for their clusters and can thus not easily be categorized. 
An overview of algorithms explained in Wikipedia can be found in the list of statistics algorithms.
There is no objectively "correct" clustering algorithm, but as it was noted, "clustering is in the eye of the beholder."
The most appropriate clustering algorithm for a particular problem often needs to be chosen experimentally, unless there is a mathematical reason to prefer one cluster model over another. 
It should be noted that an algorithm that is designed for one kind of model has no chance on a data set that contains a radically different kind of model.
 For example, k-means cannot find non-convex clusters.
Connectivity based clustering, also known as hierarchical clustering, is based on the core idea of objects being more related to nearby objects than to objects farther away. 
These algorithms connect "objects" to form "clusters" based on their distance. 
A cluster can be described largely by the maximum distance needed to connect parts of the cluster. A
t different distances, different clusters will form, which can be represented using a dendrogram, which explains where the common name "hierarchical clustering" comes from: these algorithms do not provide a single partitioning of the data set, but instead provide an extensive hierarchy of clusters that merge with each other at certain distances. 
In a dendrogram, the y-axis marks the distance at which the clusters merge, while the objects are placed along the x-axis such that the clusters don't mix.
Connectivity based clustering is a whole family of methods that differ by the way distances are computed. 
Apart from the usual choice of distance functions, the user also needs to decide on the linkage criterion (since a cluster consists of multiple objects, there are multiple candidates to compute the distance to) to use. 
Popular choices are known as single-linkage clustering (the minimum of object distances), complete linkage clustering (the maximum of object distances) or UPGMA ("Unweighted Pair Group Method with Arithmetic Mean", also known as average linkage clustering). 
Furthermore, hierarchical clustering can be agglomerative (starting with single elements and aggregating them into clusters) or divisive (starting with the complete data set and dividing it into partitions).
These methods will not produce a unique partitioning of the data set, but a hierarchy from which the user still needs to choose appropriate clusters. 
They are not very robust towards outliers, which will either show up as additional clusters or even cause other clusters to merge (known as "chaining phenomenon", in particular with single-linkage clustering). 
In the general case, the complexity is \mathcal{O}(n^3) for agglomerative clustering and \mathcal{O}(2^{n-1}) for divisive clustering,[5] which makes them too slow for large data sets. 
For some special cases, optimal efficient methods (of complexity \mathcal{O}(n^2)) are known: SLINK[6] for single-linkage and CLINK[7] for complete-linkage clustering. 
In the data mining community these methods are recognized as a theoretical foundation of cluster analysis, but often considered obsolete. 
They did however provide inspiration for many later methods such as density based clustering.
In centroid-based clustering, clusters are represented by a central vector, which may not necessarily be a member of the data set. 
When the number of clusters is fixed to k, k-means clustering gives a formal definition as an optimization problem: find the k cluster centers and assign the objects to the nearest cluster center, such that the squared distances from the cluster are minimized.
The optimization problem itself is known to be NP-hard, and thus the common approach is to search only for approximate solutions. 
A particularly well known approximative method is Lloyd's algorithm,[8] often actually referred to as "k-means algorithm". 
It does however only find a local optimum, and is commonly run multiple times with different random initializations. 
Variations of k-means often include such optimizations as choosing the best of multiple runs, but also restricting the centroids to members of the data set (k-medoids), choosing medians (k-medians clustering), choosing the initial centers less randomly (K-means++) or allowing a fuzzy cluster assignment (Fuzzy c-means).
Most k-means-type algorithms require the number of clusters - k - to be specified in advance, which is considered to be one of the biggest drawbacks of these algorithms. 
Furthermore, the algorithms prefer clusters of approximately similar size, as they will always assign an object to the nearest centroid. 
This often leads to incorrectly cut borders in between of clusters (which is not surprising, as the algorithm optimized cluster centers, not cluster borders).
K-means has a number of interesting theoretical properties. 
first, it partitions the data space into a structure known as a Voronoi diagram. 
Second, it is conceptually close to nearest neighbor classification, and as such is popular in machine learning. 
Third, it can be seen as a variation of model based classification, and Lloyd's algorithm as a variation of the Expectation-maximization algorithm for this model discussed below.
The clustering model most closely related to statistics is based on distribution models. 
Clusters can then easily be defined as objects belonging most likely to the same distribution. 
A convenient property of this approach is that this closely resembles the way artificial data sets are generated: by sampling random objects from a distribution.
While the theoretical foundation of these methods is excellent, they suffer from one key problem known as overfitting, unless constraints are put on the model complexity. 
A more complex model will usually be able to explain the data better, which makes choosing the appropriate model complexity inherently difficult.
One prominent method is known as Gaussian mixture models (using the expectation-maximization algorithm). 
Here, the data set is usually modelled with a fixed (to avoid overfitting) number of Gaussian distributions that are initialized randomly and whose parameters are iteratively optimized to fit better to the data set.
 This will converge to a local optimum, so multiple runs may produce different results. 
In order to obtain a hard clustering, objects are often then assigned to the Gaussian distribution they most likely belong to; for soft clusterings, this is not necessary.
Distribution-based clustering produces complex models for clusters that can capture correlation and dependence between attributes. 
However, these algorithms put an extra burden on the user: for many real data sets, there may be no concisely defined mathematical model (e.g. assuming Gaussian distributions is a rather strong assumption on the data).
In density-based clustering,[9] clusters are defined as areas of higher density than the remainder of the data set.
 Objects in these sparse areas - that are required to separate clusters - are usually considered to be noise and border points.
The most popular[10] density based clustering method is DBSCAN.
In contrast to many newer methods, it features a well-defined cluster model called "density-reachability".
 Similar to linkage based clustering, it is based on connecting points within certain distance thresholds. 
However, it only connects points that satisfy a density criterion, in the original variant defined as a minimum number of other objects within this radius. 
A cluster consists of all density-connected objects (which can form a cluster of an arbitrary shape, in contrast to many other methods) plus all objects that are within these objects' range. 
Another interesting property of DBSCAN is that its complexity is fairly low - it requires a linear number of range queries on the database - and that it will discover essentially the same results (it is deterministic for core and noise points, but not for border points) in each run, therefore there is no need to run it multiple times. 
The key drawback of DBSCAN and OPTICS is that they expect some kind of density drop to detect cluster borders. 
Moreover, they cannot detect intrinsic cluster structures which are prevalent in the majority of real life data. 
A variation of DBSCAN, EnDBSCAN,[14] efficiently detects such kinds of structures. 
On data sets with, for example, overlapping Gaussian distributions - a common use case in artificial data - the cluster borders produced by these algorithms will often look arbitrary, because the cluster density decreases continuously. 
On a data set consisting of mixtures of Gaussians, these algorithms are nearly always outperformed by methods such as EM clustering that are able to precisely model this kind of data.
Mean-shift is a clustering approach where each object is moved to the densest area in its vicinity, based on kernel density estimation.
 Eventually, objects converge to local maxima of density. 
Similar to k-means clustering, these "density attractors" can serve as representatives for the data set, but mean-shift can detect arbitrary-shaped clusters similar to DBSCAN. 
Due to the expensive iterative procedure and density estimation, mean-shift is usually slower than DBSCAN or k-Means.
In recent years considerable effort has been put into improving the performance of existing algorithms.
 Among them are CLARANS (Ng and Han, 1994),[17] and BIRCH (Zhang et al., 1996).
With the recent need to process larger and larger data sets (also known as big data), the willingness to trade semantic meaning of the generated clusters for performance has been increasing. 
This led to the development of pre-clustering methods such as canopy clustering, which can process huge data sets efficiently, but the resulting "clusters" are merely a rough pre-partitioning of the data set to then analyze the partitions with existing slower methods such as k-means clustering. 
Various other approaches to clustering have been tried such as seed based clustering.
For high-dimensional data, many of the existing methods fail due to the curse of dimensionality, which renders particular distance functions problematic in high-dimensional spaces. 
This led to new clustering algorithms for high-dimensional data that focus on subspace clustering (where only some attributes are used, and cluster models include the relevant attributes for the cluster) and correlation clustering that also looks for arbitrary rotated ("correlated") subspace clusters that can be modeled by giving a correlation of their attributes. 
Examples for such clustering algorithms are CLIQUE[20] and SUBCLU.
Ideas from density-based clustering methods (in particular the DBSCAN/OPTICS family of algorithms) have been adopted to subspace clustering (HiSC,[22] hierarchical subspace clustering and DiSH[23]) and correlation clustering (HiCO,[24] hierarchical correlation clustering, 4C[25] using "correlation connectivity" and ERiC[26] exploring hierarchical density-based correlation clusters).
Several different clustering systems based on mutual information have been proposed. 
One is Marina Meilă's variation of information metric;[27] another provides hierarchical clustering.
 Using genetic algorithms, a wide range of different fit-functions can be optimized, including mutual information.
Also message passing algorithms, a recent development in Computer Science and Statistical Physics, has led to the creation of new types of clustering algorithms.
Evaluation of clustering results sometimes is referred to as cluster validation.
There have been several suggestions for a measure of similarity between two clusterings. 
Such a measure can be used to compare how well different data clustering algorithms perform on a set of data. 
These measures are usually tied to the type of criterion being considered in assessing the quality of a clustering method.
When a clustering result is evaluated based on the data that was clustered itself, this is called internal evaluation. 
These methods usually assign the best score to the algorithm that produces clusters with high similarity within a cluster and low similarity between clusters.
 One drawback of using internal criteria in cluster evaluation is that high scores on an internal measure do not necessarily result in effective information retrieval applications.
 Additionally, this evaluation is biased towards algorithms that use the same cluster model. 
For example, k-Means clustering naturally optimizes object distances, and a distance-based internal criterion will likely overrate the resulting clustering.
Therefore, the internal evaluation measures are best suited to get some insight into situations where one algorithm performs better than another, but this shall not imply that one algorithm produces more valid results than another.
 Validity as measured by such an index depends on the claim that this kind of structure exists in the data set. 
An algorithm designed for some kind of models has no chance if the data set contains a radically different set of models, or if the evaluation measures a radically different criterion.
 For example, k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. 
On a data set with non-convex clusters neither the use of k-means, nor of an evaluation criterion that assumes convexity, is sound.
The following methods can be used to assess the quality of clustering algorithms based on internal criterion:
In external evaluation, clustering results are evaluated based on data that was not used for clustering, such as known class labels and external benchmarks. 
Such benchmarks consist of a set of pre-classified items, and these sets are often created by human (experts). 
Thus, the benchmark sets can be thought of as a gold standard for evaluation. 
These types of evaluation methods measure how close the clustering is to the predetermined benchmark classes. 
However, it has recently been discussed whether this is adequate for real data, or only on synthetic data sets with a factual ground truth, since classes can contain internal structure, the attributes present may not allow separation of clusters or the classes may contain anomalies.
 Additionally, from a knowledge discovery point of view, the reproduction of known knowledge may not necessarily be the intended result.
 In the special scenario of Constrained clustering, where meta information (such as class labels) is used already in the clustering process, the hold-out of information for evaluation purposes is non-trivial.
A number of measures are adapted from variants used to evaluate classification tasks. 
In place of counting the number of times a class was correctly assigned to a single data point (known as true positives), such pair counting metrics assess whether each pair of data points that is truly in the same cluster is predicted to be in the same cluster.
A decision support system (DSS) is a computer-based information system that supports business or organizational decision-making activities.
 DSSs serve the management, operations, and planning levels of an organization (usually mid and higher management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance—i.e. 
Unstructured and Semi-Structured decision problems. Decision support systems can be either fully computerized, human-powered or a combination of both.
While academics have perceived DSS as a tool to support decision making process, DSS users see DSS as a tool to facilitate organizational processes.
 Some authors have extended the definition of DSS to include any system that might support decision making.
 Sprague (1980) defines DSS by its characteristics:
DSS tends to be aimed at the less well structured, underspecified problem that upper level managers typically face;
DSS attempts to combine the use of models or analytic techniques with traditional data access and retrieval functions;
DSS specifically focuses on features which make them easy to use by noncomputer people in an interactive mode; and
DSS emphasizes flexibility and adaptability to accommodate changes in the environment and the decision making approach of the user.
DSSs include knowledge-based systems. 
A properly designed DSS is an interactive software-based system intended to help decision makers compile useful information from a combination of raw data, documents, and personal knowledge, or business models to identify and solve problems and make decisions.
Typical information that a decision support application might gather and present includes:
inventories of information assets (including legacy and relational data sources, cubes, data warehouses, and data marts),
comparative sales figures between one period and the next,
projected revenue figures based on product sales assumptions.
The concept of decision support has evolved from two main areas of research: The theoretical studies of organizational decision making done at the Carnegie Institute of Technology during the late 1950s and early 1960s, and the technical work on Technology in the 1960s.
 DSS became an area of research of its own in the middle of the 1970s, before gaining in intensity during the 1980s. 
In the middle and late 1980s, executive information systems (EIS), group decision support systems (GDSS), and organizational decision support systems (ODSS) evolved from the single user and model-oriented DSS.
According to Sol (1987)[5] the definition and scope of DSS has been migrating over the years. 
In the 1970s DSS was described as "a computer-based system to aid decision making". 
In the late 1970s the DSS movement started focusing on "interactive computer-based systems which help decision-makers utilize data bases and models to solve ill-structured problems". 
In the 1980s DSS should provide systems "using suitable and available technology to improve effectiveness of managerial and professional activities", and towards the end of 1980s DSS faced a new challenge towards the design of intelligent workstations.
In 1987, Texas Instruments completed development of the Gate Assignment Display System (GADS) for United Airlines. 
This decision support system is credited with significantly reducing travel delays by aiding the management of ground operations at various airports, beginning with O'Hare International Airport in Chicago and Stapleton Airport in Denver Colorado.
 Beginning in about 1990, data warehousing and on-line analytical processing (OLAP) began broadening the realm of DSS.
 As the turn of the millennium approached, new Web-based analytical applications were introduced.
The advent of more and better reporting technologies has seen DSS start to emerge as a critical component of management design. 
Examples of this can be seen in the intense amount of discussion of DSS in the education environment.
DSS also have a weak connection to the user interface paradigm of hypertext. 
both the University of Vermont PROMIS system (for medical decision making) and the Carnegie Mellon ZOG/KMS system (for military and business decision making) were decision support systems which also were major breakthroughs in user interface research. 
Furthermore, although hypertext researchers have generally been concerned with information overload, certain researchers, notably Douglas Engelbart, have been focused on decision makers in particular.
Using the relationship with the user as the criterion, Haettenschwiler[7] differentiates passive, active, and cooperative DSS. 
A passive DSS is a system that aids the process of decision making, but that cannot bring out explicit decision suggestions or solutions. 
An active DSS can bring out such decision suggestions or solutions. 
A cooperative DSS allows the decision maker (or its advisor) to modify, complete, or refine the decision suggestions provided by the system, before sending them back to the system for validation. 
The system again improves, completes, and refines the suggestions of the decision maker and sends them back to them for validation. 
The whole process then starts again, until a consolidated solution is generated.
Another taxonomy for DSS has been created by Danie Power.
 Using the mode of assistance as the criterion, Power differentiates communication-driven DSS, data-driven DSS, document-driven DSS, knowledge-driven DSS, and model-driven DSS.
A communication-driven DSS supports more than one person working on a shared task; examples include integrated tools like Google Docs or Groove[9]
A data-driven DSS or data-oriented DSS emphasizes access to and manipulation of a time series of internal company data and, sometimes, external data.
A document-driven DSS manages, retrieves, and manipulates unstructured information in a variety of electronic formats.
A knowledge-driven DSS provides specialized problem-solving expertise stored as facts, rules, procedures, or in similar structures.
A model-driven DSS emphasizes access to and manipulation of a statistical, financial, optimization, or simulation model. 
Model-driven DSS use data and parameters provided by users to assist decision makers in analyzing a situation; they are not necessarily data-intensive. 
Dicodess is an example of an open source model-driven DSS generator.[10]
Using scope as the criterion, Power[11] differentiates enterprise-wide DSS and desktop DSS. 
An enterprise-wide DSS is linked to large data warehouses and serves many managers in the company. 
A desktop, single-user DSS is a small system that runs on an individual manager's PC.
DSS systems are not entirely different from other systems and require a structured approach. 
Such a framework includes people, technology, and the development approach.
The Early Framework of Decision Support System consists of four phases:
Intelligence Searching for conditions that call for decision.
Design Developing and analyzing possible alternative actions of solution.
Choice Selecting a course of action among those.
Implementation Adopting the selected course of action in decision situation.
DSS technology levels (of hardware and software) may include:
The actual application that will be used by the user. 
this is the part of the application that allows the decision maker to make decisions in a particular problem area. 
The user can act upon that particular problem.
Generator contains Hardware/software environment that allows people to easily develop specific DSS applications. 
This level makes use of case tools or systems such as Crystal, Analytica and iThink.
Tools include lower level hardware/software.
DSS generators including special languages, function libraries and linking modules
An iterative developmental approach allows for the DSS to be changed and redesigned at various intervals. 
Once the system is designed, it will need to be tested and revised where necessary for the desired outcome.
There are several ways to classify DSS applications. 
Not every DSS fits neatly into one of the categories, but may be a mix of two or more architectures.
Holsapple and Whinston[15] classify DSS into the following six frameworks: text-oriented DSS, database-oriented DSS, spreadsheet-oriented DSS, solver-oriented DSS, rule-oriented DSS, and compound DSS.
A compound DSS is the most popular classification for a DSS.
 It is a hybrid system that includes two or more of the five basic structures described by Holsapple and Whinston.[15]
The support given by DSS can be separated into three distinct, interrelated categories:[16] Personal Support, Group Support, and Organizational Support.
DSS components may be classified as:
Inputs: Factors, numbers, and characteristics to analyze
User Knowledge and Expertise: Inputs requiring manual analysis by the user
Outputs: Transformed data from which DSS "decisions" are generated
Decisions: Results generated by the DSS based on user criteria
DSSs which perform selected cognitive decision-making functions and are based on artificial intelligence or intelligent agents technologies are called Intelligent Decision Support Systems (IDSS)[17]
The nascent field of Decision engineering treats the decision itself as an engineered object, and applies engineering principles such as Design and Quality assurance to an explicit representation of the elements that make up a decision.
As mentioned above, there are theoretical possibilities of building such systems in any knowledge domain.
One is the clinical decision support system for medical diagnosis. 
There are four stages in the evolution of clinical decision support system (CDSS). 
The primitive version is standalone which does not support integration. 
The second generation of CDSS supports integration with other medical systems. 
The third generation is standard-based while the fourth is service model-based.[18]
Other examples include a bank loan officer verifying the credit of a loan applicant or an engineering firm that has bids on several projects and wants to know if they can be competitive with their costs.
DSS is extensively used in business and management. 
Executive dashboard and other business performance software allow faster decision making, identification of negative trends, and better allocation of business resources. 
Due to DSS all the information from any organization is represented in the form of charts, graphs i.e. in a summarized way, which helps the management to take strategic decision. 
For example, one of the DSS applications is the management and development of complex anti-terrorism systems.
A growing area of DSS application, concepts, principles, and techniques is in agricultural production, marketing for sustainable development.
 For example, the DSSAT4 package,[20][21] developed through financial support of USAID during the 80s and 90s, has allowed rapid assessment of several agricultural production systems around the world to facilitate decision-making at the farm and policy levels. 
There are, however, many constraints to the successful adoption on DSS in agriculture.
DSS are also prevalent in forest management where the long planning horizon and the spatial dimension of planning problems demands specific requirements. 
All aspects of Forest management, from log transportation, harvest scheduling to sustainability and ecosystem protection have been addressed by modern DSSs. 
In this context the consideration of single or multiple management objectives related to the provision of goods and services that traded or non-traded and often subject to resource constraints and decision problems. 
The Community of Practice of Forest Management Decision Support Systems provides a large repository on knowledge about the construction and use of forest Decision Support Systems.
A specific example concerns the Canadian National Railway system, which tests its equipment on a regular basis using a decision support system. 
A problem faced by any railroad is worn-out or defective rails, which can result in hundreds of derailments per year. 
Under a DSS, CN managed to decrease the incidence of derailments at the same time other companies were experiencing an increase.
In imaging science, image processing is processing of images using mathematical operations by using any form of signal processing for which the input is an image, a series of images, or a video, such as a photograph or video frame; the output of image processing may be either an image or a set of characteristics or parameters related to the image.
 Most image-processing techniques involve treating the image as a two-dimensional signal and applying standard signal-processing techniques to it. 
Images are also processed as three-dimensional signals where the third-dimension being time or the z-axis.
Image processing usually refers to digital image processing, but optical and analog image processing also are possible.
 This article is about general techniques that apply to all of them. The acquisition of images (producing the input image in the first place) is referred to as imaging.[2]
Closely related to image processing are computer graphics and computer vision. 
in computer graphics, images are manually made from physical models of objects, environments, and lighting, instead of being acquired (via imaging devices such as cameras) from natural scenes, as in most animated movies. 
Computer vision, on the other hand, is often considered high-level image processing out of which a machine/computer/software intends to decipher the physical contents of an image or a sequence of images (e.g., videos or 3D full-body magnetic resonance scans).
In modern sciences and technologies, images also gain much broader scopes due to the ever growing importance of scientific visualization (of often large-scale complex scientific/experimental data). 
Examples include microarray data in genetic research, or real-time multi-asset portfolio trading in finance.
Digital image processing is the use of computer algorithms to perform image processing on digital images.
 As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. 
It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and signal distortion during processing. 
Since images are defined over two dimensions (perhaps more) digital image processing may be model in the form of multidimensional systems.
The cost of processing was fairly high, however, with the computing equipment of that era.
 That changed in the 1970s, when digital image processing proliferated as cheaper computers and dedicated hardware became available. 
Images then could be processed in real time, for some dedicated problems such as television standards conversion. 
As general-purpose computers became faster, they started to take over the role of dedicated hardware for all but the most specialized and computer-intensive operations.
With the fast computers and signal processors available in the 2000s, digital image processing has become the most common form of image processing and generally, is used because it is not only the most versatile method, but also the cheapest.
Digital image processing technology for medical applications was inducted into the Space Foundation Space Technology Hall of Fame in 1994.
In 2002 Raanan Fattel introduced Gradient domain image processing, a new way to process images in which the differences between pixels are manipulated rather than the pixel values themselves.
Digital image processing allows the use of much more complex algorithms, and hence, can offer both more sophisticated performance at simple tasks, and the implementation of methods which would be impossible by analog means.
Query optimization is a function of many relational database management systems. 
The query optimizer attempts to determine the most efficient way to execute a given query by considering the possible query plans.
Generally, the query optimizer cannot be accessed directly by users: once queries are submitted to database server, and parsed by the parser, they are then passed to the query optimizer where optimization occurs.
 However, some database engines allow guiding the query optimizer with hints.
A query is a request for information from a database.
 It can be as simple as "finding the address of a person with SS# 123-45-6789," or more complex like "finding the average salary of all the employed married men in California between the ages 30 to 39, that earn less than their wives.
" Queries results are generated by accessing relevant database data and manipulating it in a way that yields the requested information. 
Since database structures are complex, in most cases, and especially for not-very-simple queries, the needed data for a query can be collected from a database by accessing it in different ways, through different data-structures, and in different orders. 
Each different way typically requires different processing time. 
Processing times of the same query may have large variance, from a fraction of a second to hours, depending on the way selected. 
The purpose of query optimization, which is an automated process, is to find the way to process a given query in minimum time.
 The large possible variance in time justifies performing query optimization, though finding the exact optimal way to execute a query, among all possibilities, is typically very complex, time consuming by itself, may be too costly, and often practically impossible. 
Thus query optimization typically tries to approximate the optimum by comparing several common-sense alternatives to provide in a reasonable time a "good enough" plan which typically does not deviate much from the best possible result.
There is a trade-off between the amount of time spent figuring out the best query plan and the quality of the choice; the optimizer may not choose the best answer on its own.
 Different qualities of database management systems have different ways of balancing these two. 
Cost-based query optimizers evaluate the resource footprint of various query plans and use this as the basis for plan selection.
 These assign an estimated "cost" to each possible query plan, and choose the plan with the smallest cost. 
Costs are used to estimate the runtime cost of evaluating the query, in terms of the number of I/O operations required, CPU path length, amount of disk buffer space, disk storage service time, and interconnect usage between units of parallelism, and other factors determined from the data dictionary. 
The set of query plans examined is formed by examining the possible access paths (e.g., primary index access, secondary index access, full file scan) and various relational table join techniques (e.g., merge join, hash join, product join). 
The search space can become quite large depending on the complexity of the SQL query. There are two types of optimization.
 These consist of logical optimization—which generates a sequence of relational algebra to solve the query—and physical optimization—which is used to determine the means of carrying out each operation.
Most query optimizers represent query plans as a tree of "plan nodes". 
a plan node encapsulates a single operation that is required to execute the query. 
The nodes are arranged as a tree, in which intermediate results flow from the bottom of the tree to the top. 
Each node has zero or more child nodes—those are nodes whose output is fed as input to the parent node. 
For example, a join node will have two child nodes, which represent the two join operands, whereas a sort node would have a single child node (the input to be sorted). 
The leaves of the tree are nodes which produce results by scanning the disk, for example by performing an index scan or a sequential scan.
The performance of a query plan is determined largely by the order in which the tables are joined. 
For example, when joining 3 tables A, B, C of size 10 rows, 10,000 rows, and 1,000,000 rows, respectively, a query plan that joins B and C first can take several orders-of-magnitude more time to execute than one that joins A and C first. 
Most query optimizers determine join order via a dynamic programming algorithm pioneered by IBM's System R database project[citation needed]. 
This algorithm works in two stages:
First, all ways to access each relation in the query are computed.
 Every relation in the query can be accessed via a sequential scan. 
If there is an index on a relation that can be used to answer a predicate in the query, an index scan can also be used. 
For each relation, the optimizer records the cheapest way to scan the relation, as well as the cheapest way to scan the relation that produces records in a particular sorted order.
The optimizer then considers combining each pair of relations for which a join condition exists.
 For each pair, the optimizer will consider the available join algorithms implemented by the DBMS.
 It will preserve the cheapest way to join each pair of relations, in addition to the cheapest way to join each pair of relations that produces its output according to a particular sort order.
Then all three-relation query plans are computed, by joining each two-relation plan produced by the previous phase with the remaining relations in the query.
Sort order can avoid a redundant sort operation later on in processing the query. 
Second, a particular sort order can speed up a subsequent join because it clusters the data in a particular way.
Query planning for nested SQL queries
A SQL query to a modern relational DBMS does more than just selections and joins. 
In particular, SQL queries often nest several layers of SPJ blocks (Select-Project-Join), by means of group by, exists, and not exists operators. 
In some cases such nested SQL queries can be flattened into a select-project-join query, but not always.
 Query plans for nested SQL queries can also be chosen using the same dynamic programming algorithm as used for join ordering, but this can lead to an enormous escalation in query optimization time. 
So some database management systems can be use an alternative rule-based approach that uses a query graph model.
One of the hardest problems in query optimization is to accurately estimate the costs of alternative query plans.
 Optimizers cost query plans using a mathematical model of query execution costs that relies heavily on estimates of the cardinality, or number of tuples, flowing through each edge in a query plan. 
Cardinality estimation in turn depends on estimates of the selection factor of predicates in the query. 
Traditionally, database systems estimate selectivities through fairly detailed statistics on the distribution of values in each column, such as histograms. 
This technique works well for estimation of selectivities of individual predicates. 
However many queries have conjunctions of predicates such as select count(*) from R where R.make='Honda' and R.model='Accord'. 
Query predicates are often highly correlated (for example, model='Accord' implies make='Honda'), and it is very hard to estimate the selectivity of the conjunct in general. 
Poor cardinality estimates and uncaught correlation are one of the main reasons why query optimizers pick poor query plans. 
This is one reason why a database administrator should regularly update the database statistics, especially after major data loads/unloads.
Classical query optimization assumes that query plans are compared according to one single cost metric, usually execution time, and that the cost of each query plan can be calculated without uncertainty. 
Both assumptions are sometimes violated in practice[1] and multiple extensions of classical query optimization have been studied in the research literature that overcome those limitations. 
Those extended problem variants differ in how they model the cost of single query plans and in terms of their optimization goal.
Classical query optimization associates each query plan with one scalar cost value.
Parametric query optimization[2] assumes that query plan cost depends on parameters whose values are unknown at optimization time.
 Such parameters can for instance represent the selectivity of query predicates that are not fully specified at optimization time but will be provided at execution time.
 Parametric query optimization therefore associates each query plan with a cost function that maps from a multi-dimensional parameter space to a one-dimensional cost space.
The goal of optimization is usually to generate all query plans that could be optimal for any of the possible parameter value combinations.
 This yields a set of relevant query plans. At run time, the best plan is selected out of that set once the true parameter values become known. 
The advantage of parametric query optimization is that optimization (which is in general a very expensive operation) is avoided at run time.
There are often other cost metrics in addition to execution time that are relevant to compare query plans [1]. 
In a cloud computing scenario for instance, one should compare query plans not only in terms of how much time they take to execute but also in terms of how much money their execution costs. 
Or in the context of approximate query optimization, it is possible to execute query plans on randomly selected samples of the input data in order to obtain approximate results with reduced execution overhead. 
In such cases, alternative query plans must be compared in terms of their execution time but also in terms of the precision or reliability of the data they generate.
Multi-objective query optimization[3] models the cost of a query plan as a cost vector where each vector component represents cost according to a different cost metric. 
Classical query optimization can be considered as a special case of multi-objective query optimization where the dimension of the cost space (i.e., the number of cost vector components) is one.
Different cost metrics might conflict with each other (e.g., there might be one plan with minimal execution time and a different plan with minimal monetary execution fees in a cloud computing scenario). 
Therefore, the goal of optimization cannot be to find a query plan that minimizes all cost metrics but must be to find a query plan that realizes the best compromise between different cost metrics. 
What the best compromise is depends on user preferences (e.g., some users might prefer a cheaper plan while others prefer a faster plan in a cloud scenario). 
Multi-objective parametric query optimization[1] generalizes parametric and multi-objective query optimization. 
Plans are compared according to multiple cost metrics and plan costs may depend on parameters whose values are unknown at optimization time. 
The cost of a query plan is therefore modeled as a function from a multi-dimensional parameter space to a multi-dimensional cost space. 
The goal of optimization is to generate the set of query plans that can be optimal for each possible combination of parameter values and user preferences.
Ontology is the philosophical study of the nature of being, becoming, existence, or reality, as well as the basic categories of being and their relations.
 Traditionally listed as a part of the major branch of philosophy known as metaphysics, ontology often deals with questions concerning what entities exist or may be said to exist, and how such entities may be grouped, related within a hierarchy, and subdivided according to similarities and differences. 
Although ontology as a philosophical enterprise is highly theoretical, it also has practical application in information science and technology, such as ontology engineering.
In analytic philosophy, ontology deals with the determination whether categories of being are fundamental and discusses in what sense the items in those categories may be said to "be".
 It is the inquiry into being in so much as it is being ("being qua being"), or into beings insofar as they exist—and not insofar as (for instance) particular facts may be obtained about them or particular properties belong to them.
Some philosophers, notably of the Platonic school, contend that all nouns (including abstract nouns) refer to existent entities. 
Other philosophers contend that nouns do not always name entities, but that some provide a kind of shorthand for reference to a collection of either objects or events. 
In this latter view, mind, instead of referring to an entity, refers to a collection of mental events experienced by a person; society refers to a collection of persons with some shared characteristics, and geometry refers to a collection of a specific kind of intellectual activity.
 Between these poles of realism and nominalism, stand a variety of other positions. 
An ontology may give an account of which words refer to entities, which do not, why, and what categories result.
Principal questions of ontology include:
"What can be said to exist?"
"What is a thing?"[2]
"Into what categories, if any, can we sort existing things?"
"What are the meanings of being?"
"What are the various modes of being of entities?"
Various philosophers have provided different answers to these questions.
 One common approach involves dividing the extant subjects and predicates into groups called categories. 
Of course, such lists of categories differ widely from one another, and it is through the co-ordination of different categorical schemes that ontology relates to such fields as library science and artificial intelligence. 
Such an understanding of ontological categories, however, is merely taxonomic, classificatory. 
Aristotle's categories are the ways in which a being may be addressed simply as a being, such as:
what it is (its 'whatness', quidditas or essence)
how it is (its 'howness' or qualitativeness)
how much it is (quantitativeness)
where it is, its relatedness to other beings[3]
Further examples of ontological questions include:[citation needed]
What is existence, i.e. what does it mean for a being to be?
Is existence a property?
Is existence a genus or general class that is simply divided up by specific differences?
Which entities, if any, are fundamental?
Are all entities objects?
How do the properties of an object relate to the object itself?
Do physical properties actually exist?
What features are the essential, as opposed to merely accidental attributes of a given object?
How many levels of existence or ontological levels are there? And what constitutes a "level"?
What is a physical object?
Can one give an account of what it means to say that a physical object exists?
Can one give an account of what it means to say that a non-physical entity exists?
What constitutes the identity of an object?
When does an object go out of existence, as opposed to merely changing?
Do beings exist other than in the modes of objectivity and subjectivity, i.e. is the subject/object split of modern philosophy inevitable?
Concepts
Essential ontological dichotomies include:
universals and particulars
substance and accident
abstract and concrete objects
essence and existence
determinism and indeterminism
monism and dualism
idealism and materialism
Types
Philosophers can classify ontologies in various ways using criteria such as the degree of abstraction and field of application:[4]
Upper ontology: concepts supporting development of an ontology, meta-ontology
Domain ontology: concepts relevant to a particular topic or area of interest, for example, information technology or computer languages, or particular branches of science
Interface ontology: concepts relevant to the juncture of two disciplines
Process ontology: inputs, outputs, constraints, sequencing information, involved in business or engineering processes
While the etymology is Greek, the oldest extant record of the word itself, the New Latin form ontologia, appeared in 1606 in the work Ogdoas Scholastica by Jacob Lorhard (Lorhardus) and in 1613 in the Lexicon philosophicum by Rudolf Göckel (Goclenius); see classical compounds for this type of word formation.
The first occurrence in English of ontology as recorded by the OED (Oxford English Dictionary, online edition, 2008) came in a work by Gideon Harvey (1636/7–1702): Archelogia philosophica nova; or, New principles of Philosophy. 
Containing Philosophy in general, Metaphysicks or Ontology, Dynamilogy or a Discourse of Power, Religio Philosophi or Natural Theology, Physicks or Natural philosophy, London, Thomson, 1663. 
The word was first used in its Latin form by philosophers based on the Latin roots, which themselves are based on the Greek.
Leibniz is the only one of the great philosophers of the 17th century to have used the term ontology.
Parmenides was among the first in the Greek tradition to propose an ontological characterization of the fundamental nature of existence.
 In his prologue or proem he describes two views of existence; initially that nothing comes from nothing, and therefore existence is eternal. 
Consequently, our opinions about truth must often be false and deceitful. 
Most of western philosophy — including the fundamental concepts of falsifiability — have emerged from this view. 
This posits that existence is what may be conceived of by thought, created, or possessed. 
Hence, there may be neither void nor vacuum; and true reality neither may come into being nor vanish from existence. 
Rather, the entirety of creation is eternal, uniform, and immutable, though not infinite (he characterized its shape as that of a perfect sphere). 
Parmenides thus posits that change, as perceived in everyday experience, is illusory. Everything that may be apprehended is but one part of a single entity. 
This idea somewhat anticipates the modern concept of an ultimate grand unification theory that finally describes all of existence in terms of one inter-related sub-atomic reality which applies to everything.
The opposite of eleatic monism is the pluralistic conception of Being. 
In the 5th century BC, Anaxagoras and Leucippus replaced [8] the reality of Being (unique and unchanging) with that of Becoming and therefore by a more fundamental and elementary ontic plurality. 
This thesis originated in the Hellenic world, stated in two different ways by Anaxagoras and by Leucippus. 
The first theory dealt with "seeds" (which Aristotle referred to as "homeomeries") of the various substances. 
The second was the atomistic theory,[9] which dealt with reality as based on the vacuum, the atoms and their intrinsic movement in it.
The materialist Atomism proposed by Leucippus was indeterminist, but then developed by Democritus in a deterministic way. 
It was later (4th century BC) that the original atomism was taken again as indeterministic by Epicurus. 
He confirmed the reality as composed of an infinity of indivisible, unchangeable corpuscles or atoms (atomon, lit. 'uncuttable'), but he gives weight to characterize atoms while for Leucippus they are characterized by a "figure", an "order" and a "position" in the cosmos.
 They are, besides, creating the whole with the intrinsic movement in the vacuum, producing the diverse flux of being. 
Their movement is influenced by the parenklisis (Lucretius names it clinamen) and that is determined by the chance. 
These ideas foreshadowed our understanding of traditional physics until the nature of atoms was discovered in the 20th century.
Plato developed this distinction between true reality and illusion, in arguing that what is real are eternal and unchanging Forms or Ideas of which things experienced in sensation are at best merely copies, and real only in so far as they copy ('partake of') such Forms. 
In general, Plato presumes that all nouns (e.g., 'Beauty') refer to real entities, whether sensible bodies or insensible Forms. 
Hence, in The Sophist Plato argues that Being is a Form in which all existent things participate and which they have in common and argues, against Parmenides, that Forms must exist not only of Being, but also of Negation and of non-Being (or Difference).
In his Categories, Aristotle identifies ten possible kinds of things that may be the subject or the predicate of a proposition. 
For Aristotle there are four different ontological dimensions:
according to the various categories or ways of addressing a being as such
according to its truth or falsity (e.g. fake gold, counterfeit money)
whether it exists in and of itself or simply 'comes along' by accident
according to its potency, movement (energy) or finished presence (Metaphysics Book Theta).
According to Avicenna, and in an interpretation of Greek Aristotelian and Platonist ontological doctrines in medieval metaphysics, being is either necessary, contingent qua possible, or impossible.
 Necessary being is that which cannot but be, since its non-being entails a contradiction.
 Contingent qua possible being is neither necessary nor impossible for it to be or not to be. 
It is ontologically neutral, and is brought from potential existing into actual existence by way of a cause that is external to its essence. 
Its being is borrowed unlike the necessary existent, which is self-subsisting and is impossible for it not to be.
 As for the impossible, it necessarily does not exist, and the affirmation of its being is a contradiction.
The concept of 'ontological formations' refers to formations of social relations understood as dominant ways of living. 
Temporal, spatial, corporeal, epistemological and performative relations are taken to be central to understanding a dominant formation. 
That is, a particular ontological formation is based on how ontological categories of time, space, embodiment, knowing and performing are lived—objectively and subjectively. 
Different ontological formations include the customary (including the tribal), the traditional, the modern and the postmodern. 
The concept was first introduced by Paul James' Globalism, Nationalism, Tribalism[12] together with a series of writers including Damian Grenfell and Manfred Steger.
In the engaged theory approach, ontological formations are seen as layered and intersecting rather than singular formations. 
They are 'formations of being'. 
This approach avoids the usual problems of a Great Divide being posited between the modern and the pre-modern.
René Descartes, with "je pense donc je suis" or "cogito ergo sum" or "I think, therefore I am", argued that "the self" is something that we can know exists with epistemological certainty. 
Descartes argued further that this knowledge could lead to a proof of the certainty of the existence of God, using the ontological argument that had been formulated first by Anselm of Canterbury.
Certainty about the existence of "the self" and "the other", however, came under increasing criticism in the 20th century. 
Sociological theorists, most notably George Herbert Mead and Erving Goffman, saw the Cartesian Other as a "Generalized Other", the imaginary audience that individuals use when thinking about the self. 
According to Mead, "we do not assume there is a self to begin with. Self is not presupposed as a stuff out of which the world arises. 
Rather, the self arises in the world".
The Cartesian Other was also used by Sigmund Freud, who saw the superego as an abstract regulatory force, and Émile Durkheim who viewed this as a psychologically manifested entity which represented God in society at large.
Schools of subjectivism, objectivism and relativism existed at various times in the 20th century, and the postmodernists and body philosophers tried to reframe all these questions in terms of bodies taking some specific action in an environment. 
This relied to a great degree on insights derived from scientific research into animals taking instinctive action in natural and artificial settings—as studied by biology, ecology,[15] and cognitive science.
The processes by which bodies related to environments became of great concern, and the idea of being itself became difficult to really define.
 What did people mean when they said "A is B", "A must be B", "A was B"...? 
Some linguists advocated dropping the verb "to be" from the English language, leaving "E Prime", supposedly less prone to bad abstractions.
 Others, mostly philosophers, tried to dig into the word and its usage. Heidegger distinguished human being as existence from the being of things in the world. 
Heidegger proposes that our way of being human and the way the world is for us are cast historically through a fundamental ontological questioning. 
These fundamental ontological categories provide the basis for communication in an age: a horizon of unspoken and seemingly unquestionable background meanings, such as human beings understood unquestioningly as subjects and other entities understood unquestioningly as objects. 
Because these basic ontological meanings both generate and are regenerated in everyday interactions, the locus of our way of being in a historical epoch is the communicative event of language in use.
For Heidegger, however, communication in the first place is not among human beings, but language itself shapes up in response to questioning (the inexhaustible meaning of) being.
 Even the focus of traditional ontology on the 'whatness' or 'quidditas' of beings in their substantial, standing presence can be shifted to pose the question of the 'whoness' of human being itself.[17]
Some philosophers suggest that the question of "What is?" is (at least in part) an issue of usage rather than a question about facts.
 This perspective is conveyed by an analogy made by Donald Davidson: Suppose a person refers to a 'cup' as a 'chair' and makes some comments pertinent to a cup, but uses the word 'chair' consistently throughout instead of 'cup'.
 One might readily catch on that this person simply calls a 'cup' a 'chair' and the oddity is explained.
Analogously, if we find people asserting 'there are' such-and-such, and we do not ourselves think that 'such-and-such' exist, we might conclude that these people are not nuts (Davidson calls this assumption 'charity'), they simply use 'there are' differently than we do. 
The question of What is? is at least partially a topic in the philosophy of language, and is not entirely about ontology itself.
 This viewpoint has been expressed by Eli Hirsch.
Hirsch interprets Hilary Putnam as asserting that different concepts of "the existence of something" can be correct. 
This position does not contradict the view that some things do exist, but points out that different 'languages' will have different rules about assigning this property.
 How to determine the 'fitness' of a 'language' to the world then becomes a subject for investigation.
In human geography there are two types of ontology: small "o" which accounts for the practical orientation, describing functions of being a part of the group, thought to oversimplify and ignore key activities. 
The other "o", or big "O", systematically, logically, and rationally describes the essential characteristics and universal traits. This concept relates closely to Plato's view that the human mind can only perceive a bigger world if they continue to live within the confines of their "caves". 
However, in spite of the differences, ontology relies on the symbolic agreements among members. That said, ontology is crucial for the axiomatic language frameworks.
According to A.N. Whitehead, for ontology, it is useful to distinguish the terms 'reality' and 'actuality'.
In this view, an 'actual entity' has a philosophical status of fundamental ontological priority, while a 'real entity' is one which may be actual, or may derive its reality from its logical relation to some actual entity or entities. 
For example, an occasion in the life of Socrates is an actual entity. 
But Socrates' being a man does not make 'man' an actual entity, because it refers indeterminately to many actual entities, such as several occasions in the life of Socrates, and also to several occasions in the lives of Alcibiades, and of others.
 But the notion of man is real; it derives its reality from its reference to those many actual occasions, each of which is an actual entity. 
An actual occasion is a concrete entity, while terms such as 'man' are abstractions from many concrete relevant entities.
According to Whitehead, an actual entity must earn its philosophical status of fundamental ontological priority by satisfying several philosophical criteria, as follows.
There is no going behind an actual entity, to find something more fundamental in fact or in efficacy. 
This criterion is to be regarded as expressing an axiom, or postulated distinguished doctrine.
An actual entity must be completely determinate in the sense that there may be no confusion about its identity that would allow it to be confounded with another actual entity. 
In this sense an actual entity is completely concrete, with no potential to be something other than itself. 
It is what it is. 
It is of course a source of potentiality for the creation of other actual entities, of which it may be said to be a part cause. 
Likewise it is the concretion or realization of potentialities of other actual entities which are its partial causes.
Causation between actual entities is essential to their actuality. 
Consequently, for Whitehead, each actual entity has its distinct and definite extension in physical Minkowski space, and so is uniquely identifiable. 
A description in Minkowski space supports descriptions in time and space for particular observers.
It is part of the aim of the philosophy of such an ontology as Whitehead's that the actual entities should be all alike, qua actual entities; they should all satisfy a single definite set of well stated ontological criteria of actuality.
Whitehead proposed that his notion of an occasion of experience satisfies the criteria for its status as the philosophically preferred definition of an actual entity. 
From a purely logical point of view, each occasion of experience has in full measure the characters of both objective and subjective reality. 
Subjectivity and objectivity refer to different aspects of an occasion of experience, and in no way do they exclude each other.
Examples of other philosophical proposals or candidates as actual entities, in this view, are Aristotle's 'substances', Leibniz' monads, and Descartes ′res verae' , and the more modern 'states of affairs'.
 Aristotle's substances, such as Socrates, have behind them as more fundamental the 'primary substances', and in this sense do not satisfy Whitehead's criteria. 
Whitehead is not happy with Leibniz' monads as actual entities because they are "windowless" and do not cause each other.
 'States of affairs' are often not closely defined, often without specific mention of extension in physical Minkowski space; they are therefore not necessarily processes of becoming, but may be as their name suggests, simply static states in some sense. 
States of affairs are contingent on particulars, and therefore have something behind them.[26] One summary of the Whiteheadian actual entity is that it is a process of becoming. 
Another summary, referring to its causal linkage to other actual entities, is that it is "all window", in contrast with Leibniz' windowless monads.
This view allows philosophical entities other than actual entities to really exist, but not as fundamentally and primarily factual or causally efficacious; they have existence as abstractions, with reality only derived from their reference to actual entities. 
A Whiteheadian actual entity has a unique and completely definite place and time. 
Whiteheadian abstractions are not so tightly defined in time and place, and in the extreme, some are timeless and placeless, or 'eternal' entities. 
All abstractions have logical or conceptual rather than efficacious existence; their lack of definite time does not make them unreal if they refer to actual entities. 
Whitehead calls this 'the ontological principle'.
There is an established and long philosophical history of the concept of atoms as microscopic physical objects.
They are far too small to be visible to the naked eye. 
It was as recent as the nineteenth century that precise estimates of the sizes of putative physical atoms began to become plausible. 
Almost direct empirical observation of atomic effects was due to the theoretical investigation of Brownian motion by Albert Einstein in the very early twentieth century. 
But even then, the real existence of atoms was debated by some. Such debate might be labeled 'microcosmic ontology'. 
Here the word 'microcosm' is used to indicate a physical world of small entities, such as for example atoms.
Subatomic particles are usually considered to be much smaller than atoms. 
Their real or actual existence may be very difficult to demonstrate empirically.
 A distinction is sometimes drawn between actual and virtual subatomic particles. 
In computer science and information science, an ontology is a formal naming and definition of the types, properties, and interrelationships of the entities that really or fundamentally exist for a particular domain of discourse. 
It is thus a practical application of philosophical ontology, with a taxonomy.
An ontology compartmentalizes the variables needed for some set of computations and establishes the relationships between them.
The fields of artificial intelligence, the Semantic Web, systems engineering, software engineering, biomedical informatics, library science, enterprise bookmarking, and information architecture all create ontologies to limit complexity and to organize information. 
The ontology can then be applied to problem solving.
The term ontology has its origin in philosophy and has been applied in many different ways. 
The word element onto- comes from the Greek ὤν, ὄντος, ("being", "that which is"), present participle of the verb εἰμί ("be"). 
The core meaning within computer science is a model for describing the world that consists of a set of types, properties, and relationship types. 
There is also generally an expectation that the features of the model in an ontology should closely resemble the real world (related to the object).
What many ontologies have in common in both computer science and in philosophy is the representation of entities, ideas, and events, along with their properties and relations, according to a system of categories. 
In both fields, there is considerable work on problems of ontological relativity (e.g., Quine and Kripke in philosophy, Sowa and Guarino in computer science),[4] and debates concerning whether a normative ontology is viable (e.g., debates over foundationalism in philosophy, and over the Cyc project in AI).
 Differences between the two are largely matters of focus. 
Computer scientists are more concerned with establishing fixed, controlled vocabularies, while philosophers are more concerned with first principles, such as whether there are such things as fixed essences or whether enduring objects must be ontologically more primary than processes.
Other fields make ontological assumptions that are sometimes explicitly elaborated and explored. 
For instance, the definition and ontology of economics (also sometimes called the political economy) is hotly debated especially in Marxist economics[5] where it is a primary concern, but also in other subfields.
 Such concerns intersect with those of information science when a simulation or model is intended to enable decisions in the economic realm; for example, to determine what capital assets are at risk and if so by how much (see risk management). 
Some claim all social sciences have explicit ontology issues because they do not have hard falsifiability criteria like most models in physical sciences and that indeed the lack of such widely accepted hard falsification criteria is what defines a social or soft science.
Historically, ontologies arise out of the branch of philosophy known as metaphysics, which deals with the nature of reality – of what exists. 
This fundamental branch is concerned with analyzing various types or modes of existence, often with special attention to the relations between particulars and universals, between intrinsic and extrinsic properties, and between essence and existence. 
The traditional goal of ontological inquiry in particular is to divide the world "at its joints" to discover those fundamental categories or kinds into which the world’s objects naturally fall.
During the second half of the 20th century, philosophers extensively debated the possible methods or approaches to building ontologies without actually building any very elaborate ontologies themselves.
By contrast, computer scientists were building some large and robust ontologies, such as WordNet and Cyc, with comparatively little debate over how they were built.
Since the mid-1970s, researchers in the field of artificial intelligence (AI) have recognized that capturing knowledge is the key to building large and powerful AI systems. 
AI researchers argued that they could create new ontologies as computational models that enable certain kinds of automated reasoning. 
In the 1980s, the AI community began to use the term ontology to refer to both a theory of a modeled world and a component of knowledge systems. 
Some researchers, drawing inspiration from philosophical ontologies, viewed computational ontology as a kind of applied philosophy.
In the early 1990s, the widely cited Web page and paper "Toward Principles for the Design of Ontologies Used for Knowledge Sharing" by Tom Gruber[9] is credited with a deliberate definition of ontology as a technical term in computer science. 
Gruber introduced the term to mean a specification of a conceptualization:
An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents.
 This definition is consistent with the usage of ontology as set of concept definitions, but more general. And it is a different sense of the word than its use in philosophy.
Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but ontologies need not be limited to these forms. 
Ontologies are also not limited to conservative definitions — that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world.
 To specify a conceptualization, one needs to state axioms that do constrain the possible interpretations for the defined terms.
Contemporary ontologies share many structural similarities, regardless of the language in which they are expressed. 
As mentioned above, most ontologies describe individuals (instances), classes (concepts), attributes, and relations. In this section each of these components is discussed in turn.
Individuals: instances or objects (the basic or "ground level" objects)
Classes: sets, collections, concepts, classes in programming, types of objects, or kinds of things
Attributes: aspects, properties, features, characteristics, or parameters that objects (and classes) can have
Relations: ways in which classes and individuals can be related to one another
Function terms: complex structures formed from certain relations that can be used in place of an individual term in a statement
Restrictions: formally stated descriptions of what must be true in order for some assertion to be accepted as input
Rules: statements in the form of an if-then (antecedent-consequent) sentence that describe the logical inferences that can be drawn from an assertion in a particular form
Axioms: assertions (including rules) in a logical form that together comprise the overall theory that the ontology describes in its domain of application. 
This definition differs from that of "axioms" in generative grammar and formal logic. 
In those disciplines, axioms include only statements asserted as a priori knowledge. 
As used here, "axioms" also include the theory derived from axiomatic statements
Events: the changing of attributes or relations
Ontologies are commonly encoded using ontology languages.
A domain ontology (or domain-specific ontology) represents concepts which belong to part of the world. 
Particular meanings of terms applied to that domain are provided by domain ontology. 
For example, the word card has many different meanings. 
An ontology about the domain of poker would model the "playing card" meaning of the word, while an ontology about the domain of computer hardware would model the "punched card" and "video card" meanings.
Since domain ontologies represent concepts in very specific and often eclectic ways, they are often incompatible. 
As systems that rely on domain ontologies expand, they often need to merge domain ontologies into a more general representation. 
This presents a challenge to the ontology designer.
 Different ontologies in the same domain arise due to different languages, different intended usage of the ontologies, and different perceptions of the domain (based on cultural background, education, ideology, etc.).
At present, merging ontologies that are not developed from a common foundation ontology is a largely manual process and therefore time-consuming and expensive. 
Domain ontologies that use the same foundation ontology to provide a set of basic elements with which to specify the meanings of the domain ontology elements can be merged automatically. 
There are studies on generalized techniques for merging ontologies,[12] but this area of research is still largely theoretical.
An upper ontology (or foundation ontology) is a model of the common objects that are generally applicable across a wide range of domain ontologies. 
It usually employs a core glossary that contains the terms and associated object descriptions as they are used in various relevant domain sets.
There are several standardized upper ontologies available for use, including BFO, BORO method, Dublin Core, GFO, OpenCyc/ResearchCyc, SUMO, the Unified Foundational Ontology (UFO),[13] and DOLCE.
 WordNet, while considered an upper ontology by some, is not strictly an ontology. 
However, it has been employed as a linguistic tool for learning domain ontologies.
The Gellish ontology is an example of a combination of an upper and a domain ontology.
A survey of ontology visualization techniques is presented by Katifori et al.
 An evaluation of two most established ontology visualization techniques: indented tree and graph is discussed in.
 A visual language for ontologies represented in OWL is specified by the Visual Notation for OWL Ontologies (VOWL).
Ontology engineering (or ontology building) is a subfield of knowledge engineering. 
It studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tool suites and languages that support them.
Ontology engineering aims to make explicit the knowledge contained within software applications, and within enterprises and business procedures for a particular domain. 
Ontology engineering offers a direction towards solving the interoperability problems brought about by semantic obstacles, such as the obstacles related to the definitions of business terms and software classes. 
Ontology engineering is a set of tasks related to the development of ontologies for a particular domain.
Ontology editors are applications designed to assist in the creation or manipulation of ontologies. 
They often express ontologies in one of many ontology languages. 
Some provide export to other ontology languages however.
Among the most relevant criteria for choosing an ontology editor are the degree to which the editor abstracts from the actual ontology representation language used for persistence and the visual navigation possibilities within the knowledge model. 
Next come built-in inference engines and information extraction facilities, and the support of meta-ontologies such as OWL-S, Dublin Core, etc. 
Another important feature is the ability to import & export foreign knowledge representation languages for ontology matching. 
Ontologies are developed for a specific purpose and application.
a.k.a. software (Ontology, taxonomy and thesaurus management software available from The Synercon Group)
Anzo for Excel (Includes an RDFS and OWL ontology editor within Excel; generates ontologies from Excel spreadsheets)
Chimaera (Other web service by Stanford)
CmapTools Ontology Editor (COE) (Java based ontology editor from the Florida Institute for Human and Machine Cognition. Supports numerous formats)
dot15926 Editor (Open source ontology editor for data compliant to engineering ontology standard ISO 15926. Allows Python scripting and pattern-based data analysis. 
EMFText OWL2 Manchester Editor, Eclipse-based, open-source, Pellet integration
Enterprise Architect, along with UML modeling, supports OMG's Ontology Definition MetaModel which includes OWL and RDF.
Fluent Editor, a comprehensive ontology editor for OWL and SWRL with Controlled Natural Language (Controlled English). 
Supports OWL, RDF, DL and Functional rendering, unlimited imports and built-in reasoning services.
HOZO (Java-based graphical editor especially created to produce heavy-weight and well thought out ontologies, from Osaka University and Enegate Co, ltd.)
KAON (single user and server based solutions possible, open source, from FZI/AIFB Karlsruhe)
KMgen (Ontology editor for the KM language. KM: The Knowledge Machine)
Knoodl (Free web application/service that is an ontology editor, wiki, and ontology registry. 
Supports creation of communities where members can collaboratively import, create, discuss, document and publish ontologies. 
Supports OWL, RDF, RDFS, and SPARQL queries. 
Available since early Nov 2006 from Revelytix, Inc..)
Model Futures IDEAS AddIn (free) A plug-in for Sparx Systems Enterprise Architect that allows IDEAS Group 4D ontologies to be developed using a UML profile
Model Futures OWL Editor (Free) Able to work with very large OWL files (e.g. Cyc) and has extensive import and export capabilities (inc. UML, Thesaurus Descriptor, MS Word, CA ERwin Data Modeler, CSV, etc.)
myWeb (Java-based, mySQL connection, bundled with applet that allows online browsing of ontologies (including OBO))
Neologism (Web-based, open source, supports RDFS and a subset of OWL, built on Drupal)
NeOn Toolkit (Eclipse-based, open source, OWL support, several import mechanisms, support for reuse and management of networked ontologies, visualization, etc.…from NeOn Project)
OBO-Edit (Java-based, downloadable, open source, developed by the Gene Ontology Consortium for editing biological ontologies)
OntoStudio (Eclipse-based, downloadable, support for RDF(S), OWL and F-Logic, graphical rule editor, visualizations, from ontoprise)
Ontolingua (Web service offered by Stanford University)
Open Semantic Framework (OSF), an integrated software stack using semantic technologies for knowledge management, which includes an ontology editor
OWLGrEd (A graphical ontology editor, easy-to-use)
PoolParty Thesaurus Server (Commercial ontology, taxonomy and thesaurus management software available from Semantic Web Company, fully based on standards like RDFS, SKOS and SPARQL, integrated with Virtuoso Universal Server)
Protégé (Java-based, downloadable, Supports OWL, open source, many sample ontologies, from Stanford University)
ScholOnto (net-centric representations of research)
Semantic Turkey (Firefox extension - also based on Java - for managing ontologies and acquiring new knowledge from the Web; developed at University of Rome, Tor Vergata )
Sigma knowledge engineering environment is a system primarily for development of the Suggested Upper Merged Ontology
Swoop (Java-based, downloadable, open source, OWL Ontology browser and editor from the University of Maryland)
Semaphore Ontology Manager (Commercial ontology, taxonomy and thesaurus management software available from Smartlogic Semaphore Limited. 
Intuitive tool to manage the entire "build - enhance - review - maintain" ontology lifecycle.)
Synaptica (Ontology, taxonomy and thesaurus management software available from Synaptica, LLC. Web based, supports OWL and SKOS.)
TopBraid Composer (Eclipse-based, downloadable, full support for RDFS and OWL, built-in inference engine, SWRL editor and SPARQL queries, visualization, import of XML and UML, from TopQuadrant)
Transinsight (The editor is especially designed for creating text mining ontologies and part of GoPubMed.org)
WebODE (Web service offered by the Technical University of Madrid)
TwoUse Toolkit (Eclipse-based, open source, model-driven ontology editing environment especially designed for software engineers)
Be Informed Suite (Commercial tool for building large ontology based applications. 
Includes visual editors, inference engines, export to standard formats)
Thesaurus Master (Manages creation and use of ontologies for use in data management and semantic enrichment by enterprise, government, and scholarly publishers.)
TODE (A Dot Net based Tool for Ontology Development and Editing)
VocBench (Collaborative Web Application for SKOS/SKOS-XL Thesauri Management - developed on a joint effort between University of Rome, Tor Vergata and the Food and Agriculture Organization of the United Nations: FAO )
OBIS (Web based user interface that allows to input ontology instances in a user friendly way that can be accessed via SPARQL endpoint)
Ontology learning is the automatic or semi-automatic creation of ontologies, including extracting a domain's terms from natural language text.
 As building ontologies manually is extremely labor-intensive and time consuming, there is great motivation to automate the process. 
Information extraction and text mining methods have been explored to automatically link ontologies to documents, e.g. in the context of the BioCreative challenges.
There is a proposal that portions of this section be split into a new article titled Ontology language. (Discuss) (October 2013)
An ontology language is a formal language used to encode the ontology.
 There are a number of such languages for ontologies, both proprietary and standards-based:
Common Algebraic Specification Language is a general logic-based specification language developed within the IFIP working group 1.3 "Foundations of System Specifications" and functions as a de facto standard in the area of software specifications. 
It is now being applied to ontology specifications in order to provide modularity and structuring mechanisms.
Common logic is ISO standard 24707, a specification for a family of ontology languages that can be accurately translated into each other.
The Cyc project has its own ontology language called CycL, based on first-order predicate calculus with some higher-order extensions.
DOGMA (Developing Ontology-Grounded Methods and Applications) adopts the fact-oriented modeling approach to provide a higher level of semantic stability.
The Gellish language includes rules for its own extension and thus integrates an ontology with an ontology language.
IDEF5 is a software engineering method to develop and maintain usable, accurate, domain ontologies.
KIF is a syntax for first-order logic that is based on S-expressions. SUO-KIF is a derivative version supporting the Suggested Upper Merged Ontology.
MOF and UML are standards of the OMG
Olog is a category theoretic approach to ontologies, emphasizing translations between ontologies using functors.
OBO, a language used for biological and biomedical ontologies.
OntoUML is an ontologically well-founded profile of UML for conceptual modeling of domain ontologies.
OWL is a language for making ontological statements, developed as a follow-on from RDF and RDFS, as well as earlier ontology language projects including OIL, DAML, and DAML+OIL. 
OWL is intended to be used over the World Wide Web, and all its elements (classes, properties and individuals) are defined as RDF resources, and identified by URIs.
Rule Interchange Format (RIF) and F-Logic combine ontologies and rules.
Semantic Application Design Language (SADL)[23] captures a subset of the expressiveness of OWL, using an English-like language entered via an Eclipse Plug-in.
SBVR (Semantics of Business Vocabularies and Rules) is an OMG standard adopted in industry to build ontologies.
TOVE Project, TOronto Virtual Enterprise project
AURUM - Information Security Ontology,[24] An ontology for information security knowledge sharing, enabling users to collaboratively understand and extend the domain knowledge body. 
It may serve as a basis for automated information security risk and compliance management.
BabelNet, a very large multilingual semantic network and ontology, lexicalized in many languages
Basic Formal Ontology,[25] a formal upper ontology designed to support scientific research
BioPAX,[26] an ontology for the exchange and interoperability of biological pathway (cellular processes) data
BMO,[27] an e-Business Model Ontology based on a review of enterprise ontologies and business model literature
CCO and GexKB,[28] Application Ontologies (APO) that integrate diverse types of knowledge with the Cell Cycle Ontology (CCO) and the Gene Expression Knowledge Base (GexKB)
CContology (Customer Complaint Ontology),[29] an e-business ontology to support online customer complaint management
CIDOC Conceptual Reference Model, an ontology for cultural heritage[30]
COSMO,[31] a Foundation Ontology (current version in OWL) that is designed to contain representations of all of the primitive concepts needed to logically specify the meanings of any domain entity. 
It is intended to serve as a basic ontology that can be used to translate among the representations in other ontologies or databases. 
It started as a merger of the basic elements of the OpenCyc and SUMO ontologies, and has been supplemented with other ontology elements (types, relations) so as to include representations of all of the words in the Longman dictionary defining vocabulary.
Cyc, a large Foundation Ontology for formal representation of the universe of discourse
Disease Ontology,[32] designed to facilitate the mapping of diseases and associated conditions to particular medical codes
DOLCE, a Descriptive Ontology for Linguistic and Cognitive Engineering[14][15]
Dublin Core, a simple ontology for documents and publishing
Foundational, Core and Linguistic Ontologies[33]
Foundational Model of Anatomy,[34] an ontology for human anatomy
Friend of a Friend, an ontology for describing persons, their activities and their relations to other people and objects
Gene Ontology for genomics
Gellish English dictionary, an ontology that includes a dictionary and taxonomy that includes an upper ontology and a lower ontology that focusses on industrial and business applications in engineering, technology and procurement. 
See also Gellish as Open Source project on SourceForge.
Geopolitical ontology, an ontology describing geopolitical information created by Food and Agriculture Organization(FAO). 
The geopolitical ontology includes names in multiple languages (English, French, Spanish, Arabic, Chinese, Russian and Italian); maps standard coding systems (UN, ISO, FAOSTAT, AGROVOC, etc.); provides relations among territories (land borders, group membership, etc.); and tracks historical changes. 
In addition, FAO provides web services <http://www.fao.org/countryprofiles/webservices.asp?lang=en> of geopolitical ontology and a module maker <http://www.fao.org/countryprofiles/geoinfo/modulemaker/index.html> to download modules of the geopolitical ontology into different formats (RDF, XML, and EXCEL). 
See more information on the FAO Country Profiles geopolitical ontology web page <http://www.fao.org/countryprofiles/geoinfo.asp?lang=en>.
GOLD,[35] General Ontology for Linguistic Description
GUM (Generalized Upper Model),[36] a linguistically motivated ontology for mediating between clients systems and natural language technology
IDEAS Group,[37] a formal ontology for enterprise architecture being developed by the Australian, Canadian, UK and U.S. Defence Depts.
Linkbase,[38] a formal representation of the biomedical domain, founded upon Basic Formal Ontology.
LPL, Lawson Pattern Language
NCBO Bioportal,[39] biological and biomedical ontologies and associated tools to search, browse and visualise
NIFSTD Ontologies from the Neuroscience Information Framework: a modular set of ontologies for the neuroscience domain. See http://neuinfo.org
OBO-Edit,[40] an ontology browser for most of the Open Biological and Biomedical Ontologies
OBO Foundry,[41] a suite of interoperable reference ontologies in biology and biomedicine
OMNIBUS Ontology,[42] an ontology of learning, instruction, and instructional design
Ontology for Biomedical Investigations, an open access, integrated ontology for the description of biological and clinical investigations
ONSTR,[43] Ontology for Newborn Screening Follow-up and Translational Research [4], Newborn Screening Follow-up Data Integration Collaborative, Emory University, Atlanta, GA. 
See also https://nbsdc.org/projectmission.php
Plant Ontology[44] for plant structures and growth/development stages, etc.
POPE, Purdue Ontology for Pharmaceutical Engineering
PRO,[45] the Protein Ontology of the Protein Information Resource, Georgetown University
Program abstraction taxonomy program abstraction taxonomy
Protein Ontology[46] for proteomics
RXNO Ontology, for name reactions in chemistry
SNOMED CT (Systematized Nomenclature of Medicine -- Clinical Terms)
Suggested Upper Merged Ontology, a formal upper ontology
Systems Biology Ontology (SBO), for computational models in biology
SWEET,[47] Semantic Web for Earth and Environmental Terminology
ThoughtTreasure ontology
TIME-ITEM, Topics for Indexing Medical Education
Uberon,[48] representing animal anatomical structures
UMBEL, a lightweight reference structure of 20,000 subject concept classes and their relationships derived from OpenCyc
WordNet, a lexical reference system
YAMATO,[49] Yet Another More Advanced Top-level Ontology
The W3C Linking Open Data community project coordinates attempts to converge different ontologies into worldwide Semantic Web.
The development of ontologies for the Web has led to the emergence of services providing lists or directories of ontologies with search facility.
 Such directories have been called ontology libraries.
The following are libraries of human-selected ontologies.
COLORE[50] is an open repository of first-order ontologies in Common Logic with formal links between ontologies in the repository.
DAML Ontology Library[51] maintains a legacy of ontologies in DAML.
Ontology Design Patterns portal[52] is a wiki repository of reusable components and practices for ontology design, and also maintains a list of exemplary ontologies. 
Started within the NeOn EU project.
Protégé Ontology Library[53] contains a set of OWL, Frame-based and other format ontologies.
SchemaWeb[54] is a directory of RDF schemata expressed in RDFS, OWL and DAML+OIL.
The following are both directories and search engines. They include crawlers searching the Web for well-formed ontologies.
OBO Foundry is a suite of interoperable reference ontologies in biology and biomedicine.[55][56]
Bioportal (ontology repository of NCBO)
OntoSelect[57] Ontology Library offers similar services for RDF/S, DAML and OWL ontologies.
Ontaria[58] is a "searchable and browsable directory of semantic web data" with a focus on RDF vocabularies with OWL ontologies.
(NB Project "on hold" since 2004).
Swoogle is a directory and search engine for all RDF resources available on the Web, including ontologies.
OOR - the Open Ontology Repository initiative - http://oor.net
ROMULUS is a foundational ontology repository aimed at improving semantic interoperability. 
Currently there are three foundational ontologies in the repository: DOLCE, BFO and GFO.
In general, ontologies can be used beneficially in enterprise applications.
A more concrete example is SAPPHIRE (Health care) or Situational Awareness and Preparedness for Public Health Incidences and Reasoning Engines which is a semantics-based health information system capable of tracking and evaluating situations and occurrences that may affect public health.
geographic information systems bring together data from different sources and benefit therefore from ontological metadata which helps to connect the semantics of the data.
He writes 'before one is able to answer the question 'what is an ontology?', one must provide first an answer to the question 'what does the word ontology mean?
Ontology engineering in computer science and information science is a field which studies the methods and methodologies for building ontologies: formal representations of a set of concepts within a domain and the relationships between those concepts.
 A large-scale representation of abstract concepts such as actions, time, physical objects and beliefs would be an example of ontological engineering.
 Ontology engineering is one of the areas of applied ontology, and can be seen as an application of philosophical ontology. 
Core ideas and objectives of ontology engineering are also central in conceptual modeling.
[Ontology engineering] aims at making explicit the knowledge contained within software applications, and within enterprises and business procedures for a particular domain. 
Ontology engineering offers a direction towards solving the inter-operability problems brought about by semantic obstacles, i.e. the obstacles related to the definitions of business terms and software classes. 
Ontology engineering is a set of tasks related to the development of ontologies for a particular domain.
Line Pouchard, Nenad Ivezic and Craig Schlenoff, Ontology Engineering for Distributed Collaboration in Manufacturing[3]
During the last decade, increasing attention has been focused on ontologies. 
Ontological engineering is a new field of study concerning the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies,[4][5] and the tool suites and languages that support them.
Further information: ontology language
An ontology language is a formal language used to encode the ontology. 
There are a number of such languages for ontologies, both proprietary and standards-based:
Common logic is ISO standard 24707, a specification for a family of ontology languages that can be accurately translated into each other.
The Cyc project has its own ontology language called CycL, based on first-order predicate calculus with some higher-order extensions.
The Gellish language includes rules for its own extension and thus integrates an ontology with an ontology language.
IDEF5 is a software engineering method to develop and maintain usable, accurate, domain ontologies.
KIF is a syntax for first-order logic that is based on S-expressions.
Rule Interchange Format (RIF) and F-Logic combine ontologies and rules.
OWL is a language for making ontological statements, developed as a follow-on from RDF and RDFS, as well as earlier ontology language projects including OIL, DAML and DAML+OIL. 
OWL is intended to be used over the World Wide Web, and all its elements (classes, properties and individuals) are defined as RDF resources, and identified by URIs.
XBRL (Extensible Business Reporting Language) is a syntax for expressing business semantics.
OntoUML is a well-founded language for specifying reference ontologies.
Ontology engineering in life sciences
Life sciences is flourishing with ontologies that biologists use to make sense of their experiments.
 For inferring correct conclusions from experiments, ontologies have to be structured optimally against the knowledge base they represent. 
The structure of an ontology needs to be changed continuously so that it is an accurate representation of the underlying domain.
Recently, an automated method was introduced for engineering ontologies in life sciences such as Gene Ontology (GO),[7] one of the most successful and widely used biomedical ontology.
 Based on information theory, it restructures ontologies so that the levels represent the desired specificity of the concepts. 
Similar information theoretic approaches have also been used for optimal partition of Gene Ontology.
 Given the mathematical nature of such engineering algorithms, these optimizations can be automated to produce a principled and scalable architecture to restructure ontologies such as GO.
Open Biomedical Ontologies (OBO), a 2006 initiative of the U.S. National Center for Biomedical Ontology, that provides a common 'foundry' for various ontology initiatives, amongst which are:
The Generic Model Organism Project (GMOD)
Gene Ontology Consortium
Sequence Ontology
Ontology Lookup Service
The Plant Ontology Consortium
Standards and Ontologies for Functional Genomics
Applied ontology involves the practical application of ontological resources to specific domains, such as biomedicine or geography. 
Much work in applied ontology is carried out within the framework of the semantic web.
 See foundation ontology and ontology (computer science).
The challenge of applying ontology is ontology's emphasis on a world view orthogonal to epistemology.
The emphasis is on being rather than on doing (as implied by "applied") or on knowing.
One way in which that emphasis plays out[citation needed] is in the concept of "speech acts": acts of promising, ordering, apologizing, requesting, inviting or sharing. 
The study of these acts from an ontological perspective is one of the driving forces behind relationship-oriented applied ontology.[1] 
This can involve concepts[which?] championed by ordinary language philosophers[citation needed] like Wittgenstein.
Applying ontology can also involve looking at the relationship between a person's world and that person's actions. 
The context or clearing is highly influenced by the being of the subject or the field of being itself.[original research?] 
This view is highly influenced by the philosophy of phenomenology,[2] the works of Martin Heidegger, and many[which?] others
Semantics (from Ancient Greek: σημαντικός sēmantikós, "significant")[1][2] is the study of meaning. 
It focuses on the relation between signifiers, like words, phrases, signs, and symbols, and what they stand for; their denotation. 
Linguistic semantics is the study of meaning that is used for understanding human expression through language. 
Other forms of semantics include the semantics of programming languages, formal logics, and semiotics. 
In international scientific vocabulary semantics is also called semasiology.
The word semantics was first used by Michel Bréal a French philologist,[3] itself denotes a range of ideas—from the popular to the highly technical. 
It is often used in ordinary language for denoting a problem of understanding that comes down to word selection or connotation. 
This problem of understanding has been the subject of many formal enquiries, over a long period of time, especially in the field of formal semantics. 
In linguistics, it is the study of the interpretation of signs or symbols used in agents or communities within particular circumstances and contexts.
 Within this view, sounds, facial expressions, body language, and proxemics have semantic (meaningful) content, and each comprises several branches of study. 
In written language, things like paragraph structure and punctuation bear semantic content; other forms of language bear other semantic content.[4]
The formal study of semantics intersects with many other fields of inquiry, including lexicology, syntax, pragmatics, etymology and others. 
Independently, semantics is also a well-defined field in its own right, often with synthetic properties.[5] 
In the philosophy of language, semantics and reference are closely connected. 
Further related fields include philology, communication, and semiotics. 
The formal study of semantics can therefore be manifold and complex.
Semantics contrasts with syntax, the study of the combinatorics of units of a language (without reference to their meaning), and pragmatics, the study of the relationships between the symbols of a language, their meaning, and the users of the language.
Semantics as a field of study also has significant ties to various representational theories of meaning including truth theories of meaning, coherence theories of meaning, and correspondence theories of meaning.
 Each of these is related to the general philosophical study of reality and the representation of meaning.
In linguistics, semantics is the subfield that is devoted to the study of meaning, as inherent at the levels of words, phrases, sentences, and larger units of discourse (termed texts, or narratives). 
The study of semantics is also closely linked to the subjects of representation, reference and denotation. 
The basic study of semantics is oriented to the examination of the meaning of signs, and the study of relations between different linguistic units and compounds: homonymy, synonymy, antonymy, hypernymy, hyponymy, meronymy, metonymy, holonymy, paronyms.
 A key concern is how meaning attaches to larger chunks of text, possibly as a result of the composition from smaller units of meaning.
 Traditionally, semantics has included the study of sense and denotative reference, truth conditions, argument structure, thematic roles, discourse analysis, and the linkage of all of these to syntax.
In the late 1960s, Richard Montague proposed a system for defining semantic entries in the lexicon in terms of the lambda calculus.
 In these terms, the syntactic parse of the sentence John ate every bagel would consist of a subject (John) and a predicate (ate every bagel); Montague demonstrated that the meaning of the sentence altogether could be decomposed into the meanings of its parts and in relatively few rules of combination. The logical predicate thus obtained would be elaborated further, e.g. using truth theory models, which ultimately relate meanings to a set of Tarskiian universals, which may lie outside the logic. The notion of such meaning atoms or primitives is basic to the language of thought hypothesis from the 1970s.
Despite its elegance, Montague grammar was limited by the context-dependent variability in word sense, and led to several attempts at incorporating context, such as:
Situation semantics (1980s): truth-values are incomplete, they get assigned based on context
Generative lexicon (1990s): categories (types) are incomplete, and get assigned based on context
Linguistics
Theoretical
Cognitive Generative Structuralist Quantitative
Functional theories of grammar
Phonology Morphology
Morphophonology Syntax Lexis Semantics Pragmatics Graphemics Orthography Semiotics
Descriptive
Anthropological Comparative Historical Etymology Graphetics Phonetics Sociolinguistics
Applied and experimental
Computational Contrastive
Evolutionary Forensic Internet
Language acquisition
Second-language acquisition
Language assessment
Language development
Language education
Linguistic anthropology
Neurolinguistics Psycholinguistics
Related articles
History of linguistics
Linguistic prescription
List of linguists
Unsolved linguistics problems
Linguistics portal
In Chomskyan linguistics there was no mechanism for the learning of semantic relations, and the nativist view considered all semantic notions as inborn. 
Thus, even novel concepts were proposed to have been dormant in some sense. 
This view was also thought unable to address many issues such as metaphor or associative meanings, and semantic change, where meanings within a linguistic community change over time, and qualia or subjective experience. 
Another issue not addressed by the nativist model was how perceptual cues are combined in thought, e.g. in mental rotation.[7]
This view of semantics, as an innate finite meaning inherent in a lexical unit that can be composed to generate meanings for larger chunks of discourse, is now being fiercely debated in the emerging domain of cognitive linguistics[8] and also in the non-Fodorian camp in philosophy of language.
[9] The challenge is motivated by:
factors internal to language, such as the problem of resolving indexical or anaphora (e.g. this x, him, last week). 
In these situations context serves as the input, but the interpreted utterance also modifies the context, so it is also the output.
 Thus, the interpretation is necessarily dynamic and the meaning of sentences is viewed as contexts changing potentials instead of propositions.
factors external to language, i.e. language is not a set of labels stuck on things, but "a toolbox, the importance of whose elements lie in the way they function rather than their attachments to things.
 This view reflects the position of the later Wittgenstein and his famous game example, and is related to the positions of Quine, Davidson, and others.
A concrete example of the latter phenomenon is semantic underspecification – meanings are not complete without some elements of context.
 To take an example of one word, red, its meaning in a phrase such as red book is similar to many other usages, and can be viewed as compositional.
 However, the colours implied in phrases such as red wine (very dark), and red hair (coppery), or red soil, or red skin are very different. 
Indeed, these colours by themselves would not be called red by native speakers. 
These instances are contrastive, so red wine is so called only in comparison with the other kind of wine (which also is not white for the same reasons). 
This view goes back to de Saussure:
Each of a set of synonyms like redouter ('to dread'), craindre ('to fear'), avoir peur ('to be afraid') has its particular value only because they stand in contrast with one another. 
No word has a value that can be identified independently of what else is in its vicinity.[11]
and may go back to earlier Indian views on language, especially the Nyaya view of words as indicators and not carriers of meaning.[12]
An attempt to defend a system based on propositional meaning for semantic underspecification can be found in the generative lexicon model of James Pustejovsky, who extends contextual operations (based on type shifting) into the lexicon. 
Thus meanings are generated "on the fly" (as you go), based on finite context.
Another set of concepts related to fuzziness in semantics is based on prototypes. 
The work of Eleanor Rosch in the 1970s led to a view that natural categories are not characterizable in terms of necessary and sufficient conditions, but are graded (fuzzy at their boundaries) and inconsistent as to the status of their constituent members.
 One may compare it with Jung's archetype, though the concept of archetype sticks to static concept.
 Some post-structuralists are against the fixed or static meaning of the words. 
Derrida, following Nietzsche, talked about slippages in fixed meanings.
Systems of categories are not objectively out there in the world but are rooted in people's experience. 
These categories evolve as learned concepts of the world – meaning is not an objective truth, but a subjective construct, learned from experience, and language arises out of the "grounding of our conceptual systems in shared embodiment and bodily experience".
 A corollary of this is that the conceptual categories (i.e. the lexicon) will not be identical for different cultures, or indeed, for every individual in the same culture. 
This leads to another debate (see the Sapir–Whorf hypothesis or Eskimo words for snow).
Originates from Montague's work (see above). 
A highly formalized theory of natural language semantics in which expressions are assigned denotations (meanings) such as individuals, truth values, or functions from one of these to another.
 The truth of a sentence, and more interestingly, its logical relation to other sentences, is then evaluated relative to a model.
Pioneered by the philosopher Donald Davidson, another formalized theory, which aims to associate each natural language sentence with a meta-language description of the conditions under which it is true, for example: 'Snow is white' is true if and only if snow is white. 
The challenge is to arrive at the truth conditions for any sentences from fixed meanings assigned to the individual words and fixed rules for how to combine them. 
In practice, truth-conditional semantics is similar to model-theoretic semantics; conceptually, however, they differ in that truth-conditional semantics seeks to connect language with statements about the real world (in the form of meta-language statements), rather than with abstract models.
This theory is an effort to explain properties of argument structure.
 The assumption behind this theory is that syntactic properties of phrases reflect the meanings of the words that head them.
 With this theory, linguists can better deal with the fact that subtle differences in word meaning correlate with other differences in the syntactic structure that the word appears in.
 The way this is gone about is by looking at the internal structure of words.
 These small parts that make up the internal structure of words are termed semantic primitives.
A linguistic theory that investigates word meaning. 
This theory understands that the meaning of a word is fully reflected by its context. 
Here, the meaning of a word is constituted by its contextual relations.
 Therefore, a distinction between degrees of participation as well as modes of participation are made.
 In order to accomplish this distinction any part of a sentence that bears a meaning and combines with the meanings of other constituents is labeled as a semantic constituent. 
Semantic constituents that cannot be broken down into more elementary constituents are labeled minimal semantic constituents.
Computational semantics is focused on the processing of linguistic meaning. 
In order to do this concrete algorithms and architectures are described. 
Within this framework the algorithms and architectures are also analyzed in terms of decidability, time/space complexity, data structures they require and communication protocols.
In computer science, the term semantics refers to the meaning of languages, as opposed to their form (syntax). 
According to Euzenat, semantics "provides the rules for interpreting the syntax which do not provide the meaning directly but constrains the possible interpretations of what is declared."
 In other words, semantics is about interpretation of an expression. 
Additionally, the term is applied to certain types of data structures specifically designed and used for representing information content.
The semantics of programming languages and other languages is an important issue and area of study in computer science. 
Like the syntax of a language, its semantics can be defined exactly.
For instance, the following statements use different syntaxes, but cause the same instructions to be executed:
Various ways have been developed to describe the semantics of programming languages formally, building on mathematical logic:
Operational semantics: The meaning of a construct is specified by the computation it induces when it is executed on a machine. 
In particular, it is of interest how the effect of a computation is produced.
Denotational semantics: Meanings are modelled by mathematical objects that represent the effect of executing the constructs. 
Thus only the effect is of interest, not how it is obtained.
Axiomatic semantics: Specific properties of the effect of executing the constructs are expressed as assertions. 
Thus there may be aspects of the executions that are ignored.
Terms such as semantic network and semantic data model are used to describe particular types of data model characterized by the use of directed graphs in which the vertices denote concepts or entities in the world, and the arcs denote relationships between them.
The Semantic Web refers to the extension of the World Wide Web via embedding added semantic metadata, using semantic data modelling techniques such as Resource Description Framework (RDF) and Web Ontology Language (OWL).
In psychology, semantic memory is memory for meaning – in other words, the aspect of memory that preserves only the gist, the general significance, of remembered experience – while episodic memory is memory for the ephemeral details – the individual features, or the unique particulars of experience. 
The term 'episodic memory' was introduced by Tulving and Schacter in the context of 'declarative memory' which involved simple association of factual or objective information concerning its object. 
Word meaning is measured by the company they keep, i.e. the relationships among words themselves in a semantic network. 
The memories may be transferred intergenerationally or isolated in one generation due to a cultural disruption. 
Different generations may have different experiences at similar points in their own time-lines. 
This may then create a vertically heterogeneous semantic net for certain words in an otherwise homogeneous culture.
 In a network created by people analyzing their understanding of the word (such as Wordnet) the links and decomposition structures of the network are few in number and kind, and include part of, kind of, and similar links. 
In automated ontologies the links are computed vectors without explicit meaning. 
Various automated technologies are being developed to compute the meaning of words: latent semantic indexing and support vector machines as well as natural language processing, neural networks and predicate calculus techniques.
Java is a set of computer software and specifications developed by Sun Microsystems, later acquired by Oracle Corporation, that provides a system for developing application software and deploying it in a cross-platform computing environment. 
Java is used in a wide variety of computing platforms from embedded devices and mobile phones to enterprise servers and supercomputers. 
While less common, Java applets run in secure, sandboxed environments to provide many features of native applications and can be embedded in HTML pages.
Writing in the Java programming language is the primary way to produce code that will be deployed as byte code in a Java Virtual Machine (JVM); byte code compilers are also available for other languages, including Ada, JavaScript, Python, and Ruby. 
In addition, several languages have been designed to run natively on the JVM, including Scala, Clojure and Groovy. 
Java syntax borrows heavily from C and C++, but object-oriented features are modeled after Smalltalk and Objective-C.
 Java eschews certain low-level constructs such as pointers and has a very simple memory model where every object is allocated on the heap and all variables of object types are references. 
Memory management is handled through integrated automatic garbage collection performed by the JVM.
On November 13, 2006, Sun Microsystems made the bulk of its implementation of Java available under the GNU General Public License (GPL).
The latest version is Java 8, the only supported version in 2015.
The Java platform is a suite of programs that facilitate developing and running programs written in the Java programming language. 
The platform is not specific to any one processor or operating system, rather an execution engine (called a virtual machine) and a compiler with a set of libraries are implemented for various hardware and operating systems so that Java programs can run identically on all of them. There are multiple platforms, each targeting a different class of devices:
Java Card: A technology that allows small Java-based applications (applets) to be run securely on smart cards and similar small-memory devices.
Java ME (Micro Edition): Specifies several different sets of libraries (known as profiles) for devices with limited storage, display, and power capacities. 
Often used to develop applications for mobile devices, PDAs, TV set-top boxes, and printers.
Java SE (Standard Edition): For general-purpose use on desktop PCs, servers and similar devices.
Java EE (Enterprise Edition): Java SE plus various APIs useful for multi-tier client–server enterprise applications.
The Java platform consists of several programs, each of which provides a portion of its overall capabilities. 
For example, the Java compiler, which converts Java source code into Java bytecode (an intermediate language for the JVM), is provided as part of the Java Development Kit (JDK). 
The Java Runtime Environment (JRE), complementing the JVM with a just-in-time (JIT) compiler, converts intermediate bytecode into native machine code on the fly. 
An extensive set of libraries are also part of the Java platform.
The essential components in the platform are the Java language compiler, the libraries, and the runtime environment in which Java intermediate bytecode executes according to the rules laid out in the virtual machine specification.
The heart of the Java platform is the concept of a "virtual machine" that executes Java bytecode programs. 
This bytecode is the same no matter what hardware or operating system the program is running under. There is a JIT (Just In Time) compiler within the Java Virtual Machine, or JVM. 
The JIT compiler translates the Java bytecode into native processor instructions at run-time and caches the native code in memory during execution.
The use of bytecode as an intermediate language permits Java programs to run on any platform that has a virtual machine available. 
The use of a JIT compiler means that Java applications, after a short delay during loading and once they have "warmed up" by being all or mostly JIT-compiled, tend to run about as fast as native programs.
 Since JRE version 1.2, Sun's JVM implementation has included a just-in-time compiler instead of an interpreter.
Although Java programs are cross-platform or platform independent, the code of the Java Virtual Machines (JVM) that execute these programs is not.
 Every supported operating platform has its own JVM.
In most modern operating systems (OSs), a large body of reusable code is provided to simplify the programmer's job. 
This code is typically provided as a set of dynamically loadable libraries that applications can call at runtime. 
Because the Java platform is not dependent on any specific operating system, applications cannot rely on any of the pre-existing OS libraries. 
Instead, the Java platform provides a comprehensive set of its own standard class libraries containing much of the same reusable functions commonly found in modern operating systems.
 Most of the system library is also written in Java. 
For instance, Swing library paints the user interface and handles the events itself, eliminating many subtle differences between how different platforms handle even similar components.
The Java class libraries serve three purposes within the Java platform. 
First, like other standard code libraries, the Java libraries provide the programmer a well-known set of functions to perform common tasks, such as maintaining lists of items or performing complex string parsing. 
Second, the class libraries provide an abstract interface to tasks that would normally depend heavily on the hardware and operating system. 
Tasks such as network access and file access are often heavily intertwined with the distinctive implementations of each platform. 
The java.net and java.io libraries implement an abstraction layer in native OS code, then provide a standard interface for the Java applications to perform those tasks.
 Finally, when some underlying platform does not support all of the features a Java application expects, the class libraries work to gracefully handle the absent components, either by emulation to provide a substitute, or at least by providing a consistent way to check for the presence of a specific feature.
See also: List of JVM languages and JVM programming languages
The word "Java", alone, usually refers to Java programming language that was designed for use with the Java platform. 
Programming languages are typically outside of the scope of the phrase "platform", although the Java programming language was listed as a core part of the Java platform before Java 7.
 The language and runtime were therefore commonly considered a single unit. 
However, an effort was made with the Java 7 specification to more clearly treat the Java language and the Java virtual machine as separate entities, so that they are no longer considered a single unit.[14]
Third parties have produced many compilers or interpreters that target the JVM. 
Some of these are for existing languages, while others are for extensions to the Java language.
BeanShell – A lightweight scripting language for Java[15]
Clojure – A dialect of the Lisp programming language
Groovy, a dynamic language with features similar to those of Python, Ruby, Perl, and Smalltalk
JRuby – A Ruby interpreter
Jython – A Python interpreter
Kotlin – An industrial programming language for JVM with full Java interoperability
Rhino – A JavaScript interpreter
Scala – A multi-paradigm programming language designed as a "better Java"
Gosu – A general-purpose Java Virtual Machine-based programming language released under the Apache License 2.0
see also: Comparison of the Java and .NET platforms and Comparison of C# and Java
The success of Java and its write once, run anywhere concept has led to other similar efforts, notably the .NET Framework, appearing since 2002, which incorporates many of the successful aspects of Java. 
.NET in its complete form (Microsoft's implementation) is currently only fully available on Windows platforms, whereas Java is fully available on many platforms. 
.NET was built from the ground-up to support multiple programming languages, while the Java platform was initially built to support only the Java language, although many other languages have been made for JVM since.
.NET includes a Java-like language called Visual J# (formerly named J++) that is incompatible with the Java specification, and the associated class library mostly dates to the old JDK 1.1 version of the language. 
For these reasons, it is more a transitional language to switch from Java to the .NET platform, than a first class .NET language.
 Visual J# was discontinued with the release of Microsoft Visual Studio 2008.
 The existing version shipping with Visual Studio 2005 will be supported until 2015 as per the product life-cycle strategy.
Java Development Kit
Main article: Java Development Kit
The Java Development Kit (JDK) is a Sun product aimed at Java developers. 
Since the introduction of Java, it has been by far the most widely used Java software development kit (SDK).
 It contains a Java compiler, a full copy of the Java Runtime Environment (JRE), and many other important development tools.
The Java platform and language began as an internal project at Sun Microsystems in December 1990, providing an alternative to the C++/C programming languages. 
Engineer Patrick Naughton had become increasingly frustrated with the state of Sun's C++ and C application programming interfaces (APIs) and tools. 
While considering moving to NeXT, Naughton was offered[by whom?] a chance to work on new technology, and thus the Stealth Project started.
The Stealth Project was soon renamed to the Green Project, with James Gosling and Mike Sheridan joining Naughton. 
Together with other engineers, they began work in a small office on Sand Hill Road in Menlo Park, California. 
They aimed to develop new technology for programming next-generation smart appliances, which Sun expected to offer major new opportunities.
The team originally considered using C++, but rejected it for several reasons. 
Because they were developing an embedded system with limited resources, they decided that C++ needed too much memory and that its complexity led to developer errors. 
The language's lack of garbage collection meant that programmers had to manually manage system memory, a challenging and error-prone task. 
The team also worried about the C++ language's lack of portable facilities for security, distributed programming, and threading. 
Finally, they wanted a platform that would port easily to all types of devices.
Bill Joy had envisioned a new language combining Mesa and C. 
In a paper called Further, he proposed to Sun that its engineers should produce an object-oriented environment based on C++. 
Initially, Gosling attempted to modify and extend C++ (a proposed development that he referred to as "C++ ++ --") but soon abandoned that in favor of creating a new language, which he called Oak, after the tree that stood just outside his office.
By the summer of 1992, the team could demonstrate portions of the new platform, including the Green OS, the Oak language, the libraries, and the hardware. 
Their first demonstration, on September 3, 1992, focused on building a personal digital assistant (PDA) device named Star7[1] that had a graphical interface and a smart agent called "Duke" to assist the user. 
In November of that year, the Green Project was spun off to become Firstperson, a wholly owned subsidiary of Sun Microsystems, and the team relocated to Palo Alto, California.
The Firstperson team had an interest in building highly interactive devices, and when Time Warner issued a request for proposal (RFP) for a set-top box, Firstperson changed their target and responded with a proposal for a set-top box platform. 
However, the cable industry felt that their platform gave too much control to the user; Firstperson lost their bid to SGI. 
An additional deal with The 3DO Company for a set-top box also failed to materialize. 
Unable to generate interest within the television industry, the company was rolled back into Sun.
in June and July 1994 – after three days of brainstorming with John Gage (the Director of Science for Sun), Gosling, Joy, Naughton, Wayne Rosing, and Eric Schmidt – the team re-targeted the platform for the World Wide Web. 
They felt that with the advent of graphical web browsers like Mosaic the Internet could evolve into the same highly interactive medium that they had envisioned for cable TV. 
As a prototype, Naughton wrote a small browser, WebRunner (named after the movie Blade Runner), renamed HotJava[16] in 1995.
In 1994, Sun renamed the Oak language as Java after a trademark search revealed that Oak Technology used the name Oak.
 Although Java 1.0a became available for download in 1994, the first public release of Java, Java 1.0a2 with the HotJava browser, came on May 23, 1995, announced by Gage at the SunWorld conference. 
Accompanying Gage's announcement, Marc Andreessen, Executive Vice President of Netscape Communications Corporation, unexpectedly announced that Netscape browsers would include Java support.
 On January 9, 1996, Sun Microsystems formed the JavaSoft group to develop the technology.[19]
The Java language has undergone several changes since the release of JDK (Java Development Kit) 1.0 on January 23, 1996, as well as numerous additions of classes and packages to the standard library. 
Since J2SE 1.4 the Java Community Process (JCP) has governed the evolution of the Java Language. 
The JCP uses Java Specification Requests (JSRs) to propose and specify additions and changes to the Java platform. 
The Java Language Specification (JLS) specifies the language; changes to the JLS are managed under JSR 901.[20]
Sun released JDK 1.1 on February 19, 1997. 
Major additions included an extensive retooling of the AWT event model, inner classes added to the language, JavaBeans and JDBC.
J2SE 1.2 (December 8, 1998) – Codename Playground. 
This and subsequent releases through J2SE 5.0 were rebranded Java 2 and the version name "J2SE" (Java 2 Platform, Standard Edition) replaced JDK to distinguish the base platform from J2EE (Java 2 Platform, Enterprise Edition) and J2ME (Java 2 Platform, Micro Edition).
 Major additions included reflection, a collections framework, Java IDL (an interface description language implementation for CORBA interoperability), and the integration of the Swing graphical API into the core classes. 
A Java Plug-in was released, and Sun's JVM was equipped with a JIT compiler for the first time.
J2SE 1.3 (May 8, 2000) – Codename Kestrel.
 Notable changes included the bundling of the HotSpot JVM (the HotSpot JVM was first released in April, 1999 for the J2SE 1.2 JVM), JavaSound, Java Naming and Directory Interface (JNDI) and Java Platform Debugger Architecture (JPDA).
J2SE 1.4 (February 6, 2002) – Codename Merlin. 
This became the first release of the Java platform developed under the Java Community Process as JSR 59.
 Major changes included regular expressions modeled after Perl, exception chaining, an integrated XML parser and XSLT processor (JAXP), and Java Web Start.
J2SE 5.0 (September 30, 2004) – Codename Tiger. 
Originally numbered 1.5, which is still used as the internal version number.
 Developed under JSR 176, Tiger added several significant new language features including the for-each loop, generics, autoboxing and var-args.
Java SE 6 (December 11, 2006) – Codename Mustang. 
Bundled with a database manager and facilitates the use of scripting languages with the JVM (such as JavaScript using Mozilla's Rhino engine). 
As of this version, Sun replaced the name "J2SE" with Java SE and dropped the ".0" from the version number.
Other major changes include support for pluggable annotations (JSR 269), many GUI improvements, including native UI enhancements to support the look and feel of Windows Vista, and improvements to the Java Platform Debugger Architecture (JPDA) & JVM Tool Interface for better monitoring and troubleshooting.
Java SE 7 (July 28, 2011) – Codename Dolphin. 
This version developed under JSR 336.
 It added many small language changes including strings in switch, try-with-resources and type inference for generic instance creation. 
The JVM was extended with support for dynamic languages, while the class library was extended among others with a join/fork framework,[26] an improved new file I/O library and support for new network protocols such as SCTP. 
Java 7 Update 76 was released in January 2015, with expiration date April 14, 2015.[27]
The current version, Java SE 8 (March 18, 2014). 
Notable changes include language-level support for lambda expressions (closures) and default methods, the Project Nashorn JavaScript runtime, a new Date and Time API inspired by Joda Time, and the removal of PermGen. 
This version is not officially supported on the Windows XP platform.
 However, due to the end of Java 7's lifecycle it is the recommended version for XP users. 
Previously, only an unofficial manual installation method had been described for Windows XP SP3. 
It refers to JDK8, the developing platform for Java that also includes a fully functioning Java Runtime Environment.
In addition to language changes, significant changes have been made to the Java class library over the years, which has grown from a few hundred classes in JDK 1.0 to over three thousand in J2SE 5.0. Entire new APIs, such as Swing and Java 2D, have evolved, and many of the original JDK 1.0 classes and methods have been deprecated.
According to Oracle, the Java Runtime Environment is found on over 850 million PCs.
 Microsoft has not bundled a Java Runtime Environment (JRE) with its operating systems since Sun Microsystems sued Microsoft for adding Windows-specific classes to the bundled Java runtime environment, and for making the new classes available through Visual J++.
 Apple no longer includes a Java runtime with OS X as of version 10.7, but the system prompts the user to download and install it the first time an application requiring the JRE is launched.
 Many Linux distributions include the partially compatible free software package GNU Classpath[31] and increasingly mostly compatible IcedTea.
Some Java applications are in fairly widespread desktop use, including the NetBeans and Eclipse integrated development environments, and file sharing clients such as LimeWire and Vuze. 
Java is also used in the MATLAB mathematics programming environment, both for rendering the user interface and as part of the core system. 
Java provides cross platform user interface for some high end collaborative applications like Lotus Notes.
This section does not cite any sources. 
Please help improve this section by adding citations to reliable sources. 
Unsourced material may be challenged and removed. (October 2015)
Java ME has become popular in mobile devices, where it competes with Symbian, BREW, and the .NET Compact Framework.
The diversity of mobile phone manufacturers has led to a need for new unified standards so programs can run on phones from different suppliers – MIDP. 
The first standard was MIDP 1, which assumed a small screen size, no access to audio, and a 32kB program limit. 
The more recent MIDP 2 allows access to audio, and up to 64kB for the program size. 
With handset designs improving more rapidly than the standards, some manufacturers relax some limitations in the standards, for example, maximum program size.
Google's Android operating system uses the Java language, but not its class libraries, therefore the Android platform cannot be called Java. 
Android executes the code on the ART VM (formerly the Dalvik VM up to Android 4.4.4) instead of the Java VM.
The Java platform has become a mainstay of enterprise IT development since the introduction of the Enterprise Edition in 1998, in two different ways:
Through the coupling of Java to the web server, the Java platform has become a leading platform for integrating the Web with enterprise backend systems. 
This has allowed companies to move part or all of their business to the Internet environment by way of highly interactive online environments (such as highly dynamic websites) that allow the customer direct access to the business processes (e.g. online banking websites, airline booking systems and so on). 
This trend has continued from its initial Web-based start:The Java platform has matured into an Enterprise Integration role in which legacy systems are unlocked to the outside world through bridges built on the Java platform. 
This trend has been supported for Java platform support for EAI standards like messaging and Web services and has fueled the inclusion of the Java platform as a development basis in such standards as SCA, XAM and others.
Java has become the standard development platform for many companies' IT departments, which do most or all of their corporate development in Java.
 This type of development is usually related to company-specific tooling (e.g. a booking tool for an airline) and the choice for the Java platform is often driven by a desire to leverage the existing Java infrastructure to build highly intelligent and interconnected tools.
The Java platform has become the main development platform for many software tools and platforms that are produced by third-party software groups (commercial, open source and hybrid) and are used as configurable (rather than programmable) tools by companies. 
Examples in this category include Web servers, application servers, databases, enterprise service buses, business process management (BPM) tools and content management systems.
Enterprise use of Java has also long been the main driver of open source interest in the platform. 
When Sun announced that Java SE and Java ME would be released under a free software license (the GNU General Public License), they released the Duke graphics under the free BSD license at the same time.
 A new Duke personality is created every year.
 For example, in July 2011 "Future Tech Duke" included a bigger nose, a jetpack, and blue wings.
The source code for Sun's implementations of Java (that is the de facto reference implementation) has been available for some time, but until recently the license terms severely restricted what could be done with it without signing (and generally paying for) a contract with Sun. 
As such these terms did not satisfy the requirements of either the Open Source Initiative or the Free Software Foundation to be considered open source or free software, and Sun Java was therefore a proprietary platform.
While several third-party projects (e.g. GNU Classpath and Apache Harmony) created free software partial Java implementations, the large size of the Sun libraries combined with the use of clean room methods meant that their implementations of the Java libraries (the compiler and VM are comparatively small and well defined) were incomplete and not fully compatible. 
These implementations also tended to be far less optimized than Sun's.
Sun announced in JavaOne 2006 that Java would become free and open source software,[37] and on October 25, 2006, at the Oracle OpenWorld conference, Jonathan I. 
Schwartz said that the company was set to announce the release of the core Java Platform as free and open source software within 30 to 60 days.
Sun released the Java HotSpot virtual machine and compiler as free software under the GNU General Public License on November 13, 2006, with a promise that the rest of the JDK (that includes the JRE) would be placed under the GPL by March 2007 ("except for a few components that Sun does not have the right to publish in distributable source form under the GPL").
 According to Richard Stallman, this would mean an end to the "Java trap".
 Mark Shuttleworth called the initial press announcement, "A real milestone for the free software community".
Sun released the source code of the Class library under GPL on May 8, 2007, except some limited parts that were licensed by Sun from 3rd parties who did not want their code to be released under a free software and open-source license.
 Some of the encumbered parts turned out to be fairly key parts of the platform such as font rendering and 2D rasterising, but these were released as open-source later by Sun (see OpenJDK Class library).
Sun's goal was to replace the parts that remain proprietary and closed-source with alternative implementations and make the class library completely free and open source. 
In the meantime, a third party project called IcedTea created a completely free and highly usable JDK by replacing encumbered code with either stubs or code from GNU Classpath. 
Although OpenJDK has since become buildable without the encumbered parts (from OpenJDK 6 b10[43]), IcedTea is still used by the majority of distributions, such as Fedora, RHEL, Debian, Ubuntu, Gentoo, Arch Linux and Slackware, as it provides security releases and an easier means for patch inclusion. 
OpenJDK also still doesn't include a browser plugin & Web Start implementation, so IcedTea's companion project, IcedTea-Web, is needed to fill this gap.
In June 2008, it was announced that IcedTea6 (as the packaged version of OpenJDK on Fedora 9) has passed the Technology Compatibility Kit tests and can claim to be a fully compatible Java 6 implementation.
Because OpenJDK is under the GPL, it is possible to redistribute a custom version of the JRE directly with software applications,[45][46] rather than requiring the enduser (or their sysadmin) to download and install the correct version of the proprietary Oracle JRE onto each of their systems themselves.
In most cases Java support is unnecessary in Web browsers, and security experts recommend that it not be run in a browser unless absolutely necessary.
It was suggested that, if Java is required by a few Web sites, users should have a separate browser installation specifically for those sites.
Further information: Generics in Java
When generics were added to Java 5.0, there was already a large framework of classes (many of which were already deprecated), so generics were chosen to be implemented using erasure to allow for migration compatibility and re-use of these existing classes. This limited the features that could be provided by this addition as compared to some other languages.[48][49]
Java lacks native unsigned integer types.
 Unsigned data are often generated from programs written in C and the lack of these types prevents direct data interchange between C and Java. 
Unsigned large numbers are also used in many numeric processing fields, including cryptography, which can make Java less convenient to use for these tasks.[50] Although it is possible to partially circumvent this problem with conversion code and using larger data types, it makes using Java cumbersome for handling the unsigned data. While a 32-bit signed integer may be used to hold a 16-bit unsigned value with relative ease, a 32-bit unsigned value would require a 64-bit signed integer. Additionally, a 64-bit unsigned value cannot be stored using any integer type in Java because no type larger than 64 bits exists in the Java language. If abstracted using functions, function calls become necessary for many operations which are native to some other languages. Alternatively, it is possible to use Java's signed integers to emulate unsigned integers of the same size, but this requires detailed knowledge of complex bitwise operations.[51]
While Java's floating point arithmetic is largely based on IEEE 754 (Standard for Binary Floating-Point Arithmetic), certain features are not supported even when using the strictfp modifier, such as Exception Flags and Directed Roundings — capabilities mandated by IEEE Standard 754. Additionally, the extended precision floating-point types permitted in 754 and present in many processors are not permitted in Java.[52][53]
Further information: Java performance
In the early days of Java (before the HotSpot VM was implemented in Java 1.3 in 2000) there were some criticisms of performance. 
However, benchmarks typically report Java as being about 50% slower than C (a language which compiles to native code).
Java's performance has improved substantially since the early versions.
 Performance of JIT compilers relative to native compilers has in some optimized tests been shown to be quite similar.
Java bytecode can either be interpreted at run time by a virtual machine, or it can be compiled at load time or runtime into native code which runs directly on the computer's hardware. 
Interpretation is slower than native execution, and compilation at load time or runtime has an initial performance penalty for the compilation. 
Modern performant JVM implementations all use the compilation approach, so after the initial startup time the performance is equivalent to native code.
Further information: Java security
The Java platform provides a security architecture[60] which is designed to allow the user to run untrusted bytecode in a "sandboxed" manner to protect against malicious or poorly written software. 
This "sandboxing" feature is intended to protect the user by restricting access to certain platform features and APIs which could be exploited by malware, such as accessing the local filesystem, running arbitrary commands, or accessing communication networks.
In recent years, researchers have discovered numerous security flaws in some widely used Java implementations, including Oracle's, which allow untrusted code to bypass the sandboxing mechanism, exposing users to malicious attacks. 
These flaws affect only Java applications which execute arbitrary untrusted bytecode, such as web browser plug-ins that run Java applets downloaded from public websites. 
Applications where the user trusts, and has full control over, all code that is being executed are unaffected.
On August 31, 2012, Java 6 and 7 on Microsoft Windows, Mac OS X, and Linux were found to have a serious security flaw that allowed a remote exploit to take place by simply loading a malicious web page.[61] 
Java 5 was later found to be flawed as well.[62]
On January 10, 2013, three computer specialists spoke out against Java, telling Reuters that it was not secure and that people should disable Java. 
Jaime Blasco, Labs Manager with AlienVault Labs, stated that "Java is a mess. 
It’s not secure. 
You have to disable it."
 This vulnerability affects Java 7 and it is unclear if it affects Java 6, so it is suggested that consumers disable it.
 Security alerts from Oracle announce schedules of critical security-related patches to Java.[66]
On January 14, 2013, security experts said that the update still failed to protect PCs from attack.
 This exploit hole prompted a response from the United States Department of Homeland Security encouraging users to disable or uninstall Java.
Apple blacklisted Java in limited order for all computers running its Mac OS X operating system through a virus protection program.[69]
The Java browser runtime environment has a history of bundling sponsored software to be installed by default during installation and during the updates which roll out every month or so. 
This includes the "Ask.com toolbar" that will redirect browser searches to ads and "McAfee Security Scan Plus".[70]
Several authors[weasel words] inline with recent Java security and vulnerability issues have called for users to ditch Java.
 "Once promising, it has outlived its usefulness in the browser, and has become a nightmare that delights cyber-criminals at the expense of computer users."
[71] "I think everyone should uninstall Java from all their PCs and Macs, and then think carefully about whether they need to add it back. 
If you are a typical home user, you can probably do without it. 
If you are a business user, you may not have a choice."[72]
Java has yet to release an automatic updater that does not require user intervention and administrative rights[73] unlike Google Chrome[74] and Flash player.
Sun Microsystems, Inc. was a company that sold computers, computer components, computer software, and information technology services and that created the Java programming language, Solaris Unix and the Network File System (NFS). 
Sun significantly evolved several key computing technologies, among them Unix, RISC processors, thin client computing, and virtualized computing. 
Sun was founded on February 24, 1982.[3] 
At its height, Sun headquarters were in Santa Clara, California (part of Silicon Valley), on the former west campus of the Agnews Developmental Center.
On January 27, 2010, Sun was acquired by Oracle Corporation for US $7.4 billion, based on an agreement signed on April 20, 2009.[4] 
The following month, Sun Microsystems, Inc. was merged with Oracle USA, Inc. to become Oracle America, Inc.[5]
Other technologies include the Java platform, MySQL, and NFS.
 Sun was a proponent of open systems in general and Unix in particular, and a major contributor to open source software.[6]
 Sun's main manufacturing facilities were located in Hillsboro, Oregon, and Linlithgow, Scotland.
The initial design for what became Sun's first Unix workstation, the Sun-1, was conceived by Andy Bechtolsheim when he was a graduate student at Stanford University in Palo Alto, California. 
Bechtolsheim originally designed the SUN workstation for the Stanford University Network communications project as a personal CAD workstation. 
It was designed around the Motorola 68000 processor with an advanced memory management unit (MMU) to support the Unix operating system with virtual memory support.[7] 
He built the first ones from spare parts obtained from Stanford's Department of Computer Science and Silicon Valley supply houses.[8]
On February 24, 1982, Vinod Khosla, Andy Bechtolsheim, and Scott McNealy, all Stanford graduate students, founded Sun Microsystems. 
Bill Joy of Berkeley, a primary developer of the Berkeley Software Distribution (BSD), joined soon after and is counted as one of the original founders.[9] 
The Sun name is derived from the initials of the Stanford University Network.[10][11][12] Sun was profitable from its first quarter in July 1982.
By 1983 Sun was known for producing 68000-based systems with high-quality graphics that were the only computers other than DEC's VAX to run 4.2BSD. 
It licensed the computer design to other manufacturers, which typically used it to build Multibus-based systems running Unix from UniSoft.[13
Sun's initial public offering was in 1986 under the stock symbol SUNW, for Sun Workstations (later Sun Worldwide).[14][15]
 The symbol was changed in 2007 to JAVA; Sun stated that the brand awareness associated with its Java platform better represented the company's current strategy.[16]
Sun's logo, which features four interleaved copies of the word sun, was designed by professor Vaughan Pratt, also of Stanford. 
The initial version of the logo was orange and had the sides oriented horizontally and vertically, but it was subsequently rotated to stand on one corner and re-colored purple, and later blue.
The "bubble" and its aftermath
In the dot-com bubble, Sun began making much more money, and its shares rose dramatically. 
It also began spending much more, hiring workers and building itself out. 
Some of this was because of genuine demand, but much was from web start-up companies anticipating business that would never happen.
In 2000, the bubble burst.[17]
 Sales in Sun's important hardware division went into free-fall as customers closed shop and auctioned off high-end servers.
Several quarters of steep losses led to executive departures, rounds of layoffs,[18][19][20] and other cost cutting. 
In December 2001, the stock fell to the 1998, pre-bubble level of about $100. 
But it kept falling, faster than many other tech companies. 
A year later it had dipped below $10 (a tenth of what it was even in 1990) but bounced back to $20. 
In mid-2004, Sun closed their Newark, California factory and consolidated all manufacturing to Hillsboro, Oregon.[21] In 2006, that factory also closed.[22]
Post-crash focus
Aerial photograph of the Sun headquarters campus in Santa Clara, California
Buildings 21 and 22 at Sun's headquarters campus in Santa Clara
Sun in Markham, Ontario, Canada
In 2004, Sun canceled two major processor projects which emphasized high instruction level parallelism and operating frequency. 
Instead, the company chose to concentrate on processors optimized for multi-threading and multiprocessing, such as the UltraSPARC T1 processor (codenamed "Niagara"). 
The company also announced a collaboration with Fujitsu to use the Japanese company's processor chips in mid-range and high-end Sun servers. 
These servers were announced on April 17, 2007 as the M-Series, part of the SPARC Enterprise series.
In February 2005, Sun announced the Sun Grid, a grid computing deployment on which it offered utility computing services priced at US$1 per CPU/hour for processing and per GB/month for storage. 
This offering built upon an existing 3,000-CPU server farm used for internal R&D for over 10 years, which Sun marketed as being able to achieve 97% utilization. 
In August 2005, the first commercial use of this grid was announced for financial risk simulations which was later launched as its first software as a service product.[23]
In January 2005, Sun reported a net profit of $19 million for fiscal 2005 second quarter, for the first time in three years. 
This was followed by net loss of $9 million on GAAP basis for the third quarter 2005, as reported on April 14, 2005. 
In January 2007, Sun reported a net GAAP profit of $126 million on revenue of $3.337 billion for its fiscal second quarter. 
Shortly following that news, it was announced that Kohlberg Kravis Roberts (KKR) would invest $700 million in the company.[24]
Sun had engineering groups in Bangalore, Beijing, Dublin, Grenoble, Hamburg, Prague, St. 
Petersburg, Tel Aviv, Tokyo, and Trondheim.[25]
In 2007–2008, Sun posted revenue of $13.8 billion and had $2 billion in cash. 
First-quarter 2008 losses were $1.68 billion; revenue fell 7% to $12.99 billion. 
Sun's stock lost 80% of its value November 2007 to November 2008, reducing the company's market value to $3 billion. 
With falling sales to large corporate clients, Sun announced plans to lay off 5,000 to 6,000 workers, or 15–18% of its work force. 
It expected to save $700 million to $800 million a year as a result of the moves, while also taking up to $600 million in charges.[26]
Sun acquisitions
Sun server racks at Seneca College (York Campus)
This list is incomplete; you can help by expanding it.
1987: Trancept Systems, a high performance graphics hardware company[27]
1987: Sitka Corp, networking systems linking the Macintosh with IBM PCs[28]
1987: Centram Systems West, maker of networking software for PCs, Macs and Sun systems
1988: Folio, Inc., developer of intelligent font scaling technology and the F3 font format[29]
1991: Interactive Systems Corporation's Intel/Unix OS division, from Eastman Kodak Company
1992: Praxsys Technologies, Inc., developers of the Windows emulation technology that eventually became Wabi[30]
1994: Thinking Machines Corporation hardware division
1996: Lighthouse Design, Ltd.[31]
1996: Cray Business Systems Division, from Silicon Graphics[32]
1996: Integrated Micro Products, specializing in fault tolerant servers
1996: Thinking Machines Corporation software division
February 1997: LongView Technologies, LLC[33]
August 1997: Diba, technology supplier for the Information Appliance industry[34]
September 1997: Chorus Systems, creators of ChorusOS[35]
November 1997: Encore Computer Corporation's storage business[36]
1998: RedCape Software
1998: i-Planet, a small software company that produced the "Pony Espresso" mobile email client—its name (sans hyphen) for the Sun-Netscape software alliance
June 1998: Dakota Scientific Software, Inc. - development tools for high-performance computing[37]
July 1998: NetDynamics[38]—developers of the NetDynamics Application Server[39]
October 1998: Beduin,[40] small software company that produced the "Impact" small-footprint Java-based Web browser for mobile devices.
1999: StarDivision, German software company and with it StarOffice, which was later released as open source under the name OpenOffice.org
1999: MAXSTRAT Corporation, a company in Milpitas, California selling Fibre Channel storage servers.
1999: Forte, an enterprise software company specializing in integration solutions and developer of the Forte 4GL and TeamWare
1999: NetBeans, produced a modular IDE written in Java, based on a student project at Charles University in Prague
March 2000: Innosoft International, Inc. a software company specializing in highly scalable MTAs (PMDF) and Directory Services.
July 2000: Gridware, a software company whose products managed the distribution of computing jobs across multiple computers[41]
September 2000: Cobalt Networks, an Internet appliance manufacturer for $2 Billion[42]
December 2000: HighGround, with a suite of Web-based management solutions[43]
2001: LSC, Inc., an Eagan, Minnesota company that developed Storage and Archive Management File System (SAM-FS) and Quick File System QFS file systems for backup and archive
March 2002: Clustra Systems[44]
June 2002: Afara Websystems, developed SPARC processor-based technology[45]
September 2002: Pirus Networks, intelligent storage services[46]
November 2002: Terraspring, infrastructure automation software[47]
June 2003: Pixo, added to the Sun Content Delivery Server[48]
August 2003: CenterRun, Inc.[49]
December 2003: Waveset Technologies, identity management[50]
January 2004 Nauticus Networks[51]
February 2004: Kealia, founded by original Sun founder Andy Bechtolsheim, developed AMD-based 64-bit servers[52]
January 2005: SevenSpace, a multi-platform managed services provider[53]
May 2005: Tarantella, Inc. (formerly known as Santa Cruz Operation (SCO)), for $25 Million[54]
June 2005: SeeBeyond, a Service-Oriented Architecture (SOA) software company for $387m[55]
June 2005: Procom Technology, Inc.'s NAS IP Assets[56]
August 2005: StorageTek, data storage technology company for $4.1 Billion[57]
February 2006: Aduva, software for Solaris and Linux patch management[58]
October 2006: Neogent[59]
April 2007: SavaJe, the SavaJe OS, a Java OS for mobile phones
September 2007: Cluster File Systems, Inc.[60]
November 2007: Vaau, Enterprise Role Management and identity compliance solutions[61]
February 2008: MySQL AB, the company offering the open source database MySQL for $1 Billion.[62]
February 2008: Innotek GmbH, developer of the VirtualBox virtualization product[63][64]
April 2008: Montalvo Systems, x86 microprocessor startup acquired before first silicon
January 2009: Q-layer, a software company with cloud computing solutions[65]
For the first decade of Sun's history, the company positioned its products as technical workstations, competing successfully as a low-cost vendor during the Workstation Wars of the 1980s. 
It then shifted its hardware product line to emphasize servers and storage. 
High-level telecom control systems such as Operational Support Systems service predominantly used Sun equipment. 
This use is due mainly to the company basing its products around a mature and very stable version of the Unix operating system and the support service that Sun provides.[citation needed]
Motorola-based systems
Sun originally used Motorola 68000 family central processing units for the Sun-1 through Sun-3 computer series. 
The Sun-1 employed a 68000 CPU, the Sun-2 series, a 68010. 
The Sun-3 series was based on the 68020, with the later Sun-3x using the 68030.[67]
SPARC-based systems
See also: SPARC
SPARCstation 1+
In 1987, the company began using SPARC, a RISC processor architecture of its own design, in its computer systems, starting with the Sun-4 line. 
SPARC was initially a 32-bit architecture (SPARC V7) until the introduction of the SPARC V9 architecture in 1995, which added 64-bit extensions.
Sun has developed several generations of SPARC-based computer systems, including the SPARCstation, Ultra and Sun Blade series of workstations, and the SPARCserver, Netra, Enterprise and Sun Fire line of servers.
In the early 1990s the company began to extend its product line to include large-scale symmetric multiprocessing servers, starting with the four-processor SPARCserver 600MP. 
This was followed by the 8-processor SPARCserver 1000 and 20-processor SPARCcenter 2000, which were based on work done in conjunction with Xerox PARC. 
In 1995 the company introduced Sun Ultra series machines that were equipped with the first 64-bit implementation of SPARC processors (UltraSPARC). 
In the late 1990s the transformation of product line in favor of large 64-bit SMP systems was accelerated by the acquisition of Cray Business Systems Division from Silicon Graphics.[32]
 Their 32-bit, 64-processor Cray Superserver 6400, related to the SPARCcenter, led to the 64-bit Sun Enterprise 10000 high-end server (otherwise known as Starfire).
In September 2004 Sun made available systems with UltraSPARC IV[68] which was the first multi-core SPARC processor. 
It was followed by UltraSPARC IV+ in September 2005[69] and its revisions with higher clock speeds in 2007.[70] 
These CPUs were used in the most powerful, enterprise class high-end CC-NUMA servers developed by Sun, such as Sun Fire E25K.
In November 2005 Sun launched the UltraSPARC T1, notable for its ability to concurrently run 32 threads of execution on 8 processor cores. 
Its intent was to drive more efficient use of CPU resources, which is of particular importance in data centers, where there is an increasing need to reduce power and air conditioning demands, much of which comes from the heat generated by CPUs. 
The T1 was followed in 2007 by the UltraSPARC T2, which extended the number of threads per core from 4 to 8.
 Sun has open sourced the design specifications of both the T1 and T2 processors via the OpenSPARC project.
In 2006, Sun has also ventured into the blade server (high density rack-mounted systems) market with the Sun Blade (distinct from the Sun Blade workstation).
In April 2007 Sun released the SPARC Enterprise server products, jointly designed by Sun and Fujitsu and based on Fujitsu SPARC64 VI and later processors. 
The M-class SPARC Enterprise systems include high-end reliability and availability features. 
Later T-series servers have also been badged SPARC Enterprise rather than Sun Fire.
In April 2008 Sun released servers with UltraSPARC T2 Plus, which is an SMP capable version of UltraSPARC T2, available in 2 or 4 processor configurations.
 It was the first CoolThreads CPU with multi-processor capability and it made possible to build standard rack-mounted servers that could simultaneously process up to massive 256 CPU threads in hardware (Sun SPARC Enterprise T5440),[71][72] which is considered a record in the industry.
Since 2010, all further development of Sun machines based on SPARC architecture (including new SPARC T-Series servers, SPARC T3 and T4 chips) is done as a part of Oracle Corporation hardware division.
x86-based systems
In the late 1980s, Sun also marketed an Intel 80386-based machine, the Sun386i; this was designed to be a hybrid system, running SunOS but at the same time supporting DOS applications. 
This only remained on the market for a brief time. 
A follow-up "486i" upgrade was announced but only a few prototype units were ever manufactured.
Sun's brief first foray into x86 systems ended in the early 1990s, as it decided to concentrate on SPARC and retire the last Motorola systems and 386i products, a move dubbed by McNealy as "all the wood behind one arrowhead".
 Even so, Sun kept its hand in the x86 world, as a release of Solaris for PC compatibles began shipping in 1993.
In 1997 Sun acquired Diba, Inc., followed later by the acquisition of Cobalt Networks in 2000, with the aim of building network appliances (single function computers meant for consumers). Sun also marketed a network computer (a term popularized and eventually trademarked by Oracle); the JavaStation was a diskless system designed to run Java applications.
Although none of these business initiatives were particularly successful, the Cobalt purchase gave Sun a toehold for its return to the x86 hardware market. 
In 2002, Sun introduced its first general purpose x86 system, the LX50, based in part on previous Cobalt system expertise. This was also Sun's first system announced to support Linux as well as Solaris.
In 2003, Sun announced a strategic alliance with AMD to produce x86/x64 servers based on AMD's Opteron processor; this was followed shortly by Sun's acquisition of Kealia, a startup founded by original Sun founder Andy Bechtolsheim, which had been focusing on high-performance AMD-based servers.
The following year, Sun launched the Opteron-based Sun Fire V20z and V40z servers, and the Java Workstation W1100z and W2100z workstations.
On September 12, 2005, Sun unveiled a new range of Opteron-based servers: the Sun Fire X2100, X4100 and X4200 servers.[73]
 These were designed from scratch by a team led by Bechtolsheim to address heat and power consumption issues commonly faced in data centers. 
In July 2006, the Sun Fire X4500 and X4600 systems were introduced, extending a line of x64 systems that support not only Solaris, but also Linux and Microsoft Windows.
On January 22, 2007, Sun announced a broad strategic alliance with Intel.[74] 
Intel endorsed Solaris as a mainstream operating system and as its mission critical Unix for its Xeon processor-based systems, and contributed engineering resources to OpenSolaris.[75] Sun began using the Intel Xeon processor in its x64 server line, starting with the Sun Blade X6250 server module introduced in June 2007.
On May 5, 2008, AMD announced its Operating System Research Center (OSRC) expanded its focus to include optimization to Sun's OpenSolaris and xVM virtualization products for AMD based processors.[76]
Software
Although Sun was initially known as a hardware company, its software history began with its founding in 1982; co-founder Bill Joy was one of the leading Unix developers of the time, having contributed the vi editor, the C shell, and significant work developing TCP/IP and the BSD Unix OS. 
Sun later developed software such as the Java programming language and acquired software such as StarOffice, VirtualBox and MySQL.
Sun used community-based and open-source licensing of its major technologies, and for its support of its products with other open source technologies. 
GNOME-based desktop software called Java Desktop System (originally code-named "Madhatter") was first distributed as a Linux implementation then offered as part of the Solaris operating system. 
Sun supported its Java Enterprise System (a middleware stack) on Linux.
 It released the source code for Solaris under the open-source Common Development and Distribution License, via the OpenSolaris community. 
Sun's positioning includes a commitment to indemnify users of some software from intellectual property disputes concerning that software. 
It offers support services on a variety of pricing bases, including per-employee and per-socket.
A 2006 report prepared for the EU by UNU-MERIT stated that Sun was the largest corporate contributor to open source movements in the world.[77] 
According to this report, Sun's open source contributions exceed the combined total of the next five largest commercial contributors.
Operating systems
Main article: Solaris (operating system)
Sun is best known for its Unix systems, which have a reputation for system stability and a consistent design philosophy.[citation needed]
Sun's first workstation shipped with UniSoft V7 Unix. 
Later in 1982 Sun began providing SunOS, a customized 4.1BSD Unix, as the operating system for its workstations.[citation needed]
In the late 1980s, AT&T tapped Sun to help them develop the next release of their branded UNIX, and in 1988 announced they would purchase up to a 20% stake in Sun.[78] 
UNIX System V Release 4 (SVR4) was jointly developed by AT&T and Sun; Sun used SVR4 as the foundation for Solaris 2.x, which became the successor to SunOS 4.1.x (later retrospectively named Solaris 1.x). 
By the mid-1990s, the ensuing Unix wars had largely subsided, AT&T had sold off their Unix interests, and the relationship between the two companies was significantly reduced.
From 1992 Sun also sold Interactive Unix, an operating system it acquired when it bought Interactive Systems Corporation from Eastman Kodak Company. 
This was a popular Unix variant for the PC platform and a major competitor to market leader SCO UNIX. 
Sun's focus on Interactive Unix diminished in favor of Solaris on both SPARC and x86 systems; it was dropped as a product in 2001.[citation needed]
Sun dropped the Solaris 2.x version numbering scheme after the Solaris 2.6 release (1997); the following version was branded Solaris 7. 
This was the first 64-bit release, intended for the new UltraSPARC CPUs based on the SPARC V9 architecture.
 Within the next four years, the successors Solaris 8 and Solaris 9 were released in 2000 and 2002 respectively.
Following several years of difficult competition and loss of server market share to competitors' Linux-based systems, Sun began to include Linux as part of its strategy in 2002. 
Sun supported both Red Hat Enterprise Linux and SUSE Linux Enterprise Server on its x64 systems; companies such as Canonical Ltd., 
Wind River Systems and MontaVista also supported their versions of Linux on Sun's SPARC-based systems.
In 2004, after having cultivated a reputation as one of Microsoft's most vocal antagonists, Sun entered into a joint relationship with them, resolving various legal entanglements between the two companies and receiving US$1.95 billion in settlement payments from them.[79]
 Sun supported Microsoft Windows on its x64 systems, and announced other collaborative agreements with Microsoft, including plans to support each other's virtualization environments.[80]
In 2005, the company released Solaris 10. The new version included a large number of enhancements to the operating system, as well as very novel features, previously unseen in the industry. 
Solaris 10 update releases continued through the next 8 years, the last release from Sun Microsystems being Solaris 10 10/09. 
The following updates were released by Oracle under the new license agreement; the final release is Solaris 10 1/13.[81]
Previously, Sun offered a separate variant of Solaris called Trusted Solaris, which included augmented security features such as multilevel security and a least privilege access model. 
Solaris 10 included many of the same capabilities as Trusted Solaris at the time of its initial release; Solaris 10 11/06 included Solaris Trusted Extensions, which give it the remaining capabilities needed to make it the functional successor to Trusted Solaris.
Following acquisition of Sun, Oracle Corporation continued to develop Solaris operating system, and released Oracle Solaris 11 in November 2011.
Java platform
Main article: Java platform
The Java platform was developed at Sun in the early 1990s with the objective of allowing programs to function regardless of the device they were used on, sparking the slogan "Write once, run anywhere" (WORA). 
While this objective was not entirely achieved (prompting the riposte "Write once, debug everywhere"), Java is regarded as being largely hardware- and operating system-independent.
Java was initially promoted as a platform for client-side applets running inside web browsers. 
Early examples of Java applications were the HotJava web browser and the HotJava Views suite. 
However, since then Java has been more successful on the server side of the Internet.
The platform consists of three major parts: the Java programming language, the Java Virtual Machine (JVM), and several Java Application Programming Interfaces (APIs). 
The design of the Java platform is controlled by the vendor and user community through the Java Community Process (JCP).
Java is an object-oriented programming language. 
Since its introduction in late 1995, it became one of the world's most popular programming languages.[82]
Java programs are compiled to byte code, which can be executed by any JVM, regardless of the environment.
The Java APIs provide an extensive set of library routines.
 These APIs evolved into the Standard Edition (Java SE), which provides basic infrastructure and GUI functionality; the Enterprise Edition (Java EE), aimed at large software companies implementing enterprise-class application servers; and the Micro Edition (Java ME), used to build software for devices with limited resources, such as mobile devices.
On November 13, 2006, Sun announced it would be licensing its Java implementation under the GNU General Public License; it released its Java compiler and JVM at that time.[83]
In February 2009 Sun entered a battle with Microsoft and Adobe Systems, which promoted rival platforms to build software applications for the Internet.[84]
 JavaFX was a development platform for music, video and other applications that builds on the Java programming language.[84]
Office suite
In 1999, Sun acquired the German software company StarDivision and with it the office suite StarOffice, which Sun later released as OpenOffice.org under both GNU LGPL and the SISSL (Sun Industry Standards Source License).
 OpenOffice.org supported Microsoft Office file formats (though not perfectly), was available on many platforms (primarily Linux, Microsoft Windows, Mac OS X, and Solaris) and was used in the open source community.
The principal differences between StarOffice and OpenOffice.org were that StarOffice was supported by Sun, was available as either a single-user retail box kit or as per-user blocks of licensing for the enterprise, and included a wider range of fonts and document templates and a commercial quality spellchecker.[85] 
StarOffice also contained commercially licensed functions and add-ons; in OpenOffice.org these were either replaced by open-source or free variants, or are not present at all. Both packages had native support for the OpenDocument format.
Virtualization and datacenter automation software
VirtualBox, purchased by Sun
In 2007, Sun announced the Sun xVM virtualization and datacenter automation product suite for commodity hardware. 
Sun also acquired VirtualBox in 2008. 
Earlier virtualization technologies from Sun like Dynamic System Domains and Dynamic Reconfiguration were specifically designed for high-end SPARC servers, and Logical Domains only supports the UltraSPARC T1/T2/T2 Plus server platforms. Sun marketed Sun Ops Center provisioning software for datacenter automation.
On the client side, Sun offered virtual desktop solutions. 
Desktop environments and applications could be hosted in a datacenter, with users accessing these environments from a wide range of client devices, including Microsoft Windows PCs, Sun Ray virtual display clients, Apple Macintoshes, PDAs or any combination of supported devices. A variety of networks were supported, from LAN to WAN or the public Internet. Virtual desktop products included Sun Ray Server Software, Sun Secure Global Desktop and Sun Virtual Desktop Infrastructure.
Database management systems
Sun acquired MySQL AB, the developer of the MySQL database in 2008 for US$1 billion.[86]
 CEO Jonathan Schwartz mentioned in his blog that optimizing the performance of MySQL was one of the priorities of the acquisition.[87] 
In February 2008, Sun began to publish results of the MySQL performance optimization work.[88] Sun contributed to the PostgreSQL project. 
On the Java platform, Sun contributed to and supported Java DB.
Other software
Sun offered other software products for software development and infrastructure services. 
Many were developed in house; others came from acquisitions, including Tarantella, Waveset Technologies,[50] SeeBeyond, and Vaau. 
Sun acquired many of the Netscape non-browser software products as part a deal involving Netscape's merger with AOL.[89]
 These software products were initially offered under the "iPlanet" brand; once the Sun-Netscape alliance ended, they were re-branded as "Sun ONE" (Sun Open Network Environment), and then the "Sun Java System".
Sun's middleware product was branded as the Java Enterprise System (or JES), and marketed for web and application serving, communication, calendaring, directory, identity management and service-oriented architecture. Sun's Open ESB and other software suites were available free of charge on systems running Solaris, Red Hat Enterprise Linux, HP-UX, and Windows, with support available optionally.
Sun developed data center management software products, which included the Solaris Cluster high availability software, and a grid management package called Sun Grid Engine and firewall software such as SunScreen. For Network Equipment Providers and telecommunications customers, Sun developed the Sun Netra High-Availability Suite.
Sun produced compilers and development tools under the Sun Studio brand, for building and developing Solaris and Linux applications. 
Sun entered the software as a service (SaaS) market with zembly, a social cloud-based computing platform and Project Kenai, an open-source project hosting service.
Storage
Sun sold its own storage systems to complement its system offerings; it has also made several storage-related acquisitions. 
On June 2, 2005, Sun announced it would purchase Storage Technology Corporation (StorageTek) for US$4.1 billion in cash, or $37.00 per share, a deal completed in August 2005.
In 2006, Sun introduced the Sun StorageTek 5800 System, the first application-aware programmable storage solution. 
In 2008, Sun contributed the source code of the StorageTek 5800 System under the BSD license.[90]
Sun announced the Sun Open Storage platform in 2008 built with open source technologies.
 In late 2008 Sun announced the Sun Storage 7000 Unified Storage systems (codenamed Amber Road). Transparent placement of data in the systems' solid-state drives (SSD) and conventional hard drives was managed by ZFS to take advantage of the speed of SSDs and the economy of conventional hard disks.
Other storage products included Sun Fire X4500 storage server and SAM-QFS filesystem and storage management software.
HPC solutions
Sun marketed the Sun Constellation System for High-Performance Computing (HPC). 
Even before the introduction of the Sun Constellation System in 2007, Sun's products were in use in many of the TOP500 systems and supercomputing centers:
Lustre: used by seven of the top 10 supercomputers in 2008, as well as other industries that need high-performance storage: six major oil companies (including BP, Shell, and ExxonMobil), chip-design (including Synopsys and Sony), and the movie-industry (including Harry Potter and Spider-Man).
Sun Fire X4500: used by high energy physics supercomputers to run dCache
Sun Grid Engine: a popular workload scheduler for clusters and computer farms
Sun Visualization System: allows users of the TeraGrid to remotely access the 3D rendering capabilities of the Maverick system at the University of Texas at Austin
Sun Modular Datacenter (Project Blackbox): two Sun MD S20 units are used by the Stanford Linear Accelerator Center
The Sun HPC ClusterTools product was a set of Message Passing Interface (MPI) libraries and tools for running parallel jobs on Solaris HPC clusters. 
Beginning with version 7.0, Sun switched from its own implementation of MPI to Open MPI, and donated engineering resources to the Open MPI project.
Sun was a participant in the OpenMP language committee. 
Sun Studio compilers and tools implemented the OpenMP specification for shared memory parallelization.
In 2006, Sun built the TSUBAME supercomputer, which was until June 2008 the fastest supercomputer in Asia. 
Sun built Ranger at the Texas Advanced Computing Center (TACC) in 2007. 
Ranger had a peak performance of over 500 TFLOPS, and was the 6th most powerful supercomputer on the TOP500 list in November 2008. 
Sun announced an OpenSolaris distribution that integrated many of Sun's HPC products and other 3rd-party solutions.
Staff
See also: List of notable Sun Microsystems employees
Notable Sun employees included John Gilmore, Whitfield Diffie, Radia Perlman, and Marc Tremblay. 
Sun was an early advocate of Unix-based networked computing, promoting TCP/IP and especially NFS, as reflected in the company's motto coined by John Gage. 
James Gosling led the team which developed the Java programming language. 
Jon Bosak led the creation of the XML specification at W3C.
Sun staff published articles on the company's blog site.[94]
 Staff were encouraged to use the site to blog on any aspect of their work or personal life, with few restrictions placed on staff, other than commercially confidential material. 
Jonathan I. Schwartz was one of the first CEOs of large companies to regularly blog; his postings were frequently quoted and analyzed in the press.
 In 2005, Sun Microsystems was one of the first Fortune 500 companies that instituted a formal Social Media program.
Acquisition by Oracle
Main article: Sun acquisition by Oracle
Logo used on hardware products by Oracle
Sun was sold to Oracle Corporation in 2009.[66]
 Sun's staff were asked to share anecdotes about their experiences at Sun. 
A web site containing videos, stories, and photographs from 27 years at Sun was made available on September 2, 2009.
 In October, Sun announced a second round of thousands of employees to be laid off, blamed partially on delays in approval of the merger.
 The transaction completed in early 2010.[4] 
In January 2011 Oracle agreed to pay $46 million to settle charges that it submitted false claims to US federal government agencies and paid "kickbacks" to systems integrators. 
In February 2011 Sun's former Menlo Park, California campus of about 1,000,000 square feet (93,000 m2) was sold, and it was announced that it would become headquarters for Facebook.
The sprawling facility built around an enclosed courtyard had been nicknamed "Sun Quentin".
On September 1, 2011, Sun India legally became part of Oracle. 
It had been delayed due to legal issues in Indian court.
Object-oriented programming (OOP) is a programming paradigm based on the concept of "objects", which are data structures that contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. 
A distinguishing feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated (objects have a notion of "this" or "self"). 
In OO programming, computer programs are designed by making them out of objects that interact with one another.[1][2] 
There is significant diversity in object-oriented programming, but most popular languages are class-based, meaning that objects are instances of classes, which typically also determines their type.
Many of the most widely used programming languages are multi-paradigm programming languages that support object-oriented programming to a greater or lesser degree, typically in combination with imperative, procedural programming. 
Significant object-oriented languages include Common Lisp, Python, C++, Objective-C, Smalltalk, Delphi, Java, Swift, C#, Perl, Ruby, and PHP.
Object-oriented programming by definition uses objects, but not all of the associated techniques and structures are supported directly in languages which claim to support OOP. 
The features listed below are, however, common among languages considered strongly class- and object-oriented (or multi-paradigm with OOP support), with notable exceptions mentioned.[3][
See also: Comparison of programming languages (object-oriented programming) and List of object-oriented programming terms
Shared with non-OOP predecessor languages
Object-oriented programming languages typically share low-level features with high-level procedural programming languages (which were invented first). 
The fundamental tools that can be used to construct a program include:
Variables which can store information formatted in a small number of built-in data types like integers and alphanumeric characters. 
This may include data structures like strings, lists and hash tables that are either built-in or result from combining variables using memory pointers
Procedures - also known as functions, methods, routines, or subroutines - that take input, generate output, and manipulate data. 
Modern languages include structured programming constructs like loops and conditionals.
Modular programming support provides the ability to group procedures into files and modules for organizational purposes. 
Modules are namespaced so code in one module will not be accidentally confused with the same procedure or variable name in another file or module.
Objects and classes
Languages that support object-oriented programming typically use inheritance for code reuse and extensibility in the form of either classes or prototypes. 
Those that use classes support two main concepts:
Classes - the definitions for the data format and available procedures for a given type or class of object; may also contain data and procedures (known as class methods) themselves
Objects - instances of classes
Objects sometimes correspond to things found in the real world. 
For example, a graphics program may have objects such as "circle", "square", "menu". 
An online shopping system might have objects such as "shopping cart", "customer", and "product".[7] 
Sometimes objects represent more abstract entities, like an object that represents an open file, or an object which provides the service of translating measurements from U.S. customary to metric.
Each object is said to be an instance of a particular class (for example, an object with its name field set to "Mary" might be an instance of class Employee). 
Procedures in object-oriented programming are known as methods; variables are also known as fields, members, attributes, or properties. 
This leads to the following terms:
Class variables - belong to the class as a whole; there is only one copy of each one
Instance variables or attributes - data that belongs to individual objects; every object has its own copy of each one
Member variables - refers to both the class and instance variables that are defined by a particular class
Class methods - belong to the class as a whole and have access only to class variables and inputs from the procedure call
Instance methods - belong to individual objects, and have access to instance variables for the specific object they are called on, inputs, and class variables
Objects are accessed somewhat like variables with complex internal structure, and in many languages are effectively pointers, serving as actual references to a single instance of said object in memory within a heap or stack. 
They provide a layer of abstraction which can be used to separate internal from external code. 
External code can use an object by calling a specific instance method with a certain set of input parameters, read an instance variable, or write to an instance variable. 
Objects are created by calling a special type of method in the class known as a constructor. 
A program may create many instances of the same class as it runs, which operate independently. 
This is an easy way for the same procedures to be used on different sets of data.
object-oriented programming that uses classes is sometimes called class-based programming, while prototype-based programming does not typically use classes. 
As a result, a significantly different yet analogous terminology is used to define the concepts of object and instance.
In some languages classes and objects can be composed using other concepts like traits and mixins.
Dynamic dispatch/message passing
By definition, it is the responsibility of the object, not the external code, to 'on-demand' select the procedural code to run/execute in response to a method call, typically by looking up the method at run time in a table associated with the object.
 This feature is known as dynamic dispatch, and distinguishes an object from an abstract data type (or module), which has a fixed (static) implementation of the operations for all instances. 
If there are multiple methods that might be run for a given name (which may require some language support), it is known as multiple dispatch.
A method call is also known as (message passing). 
It is conceptualized as a message (the name of the method and its input parameters) being passed to the object for dispatch.
Encapsulation
If a class disallows calling code from accessing internal object data and forces access through methods only, this is a strong form of abstraction or information hiding known as encapsulation. 
Some languages (Java, for example) let classes enforce access restrictions explicitly, for example denoting internal data with the private keyword and designating methods intended for use by code outside the class with the public keyword. 
Methods may also be designed public, private, or intermediate levels such as protected (which typically allows access from other objects of the same class, but not objects of a different class).
 In other languages (like Python) this is enforced only by convention (for example, naming private methods starting with an underscore). 
This is useful because it prevents the external code from being concerned with the internal workings of an object. 
This facilitates code refactoring, for example allowing the author of the class to change how objects of that class represent their data internally without changing any external code (as long as "public" method calls work the same way). 
It also encourages programmers to put all the code that is concerned with a certain set of data in the same class, which organizes it for easy comprehension by other programmers. 
Encapsulation is often used as a technique for encouraging decoupling.
Composition, inheritance, and delegation
Objects can contain other objects in their instance variables; this is known as object composition. 
For example, an object in the Employee class might contain (point to) an object in the Address class, in addition to its own instance variables like "first_name" and "position". 
Object composition is used to represent "has-a" relationships: every employee has an address, so every Employee object has a place to store an Address object.
Languages that support classes almost always support inheritance. 
This allows classes to be arranged in a hierarchy that represents "is-a-type-of" relationships. 
For example, class Employee might inherit from class Person. 
All the data and methods available to the parent class also appear in the child class with the same names. 
For example, class Person might define variables "first_name" and "last_name" with method "make_full_name()". 
These will also be available in class Employee, which might add the variables "position" and "salary". 
This technique allows easy re-use of the same procedures and data definitions, in addition to potentially mirroring real-world relationships in an intuitive way. 
These classes and subclasses correspond to sets and subsets in mathematical logic. 
Rather than utilizing database tables and programming subroutines, the developer utilizes objects the user may be more familiar with: objects from their application domain.[8]
Subclasses can override the methods defined by superclasses. 
Multiple inheritance is allowed in some languages, though this can make resolving overrides complicated. 
Some languages have special support for mixins, though in any language with multiple inheritance, a mixin is simply a class that does not represent an is-a-type-of relationship. 
Mixins are typically used to add the same methods to multiple classes. 
For example, class UnicodeConversionMixin might provide a method unicode_to_ascii() when included in class FileReader and class WebPageScraper, which don't share a common parent.
Abstract classes cannot be instantiated into objects; they exist only for the purpose of inheritance into other "concrete" classes which can be instantiated. 
In Java, the final keyword can be used to prevent a class from being subclassed.
The doctrine of composition over inheritance advocates implementing is-a-type-of relationships using composition instead of inheritance. 
For example, instead of inheriting from class Person, class Employee could give each Employee object an internal Person object, which it then has the opportunity to hide from external code even if class Person has many public attributes or methods. 
Some languages, like Go do not support inheritance at all.
The "open/closed principle" advocates that classes and functions "should be open for extension, but closed for modification".
Delegation is another language feature that can be used as an alternative to inheritance.
Polymorphism
Subtyping, a form of polymorphism, is when calling code can be agnostic as to whether an object belongs to a parent class or one of its descendants. 
For example, a function might call "make_full_name()" on an object, which will work whether the object is of class Person or class Employee. This is another type of abstraction which simplifies code external to the class hierarchy and enables strong separation of concerns.
Open recursion
In languages that support open recursion, object methods can call other methods on the same object (including themselves), typically using a special variable or keyword called this or self. This variable is late-bound; it allows a method defined in one class to invoke another method that is defined later, in some subclass thereof.
History
Terminology invoking "objects" and "oriented" in the modern sense of object-oriented programming made its first appearance at MIT in the late 1950s and early 1960s. 
In the environment of the artificial intelligence group, as early as 1960, "object" could refer to identified items (LISP atoms) with properties (attributes);[9][10] Alan Kay was later to cite a detailed understanding of LISP internals as a strong influence on his thinking in 1966.[11] 
Another early MIT example was Sketchpad created by Ivan Sutherland in 1960–61; in the glossary of the 1963 technical report based on his dissertation about Sketchpad, Sutherland defined notions of "object" and "instance" (with the class concept covered by "master" or "definition"), albeit specialized to graphical interaction.[12]
 Also, an MIT ALGOL version, AED-0, established a direct link between data structures ("plexes", in that dialect) and procedures, prefiguring what were later termed "messages", "methods", and "member functions".[13][14]
The formal programming concept of objects was introduced in the 1961s in Simula 67, a major revision of Simula I, a programming language designed for discrete event simulation, created by Ole-Johan Dahl and Kristen Nygaard of the Norwegian Computing Center in Oslo.[15]
 Simula 67 was influenced by SIMSCRIPT and C.A.R. "Tony" Hoare's proposed "record classes".[13][16] Simula introduced the notion of classes and instances or objects (as well as subclasses, virtual procedures, coroutines, and discrete event simulation) as part of an explicit programming paradigm. 
The language also used automatic garbage collection that had been invented earlier for the functional programming language Lisp.
 Simula was used for physical modeling, such as models to study and improve the movement of ships and their content through cargo ports. 
The ideas of Simula 67 influenced many later languages, including Smalltalk, derivatives of LISP (CLOS), Object Pascal, and C++.
The Smalltalk language, which was developed at Xerox PARC (by Alan Kay and others) in the 1970s, introduced the term object-oriented programming to represent the pervasive use of objects and messages as the basis for computation. 
Smalltalk creators were influenced by the ideas introduced in Simula 67, but Smalltalk was designed to be a fully dynamic system in which classes could be created and modified dynamically rather than statically as in Simula 67.[17] 
Smalltalk and with it OOP were introduced to a wider audience by the August 1981 issue of Byte Magazine.
In the 1970s, Kay's Smalltalk work had influenced the Lisp community to incorporate object-based techniques that were introduced to developers via the Lisp machine.
Experimentation with various extensions to Lisp (such as LOOPS and Flavors introducing multiple inheritance and mixins) eventually led to the Common Lisp Object System, which integrates functional programming and object-oriented programming and allows extension via a Meta-object protocol. 
In the 1980s, there were a few attempts to design processor architectures that included hardware support for objects in memory but these were not successful. Examples include the Intel iAPX 432 and the Linn Smart Rekursiv.
In 1985, Bertrand Meyer produced the first design of the Eiffel language. 
Focused on software quality, Eiffel is among the purely object-oriented languages, but differs in the sense that the language itself is not only a programming language, but a notation supporting the entire software lifecycle. 
Meyer described the Eiffel software development method, based on a small number of key ideas from software engineering and computer science, in Object-Oriented Software Construction. 
Essential to the quality focus of Eiffel is Meyer's reliability mechanism, Design by Contract, which is an integral part of both the method and language.
Object-oriented programming developed as the dominant programming methodology in the early and mid 1990s when programming languages supporting the techniques became widely available. 
These included Visual FoxPro 3.0,[18][19][20] C++,[21] and Delphi[citation needed]. 
Its dominance was further enhanced by the rising popularity of graphical user interfaces, which rely heavily upon object-oriented programming techniques. 
An example of a closely related dynamic GUI library and OOP language can be found in the Cocoa frameworks on Mac OS X, written in Objective-C, an object-oriented, dynamic messaging extension to C based on Smalltalk. 
OOP toolkits also enhanced the popularity of event-driven programming (although this concept is not limited to OOP).
At ETH Zürich, Niklaus Wirth and his colleagues had also been investigating such topics as data abstraction and modular programming (although this had been in common use in the 1960s or earlier). 
Modula-2 (1978) included both, and their succeeding design, Oberon, included a distinctive approach to object orientation, classes, and such.
Object-oriented features have been added to many previously existing languages, including Ada, BASIC, Fortran, Pascal, and COBOL. 
Adding these features to languages that were not initially designed for them often led to problems with compatibility and maintainability of code.
More recently, a number of languages have emerged that are primarily object-oriented, but that are also compatible with procedural methodology. 
Two such languages are Python and Ruby. 
Probably the most commercially important recent object-oriented languages are Java, developed by Sun Microsystems, as well as C# and Visual Basic.NET (VB.NET), both designed for Microsoft's .NET platform. 
Each of these two frameworks shows, in its own way, the benefit of using OOP by creating an abstraction from implementation. 
VB.NET and C# support cross-language inheritance, allowing classes defined in one language to subclass classes defined in the other language.
OOP languages
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. 
Unsourced material may be challenged and removed. (August 2009)
See also: List of object-oriented programming languages
Simula (1967) is generally accepted as being the first language with the primary features of an object-oriented language. 
It was created for making simulation programs, in which what came to be called objects were the most important information representation. 
Smalltalk (1972 to 1980) is an early example, and the one with which much of the theory of object-oriented programming was developed. 
Concerning the degree of object orientation, the following distinctions can be made:
Languages called "pure" OO languages, because everything in them is treated consistently as an object, from primitives such as characters and punctuation, all the way up to whole classes, prototypes, blocks, modules, etc. 
They were designed specifically to facilitate, even enforce, OO methods. 
Examples: Eiffel, Emerald,[22] JADE, Obix, Ruby, Scala, Smalltalk, Self.
Languages designed mainly for OO programming, but with some procedural elements. 
Examples: Delphi/Object Pascal, C++, Java, C#, VB.NET.
Languages that are historically procedural languages, but have been extended with some OO features. 
Examples: Pascal, Visual Basic (derived from BASIC), MATLAB, Fortran, Perl, COBOL 2002, PHP, ABAP, Ada 95.
Languages with most of the features of objects (classes, methods, inheritance), but in a distinctly original form. 
Examples: Oberon (Oberon-1 or Oberon-2).
Languages with abstract data type support which may be used to resemble OO programming, but without all features of object-orientation. 
This includes object-based and prototype-based languages. Examples: Modula-2, Pliant, CLU, JavaScript, Lua.
Chameleon languages that support multiple paradigms, including OO.
 Tcl stands out among these for TclOO, a hybrid object system that supports both prototype-based programming and class-based OO.
OOP in dynamic languages
In recent years, object-oriented programming has become especially popular in dynamic programming languages. 
Python, PowerShell, Ruby and Groovy are dynamic languages built on OOP principles, while Perl and PHP have been adding object-oriented features since Perl 5 and PHP 4, and ColdFusion since version 6.
The Document Object Model of HTML, XHTML, and XML documents on the Internet has bindings to the popular JavaScript/ECMAScript language. 
JavaScript is perhaps the best known prototype-based programming language, which employs cloning from prototypes rather than inheriting from a class (contrast to class-based programming).
 Before ECMAScript 6, only a prototype-based object model was supported. 
Another scripting language that takes this approach is Lua.
OOP in a network protocol
The messages that flow between computers to request services in a client-server environment can be designed as the linearizations of objects defined by class objects known to both the client and the server. 
For example, a simple linearized object would consist of a length field, a code point identifying the class, and a data value. 
A more complex example would be a command consisting of the length and code point of the command and values consisting of linearized objects representing the command's parameters. 
Each such command must be directed by the server to an object whose class (or superclass) recognizes the command and is able to provide the requested service. 
Clients and servers are best modeled as complex object-oriented structures. Distributed Data Management Architecture (DDM) took this approach and used class objects to define objects at four levels of a formal hierarchy:
Fields defining the data values that form messages, such as their length, codepoint and data values.
Objects and collections of objects similar to what would be found in a Smalltalk program for messages and parameters.
Managers similar to AS/400 objects, such as a directory to files and files consisting of metadata and records. Managers conceptually provide memory and processing resources for their contained objects.
A client or server consisting of all the managers necessary to implement a full processing environment, supporting such aspects as directory services, security and concurrency control.
The initial version of DDM defined distributed file services. 
It was later extended to be the foundation of Distributed Relational Database Architecture (DRDA).
Design patterns
Challenges of object-oriented design are addressed by several methodologies.
 Most common is known as the design patterns codified by Gamma et al.. 
More broadly, the term "design patterns" can be used to refer to any general, repeatable solution to a commonly occurring problem in software design. 
Some of these commonly occurring problems have implications and solutions particular to object-oriented development.
Inheritance and behavioral subtyping
See also: Object-oriented design
It is intuitive to assume that inheritance creates a semantic "is a" relationship, and thus to infer that objects instantiated from subclasses can always be safely used instead of those instantiated from the superclass. 
This intuition is unfortunately false in most OOP languages, in particular in all those that allow mutable objects. 
Subtype polymorphism as enforced by the type checker in OOP languages (with mutable objects) cannot guarantee behavioral subtyping in any context. 
Behavioral subtyping is undecidable in general, so it cannot be implemented by a program (compiler). 
Class or object hierarchies must be carefully designed, considering possible incorrect uses that cannot be detected syntactically. 
This issue is known as the Liskov substitution principle.
Gang of Four design patterns
Main article: Design pattern (computer science)
Design Patterns: Elements of Reusable Object-Oriented Software is an influential book published in 1995 by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, often referred to humorously as the "Gang of Four". 
Along with exploring the capabilities and pitfalls of object-oriented programming, it describes 23 common programming problems and patterns for solving them. 
As of April 2007, the book was in its 36th printing.
The book describes the following patterns:
Creational patterns (5): Factory method pattern, Abstract factory pattern, Singleton pattern, Builder pattern, Prototype pattern
Structural patterns (7): Adapter pattern, Bridge pattern, Composite pattern, Decorator pattern, Facade pattern, Flyweight pattern, Proxy pattern
Behavioral patterns (11): Chain-of-responsibility pattern, Command pattern, Interpreter pattern, Iterator pattern, Mediator pattern, Memento pattern, Observer pattern, State pattern, Strategy pattern, Template method pattern, Visitor pattern
Object-orientation and databases
Main articles: Object-relational impedance mismatch, Object-relational mapping and Object database
Both object-oriented programming and relational database management systems (RDBMSs) are extremely common in software today. 
Since relational databases don't store objects directly (though some RDBMSs have object-oriented features to approximate this), there is a general need to bridge the two worlds. 
The problem of bridging object-oriented programming accesses and data patterns with relational databases is known as object-relational impedance mismatch. 
There are a number of approaches to cope with this problem, but no general solution without downsides.[23] 
One of the most common approaches is object-relational mapping, as found in IDE languages such as Visual FoxPro and libraries such as Java Data Objects and Ruby on Rails' ActiveRecord.
There are also object databases that can be used to replace RDBMSs, but these have not been as technically and commercially successful as RDBMSs.
Real-world modeling and relationships
OOP can be used to associate real-world objects and processes with digital counterparts. 
However, not everyone agrees that OOP facilitates direct real-world mapping (see Criticism section) or that real-world mapping is even a worthy goal; Bertrand Meyer argues in Object-Oriented Software Construction[24] that a program is not a model of the world but a model of some part of the world; "Reality is a cousin twice removed". 
At the same time, some principal limitations of OOP had been noted.[25]
 For example, the circle-ellipse problem is difficult to handle using OOP's concept of inheritance.
Steve Yegge and others noted that natural languages lack the OOP approach of strictly prioritizing things (objects/nouns) before actions (methods/verbs).[26]
 This problem may cause OOP to suffer more convoluted solutions than procedural programming.[27]
OOP and control flow
OOP was developed to increase the reusability and maintainability of source code.[28]
 Transparent representation of the control flow had no priority and was meant to be handled by a compiler. 
With the increasing relevance of parallel hardware and multithreaded coding, developing transparent control flow becomes more important, something hard to achieve with OOP.
Responsibility- vs. data-driven design
Responsibility-driven design defines classes in terms of a contract, that is, a class should be defined around a responsibility and the information that it shares. 
This is contrasted by Wirfs-Brock and Wilkerson with data-driven design, where classes are defined around the data-structures that must be held. 
The authors hold that responsibility-driven design is preferable.
SOLID and GRASP guidelines
SOLID is a mnemonic invented by Michael Feathers that stands for and advocates five programming practices:
Single responsibility principle
Open/closed principle
Liskov substitution principle
Interface segregation principle
Dependency inversion principle
GRASP (General Responsibility Assignment Software Patterns) is another set of guidelines advocated by Craig Larman.
Criticism
The OOP paradigm has been criticised for a number of reasons, including not meeting its stated goals of reusability and modularity,[33][34] and for overemphasizing one aspect of software design and modeling (data/objects) at the expense of other important aspects (computation/algorithms).[35][36]
Luca Cardelli has claimed that OOP code is "intrinsically less efficient" than procedural code, that OOP can take longer to compile, and that OOP languages have "extremely poor modularity properties with respect to class extension and modification", and tend to be extremely complex.[33]
 The latter point is reiterated by Joe Armstrong, the principal inventor of Erlang, who is quoted as saying:[34]
The problem with object-oriented languages is they've got all this implicit environment that they carry around with them. 
You wanted a banana but what you got was a gorilla holding the banana and the entire jungle.
A study by Potok et al. has shown no significant difference in productivity between OOP and procedural approaches.[37]
Christopher J. Date stated that critical comparison of OOP to other technologies, relational in particular, is difficult because of lack of an agreed-upon and rigorous definition of OOP;[38] however, Date and Darwen have proposed a theoretical foundation on OOP that uses OOP as a kind of customizable type system to support RDBMS.[39]
In an article Lawrence Krubner claimed that compared to other languages (LISP dialects, functional languages, etc.) 
OOP languages have no unique strengths, and inflict a heavy burden of unneeded complexity.[40]
Alexander Stepanov compares object orientation unfavourably to generic programming:[35]
I find OOP technically unsound..
 It attempts to decompose the world in terms of interfaces that vary on a single type. 
To deal with the real problems you need multisorted algebras — families of interfaces that span multiple types.
 I find OOP philosophically unsound. 
It claims that everything is an object. 
Even if it is true it is not very interesting — saying that everything is an object is saying nothing at all.
Paul Graham has suggested that OOP's popularity within large companies is due to "large (and frequently changing) groups of mediocre programmers". 
According to Graham, the discipline imposed by OOP prevents any one programmer from "doing too much damage".[41]
Steve Yegge noted that, as opposed to functional programming:[42]
Object Oriented Programming puts the Nouns first and foremost. 
Why would you go to such lengths to put one part of speech on a pedestal? Why should one kind of concept take precedence over another? It's not as if OOP has suddenly made verbs less important in the way we actually think. 
It's a strangely skewed perspective.
Rich Hickey, creator of Clojure, described object systems as overly simplistic models of the real world. 
He emphasized the inability of OOP to model time properly, which is getting increasingly problematic as software systems become more concurrent.[36]
Eric S. Raymond, a Unix programmer and open-source software advocate, has been critical of claims that present object-oriented programming as the "One True Solution", and has written that object-oriented programming languages tend to encourage thickly layered programs that destroy transparency.[43] 
Raymond compares this unfavourably to the approach taken with Unix and the C programming language.[43]
Formal semantics
see also: Formal semantics of programming languages
Objects are the run-time entities in an object-oriented system. 
They may represent a person, a place, a bank account, a table of data, or any item that the program has to handle.
There have been several attempts at formalizing the concepts used in object-oriented programming. 
The following concepts and constructs have been used as interpretations of OOP concepts:
algebraic data types[44]
abstract data types (which have existential types) allow the definition of modules but these do not support dynamic dispatch
recursive types
encapsulated state
inheritance
records are basis for understanding objects if function literals can be stored in fields (like in functional programming languages), but the actual calculi need be considerably more complex to incorporate essential features of OOP. 
Several extensions of System F<: that deal with mutable objects have been studied;[45] these allow both subtype polymorphism and parametric polymorphism (generics)
Attempts to find a consensus definition or theory behind objects have not proven very successful (however, see Abadi & Cardelli, A Theory of Objects[45] for formal definitions of many OOP concepts and constructs), and often diverge widely. 
For example, some definitions focus on mental activities, and some on program structuring. 
One of the simpler definitions is that OOP is the act of using "map" data structures or arrays that can contain functions and pointers to other maps, all with some syntactic and scoping sugar on top. 
Inheritance can be performed by cloning the maps (sometimes called "prototyping").
In computer science, a data structure is a particular way of organizing data in a computer so that it can be used efficiently.[1][2] 
Data structures can implement one or more particular abstract data types (ADT), which are the means of specifying the contract of operations and their complexity.
 In comparison, a data structure is a concrete implementation of the contract provided by an ADT.
Different kinds of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks.
 For example, relational databases most commonly use B-tree indexes for data retrieval,[3] while compiler implementations usually use hash tables to look up identifiers.
Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. 
Usually, efficient data structures are key to designing efficient algorithms. 
Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design.
 Storing and retrieving can be carried out on data stored in both main memory and in secondary memory.
Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer—a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. 
Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations; while the linked data structures are based on storing addresses of data items within the structure itself. 
Many data structures use both principles, sometimes combined in non-trivial ways (as in XOR linking).
The implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. 
The efficiency of a data structure cannot be analyzed separately from those operations. 
This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).
Examples
Main article: List of data structures
There are numerous types of data structures, generally built upon simpler primitive data types:
An array (also called list) is a number of elements in a specific order, typically all of the same type.
Elements are accessed using an integer index to specify which element is required (although the elements may be of almost any type). 
Typical implementations allocate contiguous memory words for the elements of arrays (but this is not always a necessity). Arrays may be fixed-length or resizable.
An associative array (also called dictionary or map) is a more flexible variation on an array, in which name-value pairs can be added and deleted freely. 
A hash table is a common implementation of an associative array.
A record (also called tuple or struct) is an aggregate data structure. 
A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names. 
The elements of records are usually called fields or members.
A union is a data structure that specifies which of a number of permitted primitive types may be stored in its instances, e.g. float or long integer. 
Contrast with a record, which could be defined to contain a float and an integer; whereas in a union, there is only one value at a time. 
Enough space is allocated to contain the widest member datatype.
A tagged union (also called variant, variant record, discriminated union, or disjoint union) contains an additional field indicating its current type, for enhanced type safety.
A set is an abstract data structure that can store specific values, in no particular order and with no duplicate values.
A graph and a tree are linked abstract data structures composed of nodes. 
Each node contains a value and one or more pointers to other nodes arranged in a hierarchy. 
Graphs can be used to represent networks, while variants of trees can be used for sorting and searching, having their nodes arranged in some relative order based on their values.
A class is a data structure that contains data fields, like a record, as well as various methods which operate on the contents of the record. 
In the context of object-oriented programming, records are known as plain old data structures to distinguish them from classes.[citation needed]
Language support
Most assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack built-in support for data structures. 
On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays. 
For example, the C and Pascal languages support structs and records, respectively, in addition to vectors (one-dimensional arrays) and multi-dimensional arrays.[4][5]
Most programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. 
Modern languages usually come with standard libraries that implement the most common data structures. 
Examples are the C++ Standard Template Library, the Java Collections Framework, and Microsoft's .NET Framework.
Modern languages also generally support modular programming, the separation between the interface of a library module and its implementation.
 Some provide opaque data types that allow clients to hide implementation details. 
Object-oriented programming languages, such as C++, Java and Smalltalk may use classes for this purpose.
Many known data structures have concurrent versions that allow multiple computing threads to access the data structure simultaneously.
In computer science, an abstract data type (ADT) is a mathematical model for data types where a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. 
This contrasts with data structures, which are concrete representations of data, and are the point of view of an implementer, not a user.
Formally, an ADT may be defined as a "class of objects whose logical behavior is defined by a set of values and a set of operations";[1] this is analogous to an algebraic structure in mathematics. 
What is meant by "behavior" varies by author, with the two main types of formal specifications for behavior being axiomatic (algebraic) specification and an abstract model;[2] these correspond to axiomatic semantics and operational semantics of an abstract machine, respectively. 
Some authors also include the computational complexity ("cost"), both in terms of time (for computing operations) and space (for representing values).
In practice many common data types are not ADTs, as the abstraction is not perfect, and users must be aware of issues like arithmetic overflow that are due to the representation. 
For example, integers are often stored as fixed width values (32-bit or 64-bit binary numbers), and thus experience integer overflow if the maximum value is exceeded.
ADTs are a theoretical concept in computer science, used in the design and analysis of algorithms, data structures, and software systems, and do not correspond to specific features of computer languages—mainstream computer languages do not directly support formally specified ADTs. 
However, various language features correspond to certain aspects of ADTs, and are easily confused with ADTs proper; these include abstract types, opaque data types, protocols, and design by contract. 
ADTs were first proposed by Barbara Liskov and Stephen N. Zilles in 1974, as part of the development of the CLU language.
For example, integers are an ADT, defined as the values …, −2, −1, 0, 1, 2, …, and by the operations of addition, subtraction, multiplication, and division, together with greater than, less than, etc., which behave according to familiar mathematics (with care for integer division), independently of how the integers are represented by the computer.[a] 
Explicitly, "behavior" includes obeying various axioms (associativity and commutativity of addition etc.), and preconditions on operations (cannot divide by zero). 
Typically integers are represented in a data structure as binary numbers, most often as two's complement, but might be binary-coded decimal or in ones' complement, but the user is abstracted from the concrete choice of representation, and can simply use the data as integers.
An ADT consists not only of operations, but also of values of the underlying data and of constraints on the operations.
 An "interface" typically refers only to the operations, and perhaps some of the constraints on the operations, notably pre-conditions and post-conditions, but not other constraints, such as relations between the operations.
For example, an abstract stack, which is a last-in-first-out structure, could be defined by three operations: push, that inserts a data item onto the stack; pop, that removes a data item from it; and peek or top, that accesses a data item on top of the stack without removal. 
An abstract queue, which is a first-in-first-out structure, would also have three operations: enqueue, that inserts a data item into the queue; dequeue, that removes the first data item from it; and front, that accesses and serves the first data item in the queue. 
There would be no way of differentiating these two data types, unless a mathematical constraint is introduced that for a stack specifies that each pop always returns the most recently pushed item that has not been popped yet. 
When analyzing the efficiency of algorithms that use stacks, one may also specify that all operations take the same time no matter how many data items have been pushed into the stack, and that the stack uses a constant amount of storage for each element.
Introduction
Abstract data types are purely theoretical entities, used (among other things) to simplify the description of abstract algorithms, to classify and evaluate data structures, and to formally describe the type systems of programming languages.
 However, an ADT may be implemented by specific data types or data structures, in many ways and in many programming languages; or described in a formal specification language. 
ADTs are often implemented as modules: the module's interface declares procedures that correspond to the ADT operations, sometimes with comments that describe the constraints. 
This information hiding strategy allows the implementation of the module to be changed without disturbing the client programs.
The term abstract data type can also be regarded as a generalised approach of a number of algebraic structures, such as lattices, groups, and rings.[4]
 The notion of abstract data types is related to the concept of data abstraction, important in object-oriented programming and design by contract methodologies for software development.[citation needed]
Defining an abstract data type
An abstract data type is defined as a mathematical model of the data objects that make up a data type as well as the functions that operate on these objects. 
There are no standard conventions for defining them.
 A broad division may be drawn between "imperative" and "functional" definition styles.
Imperative-style definition
In the philosophy of imperative programming languages, an abstract data structure is conceived as an entity that is mutable—meaning that it may be in different states at different times. 
Some operations may change the state of the ADT; therefore, the order in which operations are evaluated is important, and the same operation on the same entities may have different effects if executed at different times—just like the instructions of a computer, or the commands and procedures of an imperative language. 
To underscore this view, it is customary to say that the operations are executed or applied, rather than evaluated.
 The imperative style is often used when describing abstract algorithms.
 This is described by Donald Knuth and can be referenced from here The Art of Computer Programming.
Abstract variable
Imperative-style definitions of ADT often depend on the concept of an abstract variable, which may be regarded as the simplest non-trivial ADT. 
An abstract variable V is a mutable entity that admits two operations:
store(V, x) where x is a value of unspecified nature;
fetch(V), that yields a value,
with the constraint that
fetch(V) always returns the value x used in the most recent store(V, x) operation on the same variable V.
As in so many programming languages, the operation store(V, x) is often written V ← x (or some similar notation), and fetch(V) is implied whenever a variable V is used in a context where a value is required. 
Thus, for example, V ← V + 1 is commonly understood to be a shorthand for store(V,fetch(V) + 1).
In this definition, it is implicitly assumed that storing a value into a variable U has no effect on the state of a distinct variable V.
 To make this assumption explicit, one could add the constraint that
if U and V are distinct variables, the sequence { store(U, x); store(V, y) } is equivalent to { store(V, y); store(U, x) }.
More generally, ADT definitions often assume that any operation that changes the state of one ADT instance has no effect on the state of any other instance (including other instances of the same ADT) — unless the ADT axioms imply that the two instances are connected (aliased) in that sense. 
For example, when extending the definition of abstract variable to include abstract records, the operation that selects a field from a record variable R must yield a variable V that is aliased to that part of R.
The definition of an abstract variable V may also restrict the stored values x to members of a specific set X, called the range or type of V. 
As in programming languages, such restrictions may simplify the description and analysis of algorithms, and improve their readability.
Note that this definition does not imply anything about the result of evaluating fetch(V) when V is un-initialized, that is, before performing any store operation on V. 
An algorithm that does so is usually considered invalid, because its effect is not defined. 
(However, there are some important algorithms whose efficiency strongly depends on the assumption that such a fetch is legal, and returns some arbitrary value in the variable's range.[citation needed])
Instance creation
Some algorithms need to create new instances of some ADT (such as new variables, or new stacks). 
To describe such algorithms, one usually includes in the ADT definition a create() operation that yields an instance of the ADT, usually with axioms equivalent to
the result of create() is distinct from any instance in use by the algorithm.
This axiom may be strengthened to exclude also partial aliasing with other instances. 
On the other hand, this axiom still allows implementations of create() to yield a previously created instance that has become inaccessible to the program.
Preconditions, postconditions, and invariants
In imperative-style definitions, the axioms are often expressed by preconditions, that specify when an operation may be executed; postconditions, that relate the states of the ADT before and after the execution of each operation; and invariants, that specify properties of the ADT that are not changed by the operations.
Example: abstract stack (imperative)
As another example, an imperative-style definition of an abstract stack could specify that the state of a stack S can be modified only by the operations
push(S, x), where x is some value of unspecified nature;
pop(S), that yields a value as a result,
with the constraint that
For any value x and any abstract variable V, the sequence of operations { push(S, x); V ← pop(S) } is equivalent to V ← x.
Since the assignment V ← x, by definition, cannot change the state of S, this condition implies that V ← pop(S) restores S to the state it had before the push(S, x). 
From this condition and from the properties of abstract variables, it follows, for example, that the sequence
{ push(S, x); push(S, y); U ← pop(S); push(S, z); V ← pop(S); W ← pop(S) }
where x, y, and z are any values, and U, V, W are pairwise distinct variables, is equivalent to
{ U ← y; V ← z; W ← x }
Here it is implicitly assumed that operations on a stack instance do not modify the state of any other ADT instance, including other stacks; that is,
For any values x, y, and any distinct stacks S and T, the sequence { push(S, x); push(T, y) } is equivalent to { push(T, y); push(S, x) }.
An abstract stack definition usually includes also a Boolean-valued function empty(S) and a create() operation that returns a stack instance, with axioms equivalent to
create() ≠ S for any stack S (a newly created stack is distinct from all previous stacks);
empty(create()) (a newly created stack is empty);
not empty(push(S, x)) (pushing something into a stack makes it non-empty).
Single-instance style
Sometimes an ADT is defined as if only one instance of it existed during the execution of the algorithm, and all operations were applied to that instance, which is not explicitly notated. 
For example, the abstract stack above could have been defined with operations push(x) and pop(), that operate on the only existing stack. 
ADT definitions in this style can be easily rewritten to admit multiple coexisting instances of the ADT, by adding an explicit instance parameter (like S in the previous example) to every operation that uses or modifies the implicit instance.
On the other hand, some ADTs cannot be meaningfully defined without assuming multiple instances. 
This is the case when a single operation takes two distinct instances of the ADT as parameters. 
For an example, consider augmenting the definition of the abstract stack with an operation compare(S, T) that checks whether the stacks S and T contain the same items in the same order.
Functional-style definition
Another way to define an ADT, closer to the spirit of functional programming, is to consider each state of the structure as a separate entity. 
In this view, any operation that modifies the ADT is modeled as a mathematical function that takes the old state as an argument, and returns the new state as part of the result. 
Unlike the imperative operations, these functions have no side effects.
 Therefore, the order in which they are evaluated is immaterial, and the same operation applied to the same arguments (including the same input states) will always return the same results (and output states).
In the functional view, in particular, there is no way (or need) to define an "abstract variable" with the semantics of imperative variables (namely, with fetch and store operations). 
Instead of storing values into variables, one passes them as arguments to functions.
Example: abstract stack (functional)
For example, a complete functional-style definition of an abstract stack could use the three operations:
push: takes a stack state and an arbitrary value, returns a stack state;
top: takes a stack state, returns a value;
pop: takes a stack state, returns a stack state.
In a functional-style definition there is no need for a create operation. 
Indeed, there is no notion of "stack instance". 
The stack states can be thought of as being potential states of a single stack structure, and two stack states that contain the same values in the same order are considered to be identical states. 
This view actually mirrors the behavior of some concrete implementations, such as linked lists with hash cons.
Instead of create(), a functional-style definition of an abstract stack may assume the existence of a special stack state, the empty stack, designated by a special symbol like Λ or "()"; or define a bottom() operation that takes no arguments and returns this special stack state. 
Note that the axioms imply that
push(Λ, x) ≠ Λ.
In a functional-style definition of a stack one does not need an empty predicate: instead, one can test whether a stack is empty by testing whether it is equal to Λ.
Note that these axioms do not define the effect of top(s) or pop(s), unless s is a stack state returned by a push. 
Since push leaves the stack non-empty, those two operations are undefined (hence invalid) when s = Λ. 
On the other hand, the axioms (and the lack of side effects) imply that push(s, x) = push(t, y) if and only if x = y and s = t.
As in some other branches of mathematics, it is customary to assume also that the stack states are only those whose existence can be proved from the axioms in a finite number of steps. 
In the abstract stack example above, this rule means that every stack is a finite sequence of values, that becomes the empty stack (Λ) after a finite number of pops. 
By themselves, the axioms above do not exclude the existence of infinite stacks (that can be poped forever, each time yielding a different state) or circular stacks (that return to the same state after a finite number of pops).
 In particular, they do not exclude states s such that pop(s) = s or push(s, x) = s for some x. 
However, since one cannot obtain such stack states with the given operations, they are assumed "not to exist".
Whether to include complexity
Aside from the behavior in terms of axioms, it is also possible to include, in the definition of an ADT operation, their algorithmic complexity. 
Alexander Stepanov, designer of the C++ Standard Template Library, included complexity guarantees in the STL specification, arguing:
The reason for introducing the notion of abstract data types was to allow interchangeable software modules. 
You cannot have interchangeable modules unless these modules share similar complexity behavior.
 If I replace one module with another module with the same functional behavior but with different complexity tradeoffs, the user of this code will be unpleasantly surprised. 
I could tell him anything I like about data abstraction, and he still would not want to use the code.
 Complexity assertions have to be part of the interface.
— Alexander Stepanov[5]
Advantages of abstract data typing
This section needs additional citations for verification.
 Please help improve this article by adding citations to reliable sources. 
Unsourced material may be challenged and removed. (May 2011)
Encapsulation
Abstraction provides a promise that any implementation of the ADT has certain properties and abilities; knowing these is all that is required to make use of an ADT object. 
The user does not need any technical knowledge of how the implementation works to use the ADT. 
In this way, the implementation may be complex but will be encapsulated in a simple interface when it is actually used.
Localization of change
Code that uses an ADT object will not need to be edited if the implementation of the ADT is changed. 
Since any changes to the implementation must still comply with the interface, and since code using an ADT object may only refer to properties and abilities specified in the interface, changes may be made to the implementation without requiring any changes in code where the ADT is used.
Flexibility
Different implementations of the ADT, having all the same properties and abilities, are equivalent and may be used somewhat interchangeably in code that uses the ADT.
 This gives a great deal of flexibility when using ADT objects in different situations. 
For example, different implementations of the ADT may be more efficient in different situations; it is possible to use each in the situation where they are preferable, thus increasing overall efficiency.
Typical operations
Some operations that are often specified for ADTs (possibly under other names) are
compare(s, t), that tests whether two instances' states are equivalent in some sense;
hash(s), that computes some standard hash function from the instance's state;
print(s) or show(s), that produces a human-readable representation of the instance's state.
In imperative-style ADT definitions, one often finds also
create(), that yields a new instance of the ADT;
initialize(s), that prepares a newly created instance s for further operations, or resets it to some "initial state";
copy(s, t), that puts instance s in a state equivalent to that of t;
clone(t), that performs s ← create(), copy(s, t), and returns s;
free(s) or destroy(s), that reclaims the memory and other resources used by s.
The free operation is not normally relevant or meaningful, since ADTs are theoretical entities that do not "use memory". 
However, it may be necessary when one needs to analyze the storage used by an algorithm that uses the ADT. 
In that case one needs additional axioms that specify how much memory each ADT instance uses, as a function of its state, and how much of it is returned to the pool by free.
Examples
Implementing an ADT means providing one procedure or function for each abstract operation. 
The ADT instances are represented by some concrete data structure that is manipulated by those procedures, according to the ADT's specifications.
Usually there are many ways to implement the same ADT, using several different concrete data structures. 
Thus, for example, an abstract stack can be implemented by a linked list or by an array.
In order to prevent clients from depending on the implementation, an ADT is often packaged as an opaque data type in one or more modules, whose interface contains only the signature (number and types of the parameters and results) of the operations.
 The implementation of the module—namely, the bodies of the procedures and the concrete data structure used—can then be hidden from most clients of the module. 
This makes it possible to change the implementation without affecting the clients. 
If the implementation is exposed, it is known instead as a transparent data type.
When implementing an ADT, each instance (in imperative-style definitions) or each state (in functional-style definitions) is usually represented by a handle of some sort.[7]
Modern object-oriented languages, such as C++ and Java, support a form of abstract data types. 
When a class is used as a type, it is an abstract type that refers to a hidden representation. 
In this model an ADT is typically implemented as a class, and each instance of the ADT is usually an object of that class.
 The module's interface typically declares the constructors as ordinary procedures, and most of the other ADT operations as methods of that class. 
However, such an approach does not easily encapsulate multiple representational variants found in an ADT. It also can undermine the extensibility of object-oriented programs. 
In a pure object-oriented program that uses interfaces as types, types refer to behaviors not representations.
Python was conceived in the late 1980s,[29] and its implementation was started in December 1989[30] by Guido van Rossum at CWI in the Netherlands as a successor to the ABC language (itself inspired by SETL)[31] capable of exception handling and interfacing with the Amoeba operating system.[6] 
Van Rossum is Python's principal author, and his continuing central role in deciding the direction of Python is reflected in the title given to him by the Python community, benevolent dictator for life (BDFL).
About the origin of Python, Van Rossum wrote in 1996:[32]
Over six years ago, in December 1989, I was looking for a "hobby" programming project that would keep me occupied during the week around Christmas. 
My office ... would be closed, but I had a home computer, and not much else on my hands. 
I decided to write an interpreter for the new scripting language I had been thinking about lately: a descendant of ABC that would appeal to Unix/C hackers. 
I chose Python as a working title for the project, being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus).
Python 2.0 was released on 16 October 2000 and had many major new features, including a cycle-detecting garbage collector and support for Unicode. 
With this release the development process was changed and became more transparent and community-backed.[33]
Python 3.0 (which early in its development was commonly referred to as Python 3000 or py3k), a major, backwards-incompatible release, was released on 3 December 2008[34] after a long period of testing. 
Many of its major features have been backported to the backwards-compatible Python 2.6 and 2.7.[35]
Features and philosophy
Python is a multi-paradigm programming language: object-oriented programming and structured programming are fully supported, and there are a number of language features which support functional programming and aspect-oriented programming (including by metaprogramming[36] and by magic methods).
 Many other paradigms are supported using extensions, including design by contract[38][39] and logic programming.[40]
Python uses dynamic typing and a combination of reference counting and a cycle-detecting garbage collector for memory management.
 An important feature of Python is dynamic name resolution (late binding), which binds method and variable names during program execution.
The design of Python offers some support for functional programming in the Lisp tradition.
 The language has map(), reduce() and filter() functions; comprehensions for lists, dictionaries, and sets; and generator expressions.
 The standard library has two modules (itertools and functools) that implement functional tools borrowed from Haskell and Standard ML.
The core philosophy of the language is summarized by the document "PEP 20 (The Zen of Python)", which includes aphorisms such as:[43]
Beautiful is better than ugly
Explicit is better than implicit
Simple is better than complex
Complex is better than complicated
Readability counts
Rather than requiring all desired functionality to be built into the language's core, Python was designed to be highly extensible. 
Python can also be embedded in existing applications that need a programmable interface. 
This design of a small core language with a large standard library and an easily extensible interpreter was intended by Van Rossum from the very start because of his frustrations with ABC (which espoused the opposite mindset).[29]
While offering choice in coding methodology, the Python philosophy rejects exuberant syntax, such as in Perl, in favor of a sparser, less-cluttered grammar. 
As Alex Martelli put it: "To describe something as clever is not considered a compliment in the Python culture."
Python's philosophy rejects the Perl "there is more than one way to do it" approach to language design in favor of "there should be one—and preferably only one—obvious way to do it".
Python's developers strive to avoid premature optimization, and moreover, reject patches to non-critical parts of CPython that would offer a marginal increase in speed at the cost of clarity.[45]
 When speed is important, a Python programmer can move time-critical functions to extension modules written in languages such as C, or try using PyPy, a just-in-time compiler. 
Cython is also available, which translates a Python script into C and makes direct C-level API calls into the Python interpreter.
An important goal of the Python developers is making Python fun to use. 
This is reflected in the origin of the name, which comes from Monty Python,[46] and in an occasionally playful approach to tutorials and reference materials, such as using examples that refer to spam and eggs instead of the standard foo and bar.
A common neologism in the Python community is pythonic, which can have a wide range of meanings related to program style.
 To say that code is pythonic is to say that it uses Python idioms well, that it is natural or shows fluency in the language, that it conforms with Python's minimalist philosophy and emphasis on readability. 
In contrast, code that is difficult to understand or reads like a rough transcription from another programming language is called unpythonic.
Users and admirers of Python—especially those considered knowledgeable or experienced—are often referred to as Pythonists, Pythonistas, and Pythoneers.[49][50]
Syntax and semantics
Main article: Python syntax and semantics
Python is intended to be a highly readable language. It is designed to have an uncluttered visual layout, frequently using English keywords where other languages use punctuation. 
Furthermore, Python has a smaller number of syntactic exceptions and special cases than C or Pascal.
Indentation
Main article: Python syntax and semantics § Indentation
Python uses whitespace indentation, rather than curly braces or keywords, to delimit blocks; this feature is also termed the off-side rule. 
An increase in indentation comes after certain statements; a decrease in indentation signifies the end of the current block.[52]
Statements and control flow
Python's statements include (among others):
The assignment statement (token '=', the equals sign).
 This operates differently than in traditional imperative programming languages, and this fundamental mechanism (including the nature of Python's version of "variables") illuminates many other features of the language. 
Assignment in C, e.g., "x = 2", translates to "typed variable name x receives a copy of numeric value 2". 
The (right-hand) value is copied into an allocated storage location for which the (left-hand) variable name is the symbolic address. 
The memory allocated to the variable is large enough (potentially quite large) for the declared type.
 In the simplest case of Python assignment, using the same example, "x = 2", translates to "(generic) name x receives a reference to a separate, dynamically allocated object of numeric (int) type of value 2." 
This is referred to as "binding" the name to the object. 
Since the name's storage location doesn't "contain" the indicated value, it is not proper to refer to it as a "variable."
 Names may be subsequently re-bound at any time to objects of wildly varying types, including strings, procedures, complex objects with data and methods, etc. 
uccessive assignments of a common value to multiple names, e.g., "x = 2"; "y = 2"; "z = 2" result in allocating storage to (at most) three names and a single numeric object, to which all three names are bound. 
Since a name is a generic reference holder it is not reasonable to associate a fixed data type with it. 
However at a given time a name will be bound to some object, which will have a type; thus there is dynamic typing.
The if statement, which conditionally executes a block of code, along with else and elif (a contraction of else-if).
The for statement, which iterates over an iterable object, capturing each element to a local variable for use by the attached block.
The while statement, which executes a block of code as long as its condition is true.
The try statement, which allows exceptions raised in its attached code block to be caught and handled by except clauses; it also ensures that clean-up code in a finally block will always be run regardless of how the block exits.
The class statement, which executes a block of code and attaches its local namespace to a class, for use in object-oriented programming.
The def statement, which defines a function or method.
The with statement (from Python 2.5), which encloses a code block within a context manager (for example, acquiring a lock before the block of code is run and releasing the lock afterwards, or opening a file and then closing it), allowing RAII-like behavior.
The pass statement, which serves as a NOP.
 It is syntactically needed to create an empty code block.
The assert statement, used during debugging to check for conditions that ought to apply.
The yield statement, which returns a value from a generator function. 
From Python 2.5, yield is also an operator. 
This form is used to implement coroutines.
The import statement, which is used to import modules whose functions or variables can be used in the current program.
The print statement was changed to the print() function in Python 3.
Python does not support tail-call optimization or first-class continuations, and, according to Guido van Rossum, it never will. 
However, better support for coroutine-like functionality is provided in 2.5, by extending Python's generators.
Prior to 2.5, generators were lazy iterators; information was passed unidirectionally out of the generator.
 As of Python 2.5, it is possible to pass information back into a generator function, and as of Python 3.3, the information can be passed through multiple stack levels.
Expressions
Some Python expressions are similar to languages such as C and Java, while some are not:
Addition, subtraction, and multiplication are the same, but the behavior of division differs (see Mathematics for details). 
Python also added the ** operator for exponentiation.
As of Python 3.5, it supports matrix multiplication directly with the @ operator, as opposed to C and Java which implement these as library functions. 
Earlier versions of Python also used methods instead of an infix operator. 
In Python, == compares by value, in contrast to Java, which compares numerics by value[60] and objects by reference.
 (Value comparisons in Java on objects can be performed with the equals() method.)
 Python's is operator may be used to compare object identities (comparison by reference). 
In Python, comparisons may be chained, for example a <= b <= c.
Python uses the words and, or, not for its boolean operators rather than the symbolic &&, ||, ! used in Java and C.
Python has a type of expression termed a list comprehension. Python 2.4 extended list comprehensions into a more general expression termed a generator expression.[41]
Anonymous functions are implemented using lambda expressions; however, these are limited in that the body can only be a single expression.
Conditional expressions in Python are written as x if c else y[62] (different in order of operands from the ?: operator common to many other languages).
Python makes a distinction between lists and tuples. 
Lists are written as [1, 2, 3], are mutable, and cannot be used as the keys of dictionaries (dictionary keys must be immutable in Python). 
Tuples are written as (1, 2, 3), are immutable and thus can be used as the keys of dictionaries, provided all elements of the tuple are immutable. 
The parentheses around the tuple are optional in some contexts. 
Tuples can appear on the left side of an equal sign; hence a statement like x, y = y, x can be used to swap two variables.
Python has a "string format" operator %. 
This functions analogous to printf format strings in C, e.g. "spam=%s eggs=%d" % ("blah", 2) evaluates to "spam=blah eggs=2". 
In Python 3 and 2.6+, this was supplemented by the format() method of the str class, e.g. "spam={0} eggs={1}".format("blah", 2).
Python has various kinds of string literals:
Strings delimited by single or double quotation marks. 
Unlike in Unix shells, Perl and Perl-influenced languages, single quotation marks and double quotation marks function identically. 
Both kinds of string use the backslash (\) as an escape character and there is no implicit string interpolation such as "$spam".
Triple-quoted strings, which begin and end with a series of three single or double quotation marks. 
They may span multiple lines and function like here documents in shells, Perl and Ruby.
Raw string varieties, denoted by prefixing the string literal with an r. 
No escape sequences are interpreted; hence raw strings are useful where literal backslashes are common, such as regular expressions and Windows-style paths. 
Compare "@-quoting" in C#.
Python has index and slice expressions on lists, denoted as a[key], a[start:stop] or a[start:stop:step]. 
Indexes are zero-based, and negative indexes are relative to the end. 
Slices take elements from the start index up to, but not including, the stop index. 
The third slice parameter, called step or stride, allows elements to be skipped and reversed. 
Slice indexes may be omitted, for example a[:] returns a copy of the entire list.
 Each element of a slice is a shallow copy.
In Python, a distinction between expressions and statements is rigidly enforced, in contrast to languages such as Common Lisp, Scheme, or Ruby. 
This leads to some duplication of functionality. For example:
List comprehensions vs. for-loops
Conditional expressions vs. if blocks
The eval() vs. exec() built-in functions (in Python 2, exec is a statement); the former is for expressions, the latter is for statements.
Statements cannot be a part of an expression, so list and other comprehensions or lambda expressions, all being expressions, cannot contain statements. 
A particular case of this is that an assignment statement such as a = 1 cannot form part of the conditional expression of a conditional statement. 
This has the advantage of avoiding a classic C error of mistaking an assignment operator = for an equality operator == in conditions: if (c = 1) { ... } is valid C code but if c = 1: ... causes a syntax error in Python.
Methods
Methods on objects are functions attached to the object's class; the syntax instance.method(argument) is, for normal methods and functions, syntactic sugar for Class.method(instance, argument). 
Python methods have an explicit self parameter to access instance data, in contrast to the implicit self (or this) in some other object-oriented programming languages (e.g. C++, Java, Objective-C, or Ruby).[63]
Typing
Python uses duck typing and has typed objects but untyped variable names. 
Type constraints are not checked at compile time; rather, operations on an object may fail, signifying that the given object is not of a suitable type. 
Despite being dynamically typed, Python is strongly typed, forbidding operations that are not well-defined (for example, adding a number to a string) rather than silently attempting to make sense of them.
Python allows programmers to define their own types using classes, which are most often used for object-oriented programming. 
New instances of classes are constructed by calling the class (for example, SpamClass() or EggsClass()), and the classes themselves are instances of the metaclass type (itself an instance of itself), allowing metaprogramming and reflection.
Prior to version 3.0, Python had two kinds of classes: "old-style" and "new-style".[64] 
Old-style classes were eliminated in Python 3.0, making all classes new-style. 
In versions between 2.2 and 3.0, both kinds of classes could be used. 
The syntax of both styles is the same, the difference being whether the class object is inherited from, directly or indirectly (all new-style classes inherit from object and are instances of type).
Perl is a family of high-level, general-purpose, interpreted, dynamic programming languages. 
The languages in this family include Perl 5 and Perl 6.[4]
Though Perl is not officially an acronym,[5] there are various backronyms in use, the most well-known being "Practical Extraction and Reporting Language". 
Perl was originally developed by Larry Wall in 1987 as a general-purpose Unix scripting language to make report processing easier.
Since then, it has undergone many changes and revisions. Perl 6, which began as a redesign of Perl 5 in 2000, eventually evolved into a separate language. Both languages continue to be developed independently by different development teams and liberally borrow ideas from one another.
The Perl languages borrow features from other programming languages including C, shell script (sh), AWK, and sed.
They provide powerful text processing facilities without the arbitrary data-length limits of many contemporary Unix commandline tools,[9] facilitating easy manipulation of text files.
 Perl 5 gained widespread popularity in the late 1990s as a CGI scripting language, in part due to its unsurpassed[10][11][12] regular expression and string parsing abilities.
In addition to CGI, Perl 5 is used for graphics programming, system administration, network programming, finance, bioinformatics, and other applications. 
It has been nicknamed "the Swiss Army chainsaw of scripting languages" because of its flexibility and power,[14] and possibly also because of its "ugliness".
 In 1998, it was also referred to as the "duct tape that holds the Internet together", in reference to both its ubiquitous use as a glue language and its perceived inelegance.
Larry Wall began work on Perl in 1987, while working as a programmer at Unisys,[9] and released version 1.0 to the comp.sources.misc newsgroup on December 18, 1987.[17] 
The language expanded rapidly over the next few years.
Perl 2, released in 1988, featured a better regular expression engine. 
Perl 3, released in 1989, added support for binary data streams.
Originally the only documentation for Perl was a single (increasingly lengthy) man page.
 In 1991, Programming Perl, known to many Perl programmers as the "Camel Book" because of its cover, was published and became the de facto reference for the language. 
At the same time, the Perl version number was bumped to 4, not to mark a major change in the language but to identify the version that was well documented by the book.
Early Perl 5
Main article: Perl 5 version history
Perl 4 went through a series of maintenance releases, culminating in Perl 4.036 in 1993. 
At that point, Wall abandoned Perl 4 to begin work on Perl 5. Initial design of Perl 5 continued into 1994. 
The perl5-porters mailing list was established in May 1994 to coordinate work on porting Perl 5 to different platforms. 
It remains the primary forum for development, maintenance, and porting of Perl 5.[18]
Perl 5.000 was released on October 17, 1994.[19] 
It was a nearly complete rewrite of the interpreter, and it added many new features to the language, including objects, references, lexical (my) variables, and modules.
 Importantly, modules provided a mechanism for extending the language without modifying the interpreter. 
This allowed the core interpreter to stabilize, even as it enabled ordinary Perl programmers to add new language features. 
Perl 5 has been in active development since then.
Perl 5.001 was released on March 13, 1995. Perl 5.002 was released on February 29, 1996 with the new prototypes feature. This allowed module authors to make subroutines that behaved like Perl builtins. Perl 5.003 was released June 25, 1996, as a security release.
One of the most important events in Perl 5 history took place outside of the language proper and was a consequence of its module support. On October 26, 1995, the Comprehensive Perl Archive Network (CPAN) was established as a repository for Perl modules and Perl itself; as of June 2015, it carries over 150,775 modules in 31,896 distributions, written by more than 12,219 authors, and is mirrored worldwide at more than 253 locations.[20]
Perl 5.004 was released on May 15, 1997, and included among other things the UNIVERSAL package, giving Perl a base object to which all classes were automatically derived and the ability to require versions of modules. Another significant development was the inclusion of the CGI.pm module,[21] which contributed to Perl's popularity as a CGI scripting language.[22]
Perl is also now supported running under Microsoft Windows and several other operating systems.[21]
Perl 5.005 was released on July 22, 1998. 
This release included several enhancements to the regex engine, new hooks into the backend through the B::* modules, the qr// regex quote operator, a large selection of other new core modules, and added support for several more operating systems, including BeOS.[23]
Perl 5.6 was released on March 22, 2000. 
Major changes included 64-bit support, Unicode string representation, large file support (i.e. files over 2 GiB) and the "our" keyword.[25][26]
 When developing Perl 5.6, the decision was made to switch the versioning scheme to one more similar to other open source projects; after 5.005_63, the next version became 5.5.640, with plans for development versions to have odd numbers and stable versions to have even numbers.
In 2000, Wall put forth a call for suggestions for a new version of Perl from the community. 
The process resulted in 361 RFC (request for comments) documents that were to be used in guiding development of Perl 6. 
In 2001,[27] work began on the apocalypses for Perl 6, a series of documents meant to summarize the change requests and present the design of the next generation of Perl. 
They were presented as a digest of the RFCs, rather than a formal document. 
At this point, Perl 6 existed only as a description of a language.
Perl 5.8 was first released on July 18, 2002, and had nearly yearly updates since then. 
Perl 5.8 improved Unicode support, added a new I/O implementation, added a new thread implementation, improved numeric accuracy, and added several new modules.[28]
 As of 2013 this version still remains the most popular version of Perl and is used by Red Hat 5, Suse 10, Solaris 10, HP-UX 11.33 and AIX 5.
In 2004, work began on the Synopses – documents that originally summarized the Apocalypses, but which became the specification for the Perl 6 language. 
In February 2005, Audrey Tang began work on Pugs, a Perl 6 interpreter written in Haskell.[29] This was the first concerted effort towards making Perl 6 a reality. 
This effort stalled in 2006.[30]
On December 18, 2007, the 20th anniversary of Perl 1.0, Perl 5.10.0 was released. Perl 5.10.0 included notable new features, which brought it closer to Perl 6.
 These included a switch statement (called "given"/"when"), regular expressions updates, and the smart match operator, "~~".
 Around this same time, development began in earnest on another implementation of Perl 6 known as Rakudo Perl, developed in tandem with the Parrot virtual machine. 
As of November 2009, Rakudo Perl has had regular monthly releases and now is the most complete implementation of Perl 6.
A major change in the development process of Perl 5 occurred with Perl 5.11; the development community has switched to a monthly release cycle of development releases, with a yearly schedule of stable releases. 
By that plan, bugfix point releases will follow the stable releases every three months.
On April 12, 2010, Perl 5.12.0 was released. 
Notable core enhancements include new package NAME VERSION syntax, the Yada Yada operator (intended to mark placeholder code that is not yet implemented), implicit strictures, full Y2038 compliance, regex conversion overloading, DTrace support, and Unicode 5.2.[33] 
On January 21, 2011, Perl 5.12.3 was released; it contains updated modules and some documentation changes.
 Version 5.12.4 was released on June 20, 2011. The latest version of that branch, 5.12.5, was released on November 10, 2012.
On May 14, 2011, Perl 5.14 was released. 
JSON support is built-in as of 5.14.2. The latest version of that branch, 5.14.4, was released on March 10, 2013.
On May 20, 2012, Perl 5.16 was released. 
Notable new features include the ability to specify a given version of Perl that one wishes to emulate, allowing users to upgrade their version of Perl, but still run old scripts that would normally be incompatible.
 Perl 5.16 also updates the core to support Unicode 6.1.
On May 18, 2013, Perl 5.18 was released. 
Notable new features include the new dtrace hooks, lexical subs, more CORE:: subs, overhaul of the hash for security reasons, support for Unicode 6.2.
On May 27, 2014, Perl 5.20 was released.
 Notable new features include subroutine signatures, hash slices/new slice syntax, postfix dereferencing (experimental), Unicode 6.3, rand() using consistent random number generator.
Some observers credit the release of Perl 5.10 with the start of the Modern Perl movement.
 In particular, this phrase describes a style of development that embraces the use of the CPAN, takes advantage of recent developments in the language, and is rigorous about creating high quality code.
While the book "Modern Perl"[40] may be the most visible standard-bearer of this idea, other groups such as the Enlightened Perl Organization[41] have taken up the cause.
In late 2012 and 2013 several projects for alternative implementations for Perl 5 started: Perl5 in Perl6 by the Rakudo Perl team,[42] moe by Stevan Little and friends,[43] p2[44] by the Perl11 team under Reini Urban, gperl by goccy,[45] and rperl a kickstarter project led by Will Braswell and affiliated with the Perll11 project.
Perl was originally named "Pearl". 
Wall wanted to give the language a short name with positive connotations; he claims that he considered (and rejected) every three- and four-letter word in the dictionary. 
He also considered naming it after his wife Gloria. 
Wall discovered the existing PEARL programming language before Perl's official release and changed the spelling of the name.
When referring to the language, the name is normally capitalized (Perl) as a proper noun. 
When referring to the interpreter program itself, the name is often uncapitalized (perl) because most Unix-like file systems are case-sensitive. 
Before the release of the first edition of Programming Perl, it was common to refer to the language as perl; Randal L. Schwartz, however, capitalized the language's name in the book to make it stand out better when typeset. 
This case distinction was subsequently documented as canonical.
The name is occasionally expanded as Practical Extraction and Report Language, but this is a backronym.
 Other expansions have been suggested as equally canonical, including Wall's own humorous Pathologically Eclectic Rubbish Lister.
 Indeed, Wall claims that the name was intended to inspire many different expansions.[51]
Camel symbol
The Camel symbol used by O'Reilly Media
Programming Perl, published by O'Reilly Media, features a picture of a dromedary camel on the cover and is commonly called the "Camel Book".
This image of a camel has become an unofficial symbol of Perl as well as a general hacker emblem, appearing on T-shirts and other clothing items.
O'Reilly owns the image as a trademark but licenses it for non-commercial use, requiring only an acknowledgement and a link to www.perl.com. Licensing for commercial use is decided on a case by case basis.[53] 
O'Reilly also provides "Programming Republic of Perl" logos for non-commercial sites and "Powered by Perl" buttons for any site that uses Perl.[53]
Onion symbol
The onion logo used by The Perl Foundation
The Perl Foundation owns an alternative symbol, an onion, which it licenses to its subsidiaries, Perl Mongers, PerlMonks, Perl.org, and others.
 The symbol is a visual pun on pearl onion.
Overview
Main article: Perl language structure
According to Wall, Perl has two slogans. 
The first is "There's more than one way to do it", commonly known as TMTOWTDI. 
The second slogan is "Easy things should be easy and hard things should be possible".[9]
Features
The overall structure of Perl derives broadly from C. 
Perl is procedural in nature, with variables, expressions, assignment statements, brace-delimited blocks, control structures, and subroutines.
Perl also takes features from shell programming.
 All variables are marked with leading sigils, which allow variables to be interpolated directly into strings. 
However, unlike the shell, Perl uses sigils on all accesses to variables, and unlike most other programming languages that use sigils, the sigil doesn't denote the type of the variable but the type of the expression. 
So for example, to access a list of values in a hash, the sigil for an array ("@") is used, not the sigil for a hash ("%").
 Perl also has many built-in functions that provide tools often used in shell programming (although many of these tools are implemented by programs external to the shell) such as sorting, and calling operating system facilities.
Perl takes lists from Lisp, hashes ("associative arrays") from AWK, and regular expressions from sed. 
These simplify and facilitate many parsing, text-handling, and data-management tasks. 
Also shared with Lisp are the implicit return of the last value in a block, and the fact that all statements have a value, and thus are also expressions and can be used in larger expressions themselves.
Perl 5 added features that support complex data structures, first-class functions (that is, closures as values), and an object-oriented programming model.
 These include references, packages, class-based method dispatch, and lexically scoped variables, along with compiler directives (for example, the strict pragma).
 A major additional feature introduced with Perl 5 was the ability to package code as reusable modules. 
Wall later stated that "The whole intent of Perl 5's module system was to encourage the growth of Perl culture rather than the Perl core."[56]
All versions of Perl do automatic data-typing and automatic memory management. 
The interpreter knows the type and storage requirements of every data object in the program; it allocates and frees storage for them as necessary using reference counting (so it cannot deallocate circular data structures without manual intervention).
 Legal type conversions — for example, conversions from number to string — are done automatically at run time; illegal type conversions are fatal errors.
Design
The design of Perl can be understood as a response to three broad trends in the computer industry: falling hardware costs, rising labor costs, and improvements in compiler technology. 
Many earlier computer languages, such as Fortran and C, aimed to make efficient use of expensive computer hardware. 
In contrast, Perl was designed so that computer programmers could write programs more quickly and easily.
Perl has many features that ease the task of the programmer at the expense of greater CPU and memory requirements. 
These include automatic memory management; dynamic typing; strings, lists, and hashes; regular expressions; introspection; and an eval() function. 
Perl follows the theory of "no built-in limits",[52] an idea similar to the Zero One Infinity rule.
Wall was trained as a linguist, and the design of Perl is very much informed by linguistic principles.
 Examples include Huffman coding (common constructions should be short), good end-weighting (the important information should come first), and a large collection of language primitives. 
Perl favors language constructs that are concise and natural for humans to write, even where they complicate the Perl interpreter.
Perl's syntax reflects the idea that "things that are different should look different."
 For example, scalars, arrays, and hashes have different leading sigils.
 Array indices and hash keys use different kinds of braces. 
Strings and regular expressions have different standard delimiters. 
This approach can be contrasted with languages such as Lisp, where the same S-expression construct and basic syntax are used for many different purposes.
Perl does not enforce any particular programming paradigm (procedural, object-oriented, functional, or others) or even require the programmer to choose among them.
There is a broad practical bent to both the Perl language and the community and culture that surround it.
 The preface to Programming Perl begins: "Perl is a language for getting your job done."
One consequence of this is that Perl is not a tidy language. 
It includes many features, tolerates exceptions to its rules, and employs heuristics to resolve syntactical ambiguities. 
Because of the forgiving nature of the compiler, bugs can sometimes be hard to find. 
Perl's function documentation remarks on the variant behavior of built-in functions in list and scalar contexts by saying, "In general, they do what you want, unless you want consistency."
No written specification or standard for the Perl language exists for Perl versions through Perl 5, and there are no plans to create one for the current version of Perl. 
There has been only one implementation of the interpreter, and the language has evolved along with it. 
That interpreter, together with its functional tests, stands as a de facto specification of the language. 
Perl 6, however, started with a specification,[59] and several projects[60] aim to implement some or all of the specification.
Applications
Perl has many and varied applications, compounded by the availability of many standard and third-party modules.
Perl has chiefly been used to write CGI scripts: large projects written in Perl include cPanel, Slash, Bugzilla, RT, TWiki, and Movable Type; high-traffic websites that use Perl extensively include Priceline.com, Craigslist,[61] IMDb,[62] LiveJournal, DuckDuckGo,[63][64] Slashdot and Ticketmaster. 
It is also an optional component of the popular LAMP technology stack for Web development, in lieu of PHP or Python.
Perl is often used as a glue language, tying together systems and interfaces that were not specifically designed to interoperate, and for "data munging",[65] that is, converting or processing large amounts of data for tasks such as creating reports. 
In fact, these strengths are intimately linked. 
The combination makes Perl a popular all-purpose language for system administrators, particularly because short programs, often called "one-liner programs", can be entered and run on a single command line.
Perl code can be made portable across Windows and Unix; such code is often used by suppliers of software (both COTS and bespoke) to simplify packaging and maintenance of software build- and deployment-scripts.
Graphical user interfaces (GUIs) may be developed using Perl. 
For example, Perl/Tk and WxPerl are commonly used to enable user interaction with Perl scripts. 
Such interaction may be synchronous or asynchronous, using callbacks to update the GUI.
Implementation
Perl is implemented as a core interpreter, written in C, together with a large collection of modules, written in Perl and C.
 As of 2010, the stable version (5.18.2) is 16.53 MB when packaged in a tar file and gzip compressed.
The interpreter is 150,000 lines of C code and compiles to a 1 MB executable on typical machine architectures. 
Alternatively, the interpreter can be compiled to a link library and embedded in other programs. 
There are nearly 500 modules in the distribution, comprising 200,000 lines of Perl and an additional 350,000 lines of C code (much of the C code in the modules consists of character encoding tables).
The interpreter has an object-oriented architecture. 
All of the elements of the Perl language—scalars, arrays, hashes, coderefs, file handles—are represented in the interpreter by C structs. 
Operations on these structs are defined by a large collection of macros, typedefs, and functions; these constitute the Perl C API. 
The Perl API can be bewildering to the uninitiated, but its entry points follow a consistent naming scheme, which provides guidance to those who use it.
The life of a Perl interpreter divides broadly into a compile phase and a run phase.
 In Perl, the phases are the major stages in the interpreter's life-cycle. 
Each interpreter goes through each phase only once, and the phases follow in a fixed sequence.
Most of what happens in Perl's compile phase is compilation, and most of what happens in Perl's run phase is execution, but there are significant exceptions. 
Perl makes important use of its capability to execute Perl code during the compile phase. 
Perl will also delay compilation into the run phase. 
The terms that indicate the kind of processing that is actually occurring at any moment are compile time and run time. 
Perl is in compile time at most points during the compile phase, but compile time may also be entered during the run phase. 
The compile time for code in a string argument passed to the eval built-in occurs during the run phase. 
Perl is often in run time during the compile phase and spends most of the run phase in run time. 
Code in BEGIN blocks executes at run time but in the compile phase.
At compile time, the interpreter parses Perl code into a syntax tree. 
At run time, it executes the program by walking the tree. 
Text is parsed only once, and the syntax tree is subject to optimization before it is executed, so that execution is relatively efficient. 
Compile-time optimizations on the syntax tree include constant folding and context propagation, but peephole optimization is also performed.
Perl has a Turing-complete grammar because parsing can be affected by run-time code executed during the compile phase
 Therefore, Perl cannot be parsed by a straight Lex/Yacc lexer/parser combination. 
Instead, the interpreter implements its own lexer, which coordinates with a modified GNU bison parser to resolve ambiguities in the language.
It is often said that "Only perl can parse Perl",[69] meaning that only the Perl interpreter (perl) can parse the Perl language (Perl), but even this is not, in general, true.
 Because the Perl interpreter can simulate a Turing machine during its compile phase, it would need to decide the halting problem in order to complete parsing in every case. 
It is a long-standing result that the halting problem is undecidable, and therefore not even perl can always parse Perl. 
Perl makes the unusual choice of giving the user access to its full programming power in its own compile phase. 
The cost in terms of theoretical purity is high, but practical inconvenience seems to be rare.
Other programs that undertake to parse Perl, such as source-code analyzers and auto-indenters, have to contend not only with ambiguous syntactic constructs but also with the undecidability of Perl parsing in the general case. 
Adam Kennedy's PPI project focused on parsing Perl code as a document (retaining its integrity as a document), instead of parsing Perl as executable code (that not even Perl itself can always do).
 It was Kennedy who first conjectured that "parsing Perl suffers from the 'halting problem'",[70] which was later proved.
Perl is distributed with over 250,000 functional tests for core Perl language and over 250,000 functional tests for core modules. 
These run as part of the normal build process and extensively exercise the interpreter and its core modules. 
Perl developers rely on the functional tests to ensure that changes to the interpreter do not introduce software bugs; additionally, Perl users who see that the interpreter passes its functional tests on their system can have a high degree of confidence that it is working properly.
Availability
Perl is dual licensed under both the Artistic License and the GNU General Public License. 
Distributions are available for most operating systems. 
It is particularly prevalent on Unix and Unix-like systems, but it has been ported to most modern (and many obsolete) platforms. 
With only six reported exceptions, Perl can be compiled from source code on all POSIX-compliant, or otherwise-Unix-compatible platforms.
Because of unusual changes required for the Mac OS Classic environment, a special port called MacPerl was shipped independently.
The Comprehensive Perl Archive Network carries a complete list of supported platforms with links to the distributions available on each.
 CPAN is also the source for publicly available Perl modules that are not part of the core Perl distribution.
Windows
Users of Microsoft Windows typically install one of the native binary distributions of Perl for Win32, most commonly Strawberry Perl or ActivePerl. 
Compiling Perl from source code under Windows is possible, but most installations lack the requisite C compiler and build tools. 
This also makes it difficult to install modules from the CPAN, particularly those that are partially written in C.
ActivePerl is a closed source distribution from ActiveState that has regular releases that track the core Perl releases.[75] 
The distribution also includes the Perl package manager (PPM),[76] a popular tool for installing, removing, upgrading, and managing the use of common Perl modules.
 Included also is PerlScript, a Windows Script Host (WSH) engine implementing the Perl language. 
Visual Perl is an ActiveState tool that adds Perl to the Visual Studio .NET development suite.
Strawberry Perl is an open source distribution for Windows.
 It has had regular, quarterly releases since January 2008, including new modules as feedback and requests come in. 
Strawberry Perl aims to be able to install modules like standard Perl distributions on other platforms, including compiling XS modules.
The Cygwin emulation layer is another way of running Perl under Windows. 
Cygwin provides a Unix-like environment on Windows, and both Perl and CPAN are available as standard pre-compiled packages in the Cygwin setup program. 
Since Cygwin also includes gcc, compiling Perl from source is also possible.
A perl executable is included in several Windows Resource kits in the directory with other scripting tools.
Implementations of Perl come with the MKS Toolkit and UWIN.
Database interfaces
Perl's text-handling capabilities can be used for generating SQL queries; arrays, hashes, and automatic memory management make it easy to collect and process the returned data. 
For example, in Tim Bunce's Perl DBI application programming interface (API), the arguments to the API can be the text of SQL queries; thus it is possible to program in multiple languages at the same time (e.g., for generating a Web page using HTML, JavaScript, and SQL in a here document). 
The use of Perl variable interpolation to programmatically customize each of the SQL queries, and the specification of Perl arrays or hashes as the structures to programmatically hold the resulting data sets from each SQL query, allows a high-level mechanism for handling large amounts of data for post-processing by a Perl subprogram.
 In early versions of Perl, database interfaces were created by relinking the interpreter with a client-side database library.
This was sufficiently difficult that it was done for only a few of the most-important and most widely used databases, and it restricted the resulting perl executable to using just one database interface at a time.
In Perl 5, database interfaces are implemented by Perl DBI modules. 
The DBI (Database Interface) module presents a single, database-independent interface to Perl applications, while the DBD (Database Driver) modules handle the details of accessing some 50 different databases; there are DBD drivers for most ANSI SQL databases.
DBI provides caching for database handles and queries, which can greatly improve performance in long-lived execution environments such as mod perl,[78] helping high-volume systems avert load spikes as in the Slashdot effect.
In modern Perl applications, especially those written using web frameworks such as Catalyst, the DBI module is often used indirectly via object-relational mappers such as DBIx::Class, Class::DBI or Rose::DB::Object that generate SQL queries and handle data transparently to the application author.
Comparative performance
The Computer Language Benchmarks Game, a project hosted by Alioth, compares the performance of implementations of typical programming problems in several programming languages.
 The submitted Perl implementations typically perform toward the high end of the memory-usage spectrum and give varied speed results. 
Perl's performance in the benchmarks game is typical for interpreted languages.
Large Perl programs start more slowly than similar programs in compiled languages because perl has to compile the source every time it runs. 
In a talk at the YAPC::Europe 2005 conference and subsequent article "A Timely Start", Jean-Louis Leroy found that his Perl programs took much longer to run than expected because the perl interpreter spent significant time finding modules within his over-large include path.
 Unlike Java, Python, and Ruby, Perl has only experimental support for pre-compiling.
 Therefore, Perl programs pay this overhead penalty on every execution. 
The run phase of typical programs is long enough that amortized startup time is not substantial, but benchmarks that measure very short execution times are likely to be skewed due to this overhead.
A number of tools have been introduced to improve this situation. 
The first such tool was Apache's mod perl, which sought to address one of the most-common reasons that small Perl programs were invoked rapidly: CGI Web development. 
ActivePerl, via Microsoft ISAPI, provides similar performance improvements.
Once Perl code is compiled, there is additional overhead during the execution phase that typically isn't present for programs written in compiled languages such as C or C++. 
Examples of such overhead include bytecode interpretation, reference-counting memory management, and dynamic type-checking.
Optimizing
Because Perl is an interpreted language, it can give problems when efficiency is critical; in such situations, the most critical routines can be written in other languages (such as C), which can be connected to Perl via simple Inline modules or the more complex but flexible XS mechanism.
Perl 6
Main article: Perl 6
Camelia, the logo for the Perl 6 project.
At the 2000 Perl Conference, Jon Orwant made a case for a major new language-initiative.
 This led to a decision to begin work on a redesign of the language, to be called Perl 6. 
Proposals for new language features were solicited from the Perl community at large, which submitted more than 300 RFCs.
Wall spent the next few years digesting the RFCs and synthesizing them into a coherent framework for Perl 6. 
He has presented his design for Perl 6 in a series of documents called "apocalypses" - numbered to correspond to chapters in Programming Perl. 
As of January 2011, the developing specification of Perl 6 is encapsulated in design documents called Synopses - numbered to correspond to Apocalypses.
Perl 6 and Perl 5 are distinct languages with a common ancestry.
Thesis work by Bradley M. Kuhn, overseen by Wall, considered the possible use of the Java virtual machine as a runtime for Perl.
 Kuhn's thesis showed this approach to be problematic. In 2001, it was decided that Perl 6 would run on a cross-language virtual machine called Parrot. 
This will mean that other languages targeting the Parrot will gain native access to CPAN, allowing some level of cross-language development.
In 2005, Audrey Tang created the pugs project, an implementation of Perl 6 in Haskell. 
This acted as, and continues to act as, a test platform for the Perl 6 language (separate from the development of the actual implementation) - allowing the language designers to explore. 
The pugs project spawned an active Perl/Haskell cross-language community centered around the freenode #perl6 IRC channel.
As of 2012, a number of features in the Perl 6 language show similarities to Haskell.
As of 2012, Perl 6 development centers primarily around two compilers:[89]
Rakudo Perl 6, an implementation running on the Parrot virtual machine and the Java virtual machine.
 Developers are also working on MoarVM, a C language-based virtual machine designed specifically for Rakudo.
Niecza, which targets the Common Language Runtime.
Future of Perl 5
Development of Perl 5 is also continuing. 
Perl 5.12.0 was released in April 2010 with some new features influenced by the design of Perl 6,[33][92] followed by Perl 5.14.1 (released on June 17, 2011), Perl 5.16.1 (released on August 9, 2012., and Perl 5.18.0 (released on May 18, 2013).
Perl 5 development versions are released on a monthly basis, with major releases coming out once per year.
Future plans for Perl 5 include making the core language easier to extend from modules, and providing a small, extensible Meta-object protocol in core.
The relative proportion of searches for 'Perl programming', as compared with similar searches for other programming languages, steadily declined from about 10% in 2005 to about 2% in 2011, and has remained around the 2% level since.
Perl community
Perl's culture and community has developed alongside the language itself. 
Usenet was the first public venue in which Perl was introduced, but over the course of its evolution, Perl's community was shaped by the growth of broadening Internet-based services including the introduction of the World Wide Web. 
The community that surrounds Perl was, in fact, the topic of Wall's first "State of the Onion" talk.
State of the Onion
State of the Onion is the name for Wall’s yearly keynote-style summaries on the progress of Perl and its community. 
They are characterized by his hallmark humor, employing references to Perl’s culture, the wider hacker culture, Wall’s linguistic background, sometimes his family life, and occasionally even his Christian background.
Each talk is first given at various Perl conferences and is eventually also published online.
Perl pastimes
JAPHs
In email, Usenet, and message board postings, "Just another Perl hacker" (JAPH) programs are a common trend, originated by Randal L. Schwartz, one of the earliest professional Perl trainers.
 In the parlance of Perl culture, Perl programmers are known as Perl hackers, and from this derives the practice of writing short programs to print out the phrase "Just another Perl hacker,".
 In the spirit of the original concept, these programs are moderately obfuscated and short enough to fit into the signature of an email or Usenet message. 
The "canonical" JAPH as developed by Schwartz includes the comma at the end, although this is often omitted.
Perl golf
Perl "golf" is the pastime of reducing the number of characters (key "strokes") used in a Perl program to the bare minimum, much in the same way that golf players seek to take as few shots as possible in a round. 
The phrase's first use[101] emphasized the difference between pedestrian code meant to teach a newcomer and terse hacks likely to amuse experienced Perl programmers, an example of the latter being JAPHs that were already used in signatures in Usenet postings and elsewhere. 
Similar stunts had been an unnamed pastime in the language APL in previous decades. 
The use of Perl to write a program that performed RSA encryption prompted a widespread and practical interest in this pastime.
 In subsequent years, the term "code golf" has been applied to the pastime in other languages.
 A Perl Golf Apocalypse was held at Perl Conference 4.0 in Monterey, California in July 2000.
Obfuscation
As with C, obfuscated code competitions were a well known pastime in the late 1990s. 
The Obfuscated Perl Contest was a competition held by The Perl Journal from 1996 to 2000 that made an arch virtue of Perl's syntactic flexibility. 
Awards were given for categories such as "most powerful"—programs that made efficient use of space—and "best four-line signature" for programs that fit into four lines of 76 characters in the style of a Usenet signature block.
Poetry
Perl poetry is the practice of writing poems that can be compiled as legal Perl code, for example the piece known as Black Perl. 
Perl poetry is made possible by the large number of English words that are used in the Perl language.
 New poems are regularly submitted to the community at PerlMonks
There are also many examples of code written purely for entertainment on the CPAN. 
Lingua::Romana::Perligata, for example, allows writing programs in Latin.
 Upon execution of such a program, the module translates its source code into regular Perl and runs it.
The Perl community has set aside the "Acme" namespace for modules that are fun in nature (but its scope has widened to include exploratory or experimental code or any other module that is not meant to ever be used in production). 
Some of the Acme modules are deliberately implemented in amusing ways. 
This includes Acme::Bleach, one of the first modules in the Acme:: namespace,[107] which allows the program's source code to be "whitened" (i.e., all characters replaced with whitespace) and yet still work.
Perl has been referred to as "line noise" by some programmers who claim its syntax makes it a write-only language.
 According to the Perl 6 FAQ, Perl 6 was designed to mitigate "the usual suspects" that elicit the "line noise" claim from Perl 5 critics, including the removal of "the majority of the punctuation variables" and the sanitization of the regex syntax.
 The Perl 6 FAQ also states that what is sometimes referred to as Perl's line noise is "the actual syntax of the language" just as gerunds and prepositions are a part of the English language.
  He also stated that "Perl 6 has a coherence and a consistency that Perl 5 lacks."
A Java virtual machine (JVM) is an abstract computing machine that enables a computer to run a Java program.
 There are three notions of the JVM: specification, implementation, and instance. 
The specification is a document that formally describes what is required of a JVM implementation. 
Having a single specification ensures all implementations are interoperable. 
A JVM implementation is a computer program that meets the requirements of the JVM specification. 
An instance of a JVM is an implementation running in a process that executes a computer program compiled into Java bytecode.
Java Runtime Environment (JRE) is a software package that contains what is required to run a Java program. 
It includes a Java Virtual Machine implementation together with an implementation of the Java Class Library. 
The Oracle Corporation, which owns the Java trademark, distributes a Java Runtime environment with their Java Virtual Machine called HotSpot.
Java Development Kit (JDK) is a superset of a JRE and contains tools for Java programmers, e.g. a javac compiler. 
Java Development Kit is provided free of charge either by Oracle Corporation directly, or by the OpenJDK open source project, which is governed by Oracle.
The Java virtual machine is an abstract (virtual) computer defined by a specification. 
This specification omits implementation details that are not essential to ensure interoperability.
 For example, the memory layout of run-time data areas, the garbage-collection algorithm used, and any internal optimization of the Java virtual machine instructions (their translation into machine code). 
The main reason for this omission is to not unnecessarily constrain implementers. 
Any Java application can be run only inside some concrete implementation of the abstract specification of the Java virtual machine.
Starting with Java Platform, Standard Edition (J2SE) 5.0, changes to the JVM specification have been developed under the Java Community Process as JSR 924.
 As of 2006, changes to specification to support changes proposed to the class file format (JSR 202)[3] are being done as a maintenance release of JSR 924. 
The specification for the JVM was published as the blue book,[4] The preface states:
We intend that this specification should sufficiently document the Java Virtual Machine to make possible compatible clean-room implementations. 
Oracle provides tests that verify the proper operation of implementations of the Java Virtual Machine.
One of Oracle's JVMs is named HotSpot, the other, inherited from BEA Systems is JRockit. Clean-room Java implementations include Kaffe and IBM J9.
 Oracle owns the Java trademark, and may allow its use to certify implementation suites as fully compatible with Oracle's specification..
One of the organizational units of JVM byte code is a class. 
A class loader implementation must be able to recognize and load anything that conforms to the Java class file format. 
Any implementation is free to recognize other binary forms besides class files, but it must recognize class files.
The class loader performs three basic activities in this strict order:
Loading: finds and imports the binary data for a type
Linking: performs verification, preparation, and (optionally) resolution
Verification: ensures the correctness of the imported type
Preparation: allocates memory for class variables and initializing the memory to default values
Resolution: transforms symbolic references from the type into direct references.
Initialization: invokes Java code that initializes class variables to their proper starting values.
In general, there are two types of class loader: bootstrap class loader and user defined class loader.
Every Java virtual machine implementation must have a bootstrap class loader, capable of loading trusted classes. 
The Java virtual machine specification doesn't specify how a class loader should locate classes.
Bytecode instructions
Main article: Java bytecode
The JVM has instructions for the following groups of tasks:
Load and store Arithmetic Type conversion Object creation and manipulation Operand stack management (push / pop) Control transfer (branching) Method invocation and return Throwing exceptions Monitor-based concurrency
The aim is binary compatibility. 
Each particular host operating system needs its own implementation of the JVM and runtime. 
These JVMs interpret the bytecode semantically the same way, but the actual implementation may be different. 
More complex than just emulating bytecode is compatibly and efficiently implementing the Java core API that must be mapped to each host operating system.
JVM languages
Main article: List of JVM languages
A JVM language is any language with functionality that can be expressed in terms of a valid class file which can be hosted by the Java Virtual Machine.
 A class file contains Java Virtual Machine instructions (Java byte code) and a symbol table, as well as other ancillary information. 
The class file format is the hardware- and operating system-independent binary format used to represent compiled classes and interfaces.[5]
There are several JVM languages, both old languages ported to JVM and completely new languages. 
JRuby and Jython are perhaps the most well-known ports of existing languages, i.e. Ruby and Python respectively. 
Of the new languages that have been created from scratch to compile to Java bytecode, Clojure, Groovy and Scala may be the most popular ones.
 A notable feature with the JVM languages is that they are compatible with each other, so that, for example, Scala libraries can be used with Java programs and vice versa.[6]
Java 7 JVM implements JSR 292: Supporting Dynamically Typed Languages[7] on the Java Platform, a new feature which supports dynamically typed languages in the JVM. 
This feature is developed within the Da Vinci Machine project whose mission is to extend the JVM so that it supports languages other than Java.[8][9]
Bytecode verifier
Furthermore, common programmer errors that often led to data corruption or unpredictable behavior such as accessing off the end of an array or using an uninitialized pointer are not allowed to occur.
 Several features of Java combine to provide this safety, including the class model, the garbage-collected heap, and the verifier.
The JVM verifies all bytecode before it is executed.
 This verification consists primarily of three types of checks:
Branches are always to valid locations
Data is always initialized and references are always type-safe
Access to private or package private data and methods is rigidly controlled
The first two of these checks take place primarily during the verification step that occurs when a class is loaded and made eligible for use. 
The third is primarily performed dynamically, when data items or methods of a class are first accessed by another class.
The verifier permits only some bytecode sequences in valid programs, e.g. a jump (branch) instruction can only target an instruction within the same method. 
Furthermore, the verifier ensures that any given instruction operates on a fixed stack location,[10] allowing the JIT compiler to transform stack accesses into fixed register accesses. 
Because of this, that the JVM is a stack architecture does not imply a speed penalty for emulation on register-based architectures when using a JIT compiler. 
In the face of the code-verified JVM architecture, it makes no difference to a JIT compiler whether it gets named imaginary registers or imaginary stack positions that must be allocated to the target architecture's registers. 
In fact, code verification makes the JVM different from a classic stack architecture, of which efficient emulation with a JIT compiler is more complicated and typically carried out by a slower interpreter.
The original specification for the bytecode verifier used natural language that was incomplete or incorrect in some respects. 
A number of attempts have been made to specify the JVM as a formal system.
 By doing this, the security of current JVM implementations can more thoroughly be analyzed, and potential security exploits prevented. 
It will also be possible to optimize the JVM by skipping unnecessary safety checks, if the application being run is proven to be safe.[11]
Secure execution of remote code
A virtual machine architecture allows very fine-grained control over the actions that code within the machine is permitted to take. 
This is designed to allow safe execution of untrusted code from remote sources, a model used by Java applets. 
Applets run within a VM incorporated into a user's browser, executing code downloaded from a remote HTTP server.
 The remote code runs in a restricted sandbox, which is designed to protect the user from misbehaving or malicious code. 
Publishers can purchase a certificate with which to digitally sign applets as safe, giving them permission to ask the user to break out of the sandbox and access the local file system, clipboard, execute external pieces of software, or network.
Bytecode interpreter and just-in-time compiler
For each hardware architecture a different Java bytecode interpreter is needed. 
When a computer has a Java bytecode interpreter, it can run any Java bytecode program, and the same program can be run on any computer that has such an interpreter.
When Java bytecode is executed by an interpreter, the execution will always be slower than the execution of the same program compiled into native machine language. 
This problem is mitigated by just-in-time (JIT) compilers for executing Java bytecode.
A JIT compiler may translate Java bytecode into native machine language while executing the program. 
The translated parts of the program can then be executed much more quickly than they could be interpreted. 
This technique gets applied to those parts of a program frequently executed. This way a JIT compiler can significantly speed up the overall execution time.
There is no necessary connection between Java and Java bytecode.
 A program written in Java can be compiled directly into the machine language of a real computer and programs written in other languages than Java can be compiled into Java bytecode.
Java bytecode is intended to be platform-independent and secure.
Some JVM implementations do not include an interpreter, but consist only of a just-in-time compiler.[13]
JVM in the web browser
Since very early stages of the design process, Java (and JVM) has been marketed as a web technology for creating Rich Internet Applications.
Java applets
Main article: Java applet
On the client side, web browsers may be extended with a NPAPI Java plugin which executes so called Java applets embedded into HTML pages.
 The applet is allowed to draw into a rectangular region on the page assigned to it and use a restricted set of APIs that allow for example access to user's microphone or 3D acceleration. 
Java applets were superior to JavaScript both in performance and features until approximately 2011, when JavaScript engines in browsers were made significantly faster and the HTML 5 suite of web technologies started enhancing JavaScript with new APIs. 
Java applets are not able to modify the page outside its rectangular region which is not true about JavaScript. 
Adobe Flash Player, the main competing technology, works in the same way in this respect. 
Java applets are not restricted to Java and in general can be created in any JVM language.
As of April 2014, Google Chrome does not allow the use of any NPAPI plugins.
 Mozilla Firefox will also ban NPAPI plugins by the end of 2016. 
This means that Java applets can no longer be used in either browser.
As of June 2015 according to W3Techs, Java applet use had fallen to 0.1% of all web sites. Flash had fallen to 10.8% and Silverlight to 0.1% of web sites.[16]
JavaScript JVMs and interpreters
JVM implementations in JavaScript do exist, but are mostly limited to hobby projects unsuitable for production deployment or development tools to avoid having to recompile every time the developer wants to preview the changes just made.
Compilation to JavaScript
With the continuing improvements in JavaScript execution speed, combined with the increased use of mobile devices whose web browsers do not implement support for plugins, there are efforts to target those users through compilation to JavaScript.
 It is possible to either compile the source code or JVM bytecode to JavaScript. 
Compiling the JVM bytecode which is universal across JVM languages allows building upon the existing compiler to bytecode.
Main JVM bytecode to JavaScript compilers are TeaVM,[17] the compiler contained in Dragome Web SDK,[18] Bck2Brwsr,[19] and j2js-compiler.
Leading compilers from JVM languages to JavaScript include the Java to JavaScript compiler contained in Google Web Toolkit, Clojure script (Clojure), GrooScript (Groovy), Scala.js (Scala) and others.[21]
Java Runtime Environment from Oracle
Main article: HotSpot
The Java Runtime Environment (JRE) released by Oracle is a software distribution containing a stand-alone Java VM (HotSpot), browser plugin, Java standard libraries and a configuration tool. 
It is the most common Java environment installed on Windows computers. 
It is freely available for download at the website java.com.
Performance
Main article: Java performance
The JVM specification gives a lot of leeway to implementors regarding the implementation details. 
Since Java 1.3, JRE from Oracle contains a JVM called HotSpot. 
It has been designed to be a high-performance JVM.
To speed-up code execution, HotSpot relies on just-in-time compilation.
 To speed-up object allocation and garbage collection, HotSpot uses generational heap.
The Java virtual machine heap is the area of memory used by the JVM for dynamic memory allocation.
In HotSpot the heap is divided into generations:
The young generation stores short-lived objects that are created and immediately garbage collected.
Objects that persist longer are moved to the old generation (also called the tenured generation). This memory is subdivided into (two) Survivors spaces where the objects that survived the first and next garbage collections are stored.
The permanent generation (or permgen) was used for class definitions and associated metadata prior to Java 8. 
Permanent generation was not part of the heap.[23][24] The permanent generation was removed from Java 8.
Originally there was no permanent generation, and objects and classes were stored together in the same area. 
But as class unloading occurs much more rarely than objects are collected, moving class structures to a specific area allowed significant performance improvements.[23]
Security
Oracle's JRE is installed on a large number of computers. 
Since any web page the user visits may run Java applets, Java provides an easily accessible attack surface to malicious web sites that the user visits. 
Kaspersky Labs reports that the Java web browser plugin is the method of choice for computer criminals. 
Java exploits are included in many exploit packs that hackers deploy onto hacked web sites.
In the past, end users were often using an out-of-date version of JRE which was vulnerable to many known attacks.
 This led to the widely shared belief between users that Java is inherently insecure.
Since Java 1.7, Oracle's JRE for Windows includes automatic update functionality.
Toolbar controversy
Beginning in 2005, Sun's (now Oracle's) JRE included unrelated software which was installed by default. 
In the beginning it was Google Toolbar, later MSN Toolbar, Yahoo Toolbar and finally the Ask Toolbar. 
The Ask Toolbar proved to be especially controversial. 
There has been a petition asking Oracle to remove it.
 The signers voiced their belief that Oracle was "violating the trust of the hundreds of millions of users who run Java on their machines.
 They are tarnishing the reputation of a once proud platform".
 Zdnet called their conduct deceptive, since the installer continued to offer the toolbar during every update, even after the user had previously refused to install it, increasing the chances of the toolbar being installed when the user was too busy or distracted.
In June 2015, Oracle announced that it had ended its partnership with Ask.com in favor of one with Yahoo!, in which users will be, by default, asked to change their home page and default search engine to that of Yahoo.
Cloud computing, also known as on-demand computing, is a kind of Internet-based computing that provides shared processing resources and data to computers and other devices on-demand. 
It is a model for enabling ubiquitous, on-demand access to a shared pool of configurable computing resources.
 Cloud computing and storage solutions provide users and enterprises with various capabilities to store and process their data in third-party data centers.
 It relies on sharing of resources to achieve coherence and economies of scale, similar to a utility (like the electricity grid) over a network. 
At the foundation of cloud computing is the broader concept of converged infrastructure and shared services.
Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications and services) that can be rapidly provisioned and released with minimal management effort.
Proponents claim that cloud computing allows companies to avoid upfront infrastructure costs, and focus on projects that differentiate their businesses instead of on infrastructure.
 Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and enables IT to more rapidly adjust resources to meet fluctuating and unpredictable business demand.
 Cloud providers typically use a "pay as you go" model.
 This can lead to unexpectedly high charges if administrators do not adapt to the cloud pricing model.
The present availability of high-capacity networks, low-cost computers and storage devices as well as the widespread adoption of hardware virtualization, service-oriented architecture, and autonomic and utility computing have led to a growth in cloud computing.
 Companies can scale up as computing needs increase and then scale down again as demands decrease.
Cloud computing has become a highly demanded service or utility due to the advantages of high computing power, cheap cost of services, high performance, scalability, accessibility as well as availability. 
Some cloud vendors are experiencing growth rates of 50% per annum.
 But due to being in a stage of infancy, it still has pitfalls that need proper attention to make cloud computing services more reliable and user friendly
The origin of the term cloud computing is unclear. 
The word "cloud" is commonly used in science to describe a large agglomeration of objects that visually appear from a distance as a cloud and describes any set of things whose details are not inspected further in a given context.
Another explanation is that the old programs that drew network schematics surrounded the icons for servers with a circle, and a cluster of servers in a network diagram had several overlapping circles, which resembled a cloud.[15]
In analogy to above usage the word cloud was used as a metaphor for the Internet and a standardized cloud-like shape was used to denote a network on telephony schematics and later to depict the Internet in computer network diagrams. 
With this simplification, the implication is that the specifics of how the end points of a network are connected are not relevant for the purposes of understanding the diagram. 
The cloud symbol was used to represent networks of computing equipment in the original ARPANET by as early as 1977,[16] and the CSNET by 1981[17]—both predecessors to the Internet itself.
The term cloud has been used to refer to platforms for distributed computing.
 In Wired's April 1994 feature "Bill and Andy's Excellent Adventure II" on the Apple spin-off General Magic, Andy Hertzfeld comments on General Magic's distributed programming language Telescript that:
"The beauty of Telescript ... is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create sort of a virtual service. 
No one had conceived that before. 
The example Jim White [the designer of Telescript, X.400 and ASN.1] uses now is a date-arranging service where a software agent goes to the flower store and orders flowers and then goes to the ticket shop and gets the tickets for the show, and everything is communicated to both parties."
References to "cloud computing" in its modern sense appeared as early as 1996, with the earliest known mention in a Compaq internal document.
The popularization of the term can be traced to 2006 when Amazon.com introduced the Elastic Compute Cloud.
During the mid-1970s, Time-sharing was popularly known as RJE (Remote Job Entry);[citation needed] this terminology was mostly associated with large vendors such as IBM and DEC.
IBM developed the VM Operating System (first released in 1972) to provide time-sharing services[citation needed] via virtual machines.
The 1990s
In the 1990s, telecommunications companies, who previously offered primarily dedicated point-to-point data circuits, began offering virtual private network (VPN) services with comparable quality of service, but at a lower cost. 
By switching traffic as they saw fit to balance server use, they could use overall network bandwidth more effectively.
 They began to use the cloud symbol to denote the demarcation point between what the provider was responsible for and what users were responsible for.
 Cloud computing extends this boundary to cover all servers as well as the network infrastructure.[21]
As computers became more prevalent, scientists and technologists explored ways to make large-scale computing power available to more users through time-sharing.
 They experimented with algorithms to optimize the infrastructure, platform, and applications to prioritize CPUs and increase efficiency for end users.
The New Millennium: 2000s
Since 2000, cloud computing has come into existence. 
In early 2008, NASA's OpenNebula, enhanced in the RESERVOIR European Commission-funded project, became the first open-source software for deploying private and hybrid clouds, and for the federation of clouds.
 In the same year, efforts were focused on providing quality of service guarantees (as required by real-time interactive applications) to cloud-based infrastructures, in the framework of the IRMOS European Commission-funded project, resulting in a real-time cloud environment.
 Microsoft Azure was announced as "Azure" in October 2008 and released on 1 February 2010 as Windows Azure, before being renamed to Microsoft Azure on 25 March 2014.
For a time, Azure was on the TOP500 supercomputer list, before it dropped off it.
In July 2010, Rackspace Hosting and NASA jointly launched an open-source cloud-software initiative known as OpenStack. 
The OpenStack project intended to help organizations offer cloud-computing services running on standard hardware.
 The early code came from NASA's Nebula platform as well as from Rackspace's Cloud Files platform.
On March 1, 2011, IBM announced the IBM SmartCloud framework to support Smarter Planet.
 Among the various components of the Smarter Computing foundation, cloud computing is a critical piece.
On June 7, 2012, Oracle announced the Oracle Cloud.
 While aspects of the Oracle Cloud are still in development, this cloud offering is poised to be the first to provide users with access to an integrated set of IT solutions, including the Applications (SaaS), Platform (PaaS), and Infrastructure (IaaS) layers.
Similar concepts
Cloud computing is the result of the evolution and adoption of existing technologies and paradigms. 
The goal of cloud computing is to allow users to take beneﬁt from all of these technologies, without the need for deep knowledge about or expertise with each one of them. 
The cloud aims to cut costs, and helps the users focus on their core business instead of being impeded by IT obstacles.
The main enabling technology for cloud computing is virtualization. 
Virtualization software separates a physical computing device into one or more "virtual" devices, each of which can be easily used and managed to perform computing tasks.
 With operating system–level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. 
Virtualization provides the agility required to speed up IT operations, and reduces cost by increasing infrastructure utilization. 
Autonomic computing automates the process through which the user can provision resources on-demand.
 By minimizing user involvement, automation speeds up the process, reduces labor costs and reduces the possibility of human errors.
Users routinely face difficult business problems. 
Cloud computing adopts concepts from Service-oriented Architecture (SOA) that can help the user break these problems into services that can be integrated to provide a solution.
 Cloud computing provides all of its resources as services, and makes use of the well-established standards and best practices gained in the domain of SOA to allow global and easy access to cloud services in a standardized way.
Cloud computing also leverages concepts from utility computing to provide metrics for the services used. 
Such metrics are at the core of the public cloud pay-per-use models.
 In addition, measured services are an essential part of the feedback loop in autonomic computing, allowing services to scale on-demand and to perform automatic failure recovery.
Cloud computing is a kind of grid computing; it has evolved by addressing the QoS (quality of service) and reliability problems. 
Cloud computing provides the tools and technologies to build data/compute intensive parallel applications with much more affordable prices compared to traditional parallel computing techniques.
Cloud computing shares characteristics with:
Client–server model—Client–server computing refers broadly to any distributed application that distinguishes between service providers (servers) and service requestors (clients).
Grid computing—"A form of distributed and parallel computing, whereby a 'super and virtual computer' is composed of a cluster of networked, loosely coupled computers acting in concert to perform very large tasks."
Fog computing—Distributed computing paradigm that provides data, compute, storage and application services closer to client or near-user edge devices, such as network routers.
 Furthermore, fog computing handles data at the network level, on smart devices and on the end-user client side (e.g. mobile devices), instead of sending data to a remote location for processing.
Dew computing—In the existing computing hierarchy, the Dew computing is positioned as the ground level for the cloud and fog computing paradigms. 
Compared to fog computing, which supports emerging IoT applications that demand real-time and predictable latency and the dynamic network reconfigurability, Dew computing pushes the frontiers to computing applications, data, and low level services away from centralized virtual nodes to the end users.
Mainframe computer—Powerful computers used mainly by large organizations for critical applications, typically bulk data processing such as: census; industry and consumer statistics; police and secret intelligence services; enterprise resource planning; and financial transaction processing.
Utility computing—The "packaging of computing resources, such as computation and storage, as a metered service similar to a traditional public utility, such as electricity."
Peer-to-peer—A distributed architecture without the need for central coordination. 
Participants are both suppliers and consumers of resources (in contrast to the traditional client–server model).
Characteristics
Cloud computing exhibits the following key characteristics:

Agility improves with users' ability to re-provision technological infrastructure resources.[wtf?]
Cost reductions claimed by cloud providers.
A public-cloud delivery model converts capital expenditure to operational expenditure.
 This purportedly lowers barriers to entry, as infrastructure is typically provided by a third party and need not be purchased for one-time or infrequent intensive computing tasks. 
Pricing on a utility computing basis is fine-grained, with usage-based options and fewer IT skills are required for implementation (in-house).
The e-FISCAL project's state-of-the-art repository[42] contains several articles looking into cost aspects in more detail, most of them concluding that costs savings depend on the type of activities supported and the type of infrastructure available in-house.
Device and location independence[43] enable users to access systems using a web browser regardless of their location or what device they use (e.g., PC, mobile phone). 
As infrastructure is off-site (typically provided by a third-party) and accessed via the Internet, users can connect from anywhere.[41]
Maintenance of cloud computing applications is easier, because they do not need to be installed on each user's computer and can be accessed from different places.
Multitenancy enables sharing of resources and costs across a large pool of users thus allowing for:
centralization of infrastructure in locations with lower costs (such as real estate, electricity, etc.)
peak-load capacity increases (users need not engineer for highest possible load-levels)
utilisation and efficiency improvements for systems that are often only 10–20% utilised.[44][45]
Performance is monitored, and consistent and loosely coupled architectures are constructed using web services as the system interface.[41][46][47]
Productivity may be increased when multiple users can work on the same data simultaneously, rather than waiting for it to be saved and emailed. Time may be saved as information does not need to be re-entered when fields are matched, nor do users need to install application software upgrades to their computer.
Reliability improves with the use of multiple redundant sites, which makes well-designed cloud computing suitable for business continuity and disaster recovery.
Scalability and elasticity via dynamic ("on-demand") provisioning of resources on a fine-grained, self-service basis in near real-time[50][51] (Note, the VM startup time varies by VM type, location, OS and cloud providers[50]), without users having to engineer for peak loads.
 This gives the ability to scale up when the usage need increases or down if resources are not being used.
Security can improve due to centralization of data, increased security-focused resources, etc., but concerns can persist about loss of control over certain sensitive data, and the lack of security for stored kernels. 
In addition, user access to security audit logs may be difficult or impossible. 
Private cloud installations are in part motivated by users' desire to retain control over the infrastructure and avoid losing control of information security.
The National Institute of Standards and Technology's definition of cloud computing identifies "five essential characteristics":
On-demand self-service. 
A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider.
Broad network access.
Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations).
Resource pooling. 
The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand. 
Rapid elasticity.
 Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. 
To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time.
Measured service. 
Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). 
Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service.
— National Institute of Standards and Technology[57]
Service models
Though service-oriented architecture advocates "everything as a service" (with the acronyms EaaS or XaaS or simply aas),[58] cloud-computing providers offer their "services" according to different models,need quotation to verify] which happen to form a stack: infrastructure-, platform- and software-as-a-service.
Cloud-computing layers accessible within a stack
Infrastructure as a service (IaaS)
See also: Category:Cloud infrastructure
In the most basic cloud-service model—and according to the IETF (Internet Engineering Task Force)—providers of IaaS offer computers—physical or (more often) virtual machines—and other resources. 
IaaS refers to online services that abstract user from the detail of infrastructure like physical computing resources, location, data partitioning, scaling, security, backup etc.
 A hypervisor, such as Xen, Oracle VirtualBox, KVM, VMware ESX/ESXi, or Hyper-V runs the virtual machines as guests.
 Pools of hypervisors within the cloud operational system can support large numbers of virtual machines and the ability to scale services up and down according to customers' varying requirements. 
IaaS clouds often offer additional resources such as a virtual-machine disk-image library, raw block storage, file or object storage, firewalls, load balancers, IP addresses, virtual local area networks (VLANs), and software bundles.
 IaaS-cloud providers supply these resources on-demand from their large pools of equipment installed in data centers. 
For wide-area connectivity, customers can use either the Internet or carrier clouds (dedicated virtual private networks).
To deploy their applications, cloud users install operating-system images and their application software on the cloud infrastructure.
[unreliable source?] In this model, the cloud user patches and maintains the operating systems and the application software. 
Cloud providers typically bill IaaS services on a utility computing basis: cost reflects the amount of resources allocated and consumed.
Platform as a service (PaaS)
Main article: Platform as a service
See also: Category:Cloud platforms
PaaS vendors offer a development environment to application developers.
 The provider typically develops toolkit and standards for development and channels for distribution and payment. 
In the PaaS models, cloud providers deliver a computing platform, typically including operating system, programming-language execution environment, database, and web server. 
Application developers can develop and run their software solutions on a cloud platform without the cost and complexity of buying and managing the underlying hardware and software layers. 
With some PaaS offers like Microsoft Azure and Google App Engine, the underlying computer and storage resources scale automatically to match application demand so that the cloud user does not have to allocate resources manually. 
The latter has also been proposed by an architecture aiming to facilitate real-time in cloud environments.
need quotation to verify] Even more specific application types can be provided via PaaS, such as media encoding as provided by services like bitcodin.com or media.io.
Some integration and data management providers have also embraced specialized applications of PaaS as delivery models for data solutions. 
Examples include iPaaS and dPaaS. iPaaS (Integration Platform as a Service) enables customers to develop, execute and govern integration flows.
 Under the iPaaS integration model, customers drive the development and deployment of integrations without installing or managing any hardware or middleware.
 dPaaS (Data Platform as a Service) delivers integration—and data-management—products as a fully managed service.
Under the dPaaS model, the PaaS provider, not the customer, manages the development and execution of data solutions by building tailored data applications for the customer.
 dPaaS users retain transparency and control over data through data-visualization tools.
Software as a service (SaaS)
Main article: Software as a service
In the software as a service (SaaS) model, users gain access to application software and databases.
 Cloud providers manage the infrastructure and platforms that run the applications. 
SaaS is sometimes referred to as "on-demand software" and is usually priced on a pay-per-use basis or using a subscription fee.
In the SaaS model, cloud providers install and operate application software in the cloud and cloud users access the software from cloud clients.
 Cloud users do not manage the cloud infrastructure and platform where the application runs. 
This eliminates the need to install and run the application on the cloud user's own computers, which simplifies maintenance and support. 
Cloud applications differ from other applications in their scalability—which can be achieved by cloning tasks onto multiple virtual machines at run-time to meet changing work demand.
 Load balancers distribute the work over the set of virtual machines.
 This process is transparent to the cloud user, who sees only a single access-point. 
To accommodate a large number of cloud users, cloud applications can be multitenant, meaning that any machine may serve more than one cloud-user organization.
The pricing model for SaaS applications is typically a monthly or yearly flat fee per user,[75] so prices become scalable and adjustable if users are added or removed at any point.
Proponents claim that SaaS gives a business the potential to reduce IT operational costs by outsourcing hardware and software maintenance and support to the cloud provider. 
This enables the business to reallocate IT operations costs away from hardware/software spending and from personnel expenses, towards meeting other goals. 
In addition, with applications hosted centrally, updates can be released without the need for users to install new software. 
One drawback of SaaS comes with storing the users' data on the cloud provider's server. 
As a result,[citation needed] there could be unauthorized access to the data. 
For this reason, users are increasingly[quantify] adopting intelligent third-party key-management systems to help secure their data.
Cloud clients
See also: Category:Cloud clients and Cloud API
Users access cloud computing using networked client devices, such as desktop computers, laptops, tablets and smartphones and any Ethernet enabled device such as Home Automation Gadgets.
 Some of these devices—cloud clients—rely on cloud computing for all or a majority of their applications so as to be essentially useless without it. 
Examples are thin clients and the browser-based Chromebook.
 Many cloud applications do not require specific software on the client and instead use a web browser to interact with the cloud application.
 With Ajax and HTML5 these Web user interfaces can achieve a similar, or even better, look and feel to native applications. 
Some cloud applications, however, support specific client software dedicated to these applications (e.g., virtual desktop clients and most email clients).
 Some legacy applications (line of business applications that until now have been prevalent in thin client computing) are delivered via a screen-sharing technology.
Private cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third-party, and hosted either internally or externally.
Undertaking a private cloud project requires a significant level and degree of engagement to virtualize the business environment, and requires the organization to reevaluate decisions about existing resources. 
When done right, it can improve business, but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities. 
Self-run data centers[77] are generally capital intensive. 
They have a significant physical footprint, requiring allocations of space, hardware, and environmental controls. 
These assets have to be refreshed periodically, resulting in additional capital expenditures. 
They have attracted criticism because users "still have to buy, build, and manage them" and thus do not benefit from less hands-on management,[78] essentially "[lacking] the economic model that makes cloud computing such an intriguing concept".
Public cloud
A cloud is called a "public cloud" when the services are rendered over a network that is open for public use. 
Public cloud services may be free.
 Generally, public cloud service providers like Amazon AWS, Microsoft and Google own and operate the infrastructure at their data center and access is generally via the Internet. 
AWS and Microsoft also offer direct connect services called "AWS Direct Connect" and "Azure ExpressRoute" respectively, such connections require customers to purchase or lease a private connection to a peering point offered by the cloud provider.
Hybrid cloud
Hybrid cloud is a composition of two or more clouds (private, community or public) that remain distinct entities but are bound together, offering the benefits of multiple deployment models.
 Hybrid cloud can also mean the ability to connect collocation, managed and/or dedicated services with cloud resources.
Gartner, Inc. defines a hybrid cloud service as a cloud computing service that is composed of some combination of private, public and community cloud services, from different service providers.
 A hybrid cloud service crosses isolation and provider boundaries so that it can't be simply put in one category of private, public, or community cloud service. 
It allows one to extend either the capacity or the capability of a cloud service, by aggregation, integration or customization with another cloud service.
Varied use cases for hybrid cloud composition exist. 
For example, an organization may store sensitive client data in house on a private cloud application, but interconnect that application to a business intelligence application provided on a public cloud as a software service.
 This example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services. 
Hybrid cloud adoption depends on a number of factors such as data security and compliance requirements, level of control needed over data, and the applications an organization uses.
Another example of hybrid cloud is one where IT organizations use public cloud computing resources to meet temporary capacity needs that can not be met by the private cloud.
This capability enables hybrid clouds to employ cloud bursting for scaling across clouds.
 Cloud bursting is an application deployment model in which an application runs in a private cloud or data center and "bursts" to a public cloud when the demand for computing capacity increases.
 A primary advantage of cloud bursting and a hybrid cloud model is that an organization only pays for extra compute resources when they are needed.
Cloud bursting enables data centers to create an in-house IT infrastructure that supports average workloads, and use cloud resources from public or private clouds, during spikes in processing demands.
The specialized model of hybrid cloud, which is built atop heterogeneous hardware, is called "Cross-platform Hybrid Cloud".
 A cross-platform hybrid cloud is usually powered by different CPU architectures, for example, x86-64 and ARM, underneath. 
Users can transparently deploy and scale applications without knowledge of the cloud's hardware diversity.
 This kind of cloud emerges from the raise of ARM-based system-on-chip for server-class computing.
Others
Community cloud
Community cloud shares infrastructure between several organizations from a specific community with common concerns (security, compliance, jurisdiction, etc.), whether managed internally or by a third-party, and either hosted internally or externally. 
The costs are spread over fewer users than a public cloud (but more than a private cloud), so only some of the cost savings potential of cloud computing are realized.
Distributed cloud
A cloud computing platform can be assembled from a distributed set of machines in different locations, connected to a single network or hub service. 
It is possible to distinguish between two types of distributed clouds: public-resource computing and volunteer cloud.
Public-resource computing—This type of distributed cloud results from an expansive definition of cloud computing, because they are more akin to distributed computing than cloud computing. 
Nonetheless, it is considered a sub-class of cloud computing, and some examples include distributed computing platforms such as BOINC and Folding@Home.
Volunteer cloud—Volunteer cloud computing is characterized as the intersection of public-resource computing and cloud computing, where a cloud computing infrastructure is built using volunteered resources. 
Many challenges arise from this type of infrastructure, because of the volatility of the resources used to built it and the dynamic environment it operates in. 
It can also be called peer-to-peer clouds, or ad-hoc clouds. 
An interesting effort in such direction is Cloud@Home, it aims to implement a cloud computing infrastructure using volunteered resources providing a business-model to incentivize contributions through financial restitution.
Intercloud
Main article: Intercloud
The Intercloud[90] is an interconnected global "cloud of clouds"[91][92] and an extension of the Internet "network of networks" on which it is based. 
The focus is on direct interoperability between public cloud service providers, more so than between providers and consumers (as is the case for hybrid- and multi-cloud).
Multicloud
Main article: Multicloud
Multicloud is the use of multiple cloud computing services in a single heterogeneous architecture to reduce reliance on single vendors, increase flexibility through choice, mitigate against disasters, etc. 
It differs from hybrid cloud in that it refers to multiple cloud services, rather than multiple deployment modes (public, private, legacy).
Architecture
Cloud computing sample architecture
Cloud architecture,[99] the systems architecture of the software systems involved in the delivery of cloud computing, typically involves multiple cloud components communicating with each other over a loose coupling mechanism such as a messaging queue.
 Elastic provision implies intelligence in the use of tight or loose coupling as applied to mechanisms such as these and others.
Cloud engineering
Cloud engineering is the application of engineering disciplines to cloud computing. 
It brings a systematic approach to the high-level concerns of commercialization, standardization, and governance in conceiving, developing, operating and maintaining cloud computing systems.
 It is a multidisciplinary method encompassing contributions from diverse areas such as systems, software, web, performance, information, security, platform, risk, and quality engineering.
Security and privacy
Main article: Cloud computing issues
Cloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time. 
It could accidentally or deliberately alter or even delete information.
Many cloud providers can share information with third parties if necessary for purposes of law and order even without a warrant.
That is permitted in their privacy policies, which users must agree to before they start using cloud services.
 Solutions to privacy include policy and legislation as well as end users' choices for how data is stored.
 Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access.

According to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and API's, Data Loss & Leakage, and Hardware Failure—which accounted for 29%, 25% and 10% of all cloud security outages respectively. 
Together, these form shared technology vulnerabilities. 
In a cloud provider platform being shared by different users there may be a possibility that information belonging to different customers resides on same data server. 
Therefore, Information leakage may arise by mistake when information for one customer is given to other.
Additionally, Eugene Schultz, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. 
"There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into". 
Because data from hundreds or thousands of companies can be stored on large cloud servers, hackers can theoretically gain control of huge stores of information through a single attack—a process he called "hyperjacking". 
Some examples of this include the Dropbox security breach, and iCloud 2014 leak.
 Dropbox had been breached in October 2014, having over 7 million of its users passwords stolen by hackers in an effort to get monetary value from it by Bitcoins (BTC). 
By having these passwords, they are able to read private data as well as have this data be indexed by search engines (making the information public).[102]
There is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?).
 Many Terms of Service agreements are silent on the question of ownership.
Physical control of the computer equipment (private cloud) is more secure than having the equipment off site and under someone else's control (public cloud).
 This delivers great incentive to public cloud computing service providers to prioritize building and maintaining strong management of secure services.
Some small businesses that don't have expertise in IT security could find that it's more secure for them to use a public cloud.
There is the risk that end users don't understand the issues involved when signing on to a cloud service (persons sometimes don't read the many pages of the terms of service agreement, and just click "Accept" without reading).
 This is important now that cloud computing is becoming popular and required for some services to work, for example for an intelligent personal assistant (Apple's Siri or Google Now).
Fundamentally private cloud is seen as more secure with higher levels of control for the owner, however public cloud is seen to be more flexible and requires less time and money investment from the user.[105]
The future
Cloud computing is therefore still as much a research topic, as it is a market offering.
 What is clear through the evolution of cloud computing services is that the chief technical officer (CTO) is a major driving force behind cloud adoption.
 The major cloud technology developers continue to invest billions a year in cloud R&D; for example: in 2011 Microsoft committed 90% of its US$9.6bn R&D budget to its cloud.
 Centaur Partners also predict that SaaS revenue will grow from US$13.5B in 2011 to $32.8B in 2016.
This expansion also includes Finance and Accounting SaaS.
Additionally, more industries are turning to cloud technology as an efficient way to improve quality services due to its capabilities to reduce overhead costs, downtime, and automate infrastructure deployment.
A service-oriented architecture (SOA) is an architectural pattern in computer software design in which application components provide services to other components via a communications protocol, typically over a network. 
The principles of service-orientation are independent of any vendor, product or technology.
A service is a self-contained unit of functionality, such as retrieving an online bank statement.
 By that definition, a service is an operation that may be discretely invoked. 
However, in the Web Services Description Language (WSDL), a service is an interface definition that may list several discrete services/operations. 
And elsewhere, the term service is used for a component that is encapsulated behind an interface. This widespread ambiguity is reflected in what follows.
Services can be combined to provide the functionality of a large software application.
 SOA makes it easier for software components on computers connected over a network to cooperate. 
Every computer can run any number of services, and each service is built in a way that ensures that the service can exchange information with any other service in the network without human interaction and without the need to make changes to the underlying program itself.
The OASIS group[4] and the Open Group[5] have both created formal definitions. 
OASIS defines SOA as:
A paradigm for organizing and utilizing distributed capabilities that may be under the control of different ownership domains.
 It provides a uniform means to offer, discover, interact with and use capabilities to produce desired effects consistent with measurable preconditions and expectations.
The Open Group's definition is:
Service-Oriented Architecture (SOA) is an architectural style that supports service-orientation. 
Service-orientation is a way of thinking in terms of services and service-based development and the outcomes of services.
A service:
Is a logical representation of a repeatable business activity that has a specified outcome (e.g., check customer credit, provide weather data, consolidate drilling reports)
Is self-contained
May be composed of other services
Is a "black box" to consumers of the service
Overview
Services are unassociated, loosely coupled units of functionality that are self-contained. 
Each service implements at least one action, such as submitting an online application for an account, retrieving an online bank statement or modifying an online booking or airline ticket order. 
Within a SOA, services use defined protocols that describe how services pass and parse messages using description metadata, which in sufficient details describes not only the characteristics of these services, but also the data that drives them.
Programmers have made extensive use of XML in SOA to structure data that they wrap in a nearly exhaustive description-container.
 Analogously, the Web Services Description Language (WSDL) typically describes the services themselves, while SOAP (originally Simple Object Access Protocol) describes the communications protocols. 
SOA depends on data and services that are described by metadata that should meet the following two criteria:
The metadata should be provided in a form that software systems can use to configure dynamically by discovery and incorporation of defined services, and also to maintain coherence and integrity. 
For example, metadata could be used by other applications, like a catalogue, to perform auto discovery of services without modifying the functional contract of a service.
The metadata should be provided in a form that system designers can understand and manage with a reasonable expenditure of cost and effort.
The purpose of SOA is to allow users to combine fairly large chunks of functionality to form ad hoc applications built almost entirely from existing software services.
 The larger the chunks, the fewer the interfaces required to implement any given set of functionality; however, very large chunks of functionality may not prove sufficiently granular for easy reuse. 
Each interface brings with it some amount of processing overhead, so there is a performance consideration in choosing the granularity of services.
SOA as an architecture relies on service-orientation as its fundamental design principle.
 If a service presents a simple interface that abstracts away its underlying complexity, then users can access independent services without knowledge of the service's platform implementation.[6]
SOA framework
SOA-based solutions endeavour to enable business objectives while building an enterprise-quality system. 
SOA architecture is viewed as five horizontal layers:[7]
Consumer Interface Layer – These are GUI for end users or apps accessing apps/service interfaces.
Business Process Layer – These are choreographed services representing business use-cases in terms of applications.
Services – Services are consolidated together for whole-enterprise in-service inventory.
Service Components – The components used to build the services, such as functional and technical libraries, technological interfaces etc.
Operational Systems – This layer contains the data models, enterprise data repository, technological platforms etc.
There are four cross-cutting vertical layers, each of which are applied to and supported by each of the following horizontal layers:
Integration Layer – starts with platform integration (protocols support), data integration, service integration, application integration, leading to enterprise application integration supporting B2B and B2C.
Quality of Service – Security, availability, performance etc. 
constitute the quality of service parameters which are configured based on required SLAs, OLAs.
Informational – provide business information.
Governance – IT strategy is governed to each horizontal layer to achieve required operating and capability model.
Design concept
SOA is based on the concept of a service. 
Depending on the service design approach taken, each SOA service is designed to perform one or more activities by implementing one or more service operations.
 As a result, each service is built as a discrete piece of code.
 This makes it possible to reuse the code in different ways throughout the application by changing only the way an individual service interoperates with other services that make up the application, versus making code changes to the service itself. 
SOA design principles are used during software development and integration.
SOA generally provides a way for consumers of services, such as web-based applications, to be aware of available SOA-based services. 
For example, several disparate departments within a company may develop and deploy SOA services in different implementation languages; their respective clients will benefit from a well-defined interface to access them.
SOA defines how to integrate widely disparate applications for a Web-based environment and uses multiple implementation platforms.
 Rather than defining an API, SOA defines the interface in terms of protocols and functionality. 
An endpoint is the entry point for such a SOA implementation.
Service-orientation requires loose coupling of services with operating systems and other technologies that underlie applications. 
SOA separates functions into distinct units, or services,[8] which developers make accessible over a network in order to allow users to combine and reuse them in the production of applications. 
These services and their corresponding consumers communicate with each other by passing data in a well-defined, shared format, or by coordinating an activity between two or more services.[9]
For some, SOA can be seen as part of the continuum which ranges from the older concept of distributed computing[8][10] and modular programming, through SOA, and on to current practices of mashups, SaaS, and cloud computing (which some see as the offspring of SOA).
There are no industry standards relating to the exact composition of a service-oriented architecture, although many industry sources have published their own principles. 
Some of these include the following:
Standardized service contract: Services adhere to a communications agreement, as defined collectively by one or more service-description documents.
Service loose coupling: Services maintain a relationship that minimizes dependencies and only requires that they maintain an awareness of each other.
Service abstraction: Beyond descriptions in the service contract, services hide logic from the outside world.
Service reusability: Logic is divided into services with the intention of promoting reuse.
Service autonomy: Services have control over the logic they encapsulate, from a Design-time and a Run-time perspective.
Service statelessness: Services minimize resource consumption by deferring the management of state information when necessary[16]
Service discoverability: Services are supplemented with communicative meta data by which they can be effectively discovered and interpreted.
Service composability: Services are effective composition participants, regardless of the size and complexity of the composition.
Service granularity: A design consideration to provide optimal scope and right granular level of the business functionality in a service operation.
Service normalization: Services are decomposed or consolidated to a level of normal form to minimize redundancy. 
In some cases, services are denormalized for specific purposes, such as performance optimization, access, and aggregation.[17]
Service optimization: All else being equal, high-quality services are generally preferable to low-quality ones.
Service relevance: Functionality is presented at a granularity recognized by the user as a meaningful service.
Service encapsulation: Many services are consolidated for use under the SOA. 
Often such services were not planned to be under SOA.
Service location transparency: This refers to the ability of a service consumer to invoke a service regardless of its actual location in the network. 
This also recognizes the discoverability property (one of the core principle of SOA) and the right of a consumer to access the service. 
Often, the idea of service virtualization also relates to location transparency. 
This is where the consumer simply calls a logical service while a suitable SOA-enabling runtime infrastructure component, commonly a service bus, maps this logical service call to a physical service.
Service architecture
This is the physical design of an individual service that encompasses all the resources used by a service.
 This would normally include databases, software components, legacy systems, identity stores, XML schemas and any backing stores, e.g. shared directories. 
It is also beneficial to include any service agents employed by the service, as any change in these service agents would affect the message processing capabilities of the service.
The (standardized service contract) design principle keeps service contracts independent from their implementation. 
The service contract needs to be documented to formalize the required processing resources by the individual service capabilities. 
Although it is beneficial to document details about the service architecture, the service abstraction design principle dictates that any internal details about the service are invisible to its consumers so that they do not develop any unstated couplings.
 The service architecture serves as a point of reference for evolving the service or gauging the impact of any change in the service.
Service composition architecture
One of the core characteristics of services developed using the service-orientation design paradigm is that they are composition-centric. 
Services with this characteristic can potentially address novel requirements by recomposing the same services in different configurations. 
Service composition architecture is itself a composition of the individual architectures of the participating services. 
In the light of the Service Abstraction principle, this type of architecture only documents the service contract and any published service-level agreement (SLA); internal details of each service are not included.
If a service composition is a part of another (parent) composition, the parent composition can also be referenced in the child service composition. 
The design of service composition also includes any alternate paths, such as error conditions, which may introduce new services into the current service composition.
Service composition is also a key technique in software integration, including enterprise software integration, business process composition and workflow composition.
Service inventory architecture
A service inventory is composed of services that automate business processes. 
It is important to account for the combined processing requirements of all services within the service inventory.
 Documenting the requirements of services, independently from the business processes that they automate, helps identify processing bottlenecks.
 The service inventory architecture is documented from the service inventory blueprint, so that service candidates[18] can be redesigned before their implementation.
Service-oriented enterprise architecture
This umbrella architecture incorporates service, composition, and inventory architectures, plus any enterprise-wide technological resources accessed by these architectures e.g. an ERP system.
 This can be further supplemented by including enterprise-wide standards that apply to the aforementioned architecture types.
 Any segments of the enterprise that are not service-oriented can also be documented in order to consider transformation requirements if a service needs to communicate with the business processes automated by such segments. 
SOA's main goal is to deliver agility to business.
Web services approach
Web services can implement a service-oriented architecture.
 They make functional building-blocks accessible over standard Internet protocols independent of platforms and programming languages. 
These services can represent either new applications or just wrappers around existing legacy systems to make them network-enabled.
Each SOA building block can play one or both of two roles:
Service provider: The service provider creates a web service and possibly publishes its interface and access information to the service registry. 
Each provider must decide which services to expose, how to make trade-offs between security and easy availability, how to price the services, or (if no charges apply) how/whether to exploit them for other value. 
he provider also has to decide what category the service should be listed in for a given broker service and what sort of trading partner agreements are required to use the service. 
It registers what services are available within it, and lists all the potential service recipients. 
The implementer of the broker then decides the scope of the broker. 
Public brokers are available through the Internet, while private brokers are only accessible to a limited audience, for example, users of a company intranet. 
Furthermore, the amount of the offered information has to be decided. 
Some brokers specialize in many listings.
 Others offer high levels of trust in the listed services. 
Some cover a broad landscape of services and others focus within an industry. 
Some brokers catalog other brokers. 
Depending on the business model, brokers can attempt to maximize look-up requests, number of listings or accuracy of the listings. 
The Universal Description Discovery and Integration (UDDI) specification defines a way to publish and discover information about Web services. 
Other service broker technologies include (for example) ebXML (Electronic Business using eXtensible Markup Language) and those based on the ISO/IEC 11179 Metadata Registry (MDR) standard.
Service consumer: The service consumer or web service client locates entries in the broker registry using various find operations and then binds to the service provider in order to 
invoke one of its web services. 
Whichever service the service-consumers need, they have to take it into the brokers, bind it with respective service and then use it. 
They can access multiple services if the service provides multiple services.
Web service protocols
See also: List of web service protocols
Implementers commonly build SOAs using web services standards (for example, SOAP) that have gained broad industry acceptance after recommendation of Version 1.2 from the W3C[20] (World Wide Web Consortium) in 2003. 
These standards (also referred to as web service specifications) also provide greater interoperability and some protection from lock-in to proprietary vendor software. 
One can, however, implement SOA using any service-based technology, such as Jini, CORBA or REST.
API and ESB in Context of SoA
In practice it has shown that communication about SoA is often tainted by product specific concepts along with a tendency to generalize individual solutions and an unprecise usage of terminology. 
Especially the usage of the terms SoA versus ESB and Application programming interface are generally confused.
Service oriented Architecture is a design concept that breaks down every application in modules ("service") that can be requested by a client application to fulfil a certain activity based on an input and optionally return a result. 
API stands for Application Programming Interface.
Since a service runs in unattended mode it naturally has an API otherwise it could not be activated and consumed, neither locally nor remotely. 
Hence, an API is a naturally accessory of any service. 
This principle is common practice in every local operating system environment in order to keep interactive components, application logic and execution sequence strictly isolated from each other: Screen logic calls the operating system (managed mode) that in turn requests the appropriate service version. 
This allows for reuse but also easy module inspection and module replacement.
In a distributed environment it is analogous, only with even more freedom and flexibility.
 Such agents exists in every client server context, since the layer manages the different speeds and wait times of service communication. 
In a networked system they are mandatory, as applications will not be aware of the actual network conditions. 
Middleware free communication is simply not possible: even if there is no explicit middleware the functionality is individually programmed either into the drivers or the endpoints or the communication web server used instead.
While knowing this it is obvious that even peer-to-peer applications are no direct linkages and require the middleware agent as well. 
Otherwise the endpoints need to program logic to wait for delayed server response, network errors, security and encryption and any kind of Quality of Service.
 The more endpoints exchange information mutually the more the need arises for a well-designed middleware. 
Such a middleware that provides standardized communication services in a multi-peer client server architecture is labelled Enterprise Service Bus (ESB). 
The services provided by an ESB exists in any communication, however often they are programmed individually; they comprise such services like routing, filtering, QoS, encryption, archiving, message queuing, logging, communication trace and others.
Implementations can use one or more of these protocols and, for example, might use a file-system mechanism to communicate data conforming to a defined interface specification between processes conforming to the SOA concept. 
The key is independent services with defined interfaces that can be called to perform their tasks in a standard way, without a service having foreknowledge of the calling application, and without the application having or needing knowledge of how the service actually performs its tasks.
Elements of SOA, by Dirk Krafzig, Karl Banke, and Dirk Slama
SOA meta-model, The Linthicum Group, 2007
Service-Oriented Modeling Framework (SOMF) Version 2.0
SOA enables the development of applications that are built by combining loosely coupled and interoperable services.
These services inter-operate based on a formal definition (or contract, e.g., WSDL) that is independent of the underlying platform and programming language. 
The interface definition hides the implementation of the language-specific service.
 SOA-based systems can therefore function independently of development technologies and platforms (such as Java, .NET, etc.). 
Services written in C# running on .NET platforms and services written in Java running on Java EE platforms, for example, can both be consumed by a common composite application (or client). 
Applications running on either platform can also consume services running on the other as web services that facilitate reuse. 
Managed environments can also wrap COBOL legacy systems and present them as software services. 
This has extended the useful life of many core legacy systems indefinitely[citation needed], no matter what language they originally used.
SOA can support integration and consolidation activities within complex enterprise systems, but SOA does not specify or provide a methodology or framework for documenting capabilities or services.
We can distinguish the Service Object-Oriented Architecture (SOOA), where service providers are network (call/response) objects accepting remote invocations, from the Service Protocol Oriented Architecture (SPOA), where a communication (read/write) protocol is fixed and known beforehand by the provider and requestor. 
Based on that protocol and a service description obtained from the service registry, the requestor can bind to the service provider by creating own proxy used for remote communication over the fixed protocol.
 If a service provider registers its service description by name, the requestors have to know the name of the service beforehand. 
In SOOA, a proxy—an object implementing the same service interfaces as its service provider—is registered with the registries and it is always ready for use by requestors. 
Thus, in SOOA, the service provider owns and publishes the proxy as the active surrogate object with a codebase annotation, e.g., URLs to the code defining proxy behavior (Jini ERI). 
In SPOA, by contrast, a passive service description is registered (e.g., an XML document in WSDL for Web services, or an interface description in IDL for CORBA); the requestor then has to generate the proxy (a stub forwarding calls to a provider) based on a service description and the fixed communication protocol. 
This is referred to as a bind operation. 
The proxy binding operation is not required in SOOA since the requestor holds the active surrogate object obtained via the registry. 
The surrogate object is already bound to the provider that registered it with its appropriate network configuration and its code annotations. 
Web services, OGSA, RMI, and CORBA services cannot change the communication protocol between requestors and providers while the SOOA approach is protocol neutral.
Service-oriented modeling[8] is an SOA framework that identifies the various disciplines that guide SOA practitioners to conceptualize, analyze, design, and architect their service-oriented assets. 
The Service-oriented modeling framework (SOMF) offers a modeling language and a work structure or "map" depicting the various components that contribute to a successful service-oriented modeling approach. 
It illustrates the major elements that identify the “what to do” aspects of a service development scheme. 
The model enables practitioners to craft a project plan and to identify the milestones of a service-oriented initiative. 
SOMF also provides a common modeling notation to address alignment between business and IT organizations.
Organizational benefits
Some enterprise architects believe that SOA can help businesses respond more quickly and more cost-effectively to changing market conditions.
 This style of architecture promotes reuse at the macro (service) level rather than micro (classes) level.
 It can also simplify interconnection to—and usage of—existing IT (legacy) assets.
With SOA, the idea is that an organization can look at a problem holistically. 
A business has more overall control. 
Theoretically there would not be a mass of developers using whatever tool sets might please them.
 But rather they would be coding to a standard that is set within the business. 
They can also develop enterprise-wide SOA that encapsulates a business-oriented infrastructure. 
SOA has also been illustrated as a highway system providing efficiency for car drivers. 
The point being that if everyone had a car, but there was no highway anywhere, things would be limited and disorganized, in any attempt to get anywhere quickly or efficiently. 
IBM Vice President of Web Services Michael Liebow says that SOA "builds highways".
In some respects, SOA could be regarded as an architectural evolution rather than as a revolution. 
It captures many of the best practices of previous software architectures. 
In communications systems, for example, little development of solutions that use truly static bindings to talk to other equipment in the network has taken place. 
By formally embracing a SOA approach, such systems can position themselves to stress the importance of well-defined, highly inter-operable interfaces.
Some[who?] have questioned whether SOA simply revives concepts like modular programming (1970s), event-oriented design (1980s), or interface/component-based design (1990s)[citation needed]. 
SOA promotes the goal of separating users (consumers) from the service implementations. 
Services can therefore be run on various distributed platforms and be accessed across networks. 
This can also maximize reuse of services[citation needed].
A service comprises a stand-alone unit of functionality available only via a formally defined interface. 
Services can be some kind of "nano-enterprises" that are easy to produce and improve. 
Also services can be "mega-corporations" constructed as the coordinated work of subordinate services.
A mature rollout of SOA effectively defines the API of an organization.
Reasons for treating the implementation of services as separate projects from larger projects include:
Separation promotes the concept to the business that services can be delivered quickly and independently from the larger and slower-moving projects common in the organization. 
The business starts understanding systems and simplified user interfaces calling on services. 
This advocates agility. 
That is to say, it fosters business innovations and speeds up time-to-market.[28]
Separation promotes the decoupling of services from consuming projects.
 This encourages good design insofar as the service is designed without knowing who its consumers are.
Documentation and test artifacts of the service are not embedded within the detail of the larger project. 
This is important when the service needs to be reused later.
An indirect benefit of SOA involves dramatically simplified testing. 
Services are autonomous, stateless, with fully documented interfaces, and separate from the cross-cutting concerns of the implementation.
If an organization possesses appropriately defined test data, then a corresponding stub is built that reacts to the test data when a service is being built. 
A full set of regression tests, scripts, data, and responses is also captured for the service. 
The service can be tested as a 'black box' using existing stubs corresponding to the services it calls. 
Test environments can be constructed where the primitive and out-of-scope services are stubs, while the remainder of the mesh is test deployments of full services. 
As each interface is fully documented with its own full set of regression test documentation, it becomes simple to identify problems in test services. 
Testing evolves to merely validate that the test service operates according to its documentation, and finds gaps in documentation and test cases of all services within the environment. 
Managing the data state of idempotent services is the only complexity.
Examples may prove useful to aid in documenting a service to the level where it becomes useful. 
The documentation of some APIs within the Java Community Process provide good examples. As these are exhaustive, staff would typically use only important subsets. 
The 'ossjsa.pdf' file within JSR-89 exemplifies such a file.[29]
Challenges
One obvious and common challenge faced involves managing services metadata[citation needed].
 SOA-based environments can include many services that exchange messages to perform tasks. 
Depending on the design, a single application may generate millions of messages.
 Managing and providing information on how services interact can become complex.
 This becomes even more complicated when these services are delivered by different organizations within the company or even different companies (partners, suppliers, etc.). 
This creates huge trust issues across teams; hence SOA Governance comes into the picture.
Another challenge involves the lack of testing in SOA space.
 There are no sophisticated tools that provide testability of all headless services (including message and database services along with web services) in a typical architecture. 
Lack of horizontal trust requires that both producers and consumers test services on a continuous basis. 
SOA's main goal is to deliver agility to businesses[citation needed]. 
Therefore it is important to invest in a testing framework (build it or buy it) that would provide the visibility required to find the culprit in the architecture. 
Business agility requires SOA services to be controlled by the business goals and directives as defined in the business Motivation Model (BMM).[30]
Another challenge relates to providing appropriate levels of security. 
Security models built into an application may no longer suffice when an application exposes its capabilities as services that can be used by other applications. 
That is, application-managed security is not the right model for securing services.
 A number of new technologies and standards have started[when?] to emerge and provide more appropriate models for security in SOA.
Finally, the impact of changing a service that touches multiple business domains will require a higher level of change management governance
As SOA and the WS-* specifications practitioners expand, update and refine their output, they encounter a shortage of skilled people to work on SOA-based systems, including the integration of services and construction of services infrastructure.
Interoperability becomes an important aspect of SOA implementations. 
The WS-I organization has developed basic profile (BP) and basic security profile (BSP) to enforce compatibility.
 WS-I has designed testing tools to help assess whether web services conform to WS-I profile guidelines. 
Additionally, another charter has been established to work on the Reliable Secure Profile.
Significant vendor hype surrounds SOA, which can create exaggerated expectations. 
Product stacks continue to evolve as early adopters test the development and runtime products with real-world problems. 
SOA does not guarantee reduced IT costs, improved systems agility or shorter time to market. 
Successful SOA implementations may realize some or all of these benefits depending on the quality and relevance of the system architecture and design.
Internal IT delivery organizations routinely initiate SOA efforts, and some do a poor job of introducing SOA concepts to a business[citation needed] with the result that SOA remains misunderstood[by whom?] within that business. 
The adoption of SOA starts to meet IT delivery needs instead of those of the business, resulting in an organization with, for example, superlative laptop provisioning services, instead of one that can quickly respond to market opportunities. 
Business leadership also frequently becomes convinced that the organization is executing well on SOA.
One of the most important benefits of SOA is its ease of reuse. 
Therefore accountability and funding models must ultimately evolve within the organization.
 A business unit needs to be encouraged to create services that other units will use.
 Conversely, units must be encouraged to reuse services. 
This requires a few new governance components:
Each business unit creating services must have an appropriate support structure in place to deliver on its service-level obligations, and to support enhancing existing services strictly for the benefit of others. 
This is typically quite foreign to business leaders.
each business unit consuming services accepts the apparent risk of reusing services outside their own control, with the attendant external project dependencies, etc.
An innovative funding model is needed as incentive to drive these behaviors above.
 Business units normally pay the IT organization to assist during projects and then to operate the environment.
 Corporate incentives should discount these costs to service providers and create internal revenue streams from consuming business units to the service provider.
 These streams should be less than the costs of a consumer simply building it the old-fashioned way.
 This is where SOA deployments can benefit from the SaaS monetization architecture.
Criticisms
Some criticisms of SOA depend on conflating SOA with Web services.
 For example, some critics claim SOA results in the addition of XML layers, introducing XML parsing and composition.
In the absence of native or binary forms of remote procedure call (RPC), applications could run more slowly and require more processing power, increasing costs. 
Most implementations do incur these overheads, but SOA can be implemented using technologies (for example, Java Business Integration (JBI), Windows Communication Foundation (WCF) and data distribution service (DDS)) that do not depend on remote procedure calls or translation through XML. 
At the same time, emerging open-source XML parsing technologies (such as VTD-XML) and various XML-compatible binary formats promise to significantly improve SOA performance. 
Services implemented using JSON instead of XML do not suffer from this performance concern.
Stateful services require both the consumer and the provider to share the same consumer-specific context, which is either included in or referenced by messages exchanged between the provider and the consumer.
 This constraint has the drawback that it could reduce the overall scalability of the service provider if the service-provider needs to retain the shared context for each consumer. 
It also increases the coupling between a service provider and a consumer and makes switching service providers more difficult.
 Ultimately, some critics feel that SOA services are still too constrained by applications they represent.
Another concern relates to the ongoing evolution of WS-* standards and products (e. g., transaction, security), and SOA can thus introduce new risks unless properly managed and estimated with additional budget and contingency for additional proof-of-concept work.
Some critics[who?] regard SOA as merely an obvious evolution of currently well-deployed architectures (open interfaces, etc.).
IT system designs sometimes overlook the desirability of modifying systems readily. 
Many systems, including SOA-based systems, hard-code the operations, goods and services of the organization, thus restricting their online service and business agility in the global marketplace.
The next step in the design process covers the definition of a service delivery platform (SDP) and its implementation. 
In the SDP design phase one defines the business information models, identity management, products, content, devices, and the end-user service characteristics, as well as how agile the system is so that it can deal with the evolution of the business and its customers.
SOA Manifesto
in October 2009, at the 2nd International SOA Symposium, a mixed group of 17 independent SOA practitioners and vendors, the "SOA Manifesto Working Group," announced the publication of the SOA Manifesto.
The SOA Manifesto is a set of objectives and guiding principles that aim to provide a clear understanding and vision of SOA and service-orientation. 
Its purpose is rescuing the SOA concept from an excessive use of the term by the vendor community and "a seemingly endless proliferation of misinformation and confusion."
The manifesto provides a broad definition of SOA, the values it represents for the signatories and some guiding principles. 
The manifesto prioritizes:
Business value over technical strategy
Strategic goals over project-specific benefits
Intrinsic interoperability over custom integration
Shared services over specific-purpose implementations
Flexibility over optimization
Evolutionary refinement over pursuit of initial perfection
As of September 2010, the SOA Manifesto had been signed by more than 700 signatories and had been translated to nine languages.
Extensions
SOA, Web 2.0, services over the messenger, and mashups
Web 2.0, a perceived "second generation" of web activity, primarily features the ability of visitors to contribute information for collaboration and sharing. 
Web 2.0 applications often use RESTful web APIs and commonly feature AJAX based user interfaces, utilizing web syndication, blogs, and wikis. 
While there are no set standards for Web 2.0, it is characterized by building on the existing Web server architecture and using services. 
Web 2.0 can therefore be regarded as displaying some SOA characteristics.
Some commentators[who?] also regard mashups as Web 2.0 applications. 
The term "business mashups" describes web applications that combine content from more than one source into an integrated user experience that shares many of the characteristics of service-oriented business applications (SOBAs).
 SOBAs are applications composed of services in a declarative manner.
 There is ongoing debate about "the collision of Web 2.0, mashups, and SOA," with some stating that Web 2.0 applications are a realization of SOA composite and business applications.
Web 2.0
Tim O'Reilly coined the term "Web 2.0" to describe a perceived, quickly growing set of web-based applications.
 A topic that has experienced extensive coverage involves the relationship between Web 2.0 and Service-Oriented Architectures (SOAs).
SOA is the philosophy of encapsulating application logic in services with a uniformly defined interface and making these publicly available via discovery mechanisms. 
The notion of complexity-hiding and reuse, but also the concept of loosely coupling services has inspired researchers to elaborate on similarities between the two philosophies, SOA and Web 2.0, and their respective applications. 
Some argue Web 2.0 and SOA have significantly different elements and thus can not be regarded “parallel philosophies”, whereas others consider the two concepts as complementary and regard Web 2.0 as the global SOA.
The philosophies of Web 2.0 and SOA serve different user needs and thus expose differences with respect to the design and also the technologies used in real-world applications. 
However, as of 2008, use-cases demonstrated the potential of combining technologies and principles of both Web 2.0 and SOA.
In an "Internet of Services", all people, machines, and goods will have access via the network infrastructure of tomorrow.
 The Internet will thus offer services for all areas of life and business, such as virtual insurance, online banking and music, and so on. 
Those services will require a complex services infrastructure including service-delivery platforms bringing together demand and supply. 
Building blocks for the Internet of Services include SOA, Web 2.0 and semantics on the technology side; as well as novel business models, and approaches to systematic and community-based innovation.[50]
Even though Oracle indicates[citation needed] that Gartner is coining a new term, Gartner analysts indicate that they call this advanced SOA and refer to it as "SOA 2.0".
 Most of the major middleware vendors (e. g., Red Hat, webMethods, TIBCO Software, IBM, Sun Microsystems, and Oracle) have had some form of SOA 2.0 attributes for years.
Digital nervous system
SOA implementations have been described as representing a piece of the larger vision known as the digital nervous system[52][53] or the Zero Latency Enterprise.
Internet of Things
As the idea of SOA is extended to large numbers of devices, we see the emergence of the Internet of Things. 
An approach to control and manage all the flows of information through such devices connecting them as services is called BPM Everywhere.
SOAP, originally an acronym for Simple Object Access Protocol, is a protocol specification for exchanging structured information in the implementation of web services in computer networks. 
It uses XML Information Set for its message format, and relies on application layer protocols, most notably Hypertext Transfer Protocol (HTTP) or Simple Mail Transfer Protocol (SMTP), for message negotiation and transmission.
SOAP can form the foundation layer of a web services protocol stack, providing a basic messaging framework for web services.
 This XML-based protocol consists of three parts:
an envelope, which defines the message structure[1] and how to process it
a set of encoding rules for expressing instances of application-defined datatypes
a convention for representing procedure calls and responses
SOAP has three major characteristics:
extensibility (security and WS-routing are among the extensions under development)
neutrality (SOAP can operate over any transport protocol such as HTTP, SMTP, TCP, UDP, or JMS)
independence (SOAP allows for any programming model)
As an example of what SOAP procedures can do, an application can send a SOAP request to a server that has web services enabled—such as a real-estate price database—with the parameters for a search.
 The server then returns a SOAP response (an XML-formatted document with the resulting data), e.g., prices, location, features. 
Since the generated data comes in a standardized machine-parsable format, the requesting application can then integrate it directly.
The SOAP architecture consists of several layers of specifications for:
message format
Message Exchange Patterns (MEP)
underlying transport protocol bindings
message processing models
protocol extensibility
SOAP evolved as a successor of XML-RPC, though it borrows its transport and interaction neutrality and the envelope/header/body from elsewhere (probably from WDDX).
SOAP was designed as an object-access protocol in 1998 by Dave Winer, Don Box, Bob Atkinson, and Mohsen Al-Ghosein for Microsoft, where Atkinson and Al-Ghosein were working at the time.
 Due to politics within Microsoft,[3] the specification was not made available until it was submitted to IETF 13 September 1999.
 Because of Microsoft's hesitation, Dave Winer shipped XML-RPC in 1998.
The submitted Internet Draft did not reach RFC status and is therefore not considered a "standard" as such. 
Version 1.1 of the specification was published as a W3C Note on 8 May 2000.
 Since version 1.1 did not reach W3C Recommendation status, it can not be considered a "standard" either. 
Version 1.2 of the specification, however, became a W3C recommendation on June 24, 2003.
The SOAP specification[8] was maintained by the XML Protocol Working Group[9] of the World Wide Web Consortium until the group was closed 10 July 2009. 
SOAP originally stood for "Simple Object Access Protocol" but version 1.2 of the standard dropped this acronym.
After SOAP was first introduced, it became the underlying layer of a more complex set of Web Services, based on Web Services Description Language (WSDL), XML Schema and Universal Description Discovery and Integration (UDDI). 
These different services, especially UDDI, have proved to be of far less interest, but an appreciation of them gives a more complete understanding of the expected role of SOAP compared to how web services have actually evolved.
SOAP structure
The SOAP specification defines the messaging framework, which consists of:
The SOAP processing model defining the rules for processing a SOAP message
The SOAP extensibility model defining the concepts of SOAP features and SOAP modules
The SOAP underlying protocol binding framework describing the rules for defining a binding to an underlying protocol that can be used for exchanging SOAP messages between SOAP nodes
The SOAP message construct defining the structure of a SOAP message
Processing model
The SOAP processing model describes a distributed processing model, its participants, the SOAP nodes, and how a SOAP receiver processes a SOAP message. 
The following SOAP nodes are defined:
SOAP sender – a SOAP node that transmits a SOAP message
SOAP receiver – a SOAP node that accepts a SOAP message
SOAP message path – the set of SOAP nodes through which a single SOAP message passes
Initial SOAP sender (Originator) – the SOAP sender that originates a SOAP message at the starting point of a SOAP message path
SOAP intermediary – a SOAP intermediary is both a SOAP receiver and a SOAP sender and is targetable from within a SOAP message. 
It processes the SOAP header blocks targeted at it and acts to forward a SOAP message towards an ultimate SOAP receiver.
Ultimate SOAP receiver – the SOAP receiver that is a final destination of a SOAP message.
 It is responsible for processing the contents of the SOAP body and any SOAP header blocks targeted at it. 
In some circumstances, a SOAP message might not reach an ultimate SOAP receiver, for example because of a problem at a SOAP intermediary.
 An ultimate SOAP receiver cannot also be a SOAP intermediary for the same SOAP message.
Both SMTP and HTTP are valid application layer protocols used as transport for SOAP, but HTTP has gained wider acceptance as it works well with today's internet infrastructure; specifically, HTTP works well with network firewalls.
 SOAP may also be used over HTTPS (which is the same protocol as HTTP at the application level, but uses an encrypted transport protocol underneath) with either simple or mutual authentication; this is the advocated WS-I method to provide web service security as stated in the WS-I Basic Profile 1.1.
This is a major advantage over other distributed protocols like GIOP/IIOP or DCOM, which are normally filtered by firewalls. 
SOAP over AMQP is yet another possibility that some implementations support. 
SOAP also has an advantage over DCOM that it is unaffected by security rights configured on the machines that require knowledge of both transmitting and receiving nodes. 
This lets SOAP be loosely coupled in a way that is not possible with DCOM. There is also the SOAP-over-UDP OASIS standard.
Message format
XML Information Set was chosen as the standard message format because of its widespread use by major corporations and open source development efforts. 
Typically, XML Information Set is serialized as XML.
 A wide variety of freely available tools significantly eases the transition to a SOAP-based implementation. 
The somewhat lengthy syntax of XML can be both a benefit and a drawback. 
While it promotes readability for humans, facilitates error detection, and avoids interoperability problems such as byte-order (endianness), it can slow processing speed and can be cumbersome.
 For example, CORBA, GIOP, ICE, and DCOM use much shorter, binary message formats. 
On the other hand, hardware appliances are available to accelerate processing of XML messages.
Binary XML is also being explored as a means for streamlining the throughput requirements of XML.
 XML messages by their self-documenting nature usually have more 'overhead' (Headers, footers, nested tags, delimiters) than actual data in contrast to earlier protocols where the overhead was usually a relatively small percentage of the overall message.
In financial messaging SOAP was found to result in a 2–4 times larger message than previous protocols FIX (Financial Information Exchange) and CDR (Common Data Representation).
XML Information Set does not have to be serialized in XML. 
For instance, a CSV or JSON XML-infoset representation exists.
 There is also no need to specify a generic transformation framework. 
The concept of SOAP bindings allows for specific bindings for a specific application.
 The drawback is that both the senders and receivers have to support this newly defined binding.
SOAP's neutrality characteristic explicitly makes it suitable for use with any transport protocol. 
Implementations often use HTTP as a transport protocol, but obviously other popular transport protocols can be used. 
For example, SOAP can also be used over SMTP, JMS and Message Queues.
SOAP, when combined with HTTP post/response exchanges, tunnels easily through existing firewalls and proxies, and consequently doesn't require modifying the widespread computing and communication infrastructures that exist for processing HTTP post/response exchanges.
Disadvantages
When using standard implementations and the default SOAP/HTTP binding, the XML infoset is serialized as XML. 
Because of the verbose XML format, SOAP can be considerably slower than competing middleware technologies such as CORBA or ICE.
 This may not be an issue when only small messages are sent.
 To improve performance for the special case of XML with embedded binary objects, the Message Transmission Optimization Mechanism was introduced.
When relying on HTTP as a transport protocol and not using WS-Addressing or an ESB, the roles of the interacting parties are fixed. 
Only one party (the client) can use the services of the other.
The verbosity of the protocol led to the domination in the field by services using the REST architectural style
Parallel computing is a type of computation in which many calculations are carried out simultaneously,[1] operating on the principle that large problems can often be divided into smaller ones, which are then solved at the same time. 
There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. 
Parallelism has been employed for many years, mainly in high-performance computing, but interest in it has grown lately due to the physical constraints preventing frequency scaling.
As power consumption (and consequently heat generation) by computers has become a concern in recent years,[3] parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.
 In parallel computing, a computational task is typically broken down in several, often many, very similar subtasks that can be processed independently and whose results are combined afterwards, upon completion. 
In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.
Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task.
 Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.
Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.
A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law.
Traditionally, computer software has been written for serial computation. 
To solve a problem, an algorithm is constructed and implemented as a serial stream of instructions. 
These instructions are executed on a central processing unit on one computer. 
Only one instruction may execute at a time—after that instruction is finished, the next one is executed.
Parallel computing, on the other hand, uses multiple processing elements simultaneously to solve a problem. 
This is accomplished by breaking the problem into independent parts so that each processing element can execute its part of the algorithm simultaneously with the others. 
The processing elements can be diverse and include resources such as a single computer with multiple processors, several networked computers, specialized hardware, or any combination of the above.
Frequency scaling was the dominant reason for improvements in computer performance from the mid-1980s until 2004. 
The runtime of a program is equal to the number of instructions multiplied by the average time per instruction. 
Maintaining everything else constant, increasing the clock frequency decreases the average time it takes to execute an instruction. 
An increase in frequency thus decreases runtime for all compute-bound programs.
However, power consumption P by a chip is given by the equation P = C × V 2 × F, where C is the capacitance being switched per clock cycle (proportional to the number of transistors whose inputs change), V is voltage, and F is the processor frequency (cycles per second).
Increases in frequency increase the amount of power used in a processor. 
Increasing processor power consumption led ultimately to Intel's May 8, 2004 cancellation of its Tejas and Jayhawk processors, which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm.[11]
Moore's law is the empirical observation that the number of transistors in a microprocessor doubles every 18 to 24 months.
Despite power consumption issues, and repeated predictions of its end, Moore's law is still in effect. 
With the end of frequency scaling, these additional transistors (which are no longer used for frequency scaling) can be used to add extra hardware for parallel computing.
Amdahl's law and Gustafson's law
A graphical representation of Amdahl's law. 
The speedup of a program from parallelization is limited by how much of the program can be parallelized.
 For example, if 90% of the program can be parallelized, the theoretical maximum speedup using parallel computing would be 10 times no matter how many processors are used.
Assume that a task has two independent parts, A and B. 
Part B takes roughly 25% of the time of the whole computation. 
By working very hard, one may be able to make this part 5 times faster, but this only reduces the time for the whole computation by a little. 
In contrast, one may need to perform less work to make part A be twice as fast.
 This will make the computation much faster than by optimizing part B, even though part B's speedup is greater by ratio, (5 times versus 2 times).
Optimally, the speedup from parallelization would be linear—doubling the number of processing elements should halve the runtime, and doubling it a second time should again halve the runtime. 
However, very few parallel algorithms achieve optimal speedup. 
Most of them have a near-linear speedup for small numbers of processing elements, which flattens out into a constant value for large numbers of processing elements.
The potential speedup of an algorithm on a parallel computing platform is given by Amdahl's law
Slatency is the potential speedup in latency of the execution of the whole task;
s is the speedup in latency of the execution of the parallelizable part of the task;
p is the percentage of the execution time of the whole task concerning the parallelizable part of the task before parallelization.
Since Slatency < 1/(1 - p), it shows that a small part of the program which cannot be parallelized will limit the overall speedup available from parallelization.
 A program solving a large mathematical or engineering problem will typically consist of several parallelizable parts and several non-parallelizable (serial) parts. 
If the non-parallelizable part of a program accounts for 10% of the runtime (p = 0.9), we can get no more than a 10 times speedup, regardless of how many processors are added. 
This puts an upper limit on the usefulness of adding more parallel execution units. 
"When a task cannot be partitioned because of sequential constraints, the application of more effort has no effect on the schedule.
 The bearing of a child takes nine months, no matter how many women are assigned."
A graphical representation of Gustafson's law.
Amdahl's law only applies to cases where the problem size is fixed.
 In practice, as more computing resources become available, they tend to get used on larger problems (larger datasets), and the time spent in the parallelizable part often grows much faster than the inherently serial work.
 In this case, Gustafson's law gives a less pessimistic and more realistic assessment of parallel performance:[16]
S_\text{latency}(s) = 1 - p + sp.
Both Amdahl's law and Gustafson's law assume that the running time of the serial part of the program is independent of the number of processors. 
Amdahl's law assumes that the entire problem is of fixed size so that the total amount of work to be done in parallel is also independent of the number of processors, whereas Gustafson's law assumes that the total amount of work to be done in parallel varies linearly with the number of processors.
Understanding data dependencies is fundamental in implementing parallel algorithms.
 No program can run more quickly than the longest chain of dependent calculations (known as the critical path), since calculations that depend upon prior calculations in the chain must be executed in order. 
However, most algorithms do not consist of just a long chain of dependent calculations; there are usually opportunities to execute independent calculations in parallel.
Let Pi and Pj be two program segments. 
Bernstein's conditions[17] describe when the two are independent and can be executed in parallel. 
For Pi, let Ii be all of the input variables and Oi the output variables, and likewise for Pj. 
Pi and Pj are independent if they satisfy
Violation of the first condition introduces a flow dependency, corresponding to the first segment producing a result used by the second segment. 
The second condition represents an anti-dependency, when the second segment produces a variable needed by the first segment. 
The third and final condition represents an output dependency: when two segments write to the same location, the result comes from the logically last executed segment.[18]
Consider the following functions, which demonstrate several kinds of dependencies:
One thread will successfully lock variable V, while the other thread will be locked out—unable to proceed until V is unlocked again. 
This guarantees correct execution of the program. 
Locks, while necessary to ensure correct program execution, can greatly slow a program.
Locking multiple variables using non-atomic locks introduces the possibility of program deadlock.
 An atomic lock locks multiple variables all at once. 
If it cannot lock all of them, it does not lock any of them.
 If two threads each need to lock the same two variables using non-atomic locks, it is possible that one thread will lock one of them and the second thread will lock the second variable. 
In such a case, neither thread can complete, and deadlock results.
Many parallel programs require that their subtasks act in synchrony. 
This requires the use of a barrier. 
Barriers are typically implemented using a software lock.
 One class of algorithms, known as lock-free and wait-free algorithms, altogether avoids the use of locks and barriers. 
However, this approach is generally difficult to implement and requires correctly designed data structures.
Not all parallelization results in speed-up. 
Generally, as a task is split up into more and more threads, those threads spend an ever-increasing portion of their time communicating with each other. 
Eventually, the overhead from communication dominates the time spent solving the problem, and further parallelization (that is, splitting the workload over even more threads) increases rather than decreases the amount of time required to finish. 
This is known as parallel slowdown.
Fine-grained, coarse-grained, and embarrassing parallelism
Applications are often classified according to how often their subtasks need to synchronize or communicate with each other.
 An application exhibits fine-grained parallelism if its subtasks must communicate many times per second; it exhibits coarse-grained parallelism if they do not communicate many times per second, and it exhibits embarrassing parallelism if they rarely or never have to communicate.
 Embarrassingly parallel applications are considered the easiest to parallelize.
Consistency models
Main article: Consistency model
Parallel programming languages and parallel computers must have a consistency model (also known as a memory model). 
The consistency model defines rules for how operations on computer memory occur and how results are produced.
One ofthe first consistency models was Leslie Lamport's sequential consistency model.
 Sequential consistency is the property of a parallel program that its parallel execution produces the same results as a sequential program. 
Specifically, a program is sequentially consistent if "… the results of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program".[19]
Software transactional memory is a common type of consistency model. 
Software transactional memory borrows from database theory the concept of atomic transactions and applies them to memory accesses.
Mathematically, these models can be represented in several ways. 
Petri nets, which were introduced in Carl Adam Petri's 1962 doctoral thesis, were an early attempt to codify the rules of consistency models. 
Dataflow theory later built upon these, and Dataflow architectures were created to physically implement the ideas of dataflow theory. 
Beginning in the late 1970s, process calculi such as Calculus of Communicating Systems and Communicating Sequential Processes were developed to permit algebraic reasoning about systems composed of interacting components. 
More recent additions to the process calculus family, such as the π-calculus, have added the capability for reasoning about dynamic topologies. 
Logics such as Lamport's TLA+, and mathematical models such as traces and Actor event diagrams, have also been developed to describe the behavior of concurrent systems.
The single-instruction-single-data (SISD) classification is equivalent to an entirely sequential program. 
The single-instruction-multiple-data (SIMD) classification is analogous to doing the same operation repeatedly over a large data set. 
This is commonly done in signal processing applications.
 Multiple-instruction-single-data (MISD) is a rarely used classification. 
While computer architectures to deal with this were devised (such as systolic arrays), few applications that fit this class materialized. 
Multiple-instruction-multiple-data (MIMD) programs are by far the most common type of parallel programs.
According to David A. Patterson and John L. Hennessy, "Some machines are hybrids of these categories, of course, but this classic model has survived because it is simple, easy to understand, and gives a good first approximation. 
It is also—perhaps because of its understandability—the most widely used scheme."[20]
From the advent of very-large-scale integration (VLSI) computer-chip fabrication technology in the 1970s until about 1986, speed-up in computer architecture was driven by doubling computer word size—the amount of information the processor can manipulate per cycle.
 Increasing the word size reduces the number of instructions the processor must execute to perform an operation on variables whose sizes are greater than the length of the word. 
Historically, 4-bit microprocessors were replaced with 8-bit, then 16-bit, then 32-bit microprocessors. 
This trend generally came to an end with the introduction of 32-bit processors, which has been a standard in general-purpose computing for two decades. 
Not until recently (c. 2003–2004), with the advent of x86-64 architectures, have 64-bit processors become commonplace.
Instruction-level parallelism
Main article: Instruction-level parallelism
A canonical processor without pipeline. 
It takes five clock cycles to complete one instruction and thus the processor can issue subscalar performance (IPC = 0.2 < 1).
A canonical five-stage pipelined processor.
 In the best case scenario, it takes one clock cycle to complete one instruction and thus the processor can issue scalar performance (IPC = 1).
A computer program, is in essence, a stream of instructions executed by a processor. 
Without instruction-level parallelism, a processor can only issue less than one instruction per clock cycle (IPC < 1).
 These processors are known as subscalar processors. 
These instructions can be re-ordered and combined into groups which are then executed in parallel without changing the result of the program. 
This is known as instruction-level parallelism. 
Advances in instruction-level parallelism dominated computer architecture from the mid-1980s until the mid-1990s.
All modern processors have multi-stage instruction pipelines.
 Each stage in the pipeline corresponds to a different action the processor performs on that instruction in that stage; a processor with an N-stage pipeline can have up to N different instructions at different stages of completion and thus can issue one instruction per clock cycle (IPC = 1). 
These processors are known as scalar processors. 
The canonical example of a pipelined processor is a RISC processor, with five stages: instruction fetch (IF), instruction decode (ID), execute (EX), memory access (MEM), and register write back (WB). 
The Pentium 4 processor had a 35-stage pipeline.
A canonical five-stage pipelined superscalar processor. 
In the best case scenario, it takes one clock cycle to complete two instructions and thus the processor can issue superscalar performance (IPC = 2 > 1).
Most modern processors also have multiple execution units. 
They usually combine this feature with pipelining and thus can issue more than one instruction per clock cycle (IPC > 1).
 These processors are known as superscalar processors. 
Instructions can be grouped together only if there is no data dependency between them. 
Scoreboarding and the Tomasulo algorithm (which is similar to scoreboarding but makes use of register renaming) are two of the most common techniques for implementing out-of-order execution and instruction-level parallelism.
Task parallelism
Main article: Task parallelism
Task parallelisms is the characteristic of a parallel program that "entirely different calculations can be performed on either the same or different sets of data".
This contrasts with data parallelism, where the same calculation is performed on the same or different sets of data. 
Task parallelism involves the decomposition of a task into sub-tasks and then allocating each sub-task to a processor for execution.
 The processors would then execute these sub-tasks simultaneously and often cooperatively. 
Task parallelism does not usually scale with the size of a problem.
Hardware
Memory and communication
Main memory in a parallel computer is either shared memory (shared between all processing elements in a single address space), or distributed memory (in which each processing element has its own local address space).
Distributed memory refers to the fact that the memory is logically distributed, but often implies that it is physically distributed as well. 
Distributed shared memory and memory virtualization combine the two approaches, where the processing element has its own local memory and access to the memory on non-local processors. 
Accesses to local memory are typically faster than accesses to non-local memory.
A logical view of a non-uniform memory access (NUMA) architecture. 
Processors in one directory can access that directory's memory with less latency than they can access memory in the other directory's memory.
Computer architectures in which each element of main memory can be accessed with equal latency and bandwidth are known as uniform memory access (UMA) systems. 
Typically, that can be achieved only by a shared memory system, in which the memory is not physically distributed.
 A system that does not have this property is known as a non-uniform memory access (NUMA) architecture. 
Distributed memory systems have non-uniform memory access.
Computer systems make use of caches—small and fast memories located close to the processor which store temporary copies of memory values (nearby in both the physical and logical sense). 
Parallel computer systems have difficulties with caches that may store the same value in more than one location, with the possibility of incorrect program execution. 
These computers require a cache coherency system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. 
Bus snooping is one of the most common methods for keeping track of which values are being accessed (and thus should be purged).
 Designing large, high-performance cache coherence systems is a very difficult problem in computer architecture. 
As a result, shared memory computer architectures do not scale as well as distributed memory systems do.
Parallel computers based on interconnected networks need to have some kind of routing to enable the passing of messages between nodes that are not directly connected. 
The medium used for communication between the processors is likely to be hierarchical in large multiprocessor machines.
Classes of parallel computers
Parallel computers can be roughly classified according to the level at which the hardware supports parallelism.
 This classification is broadly analogous to the distance between basic computing nodes. 
These are not mutually exclusive; for example, clusters of symmetric multiprocessors are relatively common.
Multi-core computing
Main article: Multi-core processor
A multi-core processor is a processor that includes multiple processing units (called "cores") on the same chip. 
This processor differs from a superscalar processor, which includes multiple execution units and can issue multiple instructions per clock cycle from one instruction stream (thread); in contrast, a multi-core processor can issue multiple instructions per clock cycle from multiple instruction streams. 
IBM's Cell microprocessor, designed for use in the Sony PlayStation 3, is a prominent multi-core processor. 
Each core in a multi-core processor can potentially be superscalar as well—that is, on every clock cycle, each core can issue multiple instructions from one thread.
Simultaneous multithreading (of which Intel's Hyper-Threading is the best known) was an early form of pseudo-multi-coreism. 
A processor capable of simultaneous multithreading includes multiple execution units in the same processing unit—that is it has a superscalar architecture—and can issue multiple instructions per clock cycle from multiple threads. 
Temporal multithreading on the other hand includes a single execution unit in the same processing unit and can issue one instruction at a time from multiple threads.
Symmetric multiprocessing
Main article: Symmetric multiprocessing
A symmetric multiprocessor (SMP) is a computer system with multiple identical processors that share memory and connect via a bus.
 Bus contention prevents bus architectures from scaling. 
As a result, SMPs generally do not comprise more than 32 processors.
 Because of the small size of the processors and the significant reduction in the requirements for bus bandwidth achieved by large caches, such symmetric multiprocessors are extremely cost-effective, provided that a sufficient amount of memory bandwidth exists.
Distributed computing
Main article: Distributed computing
A distributed computer (also known as a distributed memory multiprocessor) is a distributed memory computer system in which the processing elements are connected by a network. 
Distributed computers are highly scalable.
Cluster computing
Main article: Computer cluster
A Beowulf cluster.
A cluster is a group of loosely coupled computers that work together closely, so that in some respects they can be regarded as a single computer.
 Clusters are composed of multiple standalone machines connected by a network.
 While machines in a cluster do not have to be symmetric, load balancing is more difficult if they are not. 
The most common type of cluster is the Beowulf cluster, which is a cluster implemented on multiple identical commercial off-the-shelf computers connected with a TCP/IP Ethernet local area network.
 Beowulf technology was originally developed by Thomas Sterling and Donald Becker. 
The vast majority of the TOP500 supercomputers are clusters.
Because grid computing systems (described below) can easily handle embarrassingly parallel problems, modern clusters are typically designed to handle more difficult problems—problems that require nodes to share intermediate results with each other more often. 
This requires a high bandwidth and, more importantly, a low-latency interconnection network. 
Many historic and current supercomputers use customized high-performance network hardware specifically designed for cluster computing, such as the Cray Gemini network.
 As of 2014, most current supercomputers use some off-the-shelf standard network hardware, often Myrinet, InfiniBand, or Gigabit Ethernet.
Massively parallel computing
Main article: Massively parallel (computing)
A cabinet from IBM's Blue Gene/L massively parallel supercomputer.
A massively parallel processor (MPP) is a single computer with many networked processors. 
MPPs have many of the same characteristics as clusters, but MPPs have specialized interconnect networks (whereas clusters use commodity hardware for networking). 
MPPs also tend to be larger than clusters, typically having "far more" than 100 processors.
 In an MPP, "each CPU contains its own memory and copy of the operating system and application. Each subsystem communicates with the others via a high-speed interconnect."
IBM's Blue Gene/L, the fifth fastest supercomputer in the world according to the June 2009 TOP500 ranking, is an MPP.
Grid computing
Main article: Grid computing
Grid computing is the most distributed form of parallel computing. 
Because of the low bandwidth and extremely high latency available on the Internet, distributed computing typically deals only with embarrassingly parallel problems. 
Many distributed computing applications have been created, of which SETI@home and Folding@home are the best-known examples.
Most grid computing applications use middleware, software that sits between the operating system and the application to manage network resources and standardize the software interface. 
The most common distributed computing middleware is the Berkeley Open Infrastructure for Network Computing (BOINC). 
Often, distributed computing software makes use of "spare cycles", performing computations at times when a computer is idling.
Specialized parallel computers
Within parallel computing, there are specialized parallel devices that remain niche areas of interest. 
While not domain-specific, they tend to be applicable to only a few classes of parallel problems.
Reconfigurable computing with field-programmable gate arrays
Reconfigurable computing is the use of a field-programmable gate array (FPGA) as a co-processor to a general-purpose computer. 
An FPGA is, in essence, a computer chip that can rewire itself for a given task.
FPGAs can be programmed with hardware description languages such as VHDL or Verilog. 
However, programming in these languages can be tedious. 
Several vendors have created C to HDL languages that attempt to emulate the syntax and semantics of the C programming language, with which most programmers are familiar. 
The best known C to HDL languages are Mitrion-C, Impulse C, DIME-C, and Handel-C. 
Specific subsets of SystemC based on C++ can also be used for this purpose.
AMD's decision to open its HyperTransport technology to third-party vendors has become the enabling technology for high-performance reconfigurable computing.
According to Michael R. D'Amour, Chief Operating Officer of DRC Computer Corporation, "when we first walked into AMD, they called us 'the socket stealers.'
 Now they call us their partners."
General-purpose computing on graphics processing units (GPGPU)
Main article: GPGPU
Nvidia's Tesla GPGPU card
General-purpose computing on graphics processing units (GPGPU) is a fairly recent trend in computer engineering research. 
GPUs are co-processors that have been heavily optimized for computer graphics processing.
Computer graphics processing is a field dominated by data parallel operations—particularly linear algebra matrix operations.
In the early days, GPGPU programs used the normal graphics APIs for executing programs.
However, several new programming languages and platforms have been built to do general purpose computation on GPUs with both Nvidia and AMD releasing programming environments with CUDA and Stream SDK respectively.
 Other GPU programming languages include BrookGPU, PeakStream, and RapidMind. Nvidia has also released specific products for computation in their Tesla series. 
The technology consortium Khronos Group has released the OpenCL specification, which is a framework for writing programs that execute across platforms consisting of CPUs and GPUs.
 AMD, Apple, Intel, Nvidia and others are supporting OpenCL.
Application-specific integrated circuits
Main article: Application-specific integrated circuit
Several application-specific integrated circuit (ASIC) approaches have been devised for dealing with parallel applications.
Because an ASIC is (by definition) specific to a given application, it can be fully optimized for that application. 
As a result, for a given application, an ASIC tends to outperform a general-purpose computer.
 However, ASICs are created by X-ray lithography.
 This process requires a mask, which can be extremely expensive. 
A single mask can cost over a million US dollars.
(The smaller the transistors required for the chip, the more expensive the mask will be.) 
Meanwhile, performance increases in general-purpose computing over time (as described by Moore's law) tend to wipe out these gains in only one or two chip generations.
 High initial cost, and the tendency to be overtaken by Moore's-law-driven general-purpose computing, has rendered ASICs unfeasible for most parallel computing applications. 
However, some have been built. 
One example is the PFLOPS RIKEN MDGRAPE-3 machine which uses custom ASICs for molecular dynamics simulation.
Vector processors
Main article: Vector processor
The Cray-1 is the most famous vector processor.
A vector processor is a CPU or computer system that can execute the same instruction on large sets of data. 
Vector processors have high-level operations that work on linear arrays of numbers or vectors. 
An example vector operation is A = B × C, where A, B, and C are each 64-element vectors of 64-bit floating-point numbers.
 They are closely related to Flynn's SIMD classification.
Cray computers became famous for their vector-processing computers in the 1970s and 1980s. 
However, vector processors—both as CPUs and as full computer systems—have generally disappeared.
 Modern processor instruction sets do include some vector processing instructions, such as with Freescale Semiconductor's AltiVec and Intel's Streaming SIMD Extensions (SSE).
Software
Parallel programming languages
Main article: List of concurrent and parallel programming languages
Concurrent programming languages, libraries, APIs, and parallel programming models (such as algorithmic skeletons) have been created for programming parallel computers.
These can generally be divided into classes based on the assumptions they make about the underlying memory architecture—shared memory, distributed memory, or shared distributed memory.
 Shared memory programming languages communicate by manipulating shared memory variables. 
Distributed memory uses message passing. 
POSIX Threads and OpenMP are two of most widely used shared memory APIs, whereas Message Passing Interface (MPI) is the most widely used message-passing system API.
 One concept used in programming parallel programs is the future concept, where one part of a program promises to deliver a required datum to another part of a program at some future time.
CAPS entreprise and Pathscale are also coordinating their effort to make hybrid multi-core parallel programming (HMPP) directives an open standard called OpenHMPP. 
The OpenHMPP directive-based programming model offers a syntax to efficiently offload computations on hardware accelerators and to optimize data movement to/from the hardware memory.
 OpenHMPP directives describe remote procedure call (RPC) on an accelerator device (e.g. GPU) or more generally a set of cores. 
The directives annotate C or Fortran codes to describe two sets of functionalities: the offloading of procedures (denoted codelets) onto a remote device and the optimization of data transfers between the CPU main memory and the accelerator memory.
Automatic parallelization
Main article: Automatic parallelization
Automatic parallelization of a sequential program by a compiler is the holy grail of parallel computing. 
Despite decades of work by compiler researchers, automatic parallelization has had only limited success.[44]
Mainstream parallel programming languages remain either explicitly parallel or (at best) partially implicit, in which a programmer gives the compiler directives for parallelization. 
A few fully implicit parallel programming languages exist—SISAL, Parallel Haskell, SequenceL, System C (for FPGAs), Mitrion-C, VHDL, and Verilog.
Application checkpointing
Main article: Application checkpointing
As a computer system grows in complexity, the mean time between failures usually decreases. 
Application checkpointing is a technique whereby the computer system takes a "snapshot" of the application—a record of all current resource allocations and variable states, akin to a core dump—; this information can be used to restore the program if the computer should fail. 
Application checkpointing means that the program has to restart from only its last checkpoint rather than the beginning. 
checkpointing provides benefits in a variety of situations, it is especially useful in highly parallel systems with a large number of processors used in high performance computing.[45]
Algorithmic methods
As parallel computers become larger and faster, it becomes feasible to solve problems that previously took too long to run. 
Parallel computing is used in a wide range of fields, from bioinformatics (protein folding and sequence analysis) to economics (mathematical finance). 
Common types of problems found in parallel computing applications are:[46]
dense linear algebra;
sparse linear algebra;
spectral methods (such as Cooley–Tukey fast Fourier transform)
N-body problems (such as Barnes–Hut simulation);
structured grid problems (such as Lattice Boltzmann methods);
unstructured grid problems (such as found in finite element analysis);
Monte Carlo method;
combinational logic (such as brute-force cryptographic techniques);
graph traversal (such as sorting algorithms);
dynamic programming;
branch and bound methods;
graphical models (such as detecting hidden Markov models and constructing Bayesian networks);
finite-state machine simulation.
Fault-tolerance
Further information: Fault-tolerant computer system
Parallel computing can also be applied to the design of fault-tolerant computer systems, particularly via lockstep systems performing the same operation in parallel. 
This provides redundancy in case one component should fail, and also allows automatic error detection and error correction if the results differ. 
These methods can be used to help prevent single event upsets caused by transient errors.
 Although additional measures may be required in embedded or specialized systems, this method can provide a cost effective approach to achieve n-modular redundancy in commercial off-the-shelf systems.
History
Main article: History of computing
ILLIAC IV, "the most infamous of supercomputers".
The origins of true (MIMD) parallelism go back to Luigi Federico Menabrea and his Sketch of the Analytic Engine Invented by Charles Babbage.
 IBM introduced the 704 in 1954, through a project in which Gene Amdahl was one of the principal architects.
 It became the first commercially available computer to use fully automatic floating-point arithmetic commands.
In April 1958, S. Gill (Ferranti) discussed parallel programming and the need for branching and waiting.
 Also in 1958, IBM researchers John Cocke and Daniel Slotnick discussed the use of parallelism in numerical calculations for the first time.
 Burroughs Corporation introduced the D825 in 1962, a four-processor computer that accessed up to 16 memory modules through a crossbar switch.
 In 1967, Amdahl and Slotnick published a debate about the feasibility of parallel processing at American Federation of Information Processing Societies Conference.
It was during this debate that Amdahl's law was coined to define the limit of speed-up due to parallelism.
In 1969, company Honeywell introduced its first Multics system, a symmetric multiprocessor system capable of running up to eight processors in parallel.
 C.mmp, a 1970s multi-processor project at Carnegie Mellon University, was among the first multiprocessors with more than a few processors.
The first bus-connected multiprocessor with snooping caches was the Synapse N+1 in 1984."[50]
SIMD parallel computers can be traced back to the 1970s. 
The motivation behind early SIMD computers was to amortize the gate delay of the processor's control unit over multiple instructions.
 In 1964, Slotnick had proposed building a massively parallel computer for the Lawrence Livermore National Laboratory.
 His design was funded by the US Air Force, which was the earliest SIMD parallel-computing effort, ILLIAC IV.
 The key to its design was a fairly high parallelism, with up to 256 processors, which allowed the machine to work on large datasets in what would later be known as vector processing. 
However, ILLIAC IV was called "the most infamous of supercomputers", because the project was only one fourth completed, but took 11 years and cost almost four times the original estimate.
When it was finally ready to run its first real application in 1976, it was outperformed by existing commercial supercomputers such as the Cray-1.
A supercomputer is a computer with a high-level computational capacity compared to a general-purpose computer.
 Performance of a supercomputer is measured in floating-point operations per second (FLOPS) instead of million instructions per second (MIPS).
 As of 2015, there are supercomputers which can perform up to quadrillions of FLOPS.[2]
Supercomputers were introduced in the 1960s, made initially, and for decades primarily, by Seymour Cray at Control Data Corporation (CDC), Cray Research and subsequent companies bearing his name or monogram. 
While the supercomputers of the 1970s used only a few processors, in the 1990s machines with thousands of processors began to appear and, by the end of the 20th century, massively parallel supercomputers with tens of thousands of "off-the-shelf" processors were the norm.
 Since its introduction in June 2013, China's Tianhe-2 supercomputer is currently the fastest in the world at 33.86 petaFLOPS (PFLOPS), or 33.86 quadrillions of FLOPS.
 Throughout their history, they have been essential in the field of cryptanalysis.[5]
 In another approach, a large number of dedicated processors are placed in close proximity to each other (e.g. in a computer cluster); this saves considerable time moving data around and makes it possible for the processors to work together (rather than on separate tasks), for example in mesh and hypercube architectures.
The use of multi-core processors combined with centralization is an emerging trend; one can think of this as a small cluster (the multicore processor in a smartphone, tablet, laptop, etc.) that both depends upon and contributes to the cloud.
The history of supercomputing goes back to the 1960s, with the Atlas at the University of Manchester and a series of computers at Control Data Corporation (CDC), designed by Seymour Cray. 
These used innovative designs and parallelism to achieve superior computational peak performance.
The Atlas was a joint venture between Ferranti and the Manchester University and was designed to operate at processing speeds approaching one microsecond per instruction, about one million instructions per second.
The first Atlas was officially commissioned on 7 December 1962 as one of the world's first supercomputers  – considered to be the most powerful computer in the world at that time by a considerable margin, and equivalent to four IBM 7094s.
The CDC 6600, released in 1964, was designed by Cray to be the fastest in the world. 
Cray switched from use of germanium to silicon transistors, which could run very fast, solving the overheating problem by introducing refrigeration.
Given that the 6600 outperformed all the other contemporary computers by about 10 times, it was dubbed a supercomputer and defined the supercomputing market when one hundred computers were sold at $8 million each.
Cray left CDC in 1972 to form his own company, Cray Research.
 Four years after leaving CDC, Cray delivered the 80 MHz Cray 1 in 1976, and it became one of the most successful supercomputers in history.
 The Cray-2 released in 1985 was an 8 processor liquid cooled computer and Fluorinert was pumped through it as it operated. 
It performed at 1.9 gigaflops and was the world's fastest until 1990.
While the supercomputers of the 1980s used only a few processors, in the 1990s, machines with thousands of processors began to appear both in the United States and Japan, setting new computational performance records. 
Fujitsu's Numerical Wind Tunnel supercomputer used 166 vector processors to gain the top spot in 1994 with a peak speed of 1.7 gigaFLOPS (GFLOPS) per processor.
 The Hitachi SR2201 obtained a peak performance of 600 GFLOPS in 1996 by using 2048 processors connected via a fast three-dimensional crossbar network.
 The Intel Paragon could have 1000 to 4000 Intel i860 processors in various configurations, and was ranked the fastest in the world in 1993. 
The Paragon was a MIMD machine which connected processors via a high speed two dimensional mesh, allowing processes to execute on separate nodes, communicating via the Message Passing Interface.
Approaches to supercomputer architecture have taken dramatic turns since the earliest systems were introduced in the 1960s. 
Early supercomputer architectures pioneered by Seymour Cray relied on compact innovative designs and local parallelism to achieve superior computational peak performance.
 However, in time the demand for increased computational power ushered in the age of massively parallel systems.
While the supercomputers of the 1970s used only a few processors, in the 1990s, machines with thousands of processors began to appear and by the end of the 20th century, massively parallel supercomputers with tens of thousands of "off-the-shelf" processors were the norm. 
Supercomputers of the 21st century can use over 100,000 processors (some being graphic units) connected by fast connections.
 The Connection Machine CM-5 supercomputer is a massively parallel processing computer capable of many billions of arithmetic operations per second.
Throughout the decades, the management of heat density has remained a key issue for most centralized supercomputers.
The large amount of heat generated by a system may also have other effects, e.g. reducing the lifetime of other system components.
There have been diverse approaches to heat management, from pumping Fluorinert through the system, to a hybrid liquid-air cooling system or air cooling with normal air conditioning temperatures.[20][32]
The CPU share of TOP500
Systems with a massive number of processors generally take one of two paths. 
In the grid computing approach, the processing power of a large number of computers, organised as distributed, diverse administrative domains, is opportunistically used whenever a computer is available.
 In another approach, a large number of processors are used in close proximity to each other, e.g. in a computer cluster.
 In such a centralized massively parallel system the speed and flexibility of the interconnect becomes very important and modern supercomputers have used various approaches ranging from enhanced Infiniband systems to three-dimensional torus interconnects.
 The use of multi-core processors combined with centralization is an emerging direction, e.g. as in the Cyclops64 system.
As the price, performance and energy efficiency of general purpose graphic processors (GPGPUs) have improved,[35] a number of petaflop supercomputers such as Tianhe-I and Nebulae have started to rely on them.
  However, GPUs are gaining ground and in 2012 the Jaguar supercomputer was transformed into Titan by retrofitting CPUs with GPUs.[39][40][41]
High performance computers have an expected life cycle of about three years.
A number of "special-purpose" systems have been designed, dedicated to a single problem. 
This allows the use of specially programmed FPGA chips or even custom VLSI chips, allowing better price/performance ratios by sacrificing generality. 
Examples of special-purpose supercomputers include Belle,[43] Deep Blue,[44] and Hydra,[45] for playing chess, Gravity Pipe for astrophysics,[46] MDGRAPE-3 for protein structure computation molecular dynamics[47] and Deep Crack,[48] for breaking the DES cipher.
Energy usage and heat management
See also: Computer cooling and Green 500
A typical supercomputer consumes large amounts of electrical power, almost all of which is converted into heat, requiring cooling. 
For example, Tianhe-1A consumes 4.04 megawatts (MW) of electricity.
The cost to power and cool the system can be significant, e.g. 4 MW at $0.10/kWh is $400 an hour or about $3.5 million per year.
An IBM HS20 blade
Heat management is a major issue in complex electronic devices, and affects powerful computer systems in various ways.
The thermal design power and CPU power dissipation issues in supercomputing surpass those of traditional computer cooling technologies. 
The supercomputing awards for green computing reflect this issue.
The packing of thousands of processors together inevitably generates significant amounts of heat density that need to be dealt with. 
The Cray 2 was liquid cooled, and used a Fluorinert "cooling waterfall" which was forced through the modules under pressure.
 However, the submerged liquid cooling approach was not practical for the multi-cabinet systems based on off-the-shelf processors, and in System X a special cooling system that combined air conditioning with liquid cooling was developed in conjunction with the Liebert company.
In the Blue Gene system, IBM deliberately used low power processors to deal with heat density.
 On the other hand, the IBM Power 775, released in 2011, has closely packed elements that require water cooling.
 The IBM Aquasar system, on the other hand uses hot water cooling to achieve energy efficiency, the water being used to heat buildings as well.
The energy efficiency of computer systems is generally measured in terms of "FLOPS per watt". 
In 2008, IBM's Roadrunner operated at 3.76 MFLOPS/W.[58][59] In November 2010, the Blue Gene/Q reached 1,684 MFLOPS/W.
 In June 2011 the top 2 spots on the Green 500 list were occupied by Blue Gene machines in New York (one achieving 2097 MFLOPS/W) with the DEGIMA cluster in Nagasaki placing third with 1375 MFLOPS/W.[62]
Because copper wires can transfer energy into a supercomputer with much higher power densities than forced air or circulating refrigerants can remove waste heat,[63] the ability of the cooling systems to remove waste heat is a limiting factor.
 As of 2015, many existing supercomputers have more infrastructure capacity than the actual peak demand of the machine  – designers generally conservatively design the power and cooling infrastructure to handle more than the theoretical peak electrical power consumed by the supercomputer. 
Designs for future supercomputers are power-limited  – the thermal design power of the supercomputer as a whole, the amount that the power and cooling infrastructure can handle, is somewhat more than the expected normal power consumption, but less than the theoretical peak power consumption of the electronic hardware.
Software and system management
Operating systems
Main article: Supercomputer operating systems
Since the end of the 20th century, supercomputer operating systems have undergone major transformations, based on the changes in supercomputer architecture.
While early operating systems were custom tailored to each supercomputer to gain speed, the trend has been to move away from in-house operating systems to the adaptation of generic software such as Linux.[68]
Although most modern supercomputers use the Linux operating system, each manufacturer has its own specific Linux-derivative, and no industry standard exists, partly due to the fact that the differences in hardware architectures require changes to optimize the operating system to each hardware design.
Software tools and message passing
Main article: Message passing in computer clusters
See also: Parallel computing and Parallel programming model
Wide-angle view of the ALMA correlator.
The parallel architectures of supercomputers often dictate the use of special programming techniques to exploit their speed.
 Software tools for distributed processing include standard APIs such as MPI and PVM, VTL, and open source-based software solutions such as Beowulf.
In the most common scenario, environments such as PVM and MPI for loosely connected clusters and OpenMP for tightly coordinated shared memory machines are used.
 Significant effort is required to optimize an algorithm for the interconnect characteristics of the machine it will be run on; the aim is to prevent any of the CPUs from wasting time waiting on data from other nodes. 
GPGPUs have hundreds of processor cores and are programmed using programming models such as CUDA.
Moreover, it is quite difficult to debug and test parallel programs. 
Special techniques need to be used for testing and debugging such applications.
Distributed supercomputing
Opportunistic approaches
Main article: Grid computing
Opportunistic Supercomputing is a form of networked grid computing whereby a "super virtual computer" of many loosely coupled volunteer computing machines performs very large computing tasks. 
Grid computing has been applied to a number of large-scale embarrassingly parallel problems that require supercomputing performance scales. 
However, basic grid and cloud computing approaches that rely on volunteer computing can not handle traditional supercomputing tasks such as fluid dynamic simulations.
The fastest grid computing system is the distributed computing project Folding@home.
 F@h reported 43.1 PFLOPS of x86 processing power as of June 2014. 
Of this, 42.5 PFLOPS are contributed by clients running on various GPUs, and the rest from various CPU systems.
The BOINC platform hosts a number of distributed computing projects. 
As of May 2011, BOINC recorded a processing power of over 5.5 PFLOPS through over 480,000 active computers on the network[76] The most active project (measured by computational power), MilkyWay@home, reports processing power of over 700 teraFLOPS (TFLOPS) through over 33,000 active computers.
As of May 2011, GIMPS's distributed Mersenne Prime search currently achieves about 60 TFLOPS through over 25,000 registered computers.
 The Internet PrimeNet Server supports GIMPS's grid computing approach, one of the earliest and most successful grid computing projects, since 1997.
Quasi-opportunistic approaches
Main article: Quasi-opportunistic supercomputing
Quasi-opportunistic supercomputing is a form of distributed computing whereby the “super virtual computer” of a large number of networked geographically disperse computers performs computing tasks that demand huge processing power.
Quasi-opportunistic supercomputing aims to provide a higher quality of service than opportunistic grid computing by achieving more control over the assignment of tasks to distributed resources and the use of intelligence about the availability and reliability of individual systems within the supercomputing network. 
However, quasi-opportunistic distributed execution of demanding parallel computing software in grids should be achieved through implementation of grid-wise allocation agreements, co-allocation subsystems, communication topology-aware allocation mechanisms, fault tolerant message passing libraries and data pre-conditioning.[79]
Performance measurement
Capability vs capacity
Supercomputers generally aim for the maximum in capability computing rather than capacity computing. 
Capability computing is typically thought of as using the maximum computing power to solve a single large problem in the shortest amount of time. 
Often a capability system is able to solve a problem of a size or complexity that no other computer can, e.g. a very complex weather simulation application.
Capacity computing, in contrast, is typically thought of as using efficient cost-effective computing power to solve a small number of somewhat large problems or a large number of small problems.
 Architectures that lend themselves to supporting many users for routine everyday tasks may have a lot of capacity, but are not typically considered supercomputers, given that they do not solve a single very complex problem.[80]
Performance metrics
See also: LINPACK benchmarks
Top supercomputer speeds: logscale speed over 60 years
In general, the speed of supercomputers is measured and benchmarked in "FLOPS" (FLoating point Operations Per Second), and not in terms of "MIPS" (Million Instructions Per Second), as is the case with general-purpose computers.
 These measurements are commonly used with an SI prefix such as tera-, combined into the shorthand "TFLOPS" (1012 FLOPS, pronounced teraflops), or peta-, combined into the shorthand "PFLOPS" (1015 FLOPS, pronounced petaflops.) 
"Petascale" supercomputers can process one quadrillion (1015) (1000 trillion) FLOPS. 
Exascale is computing performance in the exaFLOPS (EFLOPS) range.
 An EFLOPS is one quintillion (1018) FLOPS (one million TFLOPS).
No single number can reflect the overall performance of a computer system, yet the goal of the Linpack benchmark is to approximate how fast the computer solves numerical problems and it is widely used in the industry.
The TOP500 list
Main article: TOP500
Distribution of top 500 supercomputers among different countries as of November 2015
Since 1993, the fastest supercomputers have been ranked on the TOP500 list according to their LINPACK benchmark results. 
The list does not claim to be unbiased or definitive, but it is a widely cited current definition of the "fastest" supercomputer available at any given time.
This is a recent list of the computers which appeared at the top of the TOP500 list,[83] and the "Peak speed" is given as the "Rmax" rating. 
For more historical data see History of supercomputing.
The IBM Blue Gene/P computer has been used to simulate a number of artificial neurons equivalent to approximately one percent of a human cerebral cortex, containing 1.6 billion neurons with approximately 9 trillion connections. 
The same research group also succeeded in using a supercomputer to simulate a number of artificial neurons equivalent to the entirety of a rat's brain.
Modern-day weather forecasting also relies on supercomputers. 
The National Oceanic and Atmospheric Administration uses supercomputers to crunch hundreds of millions of observations to help make weather forecasts more accurate.
In 2011, the challenges and difficulties in pushing the envelope in supercomputing were underscored by IBM's abandonment of the Blue Waters petascale project.
Research and development trends
Diagram of a 3-dimensional torus interconnect used by systems such as Blue Gene, Cray XT3, etc.
Given the current speed of progress, industry experts estimate that supercomputers will reach 1 EFLOPS (1018, 1,000 PFLOPS or one quintillion FLOPS) by 2018. 
The Chinese government in particular is pushing to achieve this goal after they briefly achieved the most powerful supercomputer in the world with Tianhe-1A in 2010 (ranked fifth by 2012).
Using the Intel MIC multi-core processor architecture, which is Intel's response to GPU systems, SGI also plans to achieve a 500-fold increase in performance by 2018 in order to achieve one EFLOPS.
 Samples of MIC chips with 32 cores, which combine vector processing units with standard CPU, have become available.
The Indian government has also stated ambitions for an EFLOPS-range supercomputer, which they hope to complete by 2017.
 In November 2014, it was reported that India is working on the fastest supercomputer ever, which is set to work at 132 EFLOPS.[96]
Erik P. DeBenedictis of Sandia National Laboratories theorizes that a zettaFLOPS (1021, one sextillion FLOPS) computer is required to accomplish full weather modeling, which could cover a two-week time span accurately.
 Such systems might be built around 2030.
Many Monte Carlo simulations use the same algorithm to process a randomly generated data set; particularly, integro-differential equations describing physical transport processes, the random paths, collisions, and energy and momentum depositions of neutrons, photons, ions, electrons, etc. 
The next step for microprocessors may be into the third dimension; and specializing to Monte Carlo, the many layers could be identical, simplifying the design and manufacture process.
Energy use
High performance supercomputers usually require high energy, as well.
 However, Iceland may be a benchmark for the future with the world's first zero-emission supercomputer. 
Located at the Thor Data Center in Reykjavik, Iceland, this supercomputer relies on completely renewable sources for its power rather than fossil fuels. 
The colder climate is an added bonus for help with cooling, too, making it one of the greenest facilities in the world.
Many Science Fiction writers depicted supercomputers in their works, both before and after such computers were actually constructed. 
Much of such fiction deals with the relations of humans with the computers they created and the possibility of conflict eventually developing between them. 
Some such scenarios can be found on the AI takeover page.
Concurrent computing is a form of computing in which several computations are executing during overlapping time periods—concurrently—instead of sequentially (one completing before the next starts). 
This is a property of a system—this may be an individual program, a computer, or a network—and there is a separate execution point or "thread of control" for each computation ("process"). 
A concurrent system is one where a computation can make progress without waiting for all other computations to complete—where more than one computation can make progress at "the same time".[1]
As a programming paradigm, concurrent computing is a form of modular programming, namely factoring an overall computation into subcomputations that may be executed concurrently. 
Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare.
Concurrent computing is related to but distinct from parallel computing, though these concepts are frequently confused,[2][3] and both can be described as "multiple processes executing during the same period of time". 
In parallel computing, execution literally occurs at the same instant, for example on separate processors of a multi-processor machine, with the goal of speeding up computations—parallel computing is impossible on a (single-core) single processor, as only one computation can occur at any instant (during any single clock cycle).
[a] By contrast, concurrent computing consists of process lifetimes overlapping, but execution need not happen at the same instant. 
The goal here is to model processes in the outside world that happen concurrently, such as multiple clients accessing a server at the same time. 
Structuring software systems as composed of multiple concurrent, communicating parts can be useful for tackling complexity, regardless of whether the parts can be executed in parallel.
For example, concurrent processes can be executed on a single core by interleaving the execution steps of each process via time slices: only one process runs at a time, and if it does not complete during its time slice, it is paused, another process begins or resumes, and then later the original process is resumed. 
In this way multiple processes are part-way through execution at a single instant, but only one process is being executed at that instant.
Concurrent computations may be executed in parallel,[2][5] for example by assigning each process to a separate processor or processor core, or distributing a computation across a network, but in general, the languages, tools and techniques for parallel programming may not be suitable for concurrent programming, and vice versa.
The exact timing of when tasks in a concurrent system are executed depend on the scheduling, and tasks need not always be executed concurrently. 
For example, given two tasks, T1 and T2:
T1 may be executed and finished before T2 or vice versa (serial and sequential);
T1 and T2 may be executed alternately (serial and concurrent);
T1 and T2 may be executed simultaneously at the same instant of time (parallel and concurrent).
The word "sequential" is used as an antonym for both "concurrent" and "parallel"; when these are explicitly distinguished, concurrent/sequential and parallel/serial are used as opposing pairs.
 A schedule in which tasks execute one at a time (serially, no parallelism), without interleaving (sequentually, no concurrency: no task begins until the previous task ends) is called a serial schedule. 
A set of tasks that can be scheduled serially is serializable, which simplifies concurrency control.
Coordinating access to shared resources
The main challenge in designing concurrent programs is concurrency control: ensuring the correct sequencing of the interactions or communications between different computational executions, and coordinating access to resources that are shared among executions.
 Potential problems include race conditions, deadlocks, and resource starvation. 
For example, consider the following algorithm for making withdrawals from a checking account represented by the shared resource balance:
Suppose balance = 500, and two concurrent threads make the calls withdraw(300) and withdraw(350). 
If line 3 in both operations executes before line 5 both operations will find that balance >= withdrawal evaluates to true, and execution will proceed to subtracting the withdrawal amount.
 However, since both processes perform their withdrawals, the total amount withdrawn will end up being more than the original balance. 
These sorts of problems with shared resources require the use of concurrency control, or non-blocking algorithms.
Because concurrent systems rely on the use of shared resources (including communication media), concurrent computing in general requires the use of some form of arbiter somewhere in the implementation to mediate access to these resources.
Unfortunately, while many solutions exist to the problem of a conflict over one resource, many of those "solutions" have their own concurrency problems such as deadlock when more than one resource is involved.
Increased application throughput—parallel execution of a concurrent program allows the number of tasks completed in certain time period to increase.
High responsiveness for input/output—input/output-intensive applications mostly wait for input or output operations to complete. 
Concurrent programming allows the time that would be spent waiting to be used for another task.
More appropriate program structure—some problems and problem domains are well-suited to representation as concurrent tasks or processes.
A number of different methods can be used to implement concurrent programs, such as implementing each computational execution as an operating system process, or implementing the computational processes as a set of threads within a single operating system process.
Interaction and communication
In some concurrent computing systems, communication between the concurrent components is hidden from the programmer (e.g., by using futures), while in others it must be handled explicitly. 
Explicit communication can be divided into two classes:
Shared memory communication
Concurrent components communicate by altering the contents of shared memory locations (exemplified by Java and C#). 
This style of concurrent programming usually requires the application of some form of locking (e.g., mutexes, semaphores, or monitors) to coordinate between threads. 
A program that properly implements any of these is said to be thread-safe.
Message passing communication
Concurrent components communicate by exchanging messages (exemplified by Scala, Erlang and occam). 
The exchange of messages may be carried out asynchronously, or may use a synchronous "rendezvous" style in which the sender blocks until the message is received. 
Asynchronous message passing may be reliable or unreliable (sometimes referred to as "send and pray"). 
Message-passing concurrency tends to be far easier to reason about than shared-memory concurrency, and is typically considered a more robust form of concurrent programming.
[citation needed] A wide variety of mathematical theories for understanding and analyzing message-passing systems are available, including the Actor model, and various process calculi.
 Message passing can be efficiently implemented on symmetric multiprocessors, with or without shared coherent memory.
Shared memory and message passing concurrency have different performance characteristics. 
Typically (although not always), the per-process memory overhead and task switching overhead is lower in a message passing system, but the overhead of message passing itself is greater than for a procedure call. 
These differences are often overwhelmed by other performance factors.
History
Concurrent computing developed out of earlier work on railroads and telegraphy, from the 19th and early 20th century, and some terms date to this period, such as semaphores. 
The academic study of concurrent algorithms started in the 1960s, with Dijkstra (1965) credited with being the first paper in this field, identifying and solving mutual exclusion.[7]
Prevalence
Concurrency is pervasive in computing, occurring from low-level hardware on a single chip to world-wide networks.
 Examples follow.
At the programming language level:
Channel
Coroutine
Futures and promises
At the operating system level:
Computer multitasking, including both cooperative multitasking and preemptive multitasking
Time-sharing, which replaced sequential batch processing of jobs with concurrent use of a system
Process
Thread
At the network level, networked systems are generally concurrent by their nature, as they consist of separate devices.
Languages supporting it
Concurrent programming languages are programming languages that use language constructs for concurrency.
 These constructs may involve multi-threading, support for distributed computing, message passing, shared resources (including shared memory) or futures and promises. 
Such languages are sometimes described as Concurrency Oriented Languages or Concurrency Oriented Programming Languages (COPL).[8]
Today, the most commonly used programming languages that have specific constructs for concurrency are Java and C#. 
Both of these languages fundamentally use a shared-memory concurrency model, with locking provided by monitors (although message-passing models can and have been implemented on top of the underlying shared-memory model). 
Of the languages that use a message-passing concurrency model, Erlang is probably the most widely used in industry at present.[citation needed]
Many concurrent programming languages have been developed more as research languages (e.g. Pict) rather than as languages for production use. 
However, languages such as Erlang, Limbo, and occam have seen industrial use at various times in the last 20 years. 
Languages in which concurrency plays an important role include:
Ada—general purpose, with native support for message passing and monitor based concurrency
Alef—concurrent, with threads and message passing, for system programming in early versions of Plan 9 from Bell Labs
Alice—extension to Standard ML, adds support for concurrency via futures
Ateji PX—extension to Java with parallel primitives inspired from π-calculus
Axum—domain specific, concurrent, based on Actor model and .NET Common Language Runtime using a C-like syntax
C++—std::thread
Cω (C omega)—for research, extends C#, uses asynchronous communication
C#—supports concurrent computing since version 5.0 using lock, yield, async and await keywords
Clojure—modern Lisp for the JVM
Concurrent Clean—functional programming, similar to Haskell
Concurrent Collections (CnC)—Achieves implicit parallelism independent of memory model by explicitly defining flow of data and control
Concurrent Haskell—lazy, pure functional language operating concurrent processes on shared memory
Concurrent ML—concurrent extension of Standard ML
Concurrent Pascal—by Per Brinch Hansen
Curry
D—multi-paradigm system programming language with explicit support for concurrent programming (Actor model)
E—uses promises to disallow deadlocks
ECMAScript—promises available in various libraries, proposed for inclusion in standard in ECMAScript 6
Eiffel—through its SCOOP mechanism based on the concepts of Design by Contract
Elixir—dynamic and functional meta-programming aware language running on the Erlang VM.
Erlang—uses asynchronous message passing with nothing shared
FAUST—real-time functional, for signal processing, compiler provides automatic parallelization via OpenMP or a specific work-stealing scheduler
Fortran—coarrays and do concurrent are part of Fortran 2008 standard
Go—for system programming, with a concurrent programming model based on CSP
Hume—functional, concurrent, for bounded space and time environments where automata processes are described by synchronous channels patterns and message passing
Io—actor-based concurrency
Janus—features distinct askers and tellers to logical variables, bag channels; is purely declarative
Java—Thread class or Runnable interface.
JavaScript—via web workers, in a browser environment, promises, and callbacks.
JoCaml—concurrent and distributed channel based, extension of OCaml, implements the Join-calculus of processes
Join Java—concurrent, based on Java language
Joule—dataflow-based, communicates by message passing
Joyce—concurrent, teaching, built on Concurrent Pascal with features from CSP by Per Brinch Hansen
LabVIEW—graphical, dataflow, functions are nodes in a graph, data is wires between the nodes; includes object-oriented language
Limbo—relative of Alef, for system programming in Inferno (operating system)
MultiLisp—Scheme variant extended to support parallelism
Modula-2—for system programming, by N. Wirth as a successor to Pascal with native support for coroutines
Modula-3—modern member of Algol family with extensive support for threads, mutexes, condition variables
Newsqueak—for research, with channels as first-class values; predecessor of Alef
Node.js—a server-side runtime environment for JavaScript
occam—influenced heavily by Communicating Sequential Processes (CSP)
occam-π—a modern variant of occam, which incorporates ideas from Milner's π-calculus
Orc—heavily concurrent, nondeterministic, based on Kleene algebra
Oz—multiparadigm, supports shared-state and message-passing concurrency, and futures
Mozart Programming System—multiplatform Oz
ParaSail—object-oriented, parallel, free of pointers, race conditions
Pict—essentially an executable implementation of Milner's π-calculus
Perl with AnyEvent and Coro
Python with Twisted, greenlet and gevent
Reia—uses asynchronous message passing between shared-nothing objects
Red/System—for system programming, based on Rebol
Ruby with Concurrent Ruby and Celluloid
Rust—for system programming, focus on massive concurrency, using message-passing with move semantics, shared immutable memory, and shared mutable memory that is provably free of race conditions.[9]
SALSA—actor-based with token-passing, join, and first-class continuations for distributed computing over the Internet
Scala—general purpose, designed to express common programming patterns in a concise, elegant, and type-safe way
SequenceL—general purpose functional, main design objectives are ease of programming, code clarity-readability, and automatic parallelization for performance on multicore hardware, and provably free of race conditions
SR—for research
Stackless Python
StratifiedJS—combinator-based concurrency, based on JavaScript
SuperPascal—concurrent, for teaching, built on Concurrent Pascal and Joyce by Per Brinch Hansen
Unicon—for research
Termite Scheme—adds Erlang-like concurrency to Scheme
TNSDL—for developing telecommunication exchanges, uses asynchronous message passing
VHDL (VHSIC Hardware Description Language)—IEEE STD-1076
XC—concurrency-extended subset of C language developed by XMOS, based on Communicating Sequential Processes, built-in constructs for programmable I/O
Many other languages provide support for concurrency in the form of libraries, at levels roughly comparable with the above list.
Distributed computing is a field of computer science that studies distributed systems. 
A distributed system is a software system in which components located on networked computers communicate and coordinate their actions by passing messages.
 The components interact with each other in order to achieve a common goal. 
Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components.
 Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.
A computer program that runs in a distributed system is called a distributed program, and distributed programming is the process of writing such programs.
 There are many alternatives for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.
Distributed computing also refers to the use of distributed systems to solve computational problems. 
In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[3] which communicate with each other by message passing.
The word distributed in terms such as "distributed system", "distributed programming", and "distributed algorithm" originally referred to computer networks where individual computers were physically distributed within some geographical area.
 The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.
 While there is no single definition of a distributed system,[6] the following defining properties are commonly used:
There are several autonomous computational entities, each of which has its own local memory.
The entities communicate with each other by message passing.[8]
In this article, the computational entities are called computers or nodes.
A distributed system may have a common goal, such as solving a large computational problem.
Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.
Other typical properties of distributed systems include the following:
The system has to tolerate failures in individual computers.
The structure of the system (network topology, network latency, number of computers) is not known in advance, the system may consist of different kinds of computers and network links, and the system may change during the execution of a distributed program.
Each computer has only a limited, incomplete view of the system. 
Each computer may know only one part of the input.
Parallel and distributed computing
(a), (b): a distributed system.
(c): a parallel system.
Distributed systems are groups of networked computers, which have the same goal for their work. 
The terms "concurrent computing", "parallel computing", and "distributed computing" have a lot of overlap, and no clear distinction exists between them.
 The same system may be characterized both as "parallel" and "distributed"; the processors in a typical distributed system run concurrently in parallel.
Parallel computing may be seen as a particular tightly coupled form of distributed computing,[16] and distributed computing may be seen as a loosely coupled form of parallel computing.
[6] Nevertheless, it is possible to roughly classify concurrent systems as "parallel" or "distributed" using the following criteria:
In parallel computing, all processors may have access to a shared memory to exchange information between processors.
In distributed computing, each processor has its own private memory (distributed memory). 
Information is exchanged by passing messages between the processors.
The figure on the right illustrates the difference between distributed and parallel systems. 
Figure (a) is a schematic view of a typical distributed system; as usual, the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. 
Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. 
Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.
The situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). 
Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.
History
The use of concurrent processes that communicate by message-passing has its roots in operating system architectures studied in the 1960s.
 The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.
ARPANET, the predecessor of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s.
 E-mail became the most successful application of ARPANET,[21] and it is probably the earliest example of a large-scale distributed application.
 In addition to ARPANET, and its successor, the Internet, other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems.
The study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. 
The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its European counterpart International Symposium on Distributed Computing (DISC) was first held in 1985.
Architectures
Various hardware and software architectures are used for distributed computing.
 At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. 
At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.
Distributed programming typically falls into one of several basic architectures: client–server, three-tier, n-tier, or peer-to-peer; or categories: loose coupling, or tight coupling.
Client–server: architectures where smart clients contact the server for data then format and display it to the users. 
Input at the client is committed back to the server when it represents a permanent change.
Three-tier: architectures that move the client intelligence to a middle tier so that stateless clients can be used. 
This simplifies application deployment. Most web applications are three-tier.
n-tier: architectures that refer typically to web applications which further forward their requests to other enterprise services. 
This type of application is the one most responsible for the success of application servers.
Peer-to-peer: architectures where there is no special machines that provide a service or manage the network resources. 
Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and as servers.
Another basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. 
Through various message passing protocols, processes may communicate directly with one another, typically in a master/slave relationship. 
Alternatively, a "database-centric" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database.
Applications
Reasons for using distributed systems and distributed computing may include:
The very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location.
There are many cases in which the use of a single computer would be possible in principle, but the use of a distributed system is beneficial for practical reasons. 
For example, it may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer. 
A distributed system can provide more reliability than a non-distributed system, as there is no single point of failure. 
Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.[23]
Ghaemi et al. define a distributed query as a query "that selects data from databases located at multiple sites in a network" and offer as an SQL example:
In computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as superpixels). 
The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze.
Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. 
More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.
The result of image segmentation is a set of segments that collectively cover the entire image, or a set of contours extracted from the image (see edge detection). 
Each of the pixels in a region are similar with respect to some characteristic or computed property, such as color, intensity, or texture. 
Adjacent regions are significantly different with respect to the same characteristic(s).
 When applied to a stack of images, typical in medical imaging, the resulting contours after image segmentation can be used to create 3D reconstructions with the help of interpolation algorithms like Marching cubes.
Some of the practical applications of image segmentation are:
Content-based image retrieval
Machine vision
Medical imaging
Locate tumors and other pathologies
Measure tissue volumes
Diagnosis, study of anatomical structure
Surgery planning
Virtual surgery simulation
Intra-surgery navigation
Object detection
Pedestrian detection
Face detection
Brake light detection
Locate objects in satellite images (roads, forests, crops, etc.)
Recognition Tasks
Face recognition
Fingerprint recognition
Iris recognition
Traffic control systems
Video surveillance
Several general-purpose algorithms and techniques have been developed for image segmentation.
 To be useful, these techniques must typically be combined with a domain's specific knowledge in order to effectively solve the domain's segmentation problems.
Thresholding
The simplest method of image segmentation is called the thresholding method.
 This method is based on a clip-level (or a threshold value) to turn a gray-scale image into a binary image.
 There is also a balanced histogram thresholding.
The key of this method is to select the threshold value (or values when multiple-levels are selected). 
Several popular methods are used in industry including the maximum entropy method, Otsu's method (maximum variance), and k-means clustering.
Recently, methods have been developed for thresholding computed tomography (CT) images.
 The key idea is that, unlike Otsu's method, the thresholds are derived from the radiographs instead of the (reconstructed) image[8] .
New methods suggested the usage of multi-dimensional fuzzy rule-based non-linear thresholds. 
In these works decision over each pixel's membership to a segment is based on multi-dimensional rules derived from fuzzy logic and evolutionary algorithms based on image lighting environment and application.
Clustering methods
Main article: Data clustering
Original image
Source image.
Processed image
Image after running k-means with k = 16. ote that a common technique to improve performance for large images is to downsample the image, compute the clusters, and then reassign the values to the larger image if necessary.
The K-means algorithm is an iterative technique that is used to partition an image into K clusters.
 The basic algorithm is
Pick K cluster centers, either randomly or based on some heuristic
Assign each pixel in the image to the cluster that minimizes the distance between the pixel and the cluster center
Re-compute the cluster centers by averaging all of the pixels in the cluster
Repeat steps 2 and 3 until convergence is attained (i.e. no pixels change clusters)
In this case, distance is the squared or absolute difference between a pixel and a cluster center. 
The difference is typically based on pixel color, intensity, texture, and location, or a weighted combination of these factors.
 K can be selected manually, randomly, or by a heuristic. This algorithm is guaranteed to converge, but it may not return the optimal solution.
 The quality of the solution depends on the initial set of clusters and the value of K.
Compression-based methods
Compression based methods postulate that the optimal segmentation is the one that minimizes, over all possible segmentations, the coding length of the data.
 The connection between these two concepts is that segmentation tries to find patterns in an image and any regularity in the image can be used to compress it. 
The method describes each segment by its texture and boundary shape. 
Each of these components is modeled by a probability distribution function and its coding length is computed as follows:
The boundary encoding leverages the fact that regions in natural images tend to have a smooth contour. 
This prior is used by Huffman coding to encode the difference chain code of the contours in an image. 
Thus, the smoother a boundary is, the shorter coding length it attains.
Texture is encoded by lossy compression in a way similar to minimum description length (MDL) principle, but here the length of the data given the model is approximated by the number of samples times the entropy of the model. 
The texture in each region is modeled by a multivariate normal distribution whose entropy has closed form expression. 
An interesting property of this model is that the estimated entropy bounds the true entropy of the data from above.
 This is because among all distributions with a given mean and covariance, normal distribution has the largest entropy. 
Thus, the true coding length cannot be more than what the algorithm tries to minimize.
For any given segmentation of an image, this scheme yields the number of bits required to encode that image based on the given segmentation. 
Thus, among all possible segmentations of an image, the goal is to find the segmentation which produces the shortest coding length. 
This can be achieved by a simple agglomerative clustering method. 
The distortion in the lossy compression determines the coarseness of the segmentation and its optimal value may differ for each image. 
This parameter can be estimated heuristically from the contrast of textures in an image. For example, when the textures in an image are similar, such as in camouflage images, stronger sensitivity and thus lower quantization is required.
Histogram-based methods
Histogram-based methods are very efficient compared to other image segmentation methods because they typically require only one pass through the pixels.
 In this technique, a histogram is computed from all of the pixels in the image, and the peaks and valleys in the histogram are used to locate the clusters in the image.
Color or intensity can be used as the measure.
A refinement of this technique is to recursively apply the histogram-seeking method to clusters in the image in order to divide them into smaller clusters. 
This operation is repeated with smaller and smaller clusters until no more clusters are formed.
One disadvantage of the histogram-seeking method is that it may be difficult to identify significant peaks and valleys in the image.
Histogram-based approaches can also be quickly adapted to apply to multiple frames, while maintaining their single pass efficiency.
 The histogram can be done in multiple fashions when multiple frames are considered.
 The same approach that is taken with one frame can be applied to multiple, and after the results are merged, peaks and valleys that were previously difficult to identify are more likely to be distinguishable.
 The histogram can also be applied on a per-pixel basis where the resulting information is used to determine the most frequent color for the pixel location. 
This approach segments based on active objects and a static environment, resulting in a different type of segmentation useful in Video tracking.
Edge detection
Edge detection is a well-developed field on its own within image processing.
 Region boundaries and edges are closely related, since there is often a sharp adjustment in intensity at the region boundaries.
 Edge detection techniques have therefore been used as the base of another segmentation technique.
The edges identified by edge detection are often disconnected.
 To segment an object from an image however, one needs closed region boundaries.
 The desired edges are the boundaries between such objects or spatial-taxons.
Spatial-taxons[17] are information granules.,[18] consisting of a crisp pixel region, stationed at abstraction levels within a hierarchical nested scene architecture.
 They are similar to the Gestalt psychological designation of figure-ground, but are extended to include foreground, object groups, objects and salient object parts. 
Edge detection methods can be applied to the spatial-taxon region, in the same manner they would be applied to a silhouette. 
This method is particularly useful when the disconnected edge is part of an illusory contour[19][20]
Segmentation methods can also be applied to edges obtained from edge detectors.
 Dual clustering method
This method is a combination of three characteristics of the image: partition of the image based on histogram analysis is checked by high compactness of the clusters (objects), and high gradients of their borders. 
For that purpose two spaces has to be introduced: one space is the one-dimensional histogram of brightness H = H(B), the second space – the dual 3-dimensional space of the original image itself B = B(x, y). 
The first space allows to measure how compact is distributed the brightness of the image by calculating minimal clustering kmin. 
Threshold brightness T corresponding to kmin defines the binary (black-and-white) image – bitmap b = φ(x, y), where φ(x, y) = 0, if B(x, y) < T, and φ(x, y) = 1, if B(x, y) ≥ T. 
The bitmap b is an object in dual space. On that bitmap a measure has to be defined reflecting how compact distributed black (or white) pixels are. 
So, the goal is to find objects with good borders. 
For all T the measure MDC =G/(k-L) has to be calculated (where k is difference in brightness between the object and the background, L is length of all borders, and G is mean gradient on the borders). 
Maximum of MDC defines the segmentation.[22]
Region-growing methods
Region-growing methods rely mainly on the assumption that the neighboring pixels within one region have similar values. 
The common procedure is to compare one pixel with its neighbors. 
If a similarity criterion is satisfied, the pixel can be set to belong to the cluster as one or more of its neighbors. 
The selection of the similarity criterion is significant and the results are influenced by noise in all instances.
The method of Statistical Region Merging[23] (SRM) starts by building the graph of pixels using 4-connectedness with edges weighted by the absolute value of the intensity difference. 
Initially each pixel forms a single pixel region.
 SRM then sorts those edges in a priority queue and decide whether or not to merge the current regions belonging to the edge pixels using a statistical predicate.
One region-growing method is the seeded region growing method. 
This method takes a set of seeds as input along with the image.
 The seeds mark each of the objects to be segmented. 
The regions are iteratively grown by comparison of all unallocated neighboring pixels to the regions. 
The difference between a pixel's intensity value and the region's mean, \delta, is used as a measure of similarity. 
The pixel with the smallest difference measured in this way is assigned to the respective region. 
This process continues until all pixels are assigned to a region.
 Because seeded region growing requires seeds as additional input, the segmentation results are dependent on the choice of seeds, and noise in the image can cause the seeds to be poorly placed.
Another region-growing method is the unseeded region growing method. 
It is a modified algorithm that does not require explicit seeds. 
It starts with a single region A_1—the pixel chosen here does not markedly influence the final segmentation.
 At each iteration it considers the neighboring pixels in the same way as seeded region growing. 
It differs from seeded region growing in that if the minimum \delta is less than a predefined threshold T then it is added to the respective region A_j. 
If not, then the pixel is considered different from all current regions A_i and a new region A_{n+1} is created with this pixel.
One variant of this technique, proposed by Haralick and Shapiro (1985),[1] is based on pixel intensities. 
The mean and scatter of the region and the intensity of the candidate pixel are used to compute a test statistic.
 If the test statistic is sufficiently small, the pixel is added to the region, and the region’s mean and scatter are recomputed.
 Otherwise, the pixel is rejected, and is used to form a new region.
A special region-growing method is called \lambda-connected segmentation (see also lambda-connectedness). 
It is based on pixel intensities and neighborhood-linking paths.
 A degree of connectivity (connectedness) is calculated based on a path that is formed by pixels. 
For a certain value of \lambda, two pixels are called \lambda-connected if there is a path linking those two pixels and the connectedness of this path is at least \lambda. \lambda-connectedness is an equivalence relation.
Split-and-merge segmentation is based on a quadtree partition of an image.
 It is sometimes called quadtree segmentation.
This method starts at the root of the tree that represents the whole image. 
If it is found non-uniform (not homogeneous), then it is split into four son squares (the splitting process), and so on. 
If, in contrast, four son squares are homogeneous, they are merged as several connected components (the merging process). 
The node in the tree is a segmented node. 
This process continues recursively until no further splits or merges are possible.
 When a special data structure is involved in the implementation of the algorithm of the method, its time complexity can reach O(n\log n), an optimal algorithm of the method.
Partial differential equation-based methods
Using a partial differential equation (PDE)-based method and solving the PDE equation by a numerical scheme, one can segment the image.
 Curve propagation is a popular technique in this category, with numerous applications to object extraction, object tracking, stereo reconstruction, etc. 
The central idea is to evolve an initial curve towards the lowest potential of a cost function, where its definition reflects the task to be addressed.
 As for most inverse problems, the minimization of the cost functional is non-trivial and imposes certain smoothness constraints on the solution, which in the present case can be expressed as geometrical constraints on the evolving curve.
Parametric methods
Lagrangian techniques are based on parameterizing the contour according to some sampling strategy and then evolve each element according to image and internal terms. 
owadays, efficient "discretized" formulations have been developed to address these limitations while maintaining high efficiency. 
In both cases, energy minimization is generally conducted using a steepest-gradient descent, whereby derivatives are computed using, e.g., finite differences.
Level set methods
The level set method was initially proposed to track moving interfaces by Osher and Sethian in 1988 and has spread across various imaging domains in the late 90s. 
It can be used to efficiently address the problem of curve/surface/etc. 
propagation in an implicit manner. 
The central idea is to represent the evolving contour using a signed function whose zero corresponds to the actual contour. 
Then, according to the motion equation of the contour, one can easily derive a similar flow for the implicit surface that when applied to the zero level will reflect the propagation of the contour. 
The level set method affords numerous advantages: it is implicit, is parameter-free, provides a direct way to estimate the geometric properties of the evolving structure, allows for change of topology, and is intrinsic. 
It can be used to define an optimization framework, as proposed by Zhao, Merriman and Osher in 1996.
 One can conclude that it is a very convenient framework for addressing numerous applications of computer vision and medical image analysis.
Research into various level set data structures has led to very efficient implementations of this method.
Fast marching methods
The fast marching method has been used in image segmentation,[30] and this model has been improved (permitting a both positive and negative speed propagation speed) in an approach called the generalized fast marching method.[31]
Variational methods
The goal of variational methods is to find a segmentation which is optimal with respect to a specific energy functional. 
The functionals consist of a data fitting term and a regularizing terms. 
A classical representative is the Potts model defined for an image f by
    \operatorname*{argmin}_u \gamma \| \nabla u \|_0 + \int (u - f)^2 dx.
A minimizer u^* is a piecewise constant image which has an optimal tradeoff between the squared L2 distance to the given image f and the total length of its jump set. 
The jump set of u^* defines a segmentation. The relative weight of the energies is tuned by the parameter \gamma >0 .
 The binary variant of the Potts model, i.e., if the range of u is restricted to two values, is often called Chan-Vese model.
 An important generalization is the Mumford-Shah model [33] given by
\operatorname*{argmin}_{u, K}  \gamma |K| +
 \mu \int _{K^C} |\nabla u|^2 dx + \int (u - f)^2 dx.
The functional value is the sum of the total length of the segmentation curve K, the smoothness of the approximation u, and its distance to the original image f. 
The weight of the smoothness penalty is adjusted by \mu > 0. The Potts model is often called piecewise constant Mumford-Shah model as it can be seen as the degenerate case \mu \to \infty. 
The optimization problems are known to be NP-hard in general but near-minimizing strategies work well in practice. 
Classical algorithms are graduated non-convexity and Ambrosio-Tortorelli approximation.
Graph partitioning methods
Graph partitioning methods are an effective tools for image segmentation since they model the impact of pixel neighborhoods on a given cluster of pixels or pixel, under the assumption of homogeneity in images. 
In these methods, the image is modeled as a weighted, undirected graph. 
Usually a pixel or a group of pixels are associated with nodes and edge weights define the (dis)similarity between the neighborhood pixels. 
The graph (image) is then partitioned according to a criterion designed to model "good" clusters. 
Each partition of the nodes (pixels) output from these algorithms are considered an object segment in the image. 
Some popular algorithms of this category are normalized cuts,[34] random walker,[35] minimum cut,[36] isoperimetric partitioning,[37] minimum spanning tree-based segmentation,[38] and segmentation-based object categorization.
Markov Random Fields
The application of Markov random fields (MRF) for images was suggested in early 1984 by Geman and Geman.
Their strong mathematical foundation and ability to provide a global optima even when defined on local features proved to be the foundation for novel research in the domain of image analysis, de-noising and segmentation. 
MRFs are completely characterized by their prior probability distributions, marginal probability distributions, cliques, smoothing constraint as well as criterion for updating values. 
The criterion for image segmentation using MRFs is restated as finding the labelling scheme which has maximum probability for a given set of features. 
The broad categories of image segmentation using MRFs are supervised and unsupervised segmentation.
Supervised Image Segmentation using MRF and MAP
In terms of image segmentation, the function that MRFs seek to maximize is the probability of identifying a labelling scheme given a particular set of features are detected in the image.
 This is a restatement of the Maximum a posteriori estimation method.
Watershed transformation
The watershed transformation considers the gradient magnitude of an image as a topographic surface. 
Pixels having the highest gradient magnitude intensities (GMIs) correspond to watershed lines, which represent the region boundaries. 
Water placed on any pixel enclosed by a common watershed line flows downhill to a common local intensity minimum (LIM). 
Pixels draining to a common minimum form a catch basin, which represents a segment.
Model based segmentation
The central assumption of such an approach is that structures of interest/organs have a repetitive form of geometry. 
Therefore, one can seek for a probabilistic model towards explaining the variation of the shape of the organ and then when segmenting an image impose constraints using this model as prior. 
Such a task involves (i) registration of the training examples to a common pose, (ii) probabilistic representation of the variation of the registered samples, and (iii) statistical inference between the model and the image. 
State of the art methods in the literature for knowledge-based segmentation involve active shape and appearance models, active contours and deformable templates and level-set based methods.[citation needed]
Multi-scale segmentation
Image segmentations are computed at multiple scales in scale space and sometimes propagated from coarse to fine scales; see scale-space segmentation.
Segmentation criteria can be arbitrarily complex and may take into account global as well as local criteria. 
A common requirement is that each region must be connected in some sense.
One-dimensional hierarchical signal segmentation
Witkin's seminal work[47][48] in scale space included the notion that a one-dimensional signal could be unambiguously segmented into regions, with one scale parameter controlling the scale of segmentation.
A key observation is that the zero-crossings of the second derivatives (minima and maxima of the first derivative or slope) of multi-scale-smoothed versions of a signal form a nesting tree, which defines hierarchical relations between segments at different scales. 
Specifically, slope extrema at coarse scales can be traced back to corresponding features at fine scales. 
When a slope maximum and slope minimum annihilate each other at a larger scale, the three segments that they separated merge into one segment, thus defining the hierarchy of segments.
Image segmentation and primal sketch
There have been numerous research works in this area, out of which a few have now reached a state where they can be applied either with interactive manual intervention (usually with application to medical imaging) or fully automatically. 
The following is a brief overview of some of the main research ideas that current approaches are based upon.
The nesting structure that Witkin described is, however, specific for one-dimensional signals and does not trivially transfer to higher-dimensional images.
 Nevertheless, this general idea has inspired several other authors to investigate coarse-to-fine schemes for image segmentation. 
Koenderink[49] proposed to study how iso-intensity contours evolve over scales and this approach was investigated in more detail by Lifshitz and Pizer.
 Unfortunately, however, the intensity of image features changes over scales, which implies that it is hard to trace coarse-scale image features to finer scales using iso-intensity information.
Gauch and Pizer[53] studied the complementary problem of ridges and valleys at multiple scales and developed a tool for interactive image segmentation based on multi-scale watersheds. 
The use of multi-scale watershed with application to the gradient map has also been investigated by Olsen and Nielsen[54] and been carried over to clinical use by Dam[55] Vincken et al.
proposed a hyperstack for defining probabilistic relations between image structures at different scales. 
The use of stable image structures over scales has been furthered by Ahuja[57][58] and his co-workers into a fully automated system.
 A fully automatic brain segmentation algorithm based on closely related ideas of multi-scale watersheds has been presented by Undeman and Lindeberg[59] and been extensively tested in brain databases.
These ideas for multi-scale image segmentation by linking image structures over scales have also been picked up by Florack and Kuijper.
Bijaoui and Rué[61] associate structures detected in scale-space above a minimum noise threshold into an object tree which spans multiple scales and corresponds to a kind of feature in the original signal. 
Extracted features are accurately reconstructed using an iterative conjugate gradient matrix method.
Semi-automatic segmentation
In one kind of segmentation, the user outlines the region of interest with the mouse clicks and algorithms are applied so that the path that best fits the edge of the image is shown.
Techniques like SIOX, Livewire, Intelligent Scissors or IT-SNAPS are used in this kind of segmentation.
 In an alternative kind of semi-automatic segmentation, the algorithms return a spatial-taxon (i.e. foreground, object-group, object or object-part) selected by the user or designated via prior probabilities.]
Trainable segmentation
Most segmentation methods are based only on color information of pixels in the image.
 Humans use much more knowledge than this when doing image segmentation, but implementing this knowledge would cost considerable computation time and would require a huge domain-knowledge database, which is currently not available. 
In addition to traditional segmentation methods, there are trainable segmentation methods which can model some of this knowledge.
Neural Network segmentation relies on processing small areas of an image using an artificial neural network[64] or a set of neural networks. 
After such processing the decision-making mechanism marks the areas of an image accordingly to the category recognized by the neural network. 
A type of network designed especially for this is the Kohonen map.
Pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat’s visual cortex and developed for high-performance biomimetic image processing.
 In 1989, Eckhorn introduced a neural model to emulate the mechanism of a cat’s visual cortex.
 The Eckhorn model provided a simple and effective tool for studying the visual cortex of small mammals, and was soon recognized as having significant application potential in image processing. 
In 1994, the Eckhorn model was adapted to be an image processing algorithm by Johnson, who termed this algorithm Pulse-Coupled Neural Network. 
Over the past decade, PCNNs have been utilized for a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, noise reduction, and so on.
 A PCNN is a two-dimensional neural network. 
Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel’s color information (e.g. intensity) as an external stimulus. 
Each neuron also connects with its neighboring neurons, receiving local stimuli from them. 
The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. 
Through iterative computation, PCNN neurons produce temporal series of pulse outputs. 
The temporal series of pulse outputs contain information of input images and can be utilized for various image processing applications, such as image segmentation and feature generation.
 Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.
Computer vision is a field that includes methods for acquiring, processing, analyzing, and understanding images and, in general, high-dimensional data from the real world in order to produce numerical or symbolic information.
 A theme in the development of this field has been to duplicate the abilities of human vision by electronically perceiving and understanding an image.
 Understanding in this context means the transformation of visual images (the input of retina) into descriptions of world that can interface with other thought processes and elicit appropriate action. 
This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.
Computer vision has also been described as the enterprise of automating and integrating a wide range of processes and representations for vision perception.
As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. 
The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. 
As a technological discipline, computer vision seeks to apply its theories and models to the construction of computer vision systems.
Sub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, object pose estimation, learning, indexing, motion estimation, and image restoration.
Areas of artificial intelligence deal with autonomous planning or deliberation for robotical systems to navigate through an environment. 
A detailed understanding of these environments is required to navigate through them. 
Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot.
Artificial intelligence and computer vision share other topics such as pattern recognition and learning techniques. 
Consequently, computer vision is sometimes seen as a part of the artificial intelligence field or the computer science field in general.
Solid-state physics is another field that is closely related to computer vision. 
Most computer vision systems rely on image sensors, which detect electromagnetic radiation which is typically in the form of either visible or infra-red light. 
The sensors are designed using quantum physics. 
The process by which light interacts with surfaces is explained using physics.
 Physics explains the behavior of optics which are a core part of most imaging systems. 
Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process.
 Also, various measurement problems in physics can be addressed using computer vision, for example motion in fluids.
A third field which plays an important role is neurobiology, specifically the study of the biological vision system. 
Over the last century, there has been an extensive study of eyes, neurons, and the brain structures devoted to processing of visual stimuli in both humans and various animals. 
This has led to a coarse, yet complicated, description of how "real" vision systems operate in order to solve certain vision related tasks. 
These results have led to a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems, at different levels of complexity. 
Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in biology.
Some strands of computer vision research are closely related to the study of biological vision – indeed, just as many strands of AI research are closely tied with research into human consciousness, and the use of stored knowledge to interpret, integrate and utilize visual information. 
The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. 
Computer vision, on the other hand, studies and describes the processes implemented in software and hardware behind artificial vision systems. 
Interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.
Yet another field related to computer vision is signal processing. 
Many methods for processing of one-variable signals, typically temporal signals, can be extended in a natural way to processing of two-variable signals or multi-variable signals in computer vision. 
However, because of the specific nature of images there are many methods developed within computer vision which have no counterpart in processing of one-variable signals. 
Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.
Beside the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. 
For example, many methods in computer vision are based on statistics, optimization or geometry. 
Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance.
The fields most closely related to computer vision are image processing, image analysis and machine vision. 
There is a significant overlap in the range of techniques and applications that these cover. 
This implies that the basic techniques that are used and developed in these fields are more or less identical, something which can be interpreted as there is only one field with different names. 
Computer vision is, in some ways, the inverse of computer graphics. 
While computer graphics produces image data from 3D models, computer vision often produces 3D models from image data.
 There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality.
The following characterizations appear relevant but should not be taken as universally accepted:
Image processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. 
This characterization implies that image processing/analysis neither require assumptions nor produce interpretations about the image content.
Computer vision includes 3D analysis from 2D images. 
This analyzes the 3D scene projected onto one or several images, e.g., how to reconstruct structure or other information about the 3D scene from one or several images. 
Computer vision often relies on more or less complex assumptions about the scene depicted in an image.
Machine vision is the process of applying a range of technologies & methods to provide imaging-based automatic inspection, process control and robot guidance[8] in industrial applications.
 Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision based autonomous robots and systems for vision based inspection or measurement. 
This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasised by means of efficient implementations in hardware and software. 
It also implies that the external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms.
There is also a field called imaging which primarily focus on the process of producing images, but sometimes also deals with processing and analysis of images. 
For example, medical imaging includes substantial work on the analysis of image data in medical applications.
Finally, pattern recognition is a field which uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks. 
A significant part of this field is devoted to applying these methods to image data.
Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. stereo computer vision.
Applications for computer vision[edit]
Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. 
The computer vision and machine vision fields have significant overlap.
 Computer vision covers the core technology of automated image analysis which is used in many fields.
 Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. 
In many computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. 
Examples of applications of computer vision include systems for:
Controlling processes, e.g., an industrial robot;
Navigation, e.g., by an autonomous vehicle or mobile robot;
Detecting events, e.g., for visual surveillance or people counting;
Organizing information, e.g., for indexing databases of images and image sequences;
Modeling objects or environments, e.g., medical image analysis or topographical modeling;
Interaction, e.g., as the input to a device for computer-human interaction, and
Automatic inspection, e.g., in manufacturing applications.
File:DARPA Visual Media Reasoning Concept Video.ogv
DARPA's Visual Media Reasoning concept video
One of the most prominent application fields is medical computer vision or medical image processing. 
This area is characterized by the extraction of information from image data for the purpose of making a medical diagnosis of a patient. 
Generally, image data is in the form of microscopy images, X-ray images, angiography images, ultrasonic images, and tomography images. 
An example of information which can be extracted from such image data is detection of tumours, arteriosclerosis or other malign changes. 
It can also be measurements of organ dimensions, blood flow, etc. 
This application area also supports medical research by providing new information, e.g., about the structure of the brain, or about the quality of medical treatments. 
Applications of computer vision in the medical area also includes enhancement of images that are interpreted by humans, for example ultrasonic images or X-ray images, to reduce the influence of noise.
A second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a manufacturing process. 
One example is quality control where details or final products are being automatically inspected in order to find defects. 
Another example is measurement of position and orientation of details to be picked up by a robot arm. 
Machine vision is also heavily used in agricultural process to remove undesirable food stuff from bulk material, a process called optical sorting.
Military applications are probably one of the largest areas for computer vision. 
The obvious examples are detection of enemy soldiers or vehicles and missile guidance. 
More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data.
 Modern military concepts, such as "battlefield awareness", imply that various sensors, including image sensors, provide a rich set of information about a combat scene which can be used to support strategic decisions. 
In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.
Artist's Concept of Rover on Mars, an example of an unmanned land-based vehicle. 
Notice the stereo cameras mounted on top of the Rover.
One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). 
The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer vision based systems support a driver or a pilot in various situations. 
Fully autonomous vehicles typically use computer vision for navigation, i.e. for knowing where it is, or for producing a map of its environment (SLAM) and for detecting obstacles. 
It can also be used for detecting certain task specific events, e.g., a UAV looking for forest fires. 
Examples of supporting systems are obstacle warning systems in cars, and systems for autonomous landing of aircraft. 
Several car manufacturers have demonstrated systems for autonomous driving of cars, but this technology has still not reached a level where it can be put on the market.
 There are ample examples of military autonomous vehicles ranging from advanced missiles, to UAVs for recon missions or missile guidance. 
Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Mars Exploration Rover and ESA's ExoMars Rover.
Other application areas include:
Support of visual effects creation for cinema and broadcast, e.g., camera tracking (matchmoving).
Surveillance.
Representational and control requirements[edit]
Image-understanding systems (IUS) include three levels of abstraction as follows: Low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. 
Many of these requirements are really topics for further research.
The representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.
While inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. 
Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.[10]
Typical tasks of computer vision[edit]
Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. 
Some examples of typical computer vision tasks are presented below.
Recognition[edit]
The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. 
Different varieties of the recognition problem are described in the literature:
Object recognition (also called object classification) – one or several pre-specified or learned objects or object classes can be recognized, usually together with their 2D positions in the image or 3D poses in the scene. 
Google Goggles or LikeThat provide stand-alone programs that illustrate this functionality.
Identification – an individual instance of an object is recognized.
 Examples include identification of a specific person's face or fingerprint, identification of handwritten digits, or identification of a specific vehicle.
Detection – the image data are scanned for a specific condition. 
Examples include detection of possible abnormal cells or tissues in medical images or detection of a vehicle in an automatic road toll system.
 Detection based on relatively simple and fast computations is sometimes used for finding smaller regions of interesting image data which can be further analyzed by more computationally demanding techniques to produce a correct interpretation.
Currently, the best algorithms for such tasks are based on convolutional neural networks. 
An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and hundreds of object classes. 
Performance of convolutional neural networks, on the ImageNet tests, is now close to that of humans
The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. 
They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). 
By contrast, those kinds of images rarely trouble humans. 
Humans, however, tend to have trouble with other issues. 
For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.
Several specialized tasks based on recognition exist, such as:
Content-based image retrieval – finding all images in a larger set of images which have a specific content. 
Computer vision for people counter purposes in public places, malls, shopping centres
Pose estimation – estimating the position or orientation of a specific object relative to the camera. 
An example application for this technique would be assisting a robot arm in retrieving objects from a conveyor belt in an assembly line situation or picking parts from a bin.
Optical character recognition (OCR) – identifying characters in images of printed or handwritten text, usually with a view to encoding the text in a format more amenable to editing or indexing (e.g. ASCII).
2D Code reading Reading of 2D codes such as data matrix and QR codes.
Facial recognition
Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects
Motion analysis[edit]
Several tasks relate to motion estimation where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene, or even of the camera that produces the images .
 Examples of such tasks are:
Egomotion – determining the 3D rigid motion (rotation and translation) of the camera from an image sequence produced by the camera.
Tracking – following the movements of a (usually) smaller set of interest points or objects (e.g., vehicles or humans) in the image sequence.
Optical flow – to determine, for each point in the image, how that point is moving relative to the image plane, i.e., its apparent motion. 
This motion is a result both of how the corresponding 3D point is moving in the scene and how the camera is moving relative to the scene.
Scene reconstruction[edit]
Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. 
In the simplest case the model can be a set of 3D points.
 More sophisticated methods produce a complete 3D surface model.
 The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. 
Grid-based 3D sensing can be used to acquire 3D images from multiple angles. 
Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.
Image restoration[edit]
The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images.
 The simplest possible approach for noise removal is various types of filters such as low-pass filters or median filters. 
More sophisticated methods assume a model of how the local image structures look like, a model which distinguishes them from the noise.
 By first analysing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.
An example in this field is inpainting.
Computer vision system methods[edit]
The organization of a computer vision system is highly application dependent. 
Some systems are stand-alone applications which solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. 
The specific implementation of a computer vision system also depends on if its functionality is pre-specified or if some part of it can be learned or modified during operation. 
Many functions are unique to the application. 
There are, however, typical functions which are found in many computer vision systems.
Image acquisition – A digital image is produced by one or several image sensors, which, besides various types of light-sensitive cameras, include range sensors, tomography devices, radar, ultra-sonic cameras, etc. 
Depending on the type of sensor, the resulting image data is an ordinary 2D image, a 3D volume, or an image sequence. 
The pixel values typically correspond to light intensity in one or several spectral bands (gray images or colour images), but can also be related to various physical measures, such as depth, absorption or reflectance of sonic or electromagnetic waves, or nuclear magnetic resonance.[12]
Pre-processing – Before a computer vision method can be applied to image data in order to extract some specific piece of information, it is usually necessary to process the data in order to assure that it satisfies certain assumptions implied by the method. 
Examples are
Re-sampling in order to assure that the image coordinate system is correct.
Noise reduction in order to assure that sensor noise does not introduce false information.
Contrast enhancement to assure that relevant information can be detected.
Scale space representation to enhance image structures at locally appropriate scales.
Feature extraction – Image features at various levels of complexity are extracted from the image data.
Typical examples of such features are
Lines, edges and ridges.
Localized interest points such as corners, blobs or points.
More complex features may be related to texture, shape or motion.
Detection/segmentation – At some point in the processing a decision is made about which image points or regions of the image are relevant for further processing.
Examples are
Selection of a specific set of interest points
Segmentation of one or multiple image regions which contain a specific object of interest.
Segmentation of image into nested scene architecture comprised foreground, object groups, single objects or salient object parts (also referred to as spatial-taxon scene hierarchy)
High-level processing – At this step the input is typically a small set of data, for example a set of points or an image region which is assumed to contain a specific object.
 The remaining processing deals with, for example:
Verification that the data satisfy model-based and application specific assumptions.
Estimation of application specific parameters, such as object pose or object size.
Image recognition – classifying a detected object into different categories.
Image registration – comparing and combining two different views of the same object.
Decision making Making the final decision required for the application,[12] for example:
Pass/fail on automatic inspection applications
Match / no-match in recognition applications
Flag for further human review in medical, military, security and recognition applications
Computer vision hardware[edit]
In addition, a practical vision system contains software, as well as a display in order to monitor the system. 
Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. 
Furthermore, a completed system includes many accessories like camera supports, cables and connectors.
Video tracking is the process of locating a moving object (or multiple objects) over time using a camera. 
It has a variety of uses, some of which are: human-computer interaction, security and surveillance, video communication and compression, augmented reality, traffic control, medical imaging[1] and video editing.
 Video tracking can be a time consuming process due to the amount of data that is contained in video. 
Adding further to the complexity is the possible need to use object recognition techniques for tracking, a challenging problem in its own right.
The objective of video tracking is to associate target objects in consecutive video frames. 
The association can be especially difficult when the objects are moving fast relative to the frame rate. 
Another situation that increases the complexity of the problem is when the tracked object changes orientation over time. 
For these situations video tracking systems usually employ a motion model which describes how the image of the target might change for different possible motions of the object.
Examples of simple motion models are:
When tracking planar objects, the motion model is a 2D transformation (affine transformation or homography) of an image of the object (e.g. the initial frame).
When the target is a rigid 3D object, the motion model defines its aspect depending on its 3D position and orientation.
For video compression, key frames are divided into macroblocks.
 The motion model is a disruption of a key frame, where each macroblock is translated by a motion vector given by the motion parameters.
The image of deformable objects can be covered with a mesh, the motion of the object is defined by the position of the nodes of the mesh.
Algorithms[edit]
To perform video tracking an algorithm analyzes sequential video frames and outputs the movement of targets between the frames. 
There are a variety of algorithms, each having strengths and weaknesses. Considering the intended use is important when choosing which algorithm to use. There are two major components of a visual tracking system: target representation and localization, as well as filtering and data association.
Target representation and localization is mostly a bottom-up process. 
These methods give a variety of tools for identifying the moving object. 
Locating and tracking the target object successfully is dependent on the algorithm. 
For example, using blob tracking is useful for identifying human movement because a person's profile changes dynamically.
 Typically the computational complexity for these algorithms is low.
 The following are some common target representation and localization algorithms:
Kernel-based tracking (mean-shift tracking[7]): an iterative localization procedure based on the maximization of a similarity measure (Bhattacharyya coefficient).
Contour tracking: detection of object boundary (e.g. active contours or Condensation algorithm).
 Contour tracking methods iteratively evolve an initial contour initialized from the previous frame to its new position in the current frame. 
This approach to contour tracking directly evolves the contour by minimizing the contour energy using gradient descent.
Filtering and data association is mostly a top-down process, which involves incorporating prior information about the scene or object, dealing with object dynamics, and evaluation of different hypotheses. 
These methods allow the tracking of complex objects along with more complex object interaction like tracking objects moving behind obstructions.
 Additionally the complexity is increased if the video tracker (also named TV tracker or target tracker) is not mounted on rigid foundation (on-shore) but on a moving ship (off-shore), where typically an inertial measurement system is used to pre-stabilize the video tracker to reduce the required dynamics and bandwidth of the camera system.
 The computational complexity for these algorithms is usually much higher. 
The following are some common filtering algorithms:
Kalman filter: an optimal recursive Bayesian filter for linear functions subjected to Gaussian noise.
It is an algorithm that uses a series of measurements observed over time, containing noise (random variations) and other inaccuracies, and produces estimates of unknown variables that tend to be more precise than those based on a single measurement alone.
mage Restoration is the operation of taking a corrupt/noisy image and estimating the clean, original image. 
Corruption may come in many forms such as motion blur, noise and camera mis-focus.
 Image restoration is performed by reversing the process that blurred the image and such is performed by imaging a point source and use the point source image, which is called the Point Spread Function (PSF) to restore the image information lost to the blurring process.
Image restoration is different from image enhancement in that the latter is designed to emphasize features of the image that make the image more pleasing to the observer, but not necessarily to produce realistic data from a scientific point of view. 
Image enhancement techniques (like contrast stretching or de-blurring by a nearest neighbor procedure) provided by imaging packages use no a priori model of the process that created the image.
With image enhancement noise can effectively be removed by sacrificing some resolution, but this is not acceptable in many applications.
 In a fluorescence microscope, resolution in the z-direction is bad as it is. 
More advanced image processing techniques must be applied to recover the object.
The objective of image restoration techniques is to reduce noise and recover resolution loss.
 Image processing techniques are performed either in the image domain or the frequency domain. 
The most straightforward technique for image restoration is DeConvolution, which is performed in the frequency domain and after computing the Fourier Transform of both the image and the PSF and undo the resolution loss caused by the blurring factors. 
Deconvolution technique assumes absence of noise and that the blurring process is shift-invariant and hence more sophisticated techniques have been developed to deal with the different types of noises and blurring functions.
A digital image is a numeric representation of (normally binary) a two-dimensional image. 
Depending on whether the image resolution is fixed, it may be of vector or raster type. 
By itself, the term "digital image" usually refers to raster images or bitmapped images.
Raster[edit]
Raster images have a finite set of digital values, called picture elements or pixels. 
The digital image contains a fixed number of rows and columns of pixels. 
Pixels are the smallest individual element in an image, holding antiquated values that represent the brightness of a given color at any specific point.
Typically, the pixels are stored in computer memory as a raster image or raster map, a two-dimensional array of small integers. 
These values are often transmitted or stored in a compressed form.
Raster images can be created by a variety of input devices and techniques, such as digital cameras, scanners, coordinate-measuring machines, seismographic profiling, airborne radar, and more.
 They can also be synthesized from arbitrary non-image data, such as mathematical functions or three-dimensional geometric models; the latter being a major sub-area of computer graphics. 
The field of digital image processing is the study of algorithms for their transformation.
Raster file formats[edit]
Most users come into contact with raster images through digital cameras, which use any of several image file formats.
Some digital cameras give access to almost all the data captured by the camera, using a raw image format. 
The Universal Photographic Imaging Guidelines (UPDIG) suggests these formats be used when possible since raw files produce the best quality images. 
These file formats allow the photographer and the processing agent the greatest level of control and accuracy for output. 
Their use is inhibited by the prevalence of proprietary information (trade secrets) for some camera makers, but there have been initiatives such as OpenRAW to influence manufacturers to release these records publicly. 
An alternative may be Digital Negative (DNG), a proprietary Adobe product described as “the public, archival format for digital camera raw data”.
 Although this format is not yet universally accepted, support for the product is growing, and increasingly professional archivists and conservationists, working for respectable organizations, variously suggest or recommend DNG for archival purposes.
Vector[edit]
Vector images resulted from mathematical geometry (vector). 
In mathematical terms, a vector consists of point that has both direction and length.
Often, both raster and vector elements will be combined in one image; for example, in the case of a billboard with text (vector) and photographs (raster).
Image viewing[edit]
Image viewer software displays images. Web browsers can display standard internet image formats including GIF, JPEG, and PNG. 
Some can show SVG format which is a standard W3C format. 
In the past, when Internet was still slow, it was common to provide "preview" image that would load and appear on the web site before being replaced by the main image (to give at preliminary impression).
 Now Internet is fast enough and this preview image is seldom used.
Some scientific images can be very large (for instance, the 46 gigapixel size image of the Milky Way, about 194 Gb in size). 
Such images are difficult to download and are usually browsed online through more complex web interfaces.
Some viewers offer a slideshow utility to display a sequence of images.
The first scan done by the SEAC in 1957
The SEAC scanner
Early Digital fax machines such as the Bartlane cable picture transmission system preceded digital cameras and computers by decades. 
The first picture to be scanned, stored, and recreated in digital pixels was displayed on the Standards Eastern Automatic Computer (SEAC) at NIST.
The advancement of digital imagery continued in the early 1960s, alongside development of the space program and in medical research. 
Projects at the Jet Propulsion Laboratory, MIT, Bell Labs and the University of Maryland, among others, used digital images to advance satellite imagery, wirephoto standards conversion, medical imaging, videophone technology, character recognition, and photo enhancement.[13]
Rapid advances in digital imaging began with the introduction of microprocessors in the early 1970s, alongside progress in related storage and display technologies. 
The invention of computerized axial tomography (CAT scanning), using x-rays to produce a digital image of a "slice" through a three-dimensional object, was of great importance to medical diagnostics.
 As well as origination of digital images, digitization of analog images allowed the enhancement and restoration of archaeological artifacts and began to be used in fields as diverse as nuclear medicine, astronomy, law enforcement, defence and industry.[14]
Advances in microprocessor technology paved the way for the development and marketing of charge-coupled devices (CCDs) for use in a wide range of image capture devices and gradually displaced the use of analog film and tape in photography and videography towards the end of the 20th century. 
The computing power necessary to process digital image capture also allowed computer-generated digital images to achieve a level of refinement close to photorealism.
In information technology, lossy compression or irreversible compression is the class of data encoding methods that uses inexact approximations and partial data discarding to represent the content.
 These techniques are used to reduce data size for storage, handling, and transmitting content.
 Different versions of the photo of the cat at the right show how higher degrees of approximation create coarser images as more details are removed.
This is opposed to lossless data compression (reversible data compression) which does not degrade the image. 
The amount of data reduction possible using lossy compression is often much higher than through lossless techniques.
Well-designed lossy compression technology often reduces file sizes significantly before degradation is noticed by the end-user. 
Even when noticeable by the user, further data reduction may be desirable (e.g., for real-time communication, to reduce transmission times, or to reduce storage needs).
Lossy compression is most commonly used to compress multimedia data (audio, video, and images), especially in applications such as streaming media and internet telephony. 
By contrast, lossless compression is typically required for text and data files, such as bank records and text articles. 
In many cases it is advantageous to make a master lossless file which is to be used to produce new compressed files; for example, a multi-megabyte file can be used at full size to produce a full-page advertisement in a glossy magazine, and a 10 kilobyte lossy copy can be made for a small image on a web page.
Lossy and lossless compression[edit]
It is possible to compress many types of digital data in a way that reduces the size of a computer file needed to store it, or the bandwidth needed to transmit it, with no loss of the full information contained in the original file.
 A picture, for example, is converted to a digital file by considering it to be an array of dots and specifying the color and brightness of each dot. 
If the picture contains an area of the same color, it can be compressed without loss by saying "200 red dots" instead of "red dot, red dot, ...(197 more times)..., red dot."
The original data contains a certain amount of information, and there is a lower limit to the size of file that can carry all the information. 
Basic information theory says that there is an absolute limit in reducing the size of this data. 
When data is compressed, its entropy increases, and it cannot increase indefinitely. 
As an intuitive example, most people know that a compressed ZIP file is smaller than the original file, but repeatedly compressing the same file will not reduce the size to nothing. 
Most compression algorithms can recognize when further compression would be pointless and would in fact increase the size of the data.
In many cases, files or data streams contain more information than is needed for a particular purpose. 
For example, a picture may have more detail than the eye can distinguish when reproduced at the largest size intended; likewise, an audio file does not need a lot of fine detail during a very loud passage. 
Developing lossy compression techniques as closely matched to human perception as possible is a complex task.
 Sometimes the ideal is a file that provides exactly the same perception as the original, with as much digital information as possible removed; other times, perceptible loss of quality is considered a valid trade-off for the reduced data.
The terms 'irreversible' and 'reversible' are preferred over 'lossy' and 'lossless' respectively for some applications, such as medical image compression, to circumvent the negative implications of 'loss'.
The type and amount of loss can affect the utility of the images.
 Artifacts of compression may be clearly discernible yet the result still useful for the intended purpose. 
Or lossy compressed images may be 'visually lossless', or in the case of medical images, so-called Diagnostically Acceptable Irreversible Compression (DAIC)[1] may have been applied.
Transform coding[edit]
Main article: Transform coding
More generally, some forms of lossy compression can be thought of as an application of transform coding – in the case of multimedia data, perceptual coding: it transforms the raw data to a domain that more accurately reflects the information content. 
For example, rather than expressing a sound file as the amplitude levels over time, one may express it as the frequency spectrum over time, which corresponds more accurately to human audio perception.
This is because uncompressed audio can only reduce file size by lowering bit rate or depth, whereas compressing audio can reduce size while maintaining bit rate and depth.
 This compression becomes a selective loss of the least significant data, rather than losing data across the board.
 Further, a transform coding may provide a better domain for manipulating or otherwise editing the data – for example, equalization of audio is most naturally expressed in the frequency domain (boost the bass, for instance) rather than in the raw time domain.
From this point of view, perceptual encoding is not essentially about discarding data, but rather about a better representation of data.
Another use is for backward compatibility and graceful degradation: in color television, encoding color via a luminance-chrominance transform domain (such as YUV) means that black-and-white sets display the luminance, while ignoring the color information.
Information loss[edit]
Lossy compression formats suffer from generation loss: repeatedly compressing and decompressing the file will cause it to progressively lose quality. 
This is in contrast with lossless data compression, where data will not be lost via the use of such a procedure.
Information-theoretical foundations for lossy data compression are provided by rate-distortion theory. 
Much like the use of probability in optimal coding theory, rate-distortion theory heavily draws on Bayesian estimation and decision theory in order to model perceptual distortion and even aesthetic judgment.
There are two basic lossy compression schemes:
In lossy transform codecs, samples of picture or sound are taken, chopped into small segments, transformed into a new basis space, and quantized.
 The resulting quantized values are then entropy coded.
In lossy predictive codecs, previous and/or subsequent decoded data is used to predict the current sound sample or image frame. 
The error between the predicted data and the real data, together with any extra information needed to reproduce the prediction, is then quantized and coded.
In some systems the two techniques are combined, with transform codecs being used to compress the error signals generated by the predictive stage.
Lossy versus lossless[edit]
The advantage of lossy methods over lossless methods is that in some cases a lossy method can produce a much smaller compressed file than any lossless method, while still meeting the requirements of the application.
Lossy methods are most often used for compressing sound, images or videos. 
This is because these types of data are intended for human interpretation where the mind can easily "fill in the blanks" or see past very minor errors or inconsistencies – ideally lossy compression is transparent (imperceptible), which can be verified via an ABX test.
Transparency[edit]
Further information: Transparency (data compression)
When a user acquires a lossily compressed file, (for example, to reduce download time) the retrieved file can be quite different from the original at the bit level while being indistinguishable to the human ear or eye for most practical purposes.
 Many compression methods focus on the idiosyncrasies of human physiology, taking into account, for instance, that the human eye can see only certain wavelengths of light.
 The psychoacoustic model describes how sound can be highly compressed without degrading perceived quality. 
Flaws caused by lossy compression that are noticeable to the human eye or ear are known as compression artifacts.
Compression ratio[edit]
The compression ratio (that is, the size of the compressed file compared to that of the uncompressed file) of lossy video codecs is nearly always far superior to that of the audio and still-image equivalents.
Video can be compressed immensely (e.g. 100:1) with little visible quality loss
Audio can often be compressed at 10:1 with imperceptible loss of quality
Still images are often lossily compressed at 10:1, as with audio, but the quality loss is more noticeable, especially on closer inspection.
Transcoding and editing[edit]
For more details on this topic, see Transcoding.
An important caveat about lossy compression (formally transcoding), is that editing lossily compressed files causes digital generation loss from the re-encoding. 
This can be avoided by only producing lossy files from (lossless) originals and only editing (copies of) original files, such as images in raw image format instead of JPEG.
If data which has been compressed lossily is decoded and compressed losslessly, the size of the result can be comparable with the size of the data before lossy compression, but the data already lost cannot be recovered.
When deciding to use lossy conversion without keeping the original, one should remember that format conversion may be needed in the future to achieve compatibility with software or devices (format shifting), or to avoid paying patent royalties for decoding or distribution of compressed files.
Editing of lossy files[edit]
See also: commons:Commons:Software § JPEG and commons:Commons:Software § Ogg Vorbis (audio)
By modifying the compressed data directly without decoding and re-encoding, some editing of lossily compressed files without degradation of quality is possible. 
Editing which reduces the file size as if it had been compressed to a greater degree, but without more loss than this, is sometimes also possible.
JPEG[edit]
The primary programs for lossless editing of JPEGs are jpegtran, and the derived exiftran (which also preserves Exif information), and Jpegcrop (which provides a Windows interface).
These allow the image to be
cropped
rotated, flipped, and flopped, or
converted to grayscale (by dropping the chrominance channel).
While unwanted information is destroyed, the quality of the remaining portion is unchanged.
Some other transforms are possible to some extent, such as joining images with the same encoding (composing side by side, as on a grid) or pasting images (such as logos) onto existing images (both via Jpegjoin), or scaling.[3]
Some changes can be made to the compression without re-encoding:
optimize the compression (to reduce size without change to the decoded image)
convert between progressive and non-progressive encoding.
The freeware Windows-only IrfanView has some lossless JPEG operations in its JPG_TRANSFORM plugin.
Metadata[edit]
Metadata, such as ID3 tags, Vorbis comments, or Exif information, can usually be modified or removed without modifying the underlying data.
Downsampling/compressed representation scalability[edit]
One may wish to downsample or otherwise decrease the resolution of the represented source signal and the quantity of data used for its compressed representation without re-encoding, as in bitrate peeling, but this functionality is not supported in all designs, as not all codecs encode data in a form that allows less important detail to simply be dropped.
Some well known designs that have this capability include JPEG 2000 for still images and H.264/MPEG-4 AVC based Scalable Video Coding for video. 
Such schemes have also been standardized for older designs as well, such as JPEG images with progressive encoding, and MPEG-2 and MPEG-4 Part 2 video, although those prior schemes had limited success in terms of adoption into real-world common usage.
Without this capacity, which is often the case in practice, to produce a representation with lower resolution or lower fidelity than a given one, one needs to start with the original source signal and encode, or start with a compressed representation and then decompress and re-encode it (transcoding), though the latter tends to cause digital generation loss.
Another approach is to encode the original signal at several different bitrates, and their either choose which to use (as when streaming over the internet – as in RealNetworks' "SureStream" – or offering varying downloads, as at Apple's iTunes Store), or broadcast several, where the best that is successfully received is used, as in various implementations of hierarchical modulation.
 Similar techniques are used in mipmaps, pyramid representations, and more sophisticated scale space methods.
Some audio formats feature a combination of a lossy format and a lossless correction which when combined reproduce the original signal; the correction can be stripped, leaving a smaller, lossily compressed, file. 
Such formats include MPEG-4 SLS (Scalable to Lossless), WavPack, OptimFROG DualStream, and DTS-HD Master Audio in lossless (XLL) mode
Other data[edit]
Researchers have (semi-seriously) performed lossy compression on text by either using a thesaurus to substitute short words for long ones, or generative text techniques,[4] although these sometimes fall into the related category of lossy data conversion.
Lowering resolution[edit]
A general kind of lossy compression is to lower the resolution of an image, as in image scaling, particularly decimation. 
One may also remove less "lower information" parts of an image, such as by seam carving.
Many media transforms, such as Gaussian blur, are, like lossy compression, irreversible: the original signal cannot be reconstructed from the transformed signal. 
However, in general these will have the same size as the original, and are not a form of compression.
Lowering resolution has practical uses, as the NASA New Horizons craft will transmit thumbnails of its encounter with Pluto-Charon before it sends the higher resolution images.
Another solution for slow connections is the usage of Image interlacing which progressively defines the image. 
Thus a partial transmission is enough to preview the final image, in a lower resolution version, without creating a scaled and a full version too.
In machine learning and cognitive science, artificial neural networks (ANNs) are a family of models inspired by biological neural networks (the central nervous systems of animals, in particular the brain) and are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. 
Artificial neural networks are generally presented as systems of interconnected "neurons" which exchange messages between each other. 
The connections have numeric weights that can be tuned based on experience, making neural nets adaptive to inputs and capable of learning.
For example, a neural network for handwriting recognition is defined by a set of input neurons which may be activated by the pixels of an input image.
 After being weighted and transformed by a function (determined by the network's designer), the activations of these neurons are then passed on to other neurons. 
This process is repeated until finally, an output neuron is activated. 
This determines which character was read.
Like other machine learning methods – systems that learn from data – neural networks have been used to solve a wide variety of tasks that are hard to solve using ordinary rule-based programming, including computer vision and speech recognition.
Examinations of humans' central nervous systems inspired the concept of artificial neural networks. 
In an artificial neural network, simple artificial nodes, known as "neurons", "neurodes", "processing elements" or "units", are connected together to form a network which mimics a biological neural network.
There is no single formal definition of what an artificial neural network is. 
However, a class of statistical models may commonly be called "neural" if it possesses the following characteristics:
contains sets of adaptive weights, i.e. numerical parameters that are tuned by a learning algorithm, and
capability of approximating non-linear functions of their inputs.
The adaptive weights can be thought of as connection strengths between neurons, which are activated during training and prediction.
Neural networks are similar to biological neural networks in the performing of functions collectively and in parallel by the units, rather than there being a clear delineation of subtasks to which individual units are assigned. 
The term "neural network" usually refers to models employed in statistics, cognitive psychology and artificial intelligence.
 Neural network models which command the central nervous system and the rest of the brain are part of theoretical neuroscience and computational neuroscience[1]
In modern software implementations of artificial neural networks, the approach inspired by biology has been largely abandoned for a more practical approach based on statistics and signal processing. 
In some of these systems, neural networks or parts of neural networks (like artificial neurons) form components in larger systems that combine both adaptive and non-adaptive elements. 
While the more general approach of such systems is more suitable for real-world problem solving, it has little to do with the traditional, artificial intelligence connectionist models. 
What they do have in common, however, is the principle of non-linear, distributed, parallel and local processing and adaptation. 
History
Warren McCulloch and Walter Pitts[2] (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. 
This model paved the way for neural network research to split into two distinct approaches. 
One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.
In the late 1940s psychologist Donald Hebb[3] created a hypothesis of learning based on the mechanism of neural plasticity that is now known as Hebbian learning. 
Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation.
 Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines.
Farley and Wesley A. Clark[4] (1954) first used computational machines, then called "calculators," to simulate a Hebbian network at MIT. 
Other neural network computational machines were created by Rochester, Holland, Habit, and Duda[5] (1956).
Frank Rosenblatt[6] (1958) created the perceptron, an algorithm for pattern recognition based on a two-layer computer learning network using simple addition and subtraction. 
With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the exclusive-or circuit, a circuit whose mathematical computation could not be processed until after the backpropagation algorithm was created by Paul Werbos[7] (1975).
Neural network research stagnated after the publication of machine learning research by Marvin Minsky and Seymour Papert[8] (1969), who discovered two key issues with the computational machines that processed neural networks. 
The first was that single-layer neural networks were incapable of processing the exclusive-or circuit. 
The second significant issue was that computers didn't have enough processing power to effectively handle the long run time required by large neural networks. Neural network research slowed until computers achieved greater processing power. 
Another key advance that came later was the backpropagation algorithm which effectively solved the exclusive-or problem (Werbos 1975).[7]
The parallel distributed processing of the mid-1980s became popular under the name connectionism. 
The textbook by David E. Rumelhart and James McClelland[9] (1986) provided a full exposition of the use of connectionism in computers to simulate neural processes.
Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and the biological architecture of the brain is debated; it's not clear to what degree artificial neural networks mirror brain function.
Support vector machines and other, much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity. 
But the advent of deep learning in the late 2000s sparked renewed interest in neural nets.
Improvements since 2006
Computational devices have been created in CMOS, for both biophysical simulation and neuromorphic computing.
 More recent efforts show promise for creating nanodevices[11] for very large scale principal components analyses and convolution. 
If successful, these efforts could usher in a new era of neural computing[12] that is a step beyond digital computing, because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices.
Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning.
 For example, the bi-directional and multi-dimensional long short term memory (LSTM)[15][16][17][18] of Alex Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned.
Fast GPU-based implementations of this approach by Dan Ciresan and colleagues at IDSIA have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition,[19][20] the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge,[21] and others. 
Their neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance[22] on important benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem of Yann LeCun at NYU.
Deep, highly nonlinear neural architectures similar to the 1980 neocognitron by Kunihiko Fukushima[23] and the "standard architecture of vision",[24] inspired by the simple and complex cells identified by David H. Hubel and Torsten Wiesel in the primary visual cortex, can also be pre-trained by unsupervised methods[25][26] of Geoff Hinton's lab at University of Toronto.[
 A team from this lab won a 2012 contest sponsored by Merck to design software to help find molecules that might lead to new drugs.
Models
Neural network models in artificial intelligence are usually referred to as artificial neural networks (ANNs); these are essentially simple mathematical models defining a function \textstyle  f : X \rightarrow Y  or a distribution over \textstyle X or both \textstyle X and \textstyle Y, but sometimes models are also intimately associated with a particular learning algorithm or learning rule. 
A common use of the phrase "ANN model" is really the definition of a class of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity).
Network function
See also: Graphical models
The word network in the term 'artificial neural network' refers to the inter–connections between the neurons in the different layers of each system.
An example system has three layers. 
The first layer has input neurons which send data via synapses to the second layer of neurons, and then via more synapses to the third layer of output neurons.
 More complex systems will have more layers of neurons, some having increased layers of input neurons and output neurons. 
The synapses store parameters called "weights" that manipulate the data in the calculations.
An ANN is typically defined by three types of parameters:
The interconnection pattern between the different layers of neurons
The learning process for updating the weights of the interconnections
The activation function that converts a neuron's weighted input to its output activation.
Mathematically, a neuron's network function \textstyle f(x) is defined as a composition of other functions \textstyle g_i(x), which can further be defined as a composition of other functions.
 This can be conveniently represented as a network structure, with arrows depicting the dependencies between variables.
 A widely used type of composition is the nonlinear weighted sum, where \textstyle f (x) = K \left(\sum_i w_i g_i(x)\right) , where \textstyle K (commonly referred to as the activation function[30]) is some predefined function, such as the hyperbolic tangent. 
It will be convenient for the following to refer to a collection of functions \textstyle g_i as simply a vector \textstyle g = (g_1, g_2, \ldots, g_n).
ANN dependency graph
This figure depicts such a decomposition of \textstyle f, with dependencies between variables indicated by arrows. 
These can be interpreted in two ways.
The first view is the functional view: the input \textstyle x is transformed into a 3-dimensional vector \textstyle h, which is then transformed into a 2-dimensional vector \textstyle g, which is finally transformed into \textstyle f. 
This view is most commonly encountered in the context of optimization.
The second view is the probabilistic view: the random variable \textstyle F = f(G)  depends upon the random variable \textstyle G = g(H), which depends upon \textstyle H=h(X), which depends upon the random variable \textstyle X. This view is most commonly encountered in the context of graphical models.
The two views are largely equivalent.
 In either case, for this particular network architecture, the components of individual layers are independent of each other (e.g., the components of \textstyle g are independent of each other given their input \textstyle h). 
This naturally enables a degree of parallelism in the implementation.
Two separate depictions of the recurrent ANN dependency graph
Networks such as the previous one are commonly called feedforward, because their graph is a directed acyclic graph. 
Networks with cycles are commonly called recurrent. 
Such networks are commonly depicted in the manner shown at the top of the figure, where \textstyle f is shown as being dependent upon itself. 
However, an implied temporal dependence is not shown.
Learning
See also: Mathematical optimization, Estimation theory and Machine learning
What has attracted the most interest in neural networks is the possibility of learning.
 Given a specific task to solve, and a class of functions \textstyle F, learning means using a set of observations to find \textstyle  f^{*} \in F which solves the task in some optimal sense.
This entails defining a cost function \textstyle C : F \rightarrow \mathbb{R} such that, for the optimal solution \textstyle f^*, \textstyle C(f^*) \leq C(f) \textstyle \forall f \in F – i.e., no solution has a cost less than the cost of the optimal solution (see mathematical optimization).
The cost function \textstyle C is an important concept in learning, as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved. 
Learning algorithms search through the solution space to find a function that has the smallest possible cost.
For applications where the solution is dependent on some data, the cost must necessarily be a function of the observations, otherwise we would not be modelling anything related to the data. 
It is frequently defined as a statistic to which only approximations can be made.
 As a simple example, consider the problem of finding the model \textstyle f, which minimizes \textstyle C=E\left[(f(x) - y)^2\right], for data pairs \textstyle (x,y) drawn from some distribution \textstyle \mathcal{D}. 
In practical situations we would only have \textstyle N samples from \textstyle \mathcal{D} and thus, for the above example, we would only minimize \textstyle \hat{C}=\frac{1}{N}\sum_{i=1}^N (f(x_i)-y_i)^2. 
Thus, the cost is minimized over a sample of the data rather than the entire distribution generating the data.
When \textstyle N \rightarrow \infty some form of online machine learning must be used, where the cost is partially minimized as each new example is seen. 
While online machine learning is often used when \textstyle \mathcal{D} is fixed, it is most useful in the case where the distribution changes slowly over time. 
In neural network methods, some form of online machine learning is frequently used for finite datasets.
Choosing a cost function
While it is possible to define some arbitrary ad hoc cost function, frequently a particular cost will be used, either because it has desirable properties (such as convexity) or because it arises naturally from a particular formulation of the problem (e.g., in a probabilistic formulation the posterior probability of the model can be used as an inverse cost). 
Ultimately, the cost function will depend on the desired task. 
An overview of the three main categories of learning tasks is provided below:
Learning paradigms
There are three major learning paradigms, each corresponding to a particular abstract learning task. 
These are supervised learning, unsupervised learning and reinforcement learning.
Supervised learning
In supervised learning, we are given a set of example pairs \textstyle (x, y), x \in X, y \in Y and the aim is to find a function \textstyle  f : X \rightarrow Y  in the allowed class of functions that matches the examples.
 In other words, we wish to infer the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domain.
A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output, \textstyle f(x), and the target value \textstyle y over all the example pairs. 
When one tries to minimize this cost using gradient descent for the class of neural networks called multilayer perceptrons (MLP), one obtains the common and well-known backpropagation algorithm for training neural networks.
Tasks that fall within the paradigm of supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). 
The supervised learning paradigm is also applicable to sequential data (e.g., for speech and gesture recognition). 
This can be thought of as learning with a "teacher", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.
Unsupervised learning
In unsupervised learning, some data \textstyle x is given and the cost function to be minimized, that can be any function of the data \textstyle x and the network's output, \textstyle f.
The cost function is dependent on the task (what we are trying to model) and our a priori assumptions (the implicit properties of our model, its parameters and the observed variables).
As a trivial example, consider the model \textstyle f(x) = a where \textstyle a is a constant and the cost \textstyle C=E[(x - f(x))^2]. 
Minimizing this cost will give us a value of \textstyle a that is equal to the mean of the data. 
The cost function can be much more complicated. 
Its form depends on the application: for example, in compression it could be related to the mutual information between \textstyle x and \textstyle f(x), whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples those quantities would be maximized rather than minimized).
Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.
Reinforcement learning
See also: dynamic programming and stochastic control
In reinforcement learning, data \textstyle x are usually not given, but generated by an agent's interactions with the environment. 
At each point in time \textstyle t, the agent performs an action \textstyle y_t and the environment generates an observation \textstyle x_t and an instantaneous cost \textstyle c_t, according to some (usually unknown) dynamics.
 The aim is to discover a policy for selecting actions that minimizes some measure of a long-term cost, e.g., the expected cumulative cost. 
The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated.
ANNs are frequently used in reinforcement learning as part of the overall algorithm.
 Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.
Learning algorithms
See also: machine learning
Training a neural network model essentially means selecting one model from the set of allowed models (or, in a Bayesian framework, determining a distribution over the set of allowed models) that minimizes the cost criterion. 
There are numerous algorithms available for training neural network models; most of them can be viewed as a straightforward application of optimization theory and statistical estimation.
Most of the algorithms used in training artificial neural networks employ some form of gradient descent, using backpropagation to compute the actual gradients. 
This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a gradient-related direction. 
Evolutionary methods,[39] gene expression programming,[40] simulated annealing,[41] expectation-maximization, non-parametric methods and particle swarm optimization[42] are some commonly used methods for training neural networks.
Employing artificial neural networks
Perhaps the greatest advantage of ANNs is their ability to be used as an arbitrary function approximation mechanism that 'learns' from observed data.
 However, using them is not so straightforward, and a relatively good understanding of the underlying theory is essential.
Choice of model: This will depend on the data representation and the application. 
Overly complex models tend to lead to challenges in learning.
Learning algorithm: There are numerous trade-offs between learning algorithms.
 Almost any algorithm will work well with the correct hyperparameters for training on a particular fixed data set. 
However, selecting and tuning an algorithm for training on unseen data require a significant amount of experimentation.
Robustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can be extremely robust.
With the correct implementation, ANNs can be used naturally in online learning and large data set applications.
 Their simple implementation and the existence of mostly local dependencies exhibited in the structure allows for fast, parallel implementations in hardware.
Applications
The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations. 
This is particularly useful in applications where the complexity of the data or task makes the design of such a function by hand impractical.
Real-life applications
The tasks artificial neural networks are applied to tend to fall within the following broad categories:
function approximation, or regression analysis, including time series prediction, fitness approximation and modeling
classification, including pattern and sequence recognition, novelty detection and sequential decision making
data processing, including filtering, clustering, blind source separation and compression
robotics, including directing manipulators, prosthesis.
Control, including Computer numerical control
Neural networks produce reasonable results compared to typical time series approaches. 
Time series techniques try to detect trends and seasonal effects on time lines, but they fail to forecast for unseasonable times. 
If the exceptional cases are modeled properly on network, neural networks would succeed to predict unseasonable moments. 
A common use of neural networks for time series forecasting includes previous instances into the network as an input. [44]
Artificial neural networks have also been used to diagnose several cancers. 
An ANN based hybrid lung cancer detection system named HLND improves the accuracy of diagnosis and the speed of lung cancer radiology.
 These networks have also been used to diagnose prostate cancer. 
The diagnoses can be used to make specific models taken from a large group of patients compared to information of one given patient.
 The models do not depend on assumptions about correlations of different variables. 
Colorectal cancer has also been predicted using the neural networks. 
Neural networks could predict the outcome for a patient with colorectal cancer with more accuracy than the current clinical methods. 
After training, the networks could predict multiple patient outcomes from unrelated institutions.[48]
Neural networks and neuroscience
Theoretical and computational neuroscience is the field concerned with the theoretical analysis and the computational modeling of biological neural systems. 
Since neural systems are intimately related to cognitive processes and behavior, the field is closely related to cognitive and behavioral modeling.
The aim of the field is to create models of biological neural systems in order to understand how biological systems work. 
To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).
Types of models
Many models are used in the field, defined at different levels of abstraction and modeling different aspects of neural systems. 
They range from models of the short-term behavior of individual neurons (e.g.[49]), models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. 
These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.
Memory networks
More recently deep learning was shown to be useful in semantic hashing[51] where a deep graphical model of the word-count vectors[52] is obtained from a large set of documents. 
Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. 
Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document.
Neural Turing Machines[53] developed by Google DeepMind extend the capabilities of deep neural networks by coupling them to external memory resources, which they can interact with by attentional processes. 
The combined system is analogous to a Turing Machine but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. 
Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.
Memory Networks[54] is another extension to neural networks incorporating long-term memory which was developed by Facebook research.
 The long-term memory can be read and written to, with the goal of using it for prediction. 
These models have been applied in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response.
Neural network software
Main article: Neural network software
Neural network software is used to simulate, research, develop and apply artificial neural networks, biological neural networks and, in some cases, a wider array of adaptive systems.
Types of artificial neural networks
Main article: Types of artificial neural networks
Artificial neural network types vary from those with only one or two layers of single direction logic, to complicated multi–input many directional feedback loops and layers. 
On the whole, these systems use algorithms in their programming to determine control and organization of their functions. 
Most systems use "weights" to change the parameters of the throughput and the varying connections to the neurons. 
Artificial neural networks can be autonomous and learn by input from outside "teachers" or even self-teaching from written-in rules.
Theoretical properties
Computational power
The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem.
 However, the proof is not constructive regarding the number of neurons required or the settings of the weights.
Work by Hava Siegelmann and Eduardo D. Sontag has provided a proof that a specific recurrent architecture with rational valued weights (as opposed to full precision real number-valued weights) has the full power of a Universal Turing Machine[55] using a finite number of neurons and standard linear connections. 
Further, it has been shown that the use of irrational values for weights results in a machine with super-Turing power.
Capacity
Artificial neural network models have a property called 'capacity', which roughly corresponds to their ability to model any given function. 
It is related to the amount of information that can be stored in the network and to the notion of complexity.
Convergence
Nothing can be said in general about convergence since it depends on a number of factors.
 Firstly, there may exist many local minima.
 This depends on the cost function and the model. 
Secondly, the optimization method used might not be guaranteed to converge when far away from a local minimum. 
Thirdly, for a very large amount of data or parameters, some methods become impractical.
 In general, it has been found that theoretical guarantees regarding convergence are an unreliable guide to practical application.
Generalization and statistics
In applications where the goal is to create a system that generalizes well in unseen examples, the problem of over-training has emerged.
 This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. 
There are two schools of thought for avoiding this problem: The first is to use cross-validation and similar techniques to check for the presence of overtraining and optimally select hyperparameters such as to minimize the generalization error. 
The second is to use some form of regularization. 
Confidence analysis of a neural network
Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. 
The MSE on a validation set can be used as an estimate for variance. 
This value can then be used to calculate the confidence interval of the output of the network, assuming a normal distribution.
 A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.
By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based neural network) for categorical target variables, the outputs can be interpreted as posterior probabilities. 
This is very useful in classification as it gives a certainty measure on classifications.
The softmax activation function is:
y_i=\frac{e^{x_i}}{\sum_{j=1}^c e^{x_j}}
Criticism
Training issues
A common criticism of neural networks, particularly in robotics, is that they require a large diversity of training for real-world operation[citation needed].
 This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. 
Dean Pomerleau, in his research presented in the paper "Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving," uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.). 
A large amount of his research is devoted to (1) extrapolating multiple training scenarios from a single training experience, and (2) preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turn – it should not learn to always turn right). 
These issues are common in neural networks that must decide from amongst a wide variety of responses, but can be dealt with in several ways, for example by randomly shuffling the training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, or by grouping examples in so-called mini-batches.
A. K. Dewdney, a former Scientific American columnist, wrote in 1997, "Although neural nets do solve a few toy problems, their powers of computation are so limited that I am surprised anyone takes them seriously as a general problem-solving tool." (Dewdney, p. 82)
Hardware issues
To implement large and effective software neural networks, considerable processing and storage resources need to be committed.
While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on von Neumann architecture may compel a neural network designer to fill many millions of database rows for its connections – which can consume vast amounts of computer memory and hard disk space. 
Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons – which must often be matched with incredible amounts of CPU processing power and time.
 The use of GPUs instead of ordinary CPUs can bring training times for some networks down from months to mere days.
Computing power continues to grow roughly according to Moore's Law, which may provide sufficient resources to accomplish new tasks.
 Neuromorphic engineering addresses the hardware difficulty directly, by constructing non-von-Neumann chips with circuits designed to implement neural nets from the ground up.
Practical counterexamples to criticisms
Arguments against Dewdney's position are that neural networks have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircraft[59] to detecting credit card fraud.[citation needed]
Technology writer Roger Bridgman commented on Dewdney's statements about neural nets:
In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. 
An unreadable table that a useful machine could read would still be well worth having.
Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. 
Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles which allow a learning machine to be successful.
 For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.
Hybrid approaches
some other criticisms come from advocates of hybrid models (combining neural networks and symbolic approaches), who believe that the intermix of these two approaches can better capture the mechanisms of the human mind
Speech recognition (SR) is the inter-disciplinary sub-field of computational linguistics which incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields
This is to develop methodologies and technologies that enables the recognition and translation of spoken language into text by computers and computerized devices such as those categorized as Smart Technologies and robotics. 
It is also known as "automatic speech recognition" (ASR), "computer speech recognition", or just "speech to text" (STT).
Some SR systems use "training" (also called "enrollment") where an individual speaker reads text or isolated vocabulary into the system. 
The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. 
Systems that do not use training are called "speaker independent"[1] systems. 
Systems that use training are called "speaker dependent".
Speech recognition applications include voice user interfaces such as voice dialing (e.g. "Call home"), call routing (e.g. 
"I would like to make a collect call"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry , preparation of structured documents , speech-to-text processing , and aircraft .
The term voice recognition[2][3][4] or speaker identification[5][6] refers to identifying the speaker, rather than what they are saying. 
Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.
From the technology perspective, speech recognition has a long history with several waves of major innovations. 
Most recently, the field has benefited from advances in deep learning and big data. 
The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the world-wide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems.
 These speech industry players include Microsoft, Google, IBM, Baidu (China), Apple, Amazon, Nuance, IflyTek (China), many of which have publicized the core technology in their speech recognition systems being based on deep learning.
as early as 1932, Bell Labs researchers like Harvey Fletcher were investigating the science of speech perception.
 In 1952 three Bell Labs researchers built a system for single-speaker digit recognition. 
Their system worked by locating the formants in the power spectrum of each utterance.
The 1950s era technology was limited to single-speaker systems with vocabularies of around ten words.
Unfortunately, funding at Bell Labs dried up for several years when, in 1969, the influential John Pierce wrote an open letter that was critical of speech recognition research.
Pierce's letter compared speech recognition to "schemes for turning water into gasoline, extracting gold from the sea, curing cancer, or going to the moon."
 Pierce defunded speech recognition research at Bell Labs.
Raj Reddy was the first person to take on continuous speech recognition as a graduate student at Stanford University in the late 1960s. 
Previous systems required the users to make a pause after each word. 
Reddy's system was designed to issue spoken commands for the game of chess. 
Also around this time Soviet researchers invented the dynamic time warping algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary.
 Achieving speaker independence was a major unsolved goal of researchers during this time period.
In 1971, DARPA funded five years of speech recognition research through its Speech Understanding Research program with ambitious end goals including a minimum vocabulary size of 1,000 words. BBN. 
IBM., Carnegie Mellon and Stanford Research Institute all participated in the program.
 The government funding revived speech recognition research that had been largely abandoned in the United States after John Pierce's letter.
 Despite the fact that CMU's Harpy system met the goals established at the outset of the program, many of the predictions turned out to be nothing more than hype disappointing DARPA administrators.
 This disappointment led to DARPA not continuing the funding.[12] Several innovations happened during this time, such as the invention of beam search for use in CMU's Harpy system.
 The field also benefited from the discovery of several algorithms in other fields such as linear predictive coding and cepstral analysis.
During the late 1960s Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. 
At CMU, Raj Reddy's students James Baker and Janet Baker began using the Hidden Markov Model (HMM) for speech recognition.
 James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education.
The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.
Under Fred Jelinek's lead, IBM created a voice activated typewriter called Tangora, which could handle a 20,000 word vocabulary by the mid 1980s.
 Jelinek's statistical approach put less emphasis on emulating the way the human brain processes and understands speech in favor of using statistical modeling techniques like HMMs. 
(Jelinek's group independently discovered the application of HMMs to speech.
 This was controversial with linguists since HMMs are too simplistic to account for many common features of human languages.
 However, the HMM proved to be a highly useful way for modeling speech and replaced dynamic time warping to become the dominate speech recognition algorithm in the 1980s.
 IBM had a few competitors including Dragon Systems founded by James and Janet Baker in 1982.
 The 1980s also saw the introduction of the n-gram language model.
 Katz introduced the back-off model in 1987, which allowed language models to use multiple length n-grams.
Much of the progress in the field is owed to the rapidly increasing capabilities of computers. 
At the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB ram.
 Using these computers it could take up to 100 minutes to decode just 30 seconds of speech.
 A few decades later, researchers had access to tens of thousands of times as much computing power.
As the technology advanced and computers got faster, researchers began tackling harder problems such as larger vocabularies, speaker independence, noisy environments and conversational speech.
 In particular, this shifting to more difficult tasks has characterized DARPA funding of speech recognition since the 1980s. 
For example, progress was made on speaker independence first by training on a larger variety of speakers and then later by doing explicit speaker adaptation during decoding. 
Further reductions in word error rate came as researchers shifted acoustic models to be discriminative instead of using maximum likelihood models.
Another one of Raj Reddy's former students, Xuedong Huang, developed the Sphinx-II system at CMU. 
The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. 
Huang went on to found the speech recognition group at Microsoft in 1993.
The 1990s saw the first introduction of commercially successful speech recognition technologies.
 By this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary.
 In 2000, Lernout & Hauspie acquired Dragon Systems and was an industry leader until an accounting scandal brought an end to the company in 2001.
 The L&H speech technology was bought by ScanSoft which became Nuance in 2005. 
Apple originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri.
21st Century
In the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and Global Autonomous Language Exploitation (GALE).
 Four teams participated in the EARS program: IBM, BBN, Cambridge University and a team composed of ISCI, SRI and University of Washington. 
The GALE program focused on Mandarin broadcast news speech. 
Google's first effort at speech recognition came in 2007 after hiring some researchers from Nuance.
The first product was GOOG-411, a telephone based directory service. 
The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems. 
Google voice search is now supported in over 30 languages.
In the United States, the National Security Agency has made use of a type of speech recognition for keyword spotting since at least 2006.
 This technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords. 
Recordings can be indexed and analysts can run queries over the database to find conversations of interest. 
Some government research programs focused on intelligence applications of speech recognition, e.g. DARPA's EARS's program and IARPA's Babel program.
The use of deep learning for acoustic modeling was introduced during later part of 2009 by Geoffrey Hinton and his students at University of Toronto and by Li Deng and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and University of Toronto which was subsequently expanded to include IBM and Google .
 A Microsoft research executive called this innovation "the most dramatic change in accuracy since 1979."
 In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%. 
This innovation was quickly adopted across the field. 
Researchers have begun to use deep learning techniques for language modeling as well.
In the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 80's, 90's and a few years into 2000.
But these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.
 A number of key difficulties had been methodologically analyzed in the 1990s, including gradient diminishing and weak temporal correlation structure in the neural predictive models.
 All these difficulties were in addition to the lack of big training data and big computing power in these early days. 
Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009-2010 that had overcome all these difficulties. 
Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups  ignited the renaissance of neural networks and initiated deep learning research and applications in speech recognition.
Models, methods, and algorithms
Both acoustic modeling and language modeling are important parts of modern statistically-based speech recognition algorithms. 
Hidden Markov models (HMMs) are widely used in many systems. 
Language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation.
Hidden Markov models
Main article: Hidden Markov model
Modern general-purpose speech recognition systems are based on Hidden Markov Models. 
These are statistical models that output a sequence of symbols or quantities. 
HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. 
In a short time-scale (e.g., 10 milliseconds), speech can be approximated as a stationary process. 
Speech can be thought of as a Markov model for many stochastic purposes.
Another reason why HMMs are popular is because they can be trained automatically and are simple and computationally feasible to use. 
In speech recognition, the hidden Markov model would output a sequence of n-dimensional real-valued vectors (with n being a small integer, such as 10), outputting one of these every 10 milliseconds. 
The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. 
The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector.
 Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.
Described above are the core elements of the most common, HMM-based approach to speech recognition.
 Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. 
A typical large-vocabulary system would need context dependency for the phonemes (so phonemes with different left and right context have different realizations as HMM states); 
it would use cepstral normalization to normalize for different speaker and recording conditions; for further speaker normalization it might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general speaker adaptation.
The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition might use heteroscedastic linear discriminant analysis (HLDA); 
or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by heteroscedastic linear discriminant analysis or a global semi-tied co variance transform (also known as maximum likelihood linear transform, or MLLT). 
Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. 
Examples are maximum mutual information (MMI), minimum classification error (MCE) and minimum phone error (MPE).
A possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function (re scoring) to rate these good candidates so that we may pick the best one according to this refined score. 
The set of candidates can be kept either as a list (the N-best list approach) or as a subset of the models (a lattice). 
 The loss function is usually the Levenshtein distance, though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. 
Efficient algorithms have been devised to re score lattices represented as weighted finite state transducers with edit distances represented themselves as a finite state transducer verifying certain assumptions.
Dynamic time warping (DTW)-based speech recognition
Main article: Dynamic time warping
Dynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.
Dynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. 
For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. 
DTW has been applied to video, audio, and graphics – indeed, any data that can be turned into a linear representation can be analyzed with DTW.
A well-known application has been automatic speech recognition, to cope with different speaking speeds. 
In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions.
 That is, the sequences are "warped" non-linearly to match each other. 
This sequence alignment method is often used in the context of hidden Markov models.
Neural networks
Main article: Neural networks
Neural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. 
Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification,[40] isolated word recognition,[41] and speaker adaptation.
In contrast to HMMs, neural networks make no assumptions about feature statistical properties and have several qualities making them attractive recognition models for speech recognition. 
When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. 
Few assumptions on the statistics of input features are made with neural networks. 
However, in spite of their effectiveness in classifying short-time units such as individual phones and isolated words,[42] neural networks are rarely successful for continuous recognition tasks, largely because of their lack of ability to model temporal dependencies.
However, recently Recurrent Neural Networks(RNN's)[43] and Time Delay Neural Networks(TDNN's)[44] have been used which have been shown to be able to identify latent temporal dependencies and use this information to perform the task of speech recognition. 
This however enormously increases the computational cost involved and hence makes the process of speech recognition slower. 
A lot of research is still going on in this field to ensure that TDNN's and RNN's can be used in a more computationally affordable way to improve the Speech Recognition Accuracy immensely.
Deep Neural Networks and Denoising Autoencoders[45] are also being experimented with to tackle this problem in an effective manner.
Due to the inability of traditional Neural Networks to model temporal dependencies, an alternative approach is to use neural networks as a pre-processing e.g. feature transformation, dimensionality reduction,[46] for the HMM based recognition.
Deep Neural Networks and Other Deep Learning Models
Main article: Deep Neural Networks and Other Deep Learning Models
A deep neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers.
Similar to shallow neural networks, DNNs can model complex non-linear relationships.
 DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data.
 The DNN is the most popular type of deep learning architectures successfully used as an acoustic model for speech recognition since 2010.
The success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted.
See comprehensive reviews of this development and of the state of the art as of October 2014 in the recent Springer book from Microsoft Research.
See also the related background of automatic speech recognition and the impact of various machine learning paradigms including notably deep learning in a recent overview article.
One fundamental principle of deep learning is to do away with hand-crafted feature engineering and to use raw features. 
This principle was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features,[53] showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms. 
The true "raw" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.[54]
Since the initial successful debut of DNNs for speech recognition around 2009-2011, there have been huge new progresses made. 
This progress (as well as future directions) has been summarized into the following eight major areas:
Scaling up/out and speedup DNN training and decoding;
Sequence discriminative training of DNNs;
Feature processing by deep models with solid understanding of the underlying mechanisms;
Adaptation of DNNs and of related deep models;
Multi-task and transfer learning by DNNs and related deep models;
Convolution neural networks and how to design them to best exploit domain knowledge of speech;
Recurrent neural network and its rich LSTM variants;
Other types of deep models including tensor-based models and integrated deep generative/discriminative models.
Large-scale automatic speech recognition is the first and the most convincing successful case of deep learning in the recent history, embraced by both industry and academic across the board. 
Between 2010 and 2014, the two major conferences on signal processing and speech recognition, IEEE-ICASSP and Interspeech, have seen near exponential growth in the numbers of accepted papers in their respective annual conference papers on the topic of deep learning for speech recognition. 
More importantly, all major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) nowadays are based on deep learning methods.
See also the recent media interview with the CTO of Nuance Communications.
Applications
In-car systems
Typically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signalled to the driver by an audio prompt. 
Following the audio prompt, the system has a "listening window" during which it may accept a speech input for recognition.
Simple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. 
Voice recognition capabilities vary between car make and model. 
Some of the most recent[when?] car models offer natural-language speech recognition in place of a fixed set of commands. 
allowing the driver to use full sentences and common phrases.
 With such systems there is, therefore, no need for the user to memorize a set of fixed command words.
Health care
Medical documentation
In the health care sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. 
Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. 
Back-end or deferred speech recognition is where the provider dictates into a digital dictation system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalized. 
Deferred speech recognition is widely used in the industry currently.
One of the major issues relating to the use of speech recognition in healthcare is that the American Recovery and Reinvestment Act of 2009 (ARRA) provides for substantial financial benefits to physicians who utilize an EMR according to "Meaningful Use" standards. 
These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an Electronic Health Record or EHR). 
The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data are relatively minimal for people who are sighted and who can operate a keyboard and mouse.
A more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. 
A large part of the clinician's interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. 
As an alternative to this navigation by hand, cascaded use of speech recognition and information extraction has been studied[58] as a way to fill out a handover form for clinical proofing and sign-off. 
The results are encouraging, and the paper also opens data, together with the related performance benchmarks and some processing software, to the research and development community for studying clinical documentation and language-processing.
Therapeutic use
Prolonged use of speech recognition software in conjunction with word processors has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection. 
Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.
Military
High-performance fighter aircraft
Substantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft.
 Of particular note is the U.S. program in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), and a program in France installing speech recognition systems on Mirage aircraft, and also programs in the UK dealing with a variety of aircraft platforms. 
In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including: setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.
Working with Swedish pilots flying in the JAS-39 Gripen cockpit, Englund (2004) found recognition deteriorated with increasing G-loads. 
It was also concluded that adaptation greatly improved the results in all cases and introducing models for breathing was shown to improve recognition scores significantly. 
Contrary to what might be expected, no effects of the broken English of the speakers were found. 
It was evident that spontaneous speech caused problems for the recognizer, as could be expected. 
A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.
The Eurofighter Typhoon currently in service with the UK RAF employs a speaker-dependent system, i.e. it requires each pilot to create a template. 
The system is not used for any safety critical or weapon critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. 
The system is seen as a major design feature in the reduction of pilot workload, and even allows the pilot to assign targets to himself with two simple voice commands or to any of his wingmen with only five commands.[60]
Speaker-independent systems are also being developed and are in testing for the F35 Lightning II (JSF) and the Alenia Aermacchi M-346 Master lead-in fighter trainer. 
These systems have produced word accuracy in excess of 98%.
Helicopters
The problems of achieving high recognition accuracy under stress and noise pertain strongly to the helicopter environment as well as to the jet fighter environment. 
The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a facemask, which would reduce acoustic noise in the microphone. 
Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the U.S.
 Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK. 
Work in France has included speech recognition in the Puma helicopter. 
There has also been much useful work in Canada. 
Results have been encouraging, and voice applications have included: control of communication radios, setting of navigation systems, and control of an automated target handover system.
As in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. 
Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. 
Much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings.
Training air traffic controllers
Training for air traffic controllers (ATC) represents an excellent application for speech recognition systems. 
Many ATC training systems currently require a person to act as a "pseudo-pilot", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation. 
Speech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as pseudo-pilot, thus reducing training and support personnel. 
In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. 
In practice, this is rarely the case. 
The FAA document 7110.65 details the phrases that should be used by air traffic controllers.
 While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.
The USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors.
Telephony and other domain
ASR in the field of telephony is now commonplace and in the field of computer gaming and simulation is becoming more widespread. 
Despite the high level of integration with word processing in general personal computing. 
However, ASR in the field of document production has not seen the expected[by whom?] increases in use.
The improvement of mobile processor speeds made feasible the speech-enabled Symbian and Windows Mobile smartphones. 
Usage in education and daily life
For language learning, speech recognition can be useful for learning a second language. 
It can teach proper pronunciation, in addition to helping a person develop fluency with their speaking skills.
Students who are blind (see Blindness and education) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard.
Students who are physically disabled or suffer from Repetitive strain injury/other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. 
They can also utilize speech recognition technology to freely enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard.
Speech recognition can allow students with learning disabilities to become better writers. 
By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing.
 Also, see Learning disability.
Voice Recognition Software's use, in conjunction with a digital audio recorder, a personal computer and Microsoft Word has proven to be positive for restoring damaged short-term-memory capacity, in stroke and craniotomy individuals.
People with disabilities
people with disabilities can benefit from speech recognition programs. 
For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services.
Speech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involved disabilities that preclude using conventional computer input devices. 
In fact, people who used the keyboard a lot and developed RSI became an urgent early market for speech recognition.
 Speech recognition is used in deaf telephony, such as voicemail to text, relay services, and captioned telephone. 
Individuals with learning disabilities who have problems with thought-to-paper communication  can possibly benefit from the software but the technology is not bug proof.
Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability.[69]
This type of technology can help those with dyslexia but other disabilities are still in question. 
The effectiveness of the product is the problem that is hindering it being effective. 
Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. 
Giving them more work to fix, causing them to have to take more time with fixing the wrong word.[70]
Further applications
Aerospace (e.g. space exploration, spacecraft, etc.) NASA’s Mars Polar Lander used speech recognition from technology Sensory, Inc. in the Mars Microphone on the Lander
Automatic subtitling with speech recognition[72]
Automatic translation
Court reporting (Realtime Speech Writing)
Hands-free computing: Speech recognition computer user interface
Home automation
Interactive voice response
Mobile telephony, including mobile email
Multimodal interaction
Pronunciation evaluation in computer-aided language learning applications
Robotics
Speech-to-text reporter (transcription of speech into text, video captioning, Court reporting )
Telematics (e.g., vehicle Navigation Systems)
Transcription (digital speech-to-text)
Video games, with Tom Clancy's EndWar and Lifeline as working examples
Performance
The performance of speech recognition systems is usually evaluated in terms of accuracy and speed. 
Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor. 
Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).
However, speech recognition (by a machine) is a very complex problem. 
Vocalizations vary in terms of accent,pronunciation, articulation, roughness, nasality, pitch, volume, and speed. 
Speech is distorted by a background noise and echoes, electrical characteristics. 
Accuracy of speech recognition vary with the following[74]:
Vocabulary size and confusability
Speaker dependence vs. independence
Isolated, discontinuous, or continuous speech
Task and language constraints
Read vs. spontaneous speech
Adverse conditions
Accuracy
Crystal Clear app kedit.svg
This article may need to be rewritten entirely to comply with Wikipedia's quality standards, as section. 
You can help. 
The discussion page may contain suggestions. (June 2012)
As mentioned earlier in this article, accuracy of speech recognition varies in the following:
Error rates increase as the vocabulary size grows:
e.g. The 10 digits "zero" to "nine" can be recognized essentially perfectly, but vocabulary sizes of 200, 5000 or 100000 may have error rates of 3%, 7% or 45% respectively.
Vocabulary is hard to recognize if it contains confusable words:
e.g. The 26 letters of the English alphabet are difficult to discriminate because they are confusable words (most notoriously, the E-set: "B, C, D, E, G, P, T, V, Z"); an 8% error rate is considered good for this vocabulary.
Speaker dependence vs. independence:
A speaker-dependent system is intended for use by a single speaker.
A speaker-independent system is intended for use by any speaker, more difficult.
Isolated, Discontinuous or continuous speech
With isolated speech single words are used, therefore it becomes easier to recognize the speech.
With discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech.
With continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.
task and language constraints
e.g. Querying application may dismiss the hypothesis "The apple is red."
e.g. Constraints may be semantic; rejecting "The apple is angry."
e.g. Syntactic; rejecting "Red is apple the."
Constraints are often represented by a grammar.
Read vs. Spontaneous Speech
Adverse conditions
Environmental noise (e.g. Noise in a car or a factory)
Acoustical distortions (e.g. echoes, room acoustics)
Speech recognition is a multi-levelled pattern recognition task.
Acoustical signals are structured into a hierarchy of units;
e.g. Phonemes, Words, Phrases, and Sentences;
Each level provides additional constraints;
e.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at lower level;
This hierarchy of constraints are exploited;
By combining decisions probabilistically at all lower levels, and making more deterministic decisions only at the highest level;
Speech recognition by a machine is a process broken into several phases. 
Computationally, it is a problem in which a sound pattern has to be recognized or classified into a category that represents a meaning to a human. 
Every acoustic signal can be broken in smaller more basic sub-signals. 
The lowest level, where the sounds are the most fundamental, a machine would check for simple and more probabilistic rules of what sound should represent. 
Once these sounds are put together into more complex sound on upper level, a new set of more deterministic rules should predict what new complex sound should represent.
 The most upper level of a deterministic rule should figure out the meaning of complex expressions. 
In order to expand our knowledge about speech recognition we need to take into a consideration neural networks. 
There are four steps of neural network approaches:
Digitize the speech that we want to recognize
For telephone speech the sampling rate is 8000 samples per second;
Compute features of spectral-domain of the speech (with Fourier transform);
computed every 10 ms, with one 10 ms section called a frame;
Analysis of four-step neural network approaches can be explained by further information. 
Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. 
Basic sound creates a wave which has 2 descriptions; Amplitude (how strong is it), and frequency (how often it vibrates per second).
The sound waves can be digitized: Sample a strength at short intervals like in picture above[where?] to get bunch of numbers that approximate at each time step the strength of a wave. 
Collection of these numbers represent analog wave. This new wave is digital. Sound waves are complicated because they superimpose one on top of each other. 
Like the waves would. 
This way they create odd-looking waves. 
For example, if there are two waves that interact with each other we can add them which creates new odd-looking wave.
Neural network classifies features into phonetic-based categories;
Given basic sound blocks that a machine digitized, one has a bunch of numbers which describe a wave and waves describe words. 
Each frame has a unit block of sound, which are broken into basic sound waves and represented by numbers which, after Fourier Transform, can be statistically evaluated to set to which class of sounds it belongs. 
The nodes in the figure on a slide represent a feature of a sound in which a feature of a wave from the first layer of nodes to the second layer of nodes based on statistical analysis. 
This analysis depends on programmer's instructions. 
At this point, a second layer of nodes represents higher level features of a sound input which is again statistically evaluated to see what class they belong to. 
Last level of nodes should be output nodes that tell us with high probability what original sound really was.
Search to match the neural-network output scores for the best word, to determine the word that was most likely uttered;
Further information
Conferences and Journals
Popular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe, ICASSP, Interspeech/Eurospeech, and the IEEE ASRU. 
Conferences in the field of natural language processing, such as ACL, NAACL, EMNLP, and HLT, are beginning to include papers on speech processing. 
Books like "Fundamentals of Speech Recognition" by Lawrence Rabiner can be useful to acquire basic knowledge but may not be fully up to date (1993). 
Another good source can be "Statistical Methods for Speech Recognition" by Frederick Jelinek and "Spoken Language Processing (2001)" by Xuedong Huang etc. 
More up to date are "Computer Speech", by Manfred R. 
Schroeder, second edition published in 2004, and "Speech Processing: A Dynamic and Optimization-Oriented Approach" published in 2003 by Li Deng and Doug O'Shaughnessey. 
The recently updated textbook of "Speech and Language Processing (2008)" by Jurafsky and Martin presents the basics and the state of the art for ASR. 
Speaker recognition also uses the same features, most of the same front-end processing, and classification techniuqes as is done in speech recognition. 
A most recent comprehensive textbook, "Fundamentals of Speaker Recognition"[75] by Homayoon Beigi, is an in depth source for up to date details on the theory and practice. 
A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by DARPA.
A good and accessible introduction to speech recognition technology and its history is provided by the general audience book "The Voice in the Machine. 
Building Computers That Understand Speech" by Roberto Pieraccini (2012).
 Software
In terms of freely available resources, Carnegie Mellon University's Sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting. 
Another resource (free but copyrighted) is the HTK book (and the accompanying HTK toolkit). 
The AT&T libraries GRM and DCD are also general software libraries for large-vocabulary speech recognition. 
For more recent and state-of-the-art techniques, Kaldi toolkit can be used.
For more software resources, see List of speech recognition software.
Speaker recognition is the identification of a person from characteristics of voices (voice biometrics).
 It is also called voice recognition.
There is a difference between speaker recognition (recognizing who is speaking) and speech recognition (recognizing what is being said).
 These two terms are frequently confused, and "voice recognition" can be used for both. 
In addition, there is a difference between the act of authentication (commonly referred to as speaker verification or speaker authentication) and identification. 
Finally, there is a difference between speaker recognition (recognizing who is speaking) and speaker diarisation (recognizing when the same speaker is speaking). 
Recognizing the speaker can simplify the task of translating speech in systems that have been trained on specific person's voices or it can be used to authenticate or verify the identity of a speaker as part of a security process.
Speaker recognition has a history dating back some four decades and uses the acoustic features of speech that have been found to differ between individuals. 
These acoustic patterns reflect both anatomy (e.g., size and shape of the throat and mouth) and learned behavioral patterns (e.g., voice pitch, speaking style). 
Speaker verification has earned speaker recognition its classification as a "behavioral biometric".
Verification versus identification[edit]
There are two major applications of speaker recognition technologies and methodologies. 
If the speaker claims to be of a certain identity and the voice is used to verify this claim, this is called verification or authentication. 
On the other hand, identification is the task of determining an unknown speaker's identity. 
In a sense speaker verification is a 1:1 match where one speaker's voice is matched to one template (also called a "voice print" or "voice model") whereas speaker identification is a 1:N match where the voice is compared against N templates.
From a security perspective, identification is different from verification. 
For example, presenting your passport at border control is a verification process: the agent compares your face to the picture in the document. 
Conversely, a police officer comparing a sketch of an assailant against a database of previously documented criminals to find the closest match(es) is an identification process.
Speaker verification is usually employed as a "gatekeeper" in order to provide access to a secure system (e.g. telephone banking). 
These systems operate with the users' knowledge and typically require their cooperation. 
Speaker identification systems can also be implemented covertly without the user's knowledge to identify talkers in a discussion, alert automated systems of speaker changes, check if a user is already enrolled in a system, etc.
In forensic applications, it is common to first perform a speaker identification process to create a list of "best matches" and then perform a series of verification processes to determine a conclusive match.[citation needed]
Variants of speaker recognition[edit]
Each speaker recognition system has two phases: Enrollment and verification.
 During enrollment, the speaker's voice is recorded and typically a number of features are extracted to form a voice print, template, or model. 
In the verification phase, a speech sample or "utterance" is compared against a previously created voice print. 
For identification systems, the utterance is compared against multiple voice prints in order to determine the best match(es) while verification systems compare an utterance against a single voice print.
 Because of the process involved, verification is faster than identification.
Speaker recognition systems fall into two categories: text-dependent and text-independent.
Text-Dependent:
If the text must be the same for enrollment and verification this is called text-dependent recognition.
 In a text-dependent system, prompts can either be common across all speakers (e.g.: a common pass phrase) or unique. 
In addition, the use of shared-secrets (e.g.: passwords and PINs) or knowledge-based information can be employed in order to create a multi-factor authentication scenario.
Text-Independent:
Text-independent systems are most often used for speaker identification as they require very little if any cooperation by the speaker. 
In this case the text during enrollment and test is different.
In fact, the enrollment may happen without the user's knowledge, as in the case for many forensic applications. 
As text-independent technologies do not compare what was said at enrollment and verification, verification applications tend to also employ speech recognition to determine what the user is saying at the point of authentication.
In text independent systems both acoustics and speech analysis techniques are used.
Technology[edit]
Speaker recognition is a pattern recognition problem. 
The various technologies used to process and store voice prints include frequency estimation, hidden Markov models, Gaussian mixture models, pattern matching algorithms, neural networks, matrix representation, Vector Quantization and decision trees.
 Some systems also use "anti-speaker" techniques, such as cohort models, and world models.
 Spectral features are predominantly used in representing speaker characteristics.
Ambient noise levels can impede both collections of the initial and subsequent voice samples. 
Noise reduction algorithms can be employed to improve accuracy, but incorrect application can have the opposite effect.
 Performance degradation can result from changes in behavioural attributes of the voice and from enrollment using one telephone and verification on another telephone ("cross channel"). 
Integration with two-factor authentication products is expected to increase. 
Voice changes due to ageing may impact system performance over time. 
Some systems adapt the speaker models after each successful verification to capture such long-term changes in the voice, though there is debate regarding the overall security impact imposed by automated adaptation.
Capture of the biometric is seen as non-invasive. 
The technology traditionally uses existing microphones and voice transmission technology allowing recognition over long distances via ordinary telephones (wired or wireless).
Digitally recorded audio voice identification and analogue recorded voice identification uses electronic measurements as well as critical listening skills that must be applied by a forensic expert in order for the identification to be accurate.
Applications[edit]
The first international patent was filed in 1983, coming from the telecommunication research in CSELT[9] (Italy) as a basis for both future telco services to final customers and to improve the noise-reduction techniques across the network.
In May 2013 it was announced that Barclays Wealth was to use passive speaker recognition to verify the identity of telephone customers within 30 seconds of normal conversation.
of customer users had rated the system at "9 out of 10" for speed, ease of use and security.
Since then, Nuance Voice Biometrics solutions have been deployed across several major financial institutions across the world including Banco Santander, Royal Bank of Canada, Tangerine Bank, and Manulife.
In August 2014 GoVivace Inc. deployed a speaker identification system that allowed its telecom industry client to positively search for an individual among millions of speakers by using just a single example recording of their voice.
Speaker recognition may also be used in criminal investigations, such as those of the 2014 executions of, amongst others, James Foley and Steven Sotloff,.
In February 2016 UK high-street bank HSBC and its internet-based retail bank First Direct announced that it would offer 15 million customers its biometric banking software to access online and phone accounts using their fingerprint or voice.
Deep learning (deep structured learning, hierarchical learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers with complex structures, or otherwise composed of multiple non-linear transformations.
Deep learning is part of a broader family of machine learning methods based on learning representations of data. 
An observation (e.g., an image) can be represented in many ways such as a vector of intensity values per pixel, or in a more abstract way as a set of edges, regions of particular shape, etc. 
Some representations are better than others at simplifying the learning task (e.g., face recognition or facial expression recognition[7]) from examples. 
One of the promises of deep learning is replacing handcrafted features with efficient algorithms for unsupervised or semi-supervised feature learning and hierarchical feature extraction.
Research in this area attempts to make better representations and create models to learn these representations from large-scale unlabeled data. 
Some of the representations are inspired by advances in neuroscience and are loosely based on interpretation of information processing and communication patterns in a nervous system, such as neural coding which attempts to define a relationship between various stimuli and associated neuronal responses in the brain.
Various deep learning architectures such as deep neural networks, convolutional deep neural networks, deep belief networks and recurrent neural networks have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks.
Deep learning has been characterized as a buzzword, or a rebranding of neural networks
There are a number of ways that the field of deep learning has been characterized. 
Deep learning is a class of machine learning algorithms that[1](pp199–200)
use a cascade of many layers of nonlinear processing units for feature extraction and transformation. 
Each successive layer uses the output from the previous layer as input. 
The algorithms may be supervised or unsupervised and applications include pattern analysis (unsupervised) and classification (supervised).
are based on the (unsupervised) learning of multiple levels of features or representations of the data. 
Higher level features are derived from lower level features to form a hierarchical representation.
are part of the broader machine learning field of learning representations of data.
learn multiple levels of representations that correspond to different levels of abstraction; the levels form a hierarchy of concepts.
These definitions have in common (1) multiple layers of nonlinear processing units and (2) the supervised or unsupervised learning of feature representations in each layer, with the layers forming a hierarchy from low-level to high-level features.
 The composition of a layer of nonlinear processing units used in a deep learning algorithm depends on the problem to be solved. 
Layers that have been used in deep learning include hidden layers of an artificial neural network and sets of complicated propositional formulas.
 They may also include latent variables organized layer-wise in deep generative models such as the nodes in Deep Belief Networks and Deep Boltzmann Machines.
Deep learning algorithms are contrasted with shallow learning algorithms by the number of parameterized transformations a signal encounters as it propagates from the input layer to the output layer, where a parameterized transformation is a processing unit that has trainable parameters, such as weights and thresholds.
 A chain of transformations from input to output is a credit assignment path (CAP). 
CAPs describe potentially causal connections between input and output and may vary in length. 
For a feedforward neural network, the depth of the CAPs, and thus the depth of the network, is the number of hidden layers plus one (the output layer is also parameterized). 
For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP is potentially unlimited in length. 
There is no universally agreed upon threshold of depth dividing shallow learning from deep learning, but most researchers in the field agree that deep learning has multiple nonlinear layers (CAP > 2) and Schmidhuber considers CAP > 10 to be very deep learning.
Fundamental concepts
Deep learning algorithms are based on distributed representations.
 The underlying assumption behind distributed representations is that observed data are generated by the interactions of factors organized in layers. 
Deep learning adds the assumption that these layers of factors correspond to levels of abstraction or composition. 
Varying numbers of layers and layer sizes can be used to provide different amounts of abstraction.
Deep learning exploits this idea of hierarchical explanatory factors where higher level, more abstract concepts are learned from the lower level ones. 
These architectures are often constructed with a greedy layer-by-layer method. 
Deep learning helps to disentangle these abstractions and pick out which features are useful for learning.[3]
For supervised learning tasks, deep learning methods obviate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures which remove redundancy in representation.[1]
Many deep learning algorithms are applied to unsupervised learning tasks. 
This is an important benefit because unlabeled data are usually more abundant than labeled data. 
An example of a deep structure that can be trained in an unsupervised manner is a deep belief network.[3]
Interpretations
Deep neural networks are generally interpreted in terms of: Universal approximation theorem or Probabilistic inference.
Universal approximation theorem interpretation
The universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.
In 1989, the first proof was published by George Cybenko for sigmoid activation functions[13] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.
probabilistic interpretation
The probabilistic interpretation[17] derives from the field of machine learning. 
It features inference,[1][2][3][4][17][18] as well as the optimization concepts of training and testing, related to fitting and generalization respectively. 
More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[17] 
See Deep belief network. 
The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.[19]
The probabilistic interpretation was introduced and popularized by luminaries such as Geoff Hinton, Yoshua Bengio, Yann LeCun and Juergen Schmidhuber.
History
Deep learning architectures, specifically those built from artificial neural networks (ANN), date back at least to the Neocognitron introduced by Kunihiko Fukushima in 1980.
 The ANNs themselves date back even further. The challenge was how to train networks with multiple layers. 
In 1989, Yann LeCun et al. were able to apply the standard backpropagation algorithm, which had been around since 1974,[21] to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail.
 Despite the success of applying the algorithm, the time to train the network on this dataset was approximately 3 days, making it impractical for general use.
 In 1995, Brendan Frey demonstrated that it was possible to train a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, which was co-developed with Peter Dayan and Geoffrey Hinton.
However, training took two days.
Many factors contribute to the slow speed, one being the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.
While by 1991 such neural networks were used for recognizing isolated 2-D hand-written digits, recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. 
Juyang Weng et al. suggested that a human brain does not use a monolithic 3-D object model, and in 1992 they published Cresceptron,[26][27][28] a method for performing 3-D object recognition directly from cluttered scenes. 
Cresceptron is a cascade of layers similar to Neocognitron.
 But while Neocognitron required a human programmer to hand-merge features, Cresceptron automatically learned an open number of unsupervised features in each layer, where each feature is represented by a convolution kernel. 
Cresceptron also segmented each learned object from a cluttered scene through back-analysis through the network.
 Max-pooling, now often adopted by deep neural networks (e.g., ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization. 
Despite these advantages, simpler models that use task-specific handcrafted features such as Gabor filter and support vector machines (SVMs) were of popular choice in the 1990s and 2000s, because of the computational cost by ANNs then and a great lack of understanding of how the brain autonomously wires its biological networks.
In the long history of speech recognition, both shallow and deep learning (e.g., recurrent nets) of artificial neural networks had been explored for many years.
But these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.
 A number of key difficulties had been methodologically analyzed, including gradient diminishing and weak temporal correlation structure in the neural predictive models.
 Additional difficulties were the lack of big training data and weaker computing power in these early days. 
Thus, most speech recognition researchers who understood such barriers moved away from neural nets to pursue generative modeling, until a recent resurgence of deep learning overcame all these difficulties.
 Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with cross-group colleagues re-ignited neural networks research and initiated deep learning research and applications in speech recognition.
The term "deep learning" gained traction in the mid-2000s after a publication by Geoffrey Hinton and Ruslan Salakhutdinov showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuned using supervised backpropagation.
 In 1992, Schmidhuber had already implemented a very similar idea for the more general case of unsupervised deep hierarchies of recurrent neural networks, and also experimentally shown its benefits for speeding up supervised learning.
Since its resurgence, deep learning has become part of many state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). 
Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks are constantly being improved with new applications of deep learning.
 Recently, it was shown that deep learning architectures in the form of convolutional neural networks have been nearly best performing;[44][45] however, these are more widely used in computer vision than in ASR.
The real impact of deep learning in industry began in large-scale speech recognition around 2010.
 In late 2009, Li Deng invited Geoffrey Hinton to work with him and colleagues at Microsoft Research to apply deep learning to speech recognition. 
They co-organized the 2009 NIPS Workshop on Deep Learning for Speech Recognition. 
The workshop was motivated by the limitations of deep generative models of speech, and the possibility that the big-compute, big-data era warranted a serious try of deep neural nets (DNN).
 It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets encountered in the 1990s.
 However, early into this research at Microsoft, it was discovered that without pre-training, but using large amounts of training data, and especially DNNs designed with corresponding large, context-dependent output layers, produced error dramatically lower than then-state-of-the-art GMM-HMM and more advanced generative model-based speech recognition systems. 
This finding was verified by several other major speech recognition research groups.
 Further, the nature of recognition errors produced by the two types of systems was found to be characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major players in speech recognition industry. 
The history of this significant development in deep learning has been described and analyzed in recent books.
Advances in hardware have also been important in enabling the renewed interest of deep learning. 
In particular, powerful graphics processing units (GPUs) are well-suited for the kind of number crunching, matrix/vector math involved in machine learning. 
GPUs have been shown to speed up training algorithms by orders of magnitude, bringing running times of weeks back to days.
Artificial neural networks
Some of the most successful deep learning methods involve artificial neural networks. 
Artificial neural networks are inspired by the 1959 biological model proposed by Nobel laureates David H. Hubel & Torsten Wiesel, who found two types of cells in the primary visual cortex: simple cells and complex cells. 
Many artificial neural networks can be viewed as cascading models [26][27][28][51] of cell types inspired by these biological observations.
Fukushima's Neocognitron introduced convolutional neural networks partially trained by unsupervised learning with human-directed features in the neural plane.
 Yann LeCun et al. (1989) applied supervised backpropagation to such architectures.
 Weng et al. (1992) published convolutional neural networks Cresceptron[26][27][28] for 3-D object recognition from images of cluttered scenes and segmentation of such objects from images.
An obvious need for recognizing general 3-D objects is least shift invariance and tolerance to deformation. 
Max-pooling appeared to be first proposed by Cresceptron[26][27] to enable the network to tolerate small-to-large deformation in a hierarchical way, while using convolution. 
Max-pooling helps, but does not guarantee, shift-invariance at the pixel level.[28]
With the advent of the back-propagation algorithm being discovered independently by several groups in the 1970s and 1980s,[21][53] many researchers tried to train supervised deep artificial neural networks from scratch, initially with little success. 
Sepp Hochreiter's diploma thesis of 1991[24][25] formally identified the reason for this failure as the vanishing gradient problem, which affects many-layered feedforward networks and recurrent neural networks. 
Recurrent networks are trained by unfolding them into very deep feedforward networks, where a new layer is created for each time step of an input sequence processed by the network. 
As errors propagate from layer to layer, they shrink exponentially with the number of layers, impeding the tuning of neuron weights which is based on those errors.
To overcome this problem, several methods were proposed. 
One is Jürgen Schmidhuber's multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning, fine-tuned by backpropagation.
 Here each level learns a compressed representation of the observations that is fed to the next level.
Another method is the long short term memory (LSTM) network of Hochreiter & Schmidhuber (1997).
 In 2009, deep multidimensional LSTM networks won three ICDAR 2009 competitions in connected handwriting recognition, without any prior knowledge about the three languages to be learned.[55][56]
Sven Behnke in 2003 relied only on the sign of the gradient (Rprop) when training his Neural Abstraction Pyramid[57] to solve problems like image reconstruction and face localization.
Other methods also use unsupervised pre-training to structure a neural network, making it first learn generally useful feature detectors. 
Then the network is trained further by supervised back-propagation to classify labeled data. 
The deep model of Hinton et al. (2006) involves learning the distribution of a high-level representation using successive layers of binary or real-valued latent variables. 
It uses a restricted Boltzmann machine (Smolensky, 1986[58]) to model each new layer of higher level features.
Each new layer guarantees an increase on the lower-bound of the log likelihood of the data, thus improving the model, if trained properly. 
Once sufficiently many layers have been learned, the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an "ancestral pass") from the top level feature activations.
 Hinton reports that his models are effective feature extractors over high-dimensional, structured data.
The Google Brain team led by Andrew Ng and Jeff Dean created a neural network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.[61][62]
Other methods rely on the sheer processing power of modern computers, in particular, GPUs.
In 2010, Dan Ciresan and colleagues[49] in Jürgen Schmidhuber's group at the Swiss AI Lab IDSIA showed that despite the above-mentioned "vanishing gradient problem," the superior processing power of GPUs makes plain back-propagation feasible for deep feedforward neural networks with many layers. 
The method outperformed all other machine learning techniques on the old, famous MNIST handwritten digits problem of Yann LeCun and colleagues at NYU.
At about the same time, in late 2009, deep learning made inroads into speech recognition, as marked by the NIPS Workshop on Deep Learning for Speech Recognition. 
Intensive collaborative work between Microsoft Research and University of Toronto researchers demonstrated by mid-2010 in Redmond that deep neural networks interfaced with a hidden Markov model with context-dependent states that define the neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search.
 The same deep neural net model was shown to scale up to Switchboard tasks about one year later at Microsoft Research Asia.
As of 2011, the state of the art in deep learning feedforward networks alternates convolutional layers and max-pooling layers,[63][64] topped by several fully connected or sparsely connected layer followed by a final classification layer. 
Training is usually done without any unsupervised pre-training.
 Since 2011, GPU-based implementations[63] of this approach won many pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge,[66] and others.
Such supervised deep learning methods also were the first artificial pattern recognizers to achieve human-competitive performance on certain tasks.[67]
To overcome the barriers of weak AI represented by deep learning, it is necessary to go beyond deep learning architectures, because biological brains use both shallow and deep circuits as reported by brain anatomy[68] displaying a wide variety of invariance. 
Weng[69] argued that the brain self-wires largely according to signal statistics and, therefore, a serial cascade cannot catch all major statistical dependencies. 
ANNs were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes, only when invariance extended beyond shift, to all ANN-learned concepts, such as location, type (object class label), scale, lighting. 
This was realized in Developmental Networks (DNs)[70] whose embodiments are Where-What Networks, WWN-1 (2008)[71] through WWN-7 (2013).
Architectures
There are huge number of variants of deep architectures.
 Most of them are branched from some original parent architectures.
 It is not always possible to compare the performance of multiple architectures all together, because they are not all evaluated on the same data sets.
 Deep learning is a fast-growing field, and new architectures, variants, or algorithms appear every few weeks.
Deep neural networks
A deep neural network (DNN) is an artificial neural network (ANN) with multiple hidden layers of units between the input and output layers.
 Similar to shallow ANNs, DNNs can model complex non-linear relationships.
 DNN architectures, e.g., for object detection and parsing generate compositional models where the object is expressed as a layered composition of image primitives.
 The extra layers enable composition of features from lower layers, giving the potential of modeling complex data with fewer units than a similarly performing shallow network.[2]
DNNs are typically designed as feedforward networks, but recent research has successfully applied the deep learning architecture to recurrent neural networks for applications such as language modeling.
 Convolutional deep neural networks (CNNs) are used in computer vision where their success is well-documented.
 More recently, CNNs have been applied to acoustic modeling for automatic speech recognition (ASR), where they have shown success over previous models.
For simplicity, a look at training DNNs is given here.
A DNN can be discriminatively trained with the standard backpropagation algorithm. 
The weight updates can be done via stochastic gradient descent using the following equation:
 w_{ij}(t + 1) = w_{ij}(t) + \eta\frac{\partial C}{\partial w_{ij}} 
here,  \eta  is the learning rate, and  C  is the cost function. 
The choice of the cost function depends on factors such as the learning type (supervised, unsupervised, reinforcement, etc.) and the activation function. 
For example, when performing supervised learning on a multiclass classification problem, common choices for the activation function and cost function are the softmax function and cross entropy function, respectively. 
The softmax function is defined as  p_j = \frac{\exp(x_j)}{\sum_k \exp(x_k)}  where  p_j  represents the class probability (output of the unit  j ) and  x_j  and  x_k  represent the total input to units  j  and  k  of the same level respectively. 
Cross entropy is defined as  C = -\sum_j d_j \log(p_j)  where  d_j  represents the target probability for output unit  j  and  p_j  is the probability output for  j  after applying the activation function.[76]
These can be used to output object bounding boxes in form of a binary mask. 
They are also used for multi-scale regression to increase localization precision. 
DNN-based regression can learn features that capture geometric information in addition to being a good classifier. 
They remove the limitation of designing a model which will capture parts and their relations explicitly. 
This helps to learn a wide variety of objects. 
The model consists of multiple layers, each of which has a rectified linear unit for non-linear transformation. 
Some layers are convolutional, while others are fully connected. 
Every convolutional layer has an additional max pooling.
 The network is trained to minimize L2 error for predicting the mask ranging over the entire training set containing bounding boxes represented as masks.
Problems with deep neural networks
As with ANNs, many issues can arise with DNNs if they are naively trained. 
Two common issues are overfitting and computation time.
DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. 
Regularization methods such as weight decay ( \ell_2 -regularization) or sparsity ( \ell_1 -regularization) can be applied during training to help combat overfitting.
A more recent regularization method applied to DNNs is dropout regularization. 
In dropout, some number of units are randomly omitted from the hidden layers during training. 
This helps to break the rare dependencies that can occur in the training data.
The dominant method for training these structures has been error-correction training (such as backpropagation with gradient descent) due to its ease of implementation and its tendency to converge to better local optima than other training methods. 
However, these methods can be computationally expensive, especially for DNNs. 
There are many training parameters to be considered with a DNN, such as the size (number of layers and number of units per layer), the learning rate and initial weights. 
Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. 
Various 'tricks' such as using mini-batching (computing the gradient on several training examples at once rather than individual examples) have been shown to speed up computation. 
The large processing throughput of GPUs has produced significant speedups in training, due to the matrix and vector computations required being well suited for GPUs.
 Radical alternatives to backprop such as Extreme Learning Machines,[80] "No-prop" networks,[81] training without backtracking,[82] "weightless" networks,[83] and non-connectionist neural networks are gaining attention.
Deep belief networks
Main article: Deep belief network
A restricted Boltzmann machine (RBM) with fully connected visible and hidden units.
 Note there are no hidden-hidden or visible-visible connections.
A deep belief network (DBN) is a probabilistic, generative model made up of multiple layers of hidden units. 
It can be considered a composition of simple learning modules that make up each layer.
A DBN can be used to generatively pre-train a DNN by using the learned DBN weights as the initial DNN weights. 
Back-propagation or other discriminative algorithms can then be applied for fine-tuning of these weights. 
This is particularly helpful when limited training data are available, because poorly initialized weights can significantly hinder the learned model's performance. 
These pre-trained weights are in a region of the weight space that is closer to the optimal weights than are randomly chosen initial weights. 
This allows for both improved modeling and faster convergence of the fine-tuning phase.
A DBN can be efficiently trained in an unsupervised, layer-by-layer manner, where the layers are typically made of restricted Boltzmann machines (RBM). 
An RBM is an undirected, generative energy-based model with a "visible" input layer and a hidden layer, and connections between the layers but not within layers. 
The training method for RBMs proposed by Geoffrey Hinton for use with training "Product of Expert" models is called contrastive divergence (CD).
 CD provides an approximation to the maximum likelihood method that would ideally be applied for learning the weights of the RBM.
 In training a single RBM, weight updates are performed with gradient ascent via the following equation:  \Delta w_{ij}(t+1) = w_{ij}(t) + \eta\frac{\partial \log(p(v))}{\partial w_{ij}} . 
Here, p(v) is the probability of a visible vector, which is given by p(v) = \frac{1}{Z}\sum_he^{-E(v,h)}. 
 Z  is the partition function (used for normalizing) and E(v,h) is the energy function assigned to the state of the network. 
A lower energy indicates the network is in a more "desirable" configuration.
 The gradient \frac{\partial \log(p(v))}{\partial w_{ij}} has the simple form \langle v_ih_j\rangle_\text{data} - \langle v_ih_j\rangle_\text{model} where \langle\cdots\rangle_p represent averages with respect to distribution p. 
The issue arises in sampling \langle v_ih_j\rangle_\text{model} because this requires running alternating Gibbs sampling for a long time. 
CD replaces this step by running alternating Gibbs sampling for n steps (values of n = 1 have empirically been shown to perform well). 
After n steps, the data are sampled and that sample is used in place of \langle v_ih_j\rangle_\text{model}. 
The CD procedure works as follows:[79]
Initialize the visible units to a training vector.
Update the hidden units in parallel given the visible units: p(h_j = 1 \mid \textbf{V}) = \sigma(b_j + \sum_i v_iw_{ij}). 
\sigma is the sigmoid function and b_j is the bias of h_j.
Update the visible units in parallel given the hidden units: p(v_i = 1 \mid \textbf{H}) = \sigma(a_i + \sum_j h_jw_{ij}). 
a_i is the bias of v_i.
 This is called the "reconstruction" step.
Re-update the hidden units in parallel given the reconstructed visible units using the same equation as in step 2.
Perform the weight update: \Delta w_{ij} \propto \langle v_ih_j\rangle_\text{data} - \langle v_ih_j\rangle_\text{reconstruction}.
Once an RBM is trained, another RBM is "stacked" atop it, taking its input from the final already-trained layer. 
The new visible layer is initialized to a training vector, and values for the units in the already-trained layers are assigned using the current weights and biases. 
The new RBM is then trained with the procedure above. 
This whole process is repeated until some desired stopping criterion is met.[2]
Although the approximation of CD to maximum likelihood is very crude (CD has been shown to not follow the gradient of any function), it has been empirically shown to be effective in training deep architectures.
Convolutional neural networks
Main article: Convolutional neural network
A CNN is composed of one or more convolutional layers with fully connected layers (matching those in typical artificial neural networks) on top. 
It also uses tied weights and pooling layers. 
This architecture allows CNNs to take advantage of the 2D structure of input data. 
In comparison with other deep architectures, convolutional neural networks are starting to show superior results in both image and speech applications.
 They can also be trained with standard backpropagation.
 CNNs are easier to train than other regular, deep, feed-forward neural networks and have many fewer parameters to estimate, making them a highly attractive architecture to use.
Examples of applications in Computer Vision include DeepDream.
Convolutional deep belief networks
A recent achievement in deep learning is the use of convolutional deep belief networks (CDBN). 
CDBNs have structure very similar to a convolutional neural networks and are trained similar to deep belief networks. 
Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks.
 They provide a generic structure which can be used in many image and signal processing tasks. 
Recently, many benchmark results on standard image datasets like CIFAR [89] have been obtained using CDBNs.
Large memory storage and retrieval neural networks
Large memory storage and retrieval neural networks (LAMSTAR)[91][92] are fast deep learning neural networks of many layers which can use many filters simultaneously.
 These filters may be nonlinear, stochastic, logic, non-stationary, or even non-analytical. 
They are biologically motivated and continuously learning.
A LAMSTAR neural network may serve as a dynamic neural network in spatial or time domain or both. 
Its speed is provided by Hebbian link-weights (Chapter 9 of in D. Graupe, 2013[93]), which serve to integrate the various and usually different filters (preprocessing functions) into its many layers and to dynamically rank the significance of the various layers and functions relative to a given task for deep learning. 
This grossly imitates biological learning which integrates outputs various preprocessors (cochlea, retina, etc.) and cortexes (auditory, visual, etc.) and their various regions. 
Its deep learning capability is further enhanced by using inhibition, correlation and by its ability to cope with incomplete data, or "lost" neurons or layers even at the midst of a task. 
Furthermore, it is fully transparent due to its link weights. 
The link-weights also allow dynamic determination of innovation and redundancy, and facilitate the ranking of layers, of filters or of individual neurons relative to a task.
LAMSTAR has been applied to many medical[94][95][96] and financial predictions (see Graupe, 2013[97] Section 9C), adaptive filtering of noisy speech in unknown noise,[98] still-image recognition[99] (Graupe, 2013[100] Section 9D), video image recognition,[101] software security,[102] adaptive control of non-linear systems,[103] and others. 
LAMSTAR had a much faster computing speed and somewhat lower error than a convolutional neural network based on ReLU-function filters and max pooling, in a comparative character recognition study.
LAMSTAR was proposed in 1996 (A U.S. Patent 5,920,852 A) and was further developed by D Graupe and H Kordylewski 1997-2002.
 A modified version, known as LAMSTAR 2, was developed by N C Schneider and D Graupe in 2008.
Deep Boltzmann machines
A deep Boltzmann machine (DBM) is a type of binary pairwise Markov random field (undirected probabilistic graphical model) with multiple layers of hidden random variables. 
It is a network of symmetrically coupled stochastic binary units. 
It comprises a set of visible units \boldsymbol{\nu} \in \{0,1\}^D, and a series of layers of hidden units \boldsymbol{h}^{(1)} \in \{0,1\}^{F_1}, \boldsymbol{h}^{(2)} \in \{0,1\}^{F_2}, \ldots, \boldsymbol{h}^{(L)} \in \{0,1\}^{F_L}. 
There is no connection between units of the same layer (like RBM). 
For the DBM, the probability assigned to vector \nu is
p(\boldsymbol{\nu}) = \frac{1}{Z}\sum_h e^{\sum_{ij}W_{ij}^{(1)}\nu_i h_j^{(1)} + \sum_{jl}W_{jl}^{(2)}h_j^{(1)}h_l^{(2)}+\sum_{lm}W_{lm}^{(3)}h_l^{(2)}h_m^{(3)}},
where \boldsymbol{h} = \{\boldsymbol{h}^{(1)}, \boldsymbol{h}^{(2)}, \boldsymbol{h}^{(3)} \} are the set of hidden units, and \theta = \{\boldsymbol{W}^{(1)}, \boldsymbol{W}^{(2)}, \boldsymbol{W}^{(3)} \}  are the model parameters, representing visible-hidden and hidden-hidden interactions.
 If \boldsymbol{W}^{(2)} = 0 and \boldsymbol{W}^{(3)} = 0 the network is the well-known restricted Boltzmann machine.
Interactions are symmetric because links are undirected. 
by contrast, in a deep belief network (DBN) only the top two layers form a restricted Boltzmann machine (which is an undirected graphical model), but lower layers form a directed generative model.
Like DBNs, DBMs can learn complex and abstract internal representations of the input in tasks such as object or speech recognition, using limited labeled data to fine-tune the representations built using a large supply of unlabeled sensory input data.
 However, unlike DBNs and deep convolutional neural networks, they adopt the inference and training procedure in both directions, bottom-up and top-down pass, which allow the DBMs to better unveil the representations of the ambiguous and complex input structures.
However, the speed of DBMs limits their performance and functionality. 
Because exact maximum likelihood learning is intractable for DBMs, we may perform approximate maximum likelihood learning. 
Another option is to use mean-field inference to estimate data-dependent expectations, and approximation the expected sufficient statistics of the model by using Markov chain Monte Carlo (MCMC).
This approximate inference, which must be done for each test input, is about 25 to 50 times slower than a single bottom-up pass in DBMs. 
This makes the joint optimization impractical for large data sets, and seriously restricts the use of DBMs for tasks such as feature representation.
Stacked (de-noising) auto-encoders
The auto encoder idea is motivated by the concept of a good representation. 
For example, for a classifier, a good representation can be defined as one that will yield a better performing classifier.
An encoder is a deterministic mapping f_\theta that transforms an input vector x into hidden representation y, where \theta = \{\boldsymbol{W}, b\}, \boldsymbol{W} is the weight matrix and b is an offset vector (bias). 
A decoder maps back the hidden representation y to the reconstructed input z via g_\theta. 
The whole process of auto encoding is to compare this reconstructed input to the original and try to minimize this error to make the reconstructed value as close as possible to the original.
In stacked denoising auto encoders, the partially corrupted output is cleaned (de-noised). 
This idea was introduced in 2010 by Vincent et al.[114] with a specific approach to good representation, a good representation is one that can be obtained robustly from a corrupted input and that will be useful for recovering the corresponding clean input.
 Implicit in this definition are the following ideas:
The higher level representations are relatively stable and robust to input corruption;
It is necessary to extract features that are useful for representation of the input distribution.
The algorithm consists of multiple steps; starts by a stochastic mapping of \boldsymbol{x} to \tilde{\boldsymbol{x}} through q_D(\tilde{\boldsymbol{x}}|\boldsymbol{x}), this is the corrupting step.
 Then the corrupted input \tilde{\boldsymbol{x}} passes through a basic auto encoder process and is mapped to a hidden representation \boldsymbol{y} = f_\theta(\tilde{\boldsymbol{x}}) = s(\boldsymbol{W}\tilde{\boldsymbol{x}}+b). 
From this hidden representation, we can reconstruct \boldsymbol{z} = g_\theta(\boldsymbol{y}).
 In the last stage, a minimization algorithm runs in order to have z as close as possible to uncorrupted input \boldsymbol{x}. 
The reconstruction error L_H(\boldsymbol{x},\boldsymbol{z}) might be either the cross-entropy loss with an affine-sigmoid decoder, or the squared error loss with an affine decoder.
In order to make a deep architecture, auto encoders stack one on top of another. 
Once the encoding function f_\theta of the first denoising auto encoder is learned and used to uncorrupt the input (corrupted input), we can train the second level.
Once the stacked auto encoder is trained, its output can be used as the input to a supervised learning algorithm such as support vector machine classifier or a multi-class logistic regression.
Deep stacking networks
One deep architecture based on a hierarchy of blocks of simplified neural network modules is a deep convex network, introduced in 2011.
Here, the weights learning problem is formulated as a convex optimization problem with a closed-form solution. 
This architecture is also called a deep stacking network (DSN),[116] emphasizing the mechanism's similarity to stacked generalization.
 Each DSN block is a simple module that is easy to train by itself in a supervised fashion without back-propagation for the entire blocks.
As designed by Deng and Dong,[115] each block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. 
The hidden layer h has logistic sigmoidal units, and the output layer has linear units. 
Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W.
 Target vectors t form the columns of matrix T, and the input data vectors x form the columns of matrix X. 
The matrix of hidden units is \boldsymbol{H} = \sigma(\boldsymbol{W}^T\boldsymbol{X}). 
Modules are trained in order, so lower-layer weights W are known at each stage. 
The function performs the element-wise logistic sigmoid operation. 
Each block estimates the same final label class y, and its estimate is concatenated with original input X to form the expanded input for the next block. 
Thus, the input to the first block contains the original data only, while downstream blocks' input also has the output of preceding blocks. 
Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem:
\min_{U^T} f = ||\boldsymbol{U}^T \boldsymbol{H} - \boldsymbol{T}||^2_F,
which has a closed-form solution.
Unlike other deep architectures, such as DBNs, the goal is not to discover the transformed feature representation. 
The structure of the hierarchy of this kind of architecture makes parallel learning straightforward, as a batch-mode optimization problem.
 In purely discriminative tasks, DSNs perform better than conventional DBN.
Tensor deep stacking networks
This architecture is an extension of deep stacking networks (DSN). 
It improves on DSN in two important ways: it uses higher-order information from covariance statistics, and it transforms the non-convex problem of a lower-layer to a convex sub-problem of an upper-layer.
 TDSNs use covariance statistics of the data by using a bilinear mapping from each of two distinct sets of hidden units in the same layer to predictions, via a third-order tensor.
While parallelization and scalability are not considered seriously in conventional DNNs, all learning for DSNs and TDSNs is done in batch mode, to allow parallelization on a cluster of CPU or GPU nodes.
 Parallelization allows scaling the design to larger (deeper) architectures and data sets.
The basic architecture is suitable for diverse tasks such as classification and regression.
Spike-and-slab RBMs
The need for deep learning with real-valued inputs, as in Gaussian restricted Boltzmann machines, motivates the spike-and-slab RBM (ssRBMs), which models continuous-valued inputs with strictly binary latent variables.
 Similar to basic RBMs and its variants, a spike-and-slab RBM is a bipartite graph, while like GRBMs, the visible units (input) are real-valued.
 The difference is in the hidden layer, where each hidden unit has a binary spike variable and a real-valued slab variable. 
A spike is a discrete probability mass at zero, while a slab is a density over continuous domain;[124][124] their mixture forms a prior. 
The terms come from the statistics literature.
An extension of ssRBM called µ-ssRBM provides extra modeling capacity using additional terms in the energy function. 
One of these terms enables the model to form a conditional distribution of the spike variables by marginalizing out the slab variables given an observation.
Compound hierarchical-deep models
Compound hierarchical-deep models compose deep networks with non-parametric Bayesian models. 
Features can be learned using deep architectures such as DBNs,[59] DBMs,[111] deep auto encoders,[126] convolutional variants,[127][128] ssRBMs,[124] deep coding networks,[129] DBNs with sparse feature learning,[130] recursive neural networks,[131] conditional DBNs,[132] de-noising auto encoders.
 This provides a better representation, allowing faster learning and more accurate classification with high-dimensional data.
 However, these architectures are poor at learning novel classes with few examples, because all network units are involved in representing the input (a distributed representation) and must be adjusted together (high degree of freedom). 
Limiting the degree of freedom reduces the number of parameters to learn, facilitating learning of new classes from few examples. 
Hierarchical Bayesian (HB) models allow learning from few examples, for example for computer vision, statistics, and cognitive science.
Compound HD architectures aim to integrate characteristics of both HB and deep networks. 
The compound HDP-DBM architecture, a hierarchical Dirichlet process (HDP) as a hierarchical model, incorporated with DBM architecture. 
It is a full generative model, generalized from abstract concepts flowing through the layers of the model, which is able to synthesize new examples in novel classes that look reasonably natural. 
All the levels are learned jointly by maximizing a joint log-probability score.
In a DBM with three hidden layers, the probability of a visible input \boldsymbol{\nu} is:
p(\boldsymbol{\nu}, \psi) = \frac{1}{Z}\sum_h e^{\sum_{ij}W_{ij}^{(1)}\nu_i h_j^1 + \sum_{jl}W_{jl}^{(2)}h_j^{1}h_l^{2}+\sum_{lm}W_{lm}^{(3)}h_l^{2}h_m^{3}},
where \boldsymbol{h} = \{\boldsymbol{h}^{(1)}, \boldsymbol{h}^{(2)}, \boldsymbol{h}^{(3)} \} is the set of hidden units, and \psi = \{\boldsymbol{W}^{(1)}, \boldsymbol{W}^{(2)}, \boldsymbol{W}^{(3)} \}  are the model parameters, representing visible-hidden and hidden-hidden symmetric interaction terms.
After a DBM model is learned, we have an undirected model that defines the joint distribution P(\nu, h^1, h^2, h^3). 
One way to express what has been learned is the conditional model P(\nu, h^1, h^2|h^3) and a prior term P(h^3).
Here P(\nu, h^1, h^2|h^3) represents a conditional DBM model, which can be viewed as a two-layer DBM but with bias terms given by the states of h^3:
P(\nu, h^1, h^2|h^3) = \frac{1}{Z(\psi, h^3)}e^{\sum_{ij}W_{ij}^{(1)}\nu_i h_j^1 + \sum_{jl}W_{jl}^{(2)}h_j^{1}h_l^{2}+\sum_{lm}W_{lm}^{(3)}h_l^{2}h_m^{3}}.
Deep coding networks
there are advantages of a model which can actively update itself from the context in data. 
Deep coding network (DPCN) is a predictive coding scheme where top-down information is used to empirically adjust the priors needed for a bottom-up inference procedure by means of a deep locally connected generative model. 
This works by extracting sparse features from time-varying observations using a linear dynamical model. 
Then, a pooling strategy is used to learn invariant feature representations.
 These units compose to form a deep architecture, and are trained by greedy layer-wise unsupervised learning. 
The layers constitute a kind of Markov chain such that the states at any layer only depend on the preceding and succeeding layers.
Deep predictive coding network (DPCN)[140] predicts the representation of the layer, by using a top-down approach using the information in upper layer and temporal dependencies from the previous states.
DPCNs can be extended to form a convolutional network.
Multilayer kernel machine
Multilayer kernel machines (MKM) as introduced in [141] are a way of learning highly nonlinear functions by iterative application of weakly nonlinear kernels.
 They use the kernel principal component analysis (KPCA), in[142] as method for unsupervised greedy layer-wise pre-training step of the deep learning architecture.
Layer l+1-th learns the representation of the previous layer l, extracting the n_l principal component (PC) of the projection layer l output in the feature domain induced by the kernel. 
For the sake of dimensionality reduction of the updated representation in each layer, a supervised strategy is proposed to select the best informative features among features extracted by KPCA.
The process is:
rank the n_l features according to their mutual information with the class labels;
for different values of K and m_l \in\{1, \ldots, n_l\}, compute the classification error rate of a K-nearest neighbor (K-NN) classifier using only the m_l most informative features on a validation set;
the value of m_l with which the classifier has reached the lowest error rate determines the number of features to retain.
There are some drawbacks in using the KPCA method as the building cells of an MKM.
A more straightforward way to use kernel machines for deep learning was developed by Microsoft researchers for spoken language understanding.
 The main idea is to use a kernel machine to approximate a shallow neural net with an infinite number of hidden units, then use stacking to splice the output of the kernel machine and the raw input in building the next, higher level of the kernel machine. 
The number of levels in the deep convex network is a hyper-parameter of the overall system, to be determined by cross validation.
Deep q-networks
This is the latest class of deep learning models, using Q-learning, a form of reinforcement learning, from Google DeepMind. 
Preliminary results were presented in 2014, with a paper published in February 2015 in Nature[144] The application discussed in this paper is limited to Atari 2600 gaming, but the implications for other applications are profound.
Memory networks
Integrating external memory with artificial neural networks dates to early research in distributed representations [145] and self-organizing maps. 
For example, in sparse distributed memory or hierarchical temporal memory, the patterns encoded by neural networks are used as addresses for content-addressable memory, with "neurons" essentially serving as address encoders and decoders.
Long short-term memory
In the 1990s and 2000s, there was much related work with long short-term memory (LSTM - adding differentiable memory to recurrent functions). For example:
Differentiable push and pop actions for alternative memory networks called neural stack machines[146][147]
Memory networks where the control network's external differentiable storage is in the fast weights of another network [148]
LSTM "forget gates" [149]
Self-referential recurrent neural networks (RNNs) with special output units for addressing and rapidly manipulating each of the RNN's own weights in differentiable fashion (internal storage) [150][151]
Learning to transduce with unbounded memory[152]
Semantic hashing
Approaches which represent previous experiences directly and use a similar experience to form a local model are often called nearest neighbour or k-nearest neighbors methods.
 More recently, deep learning was shown to be useful in semantic hashing[154] where a deep graphical model the word-count vectors[155] obtained from a large set of documents. 
Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. 
Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. 
Unlike sparse distributed memory which operates on 1000-bit addresses, semantic hashing works on 32 or 64-bit addresses found in a conventional computer architecture.
Neural Turing machines
Neural Turing machines,[156] developed by Google DeepMind, couple deep neural networks to external memory resources, which they can interact with by attentional processes. 
The combined system is analogous to a Turing machine but is differentiable end-to-end, allowing it to be efficiently trained by gradient descent. 
Preliminary results demonstrate that neural Turing machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.
Memory networks
Memory networks [157][158] is another extension to neural networks incorporating long-term memory, which was developed by the Facebook research team. 
The long-term memory can be read and written to, with the goal of using it for prediction.
 These models have been applied in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response.[159]
Pointer networks
Deep neural networks can be potentially improved if they get deeper and have fewer parameters, while maintaining trainability.
 Such systems operate on probability distribution vectors stored in memory cells and registers. 
Thus, the model is fully differentiable and trains end-to-end. 
The key characteristic of these models is that their depth, the size of their short-term memory, and the number of parameters can be altered independently — unlike models like LSTM, whose number of parameters grows quadratically with memory size.
Encoder–decoder networks
An encoder–decoder framework is a framework based on neural networks that aims to map highly structured input to highly structured output. 
It was proposed recently in the context of machine translation,[162][163][164] where the input and output are written sentences in two natural languages. 
In that work, an recurrent neural network (RNN) or convolutional neural network (CNN) was used as an encoder to summarize a source sentence, and the summary was decoded using a conditional recurrent neural network language model to produce the translation.[165] All these systems have the same building blocks: gated RNNs and CNNs, and trained attention mechanisms.
Applications
Automatic speech recognition
The results shown in the table below are for automatic speech recognition on the popular TIMIT data set.
 This is a common data set used for initial evaluations of deep learning architectures. 
The entire set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.
 Its small size allows many configurations to be tried effectively. 
More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows very weak "language models" and thus the weaknesses in acoustic modeling aspects of speech recognition can be more easily analyzed. 
Such analysis on TIMIT by Li Deng and collaborators around 2009-2010, contrasting the GMM (and other generative models of speech) vs. 
DNN models, stimulated early industrial investment in deep learning for speech recognition from small to large scales,[36][47] eventually leading to pervasive and dominant use in that industry. 
That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. 
The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized over a time span of the past 20 years:
In 2010, industrial researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.
 Comprehensive reviews of this development and of the state of the art as of October 2014 are in the recent Springer book from Microsoft Research.
 An article[171] reviews of background of automatic speech recognition and the impact of various machine learning paradigms, including deep learning.
One fundamental principle of deep learning is to do away with hand-crafted feature engineering and to use raw features. 
This principle was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features, showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms. 
The true "raw" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.
Since the initial successful debut of DNNs for speech recognition around 2009-2011, progress (and future directions) can be summarized into eight major areas:[1][38][48]
Scaling up/out and speedup DNN training and decoding;
Sequence discriminative training of DNNs;
Feature processing by deep models with solid understanding of the underlying mechanisms;
Adaptation of DNNs and of related deep models;
Multi-task and transfer learning by DNNs and related deep models;
Convolution neural networks and how to design them to best exploit domain knowledge of speech;
Recurrent neural network and its rich LSTM variants;
Other types of deep models including tensor-based models and integrated deep generative/discriminative models.
Large-scale automatic speech recognition is the first and most convincing successful case of deep learning in the recent history, embraced by both industry and academia across the board.
 Between 2010 and 2014, the two major conferences on signal processing and speech recognition, IEEE-ICASSP and Interspeech, have seen a large increase in the numbers of accepted papers in their respective annual conference papers on the topic of deep learning for speech recognition. 
More importantly, all major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning methods.
 See also the recent media interview with the CTO of Nuance Communications.
The wide-spreading success in speech recognition achieved by 2011 was followed shortly in large-scale image recognition.
Image recognition
A common evaluation set for image classification is the MNIST database data set. 
MNIST is composed of handwritten digits and includes 60000 training examples and 10000 test examples. 
As with TIMIT, its small size allows multiple configurations to be tested. 
A comprehensive list of results on this set can be found in.
 The current best result on MNIST is an error rate of 0.23%, achieved by Ciresan et al. in 2012.
The real impact of deep learning in image or object recognition, a major branch of computer vision, was felt in the fall of 2012 after the team of Geoff Hinton and his students won the large-scale ImageNet competition by a significant margin over the then-state-of-the-art shallow machine learning methods. 
The technology is based on 20-year-old deep convolutional nets, but with much larger scale on a much larger task, since it had been learned that deep learning works well for large-scale speech recognition. 
In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced quickly, following a similar trend in large-scale speech recognition.
As in the ambitious moves from automatic speech recognition toward automatic speech translation and understanding, image classification has recently been extended to the more challenging task of automatic image captioning, in which deep learning is the essential underlying technology.
One example application is a car computer said to be trained with deep learning, which may enable cars to interpret 360° camera views.
 Another example is the technology known as Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.
Natural language processing
Neural networks have been used for implementing language models since the early 2000s.
 Key techniques in this field are negative sampling[185] and word embedding.
 Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture, that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space.
 Using a word embedding as an input layer to a recursive neural network (RNN) allows to train the network to parse sentences and phrases using an effective compositional vector grammar. 
A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by a recursive neural network.
 Recursive auto-encoders built atop word embeddings have been trained to assess sentence similarity and detect paraphrasing.
 Deep neural architectures have achieved state-of-the-art results in many natural language processing tasks such as constituency parsing, sentiment analysis, information retrieval, machine translation, contextual entity linking,[193] and others.
Drug discovery and toxicology
The pharmaceutical industry faces the problem that a large percentage of candidate drugs fail to reach the market. 
These failures of chemical compounds are caused by insufficient efficacy on the biomolecular target (on-target effect), undetected and undesired interactions with other biomolecules (off-target effects), or unanticipated toxic effects.
 In 2012, a team led by George Dahl won the "Merck Molecular Activity Challenge" using multi-task deep neural networks to predict the biomolecular target of a compound.
 In 2014, Sepp Hochreiter's group used Deep Learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the "Tox21 Data Challenge" of NIH, FDA and NCATS.
 These impressive successes show that deep learning may be superior to other virtual screening methods.
 Researchers from Google and Stanford enhanced deep learning for drug discovery by combining data from a variety of sources.
In 2015, Atomwise introduced AtomNet, the first deep learning neural networks for structure-based rational drug design.
Subsequently, AtomNet was used to predict novel candidate biomolecules for several disease targets, most notably treatments for the Ebola virus[205] and multiple sclerosis.

Customer relationship management
Recently success has been reported with application of deep reinforcement learning in direct marketing settings, illustrating suitability of the method for CRM automation. 
A neural network was used to approximate the value of possible direct marketing actions over the customer state space, defined in terms of RFM variables. 
The estimated value function was shown to have a natural interpretation as customer lifetime value.
Recommendation systems
Recommendation systems have used deep learning to extract meaningful deep features for latent factor model for content-based recommendation for music.
Recently, a more general approach for learning user preferences from multiple domains using mulitview deep learning has been introduced.
 The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.
Bioinformatics
Recently, a deep-learning approach based on an autoencoder artificial neural network has been used in bioinformatics, to predict Gene Ontology annotations and gene-function relationships.
Theories of the human brain
Computational deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.
 An approachable summary of this work is Elman, et al.'s 1996 book "Rethinking Innateness" [213] (see also: Shrager and Johnson;[214] Quartz and Sejnowski [215]).
 As these developmental theories were also instantiated in computational models, they are technical predecessors of purely computationally motivated deep learning models. 
This process yields a self-organizing stack of transducers, well-tuned to their operating environment. 
As described in The New York Times in 1995: "...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature.
The importance of deep learning with respect to the evolution and development of human cognition did not escape the attention of these researchers. 
One aspect of human development that distinguishes us from our nearest primate neighbors may be changes in the timing of development.
 Among primates, the human brain remains relatively plastic until late in the post-natal period, whereas the brains of our closest relatives are more completely formed by birth. 
Thus, humans have greater access to the complex experiences afforded by being out in the world during the most formative period of brain development.
 This may enable us to "tune in" to rapidly changing features of the environment that other animals, more constrained by evolutionary structuring of their brains, are unable to take account of. 
To the extent that these changes are reflected in similar timing changes in hypothesized wave of cortical development, they may also lead to changes in the extraction of information from the stimulus environment during the early self-organization of the brain. 
Of course, along with this flexibility comes an extended period of immaturity, during which we are dependent upon our caretakers and our community for both support and training. 
The theory of deep learning therefore sees the coevolution of culture and cognition as a fundamental condition of human evolution.
Commercial activities
Deep learning is often presented as a step towards realising strong AI[219] and thus many organizations have become interested in its use for particular applications. 
In December 2013, Facebook hired Yann LeCun to head its new artificial intelligence (AI) lab that was to have operations in California, London, and New York.
 The AI lab will develop deep learning techniques to help Facebook do tasks such as automatically tagging uploaded pictures with the names of the people in them.
 Late in 2014, Facebook also hired Vladimir Vapnik, a main developer of the Vapnik–Chervonenkis theory of statistical learning, and co-inventor of the support vector machine method.
In March 2013, Google hired Geoffrey Hinton and two of his graduate students, Alex Krizhevsky and Ilya Sutskever. 
Their work was to focus on both improving existing machine learning products at Google and help deal with the growing amount of data Google has. Google also bought Hinton's company, DNNresearch.
In 2014, Google also bought DeepMind Technologies, a British start-up that developed a system capable of learning how to play Atari video games using only raw pixels as data input. 
In 2015 they demonstrated AlphaGo system which achieved one of the long-standing "grand challenges" of AI by learning the game of Go well enough to beat a human professional Go player.
Also in 2014, Microsoft established The Deep Learning Technology Center in its MSR division, amassing deep learning experts for application-focused activities.
Baidu hired Andrew Ng to head its new Silicon Valley-based research lab focusing on deep learning.
Criticism and comment
Given the far-reaching implications of artificial intelligence coupled with the realization that deep learning is emerging as one of its most powerful techniques, the subject is understandably attracting both criticism and comment, and in some cases from outside the field of computer science itself.
A main criticism of deep learning concerns the lack of theory surrounding many of the methods. 
Most of the learning in deep architectures is just some form of gradient descent. 
While gradient descent has been understood for a while now, the theory surrounding other algorithms, such as contrastive divergence is less clear (i.e., Does it converge? If so, how fast? What is it approximating?). 
Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.
Others point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. 
Despite the power of deep learning methods, they still lack much of the functionality needed for realizing this goal entirely. 
Research psychologist Gary Marcus has noted that:
"Realistically, deep learning is only part of the larger challenge of building intelligent machines. 
Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used.
 The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning." 
To the extent that such a viewpoint implies, without intending to, that deep learning will ultimately constitute nothing more than the primitive discriminatory levels of a comprehensive future machine intelligence, a recent pair of speculations regarding art and artificial intelligence[226] offers an alternative and more expansive outlook. 
The first such speculation is that it might be possible to train a machine vision stack to perform the sophisticated task of discriminating between "old master" and amateur figure drawings; and the second is that such a sensitivity might in fact represent the rudiments of a non-trivial machine empathy. 
It is suggested, moreover, that such an eventuality would be in line with both anthropology, which identifies a concern with aesthetics as a key element of behavioral modernity, and also with a current school of thought which suspects that the allied phenomenon of consciousness may in fact have roots deep within the structure of the universe itself.
Some currently popular and successful deep learning architectures display certain problematic behaviors,such as confidently classifying random data as belonging to a familiar category of nonrandom images[232] and misclassifying minuscule perturbations of correctly classified images.
 The creator of OpenCog, Ben Goertzel, hypothesized that these behaviors are due to limitations in the internal representations learned by these architectures, and that these limitations would inhibit integration of these architectures into heterogeneous multi-component AGI architectures.
 It is suggested that these issues can be worked around by developing deep learning architectures that internally form states homologous to image-grammar[234] decompositions of observed entities and events.
 Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning which operates on concepts in terms of production rules of the grammar, and is a basic goal of both human language acquisition  and AI.
nowledge engineering (KE) refers to all technical, scientific and social aspects involved in building, maintaining and using knowledge-based systems.
Expert systems
One of the first examples of an expert system was MYCIN, an application to perform medical diagnosis. 
In the MYCIN example, the domain experts were medical doctors and the knowledge represented was their expertise in diagnosis.
Expert systems were first developed in artificial intelligence laboratories as an attempt to understand complex human decision making. 
Based on positive results from these initial prototypes, the technology was adopted by the US business community (and later worldwide) in the 1980s. 
The Stanford heuristic programming projects led by Edward Feigenbaum was one of the leaders in defining and developing the first expert systems.
History
In the earliest "cowboy" days of expert systems there was little or no formal process for the creation of the software. 
Researchers just sat down with domain experts and started programming, often developing the required tools (e.g. inference engines) at the same time as the applications themselves. 
As expert systems moved from academic prototypes to deployed business systems it was realized that a methodology was required to bring predictability and control to the process of building the software. 
There were essentially two approaches that were attempted:
Use conventional software development methodologies
Develop special methodologies tuned to the requirements of building expert systems
Many of the early expert systems were developed by large consulting and system integration firms such as Andersen Consulting. 
These firms already had well tested conventional waterfall methodologies (e.g. Method/1 for Andersen) that they trained all their staff in and that were virtually always used to develop software for their clients.
One trend in early expert systems development was to simply apply these waterfall methods to expert systems development.
Another issue with using conventional methods to develop expert systems was that due to the unprecedented nature of expert systems they were one of the first applications to adopt rapid application development methods that feature iteration and prototyping as well as or instead of detailed analysis and design.
 In the 1980s few conventional software methods supported this type of approach.
The final issue with using conventional methods to develop expert systems was the need for knowledge acquisition. 
Knowledge acquisition refers to the process of gathering expert knowledge and capturing it in the form of rules and ontologies. 
Knowledge acquisition has special requirements beyond the conventional specification process used to capture most business requirements.
These issues led to the second approach to knowledge engineering: development of custom methodologies specifically designed to build expert systems.
 One of the first and most popular of such methodologies custom designed for expert systems was the Knowledge Acquisition and Documentation Structuring (KADS) methodology developed in Europe.
 KADS had great success in Europe and was also used in the United States.
In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert.
 Expert systems are designed to solve complex problems by reasoning about knowledge, represented primarily as if–then rules rather than through conventional procedural code.
 The first expert systems were created in the 1970s and then proliferated in the 1980s.
 Expert systems were among the first truly successful forms of AI software.
An expert system is divided into two sub-systems: the inference engine and the knowledge base. 
The knowledge base represents facts and rules.
 The inference engine applies the rules to the known facts to deduce new facts. 
Inference engines can also include explanation and debugging capabilities.
Edward Feigenbaum said that the key insight of early expert systems was that "intelligent systems derive their power from the knowledge they possess rather than from the specific formalisms and inference schemes they use."
 Although, in retrospect, this seems a rather straightforward insight, it was a significant step forward at the time.
 Until then, research had been focused on attempts to develop very general-purpose problem solvers such as those described by Allen Newell and Herb Simon.
Expert systems were introduced by the Stanford Heuristic Programming Project led by Feigenbaum, who is sometimes referred to as the "father of expert systems".
 The Stanford researchers tried to identify domains where expertise was highly valued and complex, such as diagnosing infectious diseases (Mycin) and identifying unknown organic molecules (Dendral).
In addition to Feigenbaum key early contributors were Edward Shortliffe, Bruce Buchanan, and Randall Davis. 
Expert systems were among the first truly successful forms of AI software.
Research on expert systems was also active in France. 
In the US the focus tended to be on rule-based systems, first on systems hard coded on top of LISP programming environments and then on expert system shells developed by vendors such as Intellicorp. 
In France research focused more on systems developed in Prolog. 
The advantage of expert system shells was that they were somewhat easier for non-programmers to use. 
The advantage of Prolog environments was that they weren't focused only on IF-THEN rules. 
Prolog environments provided a much fuller realization of a complete First Order Logic environment.
In the 1980s, expert systems proliferated.
 Universities offered expert system courses and two thirds of the Fortune 1000 companies applied the technology in daily business activities.
Interest was international with the Fifth Generation Computer Systems project in Japan and increased research funding in Europe.
In 1981 the first IBM PC was introduced, with the MS-DOS operating system. 
The imbalance between the relatively powerful chips in the highly affordable PC compared to the much more expensive price of processing power in the mainframes that dominated the corporate IT world at the time created a whole new type of architecture for corporate computing known as the client-server model.
 Calculations and reasoning could be performed at a fraction of the price of a mainframe using a PC. 
This model also enabled business units to bypass corporate IT departments and directly build their own applications. 
As a result, client server had a tremendous impact on the expert systems market. 
Expert systems were already outliers in much of the business world, requiring new skills that many IT departments did not have and were not eager to develop. 
They were a natural fit for new PC-based shells that promised to put application development into the hands of end users and experts. 
Up until that point the primary development environment for expert systems had been high end Lisp machines from Xerox, Symbolics and Texas Instruments.
 With the rise of the PC and client server computing vendors such as Intellicorp and Inference Corporation shifted their priorities to developing PC based tools. 
In addition new vendors often financed by Venture Capital started appearing regularly. 
These new vendors included Aion Corporation, Neuron Data, Exsys, and many others.
In the 1990s and beyond the term "expert system" and the idea of a standalone AI system mostly dropped from the IT lexicon. 
There are two interpretations of this.
 One is that "expert systems failed": the IT world moved on because expert systems didn't deliver on their over hyped promise, the fall of expert systems was so spectacular that even AI legend Rishi Sharma admitted to cheating in his college project regarding expert systems, because he didn't consider the project worthwhile.
 The other is the mirror opposite, that expert systems were simply victims of their success.
 As IT professionals grasped concepts such as rule engines such tools migrated from standalone tools for the development of special purpose "expert" systems to one more tool that an IT professional has at their disposal.
 Many of the leading major business application suite vendors such as SAP, Siebel, and Oracle integrated expert system capabilities into their suite of products as a way of specifying business logic. 
Rule engines are no longer simply for defining the rules an expert would use but for any type of complex, volatile, and critical business logic. 
They often go hand in hand with business process automation and integration environments.
Software architecture
An expert system is an example of a knowledge-based system. 
Expert systems were the first commercial systems to use a knowledge-based architecture. 
A knowledge-based system is essentially composed of two sub-systems: the knowledge base and the inference engine.
The knowledge base represents facts about the world. 
In early expert systems such as Mycin and Dendral these facts were represented primarily as flat assertions about variables. 
In later expert systems developed with commercial shells the knowledge base took on more structure and utilized concepts from object-oriented programming. 
The world was represented as classes, subclasses, and instances and assertions were replaced by values of object instances. 
The rules worked by querying and asserting values of the objects.
The inference engine is an automated reasoning system that evaluates the current state of the knowledge-base, applies relevant rules, and then asserts new knowledge into the knowledge base.
 The inference engine may also include capabilities for explanation, so that it can explain to a user the chain of reasoning used to arrive at a particular conclusion by tracing back over the firing of rules that resulted in the assertion.
There are primarily two modes for an inference engine: forward chaining and backward chaining. 
The different approaches are dictated by whether the inference engine is being driven by the antecedent (left hand side) or the consequent (right hand side) of the rule. 
In forward chaining an antecedent fires and asserts the consequent. For example, consider the following rule:
R1: Man(x) => Mortal(x)
A simple example of forward chaining would be to assert Man(Socrates) to the system and then trigger the inference engine.
 It would match R1 and assert Mortal(Socrates) into the knowledge base.
Backward chaining is a bit less straight forward.
 In backward chaining the system looks at possible conclusions and works backward to see if they might be true. 
So if the system was trying to determine if Mortal(Socrates) is true it would find R1 and query the knowledge base to see if Man(Socrates) is true. 
One of the early innovations of expert systems shells was to integrate inference engines with a user interface. 
This could be especially powerful with backward chaining.
 If the system needs to know a particular fact but doesn't it can simply generate an input screen and ask the user if the information is known. 
So in this example, it could use R1 to ask the user if Socrates was a Man and then use that new information accordingly.
The use of rules to explicitly represent knowledge also enabled explanation capabilities.
 In the simple example above if the system had used R1 to assert that Socrates was Mortal and a user wished to understand why Socrates was mortal they could query the system and the system would look back at the rules which fired to cause the assertion and present those rules to the user as an explanation. 
In English if the user asked "Why is Socrates Mortal?" the system would reply "Because all men are mortal and Socrates is a man". 
A significant area for research was the generation of explanations from the knowledge base in natural English rather than simply by showing the more formal but less intuitive rules.
As Expert Systems evolved many new techniques were incorporated into various types of inference engines.
 Some of the most important of these were:
Truth Maintenance. 
Truth maintenance systems record the dependencies in a knowledge-base so that when facts are altered dependent knowledge can be altered accordingly. 
For example, if the system learns that Socrates is no longer known to be a man it will revoke the assertion that Socrates is mortal.
Hypothetical Reasoning.
 In hypothetical reasoning, the knowledge base can be divided up into many possible views, a.k.a. worlds.
 This allows the inference engine to explore multiple possibilities in parallel. 
In this simple example, the system may want to explore the consequences of both assertions, what will be true if Socrates is a Man and what will be true if he is not?
Fuzzy Logic. 
One of the first extensions of simply using rules to represent knowledge was also to associate a probability with each rule. So, not to assert that Socrates is mortal but to assert Socrates may be mortal with some probability value. 
Simple probabilities were extended in some systems with sophisticated mechanisms for uncertain reasoning and combination of probabilities.
Ontology Classification. With the addition of object classes to the knowledge base a new type of reasoning was possible. 
Rather than reason simply about the values of the objects the system could also reason about the structure of the objects as well. 
In this simple example Man can represent an object class and R1 can be redefined as a rule that defines the class of all men. 
These types of special purpose inference engines are known as classifiers.
 Although they were not highly used in expert systems, classifiers are very powerful for unstructured volatile domains and are a key technology for the Internet and the emerging Semantic Web.
Advantages
The goal of knowledge-based systems is to make the critical information required for the system to work explicit rather than implicit.
 In a traditional computer program the logic is embedded in code that can typically only be reviewed by an IT specialist. 
With an expert system the goal was to specify the rules in a format that was intuitive and easily understood, reviewed, and even edited by domain experts rather than IT experts. 
The benefits of this explicit knowledge representation were rapid development and ease of maintenance.
Ease of maintenance is the most obvious benefit. 
This was achieved in two ways. 
First, by removing the need to write conventional code many of the normal problems that can be caused by even small changes to a system could be avoided with expert systems. 
Essentially, the logical flow of the program (at least at the highest level) was simply a given for the system, simply invoke the inference engine. 
This also was a reason for the second benefit: rapid prototyping. 
With an expert system shell it was possible to enter a few rules and have a prototype developed in days rather than the months or year typically associated with complex IT projects.
A claim for expert system shells that was often made was that they removed the need for trained programmers and that experts could develop systems themselves. 
In reality this was seldom if ever true.
 While the rules for an expert system were more comprehensible than typical computer code they still had a formal syntax where a misplaced comma or other character could cause havoc as with any other computer language. 
In addition, as expert systems moved from prototypes in the lab to deployment in the business world, issues of integration and maintenance became far more critical. 
Inevitably demands to integrate with and take advantage of large legacy databases and systems arose. 
To accomplish this integration required the same skills as any other type of system.
Disadvantages
The most common disadvantage cited for expert systems in the academic literature is the knowledge acquisition problem. 
Obtaining the time of domain experts for any software application is always difficult but for expert systems it was especially difficult because the experts were by definition highly valued and in constant demand by the organization. 
As a result of this problem a great deal of research in the later years of expert systems was focused on tools for knowledge acquisition, to help automate the process of designing, debugging, and maintaining rules defined by experts. 
However, when looking at the life-cycle of expert systems in actual use other problems seem at least as critical as knowledge acquisition. 
These problems were essentially the same as those of any other large system: integration, access to large databases, and performance.
Performance was especially problematic because early expert systems were built using tools such as Lisp, which executed interpreted rather than compiled code. 
Interpreting provided an extremely powerful development environment but with the drawback that it was virtually impossible to match the efficiency of the fastest compiled languages of the time, such as C.
 System and database integration were difficult for early expert systems because the tools were mostly in languages and platforms that were neither familiar to nor welcomed in most corporate IT environments – programming languages such as Lisp and Prolog and hardware platforms such as Lisp Machines and personal computers. 
As a result, a great deal of effort in the later stages of expert system tool development was focused on integration with legacy environments such as COBOL, integration with large database systems, and porting to more standard platforms. 
These issues were resolved primarily by the client-server paradigm shift as PCs were gradually accepted in the IT world as a legitimate platform for serious business system development and as affordable minicomputer servers provided the processing power needed for AI applications.
Applications
Hayes-Roth divides expert systems applications into 10 categories illustrated in the following table. 
Note that the example applications were not in the original Hayes-Roth table and some of the example applications came along quite a bit later. 
Any application that is not foot noted is described in the Hayes-Roth book.
 Also, while these categories provide an intuitive framework for describing the space of expert systems applications, they are not rigid categories and in some cases an application may show characteristics of more than one category.
Hearsay was an early attempt at solving voice recognition through an expert systems approach. 
For the most part this category or expert systems was not all that successful.
 Hearsay and all interpretation systems are essentially pattern recognition systems—looking for patterns in noisy data. 
In the case of Hearsay recognizing phonemes in an audio stream. Other early examples were analyzing sonar data to detect Russian submarines. 
These kinds of systems proved much more amenable to a neural network AI solution than a rule-based approach.
CADUCEUS and MYCIN were medical diagnosis systems. 
The user describes their symptoms to the computer as they would to a doctor and the computer returns a medical diagnosis.
Dendral was a tool to study hypothesis formation in the identification of organic molecules. 
The general problem it solved—designing a solution given a set of constraints—was one of the most successful areas for early expert systems applied to business domains such as salespeople configuring DEC VAX computers and mortgage loan application development.
SMH.PAL is an expert system for the assessment of students with multiple disabilities.
Mistral [35] is an expert system for the monitoring of dam safety developed in the 90's by Ismes (Italy).
 It gets data from an automatic monitoring system and performs a diagnosis of the state of the dam. 
Its first copy, installed in 1992 on the Ridracoli Dam (Italy), is still operational 24/7/365. 
It has been installed on several dams in Italy and abroad (e.g. Itaipu Dam in Brazil), as well as on landslides under the name of Eydenet,[36] and on monuments under the name of Kaleidos.
 Mistral is a registered trade mark of CESI.
Automated reasoning is an area of computer science and mathematical logic dedicated to understanding different aspects of reasoning. 
The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically.
 Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy.
The most developed subareas of automated reasoning are automated theorem proving (and the less automated but more pragmatic subfield of interactive theorem proving) and automated proof checking (viewed as guaranteed correct reasoning under fixed assumptions). 
Extensive work has also been done in reasoning by analogy induction and abduction.
Other important topics include reasoning under uncertainty and non-monotonic reasoning.
 An important part of the uncertainty field is that of argumentation, where further constraints of minimality and consistency are applied on top of the more standard automated deduction. 
John Pollock's OSCAR system[1] is an example of an automated argumentation system that is more specific than being just an automated theorem prover.
Tools and techniques of automated reasoning include the classical logics and calculi, fuzzy logic, Bayesian inference, reasoning with maximal entropy and a large number of less formal ad hoc techniques.
The development of formal logic played a big role in the field of automated reasoning, which itself led to the development of artificial intelligence.
 A formal proof is a proof in which every logical inference has been checked back to the fundamental axioms of mathematics.
 All the intermediate logical steps are supplied, without exception.
 No appeal is made to intuition, even if the translation from intuition to logic is routine. 
Thus, a formal proof is less intuitive, and less susceptible to logical errors.[2]
Some consider the Cornell Summer meeting of 1957, which brought together a large number of logicians and computer scientists, as the origin of automated reasoning, or automated deduction.
 Others say that it began before that with the 1955 Logic Theorist program of Newell, Shaw and Simon, or with Martin Davis’ 1954 implementation of Presburger’s decision procedure (which proved that the sum of two even numbers is even).
 Automated reasoning, although a significant and popular area of research, went through an "AI winter" in the eighties and early nineties.
 Luckily, it got revived after that. 
For example, in 2005, Microsoft started using verification technology in many of their internal projects and is planning to include a logical specification and checking language in their 2012 version of Visual C.
Significant contributions
principia Mathematica was a milestone work in formal logic written by Alfred North Whitehead and Bertrand Russell. 
Principia Mathematica - also meaning Principles of Mathematics - was written with a purpose to derive all or some of the mathematical expressions, in terms of symbolic logic. 
Principia Mathematica was initially published in three volumes in 1910, 1912 and 1913.[5]
Logic Theorist (LT) was the first ever program developed in 1956 by Allen Newell, Cliff Shaw and Herbert A. 
Simon to "mimic human reasoning" in proving theorems and was demonstrated on fifty-two theorems from chapter two of Principia Mathematica, proving thirty-eight of them.
 In addition to proving the theorems, the program found a proof for one of the theorems that was more elegant than the one provided by Whitehead and Russell.
 After an unsuccessful attempt at publishing their results, Newell, Shaw, and Herbert reported in their publication in 1958, The Next Advance in Operation Research:
"There are now in the world machines that think, that learn and that create. 
Moreover, their ability to do these things is going to increase rapidly until (in a visible future) the range of problems they can handle will be co- extensive with the range to which the human mind has been applied.
Boyer-Moore Theorem Prover (NQTHM)
The design of this system was influenced by John McCarthy and Woody Bledsoe. 
Started in 1971 at Edinburgh, Scotland, this was a fully automatic theorem prover built using Pure Lisp.
 The main aspects of NQTHM were:
the use of Lisp as a working logic.
the reliance on a principle of definition for total recursive functions.
the extensive use of rewriting and "symbolic evaluation".
an induction heuristic based the failure of symbolic evaluation.
HOL Light
Written in OCaml, HOL Light is designed to have a simple and clean logical foundation and an uncluttered implementation. 
It is essentially another proof assistant for classical higher order logic.[12]
Coq
Developed in France, Coq is another automated proof assistant, which can automatically extract executable programs from specifications, as either Objective CAML or Haskell source code. 
Properties, programs and proofs are formalized in the same language called the Calculus of Inductive Constructions (CIC).
Applications
Automated reasoning has been most commonly used to build automated theorem provers. 
In some cases such provers have come up with new approaches to proving a theorem. 
Logic Theorist is a good example of this. 
The program came up with a proof for one of the theorems in Principia Mathematica that was more efficient (requiring fewer steps) than the proof provided by Whitehead and Russell. 
Automated reasoning programs are being applied to solve a growing number of problems in formal logic, mathematics and computer science, logic programming, software and hardware verification, circuit design, and many others. 
The TPTP (Sutcliffe and Suttner 1998) is a library of such problems that is updated on a regular basis. 
There is also a competition among automated theorem provers held regularly at the CADE conference (Pelletier, Sutcliffe and Suttner 2002); the problems for the competition are selected from the TPTP library.
Knowledge management (KM) is the process of capturing, developing, sharing, and effectively using organizational knowledge.
It refers to a multi-disciplinary approach to achieving organizational objectives by making the best use of knowledge.
An established discipline since 1991 (see Nonaka 1991), KM includes courses taught in the fields of business administration, information systems, management, and library, and information sciences.
 More recently, other fields have started contributing to KM research, including information and media, computer science, public health, and public policy.
 Several Universities now offer dedicated Master of Science degrees in Knowledge Management.
Many large companies, public institutions, and non-profit organisations have resources dedicated to internal KM efforts, often as a part of their business strategy, information technology, or human resource management departments.
 Several consulting companies provide strategy and advice regarding KM to these organisations.
Knowledge management efforts typically focus on organisational objectives such as improved performance, competitive advantage, innovation, the sharing of lessons learned, integration, and continuous improvement of the organisation.
 KM efforts overlap with organisational learning and may be distinguished from that by a greater focus on the management of knowledge as a strategic asset and a focus on encouraging the sharing of knowledge.
It is an enabler of organisational learning.
Knowledge management efforts have a long history, to include on-the-job discussions, formal apprenticeship, discussion forums, corporate libraries, professional training, and mentoring programs.
 With increased use of computers in the second half of the 20th century, specific adaptations of technologies such as knowledge bases, expert systems, knowledge repositories, group decision support systems, intranets, and computer-supported cooperative work have been introduced to further enhance such efforts.
In 1999, the term personal knowledge management was introduced; it refers to the management of knowledge at the individual level.
In the enterprise, early collections of case studies recognized the importance of knowledge management dimensions of strategy, process, and measurement.
  In short, knowledge management programs can yield impressive benefits to individuals and organizations if they are purposeful, concrete, and action-orientated.
KM emerged as a scientific discipline in the earlier 1990s.
It was initially supported solely by practitioners, when Skandia hired Leif Edvinsson of Sweden as the world's first Chief Knowledge Officer (CKO).
 Hubert Saint-Onge (formerly of CIBC, Canada), started investigating KM long before that.
 The objective of CKOs is to manage and maximize the intangible assets of their organisations.
 Gradually, CKOs became interested in practical and theoretical aspects of KM, and the new research field was formed.
 Discussion of the KM idea has been taken up by academics, such as Ikujiro Nonaka (Hitotsubashi University), Hirotaka Takeuchi (Hitotsubashi University), Thomas H. Davenport (Babson College) and Baruch Lev (New York University).
 In 2001, Thomas A. Stewart, former editor at Fortune magazine and subsequently the editor of Harvard Business Review, published a cover story highlighting the importance of intellectual capital in organisations.
 Since its establishment, the KM discipline has been gradually moving towards academic maturity.
 First, there is a trend toward higher cooperation among academics; particularly, there has been a drop in single-authored publications. 
Second, the role of practitioners has changed.
 Their contribution to academic research has been dramatically declining from 30% of overall contributions up to 2002, to only 10% by 2009 (Serenko et al. 2010).[19]
A broad range of thoughts on the KM discipline exist; approaches vary by author and school.
 As the discipline matures, academic debates have increased regarding both the theory and practice of KM, to include the following perspectives:
Techno-centric with a focus on technology, ideally those that enhance knowledge sharing and creation.
Organisational with a focus on how an organisation can be designed to facilitate knowledge processes best.[6]
Ecological with a focus on the interaction of people, identity, knowledge, and environmental factors as a complex adaptive system akin to a natural ecosystem.
Regardless of the school of thought, core components of KM include people, processes, technology (or) culture, structure, technology, depending on the specific perspective (Spender & Scherer 2007). 
Different KM schools of thought include lenses through which KM can be viewed and explained, to include:
community of practice [25]
social network analysis[26]
intellectual capital (Bontis & Choo 2002)
information theory (McInerney 2002)[15]
complexity science[27]
constructivism (Nanjappa & Grant 2003)[28]
The practical relevance of academic research in KM has been questioned (Ferguson 2005) with action research suggested as having more relevance (Andriessen 2004) and the need to translate the findings presented in academic journals to a practice (Booker, Bontis & Serenko 2008).
Dimensions
Different frameworks for distinguishing between different 'types of' knowledge exist.
One proposed framework for categorizing the dimensions of knowledge distinguishes between tacit knowledge and explicit knowledge.
 Tacit knowledge represents internalized knowledge that an individual may not be consciously aware of, such as how he or she accomplishes particular tasks. 
At the opposite end of the spectrum, explicit knowledge represents knowledge that the individual holds consciously in mental focus, in a form that can easily be communicated to others.
 Similarly, Hayes and Walsham (2003) describe content and relational perspectives of knowledge and knowledge management as two fundamentally different epistemological perspectives.
 The content perspective suggest that knowledge is easily stored because it may be codified, while the relational perspective recognizes the contextual and relational aspects of knowledge which can make knowledge difficult to share outside of the specific location where the knowledge is developed.[31]
The Knowledge Spiral as described by Nonaka & Takeuchi.
Early research suggested that a successful KM effort needs to convert internalized tacit knowledge into explicit knowledge to share it, and the same effort must permit individuals to internalize and make personally meaningful any codified knowledge retrieved from the KM effort.
 Subsequent research into KM suggested that a distinction between tacit knowledge and explicit knowledge represented an oversimplification and that the notion of explicit knowledge is self-contradictory.
 Specifically, for knowledge to be made explicit, it must be translated into information (i.e., symbols outside of our heads) (Serenko & Bontis 2004).
Later on, Ikujiro Nonaka proposed a model (SECI for Socialization, Externalization, Combination, Internalization) which considers a spiraling knowledge process interaction between explicit knowledge and tacit knowledge (Nonaka & Takeuchi 1995).
In this model, knowledge follows a cycle in which implicit knowledge is 'extracted' to become explicit knowledge, and explicit knowledge is 're-internalized' into implicit knowledge.
 More recently, together with Georg von Krogh and Sven Voelpel, Nonaka returned to his earlier work in an attempt to move the debate about knowledge conversion forwards (Nonaka, von Krogh & Voelpel 2006);[34] (Nonaka, von Krogh & 2009).[4]
A second proposed framework for categorizing the dimensions of knowledge distinguishes between embedded knowledge of a system outside of a human individual (e.g., an information system may have knowledge embedded into its design) and embodied knowledge representing a learned capability of a human body's nervous and endocrine systems (Sensky 2002).[35]
A third proposed framework for categorizing the dimensions of knowledge distinguishes between the exploratory creation of "new knowledge" (i.e., innovation) vs. the transfer or exploitation of "established knowledge" within a group, organisation, or community.
 Collaborative environments such as communities of practice or the use of social computing tools can be used for both knowledge creation and transfer.
Strategies
Knowledge may be accessed at three stages: before, during, or after KM-related activities.
Organisations have tried knowledge capture incentives, including making content submission mandatory and incorporating rewards into performance measurement plans.
 Considerable controversy exists over whether incentives work or not in this field and no consensus has emerged.
One strategy to KM involves actively managing knowledge (push strategy).
 In such an instance, individuals strive to explicitly encode their knowledge into a shared knowledge repository, such as a database, as well as retrieving knowledge they need that other individuals have provided to the repository.
 This is commonly known as the Codification approach to KM.
Another strategy to KM involves individuals making knowledge requests of experts associated with a particular subject on an ad hoc basis (pull strategy).
 In such an instance, expert individual(s) can provide their insights to the particular person or people needing this (Snowden 2002).
 This is commonly known as the Personalisation approach to KM.
Hansen et al. propose a simple framework, distinguishing two opposing KM strategies: codification and personalization.
 Codification focuses on collecting and storing codified knowledge in previously designed electronic databases to make it accessible to the organisation.
Codification can therefore refer to both tacit and explicit knowledge.
In contrast, the personalization strategy aims at encouraging individuals to share their knowledge directly.
 Information technology plays a less important role, as it is only supposed to facilitate communication and knowledge sharing among members of an organisation.
Other knowledge management strategies and instruments for companies include:
Knowledge Sharing (fostering a culture that encourages the sharing of information, based on the concept that knowledge is not irrevocable and should be shared and updated to remain relevant)
Storytelling (as a means of transferring tacit knowledge)
Cross-project learning
After action reviews
Knowledge mapping (a map of knowledge repositories within a company accessible by all)
Communities of practice
Expert directories (to enable knowledge seeker to reach to the experts)
Expert Systems (knowledge seeker responds to one or more specific questions to reach knowledge in a repository)
Best practice transfer
Knowledge fairs
Competence management (systematic evaluation and planning of competences of individual organisation members)
Proximity & architecture (the physical situation of employees can be either conducive or obstructive to knowledge sharing)
Master-apprentice relationship
Collaborative technologies (groupware, etc.)
Knowledge repositories (databases, bookmarking engines, etc.)
Measuring and reporting intellectual capital (a way of making explicit knowledge for companies)
Knowledge brokers (some organisational members take on responsibility for a specific "field" and act as first reference on whom to talk about a specific subject)
Social software (wikis, social bookmarking, blogs, etc.)
Inter-project knowledge transfer
Motivations
There are a number of claims as to the motivation leading organisations to undertake a KM effort.
Typical considerations driving a KM effort include:[27]
Making available increased knowledge content in the development and provision of products and services
Achieving shorter new product development cycles
Facilitating and managing innovation and organisational learning
Leveraging the expertise of people across the organisation
Increasing network connectivity between internal and external individuals
Managing business environments and allowing employees to obtain relevant insights and ideas appropriate to their work
Solving intractable or wicked problems
Managing intellectual capital and intellectual assets in the workforce (such as the expertise and know-how possessed by key individuals or stored in repositories)
Debate exists whether KM is more than a passing fad, though increasing amount of research in this field may help to answer this question, as well as create consensus on what elements of KM help determine the success or failure of such efforts (Wilson 2002).
 Knowledge sharing remains a challenging issue for knowledge management, while there is no clear agreement barriers may include time issues for knowledge works, the level of trust, lack of effective support technologies and culture (Jennex 2008).
KM Technologies
Knowledge Management (KM) technology can be divided into the following general categories:
Groupware
Groupware refers to technologies that facilitate collaboration and sharing of organizational information. 
One of the earliest very successful products in this category was Lotus Notes. 
Notes provided tools for threaded discussions, sharing of documents, organization wide uniform email, etc.
Workflow
Workflow tools allow the representation of processes associated with the creation, use, and maintenance of organizational knowledge.
 For example the process to create and utilize forms and documents within an organization. 
For example, a workflow system can do things such as send notifications to appropriate supervisors when a new document has been produced and is waiting their approval.
Content/Document Management
Content/Document Management systems are systems designed to automate the process of creating web content and/or documents within an organization.
 The various roles required such as editors, graphic designers, writers, and producers can be explicitly modeled along with the various tasks in the process and validation criteria for moving from one step to another. 
All this information can be used to automate and control the process.
 Commercial vendors of these tools started to start either as tools to primarily support documents (e.g., Documentum) or as tools designed to support web content (e.g., Interwoven) but as the Internet grew these functions merged and most vendors now perform both functions, management of web content and of documents. 
As Internet standards became adopted within most organization Intranets and Extranets the distinction between the two essentially went away.
Enterprise Portals
Enterprise Portals are web sites that aggregate information across the entire organization or for groups within the organization such as project teams.
eLearning
eLearning technology enables organizations to create customized training and education software. 
This can include lesson plans, monitoring progress against learning goals, online classes, etc.
 eLearning technology enables organizations to significantly reduce the cost of training and educating their members.
As with most KM technology in the business world this was most useful for companies that employ knowledge workers; highly trained staff with areas of deep expertise such as the staff of a consulting firm.
 Such firms spend a significant amount on the continuing education of their employees and even have their own internal full-time schools and internal education staff.
Scheduling and planning
Scheduling and planning tools automate the creation and maintenance of an organization's schedule: scheduling meetings,notifying people of a meeting, etc. 
An example of a well known scheduling tool is Microsoft Outlook. 
The planning aspect can integrate with project management tools such as Microsoft Project. 
Some of the earliest successful uses of KM technology in the business world were the development of these types of tools, for example online versions of corporate "yellow pages" with listing of contact info and relevant knowledge and work history.
Telepresence
Telepresence technology enables individuals to have virtual meetings rather than having to be in the same place. 
Videoconferencing is the most obvious example.
These categories are neither rigidly defined nor exhaustive. 
Workflow for example is a significant aspect of a content or document management system and most content and document management systems have tools for developing enterprise portals.
One of the most important trends in KM technology was the adoption of Internet standards. 
Original KM technology products such as Lotus Notes defined their own proprietary formats for email, documents, forms, etc. 
The explosive growth of the Internet drove most vendors to abandon proprietary formats and adopt Internet formats such as HTML, HTTP, and XML.
 In addition, open source and freeware tools for the creation of blogs and wikis now enable capabilities that used to require expensive commercial tools to be available for little or no cost.
One of the most important ongoing developments in KM technology is adoption of tools that enable organizations to work at the semantic level.
 Many of these tools are being developed as part of the Semantic Web.
 For example, the Stanford Protege Ontology Editor.
Information management (IM) concerns a cycle of organisational activity: the acquisition of information from one or more sources, the custodianship and the distribution of that information to those who need it, and its ultimate disposition through archiving or deletion.
This cycle of organisational involvement with information involves a variety of stakeholders: for example those who are responsible for assuring the quality, accessibility and utility of acquired information, those who are responsible for its safe storage and disposal, and those who need it for decision making. 
Stakeholders might have rights to originate, change, distribute or delete information according to organisational information management policies.
Information management embraces all the generic concepts of management, including: planning, organizing, structuring, processing, controlling, evaluation and reporting of information activities, all of which is needed in order to meet the needs of those with organisational roles or functions that depend on information.
Information management is closely related to, and overlaps, the management of data, systems, technology, processes and – where the availability of information is critical to organisational success – strategy. 
This broad view of the realm of information management contrasts with the earlier, more traditional view, that the life cycle of managing information is an operational matter that requires specific procedures, organisational capabilities and standards that deal with information as a product or a service.
Emergent ideas out of data management
In the 1970s the management of information largely concerned matters closer to what would now be called data management: punched cards, magnetic tapes and other record-keeping media, involving a life cycle of such formats requiring origination, distribution, backup, maintenance and disposal. 
At this time the huge potential of information technology began to be recognised: for example a single chip storing a whole book, or electronic mail moving messages instantly around the world, remarkable ideas at the time.
 With the proliferation of information technology and the extending reach of information systems in the 1980s and 1990s,[2] information management took on a new form. 
Progressive businesses such as British Petroleum transformed the vocabulary of what was then "IT management", so that “systems analysts” became “business analysts”, “monopoly supply” became a mixture of “insourcing” and “outsourcing”, and the large IT function was transformed into “lean teams” that began to allow some agility in the processes that harness information for business benefit.
 The scope of senior management interest in information at British Petroleum extended from the creation of value through improved business processes, based upon the effective management of information, permitting the implementation of appropriate information systems (or “applications”) that were operated on IT infrastructure that was outsourced.
In this way, information management was no longer a simple job that could be performed by anyone who had nothing else to do, it became highly strategic and a matter for senior management attention.

 An understanding of the technologies involved, an ability to manage information systems projects and business change well, and a willingness to align technology and business strategies all became necessary.[4]
Positioning information management in the bigger picture
In the transitional period leading up to the strategic view of information management, Venkatraman (a strong advocate of this process of transition and transformation,[5] proffered a simple arrangement of ideas that succinctly brought data management, information management and knowledge management together (see the figure). 
He argued that:
Data that is maintained in IT infrastructure has to be interpreted in order to render information.
The information in our information systems has to be understood in order to emerge as knowledge.
Knowledge allows managers to take effective decisions.
Effective decisions have to lead to appropriate actions.
Appropriate actions are expected to deliver meaningful results.
This simple model summarises a presentation by Venkatraman in 1996, as reported by Ward and Peppard (2002, page 207).
This is often referred to as the DIKAR model: Data, Information, Knowledge, Action and Result,[5] it gives a strong clue as to the layers involved in aligning technology and organisational strategies, and it can be seen as a pivotal moment in changing attitudes to information management.
 The recognition that information management is an investment that must deliver meaningful results is important to all modern organisations that depend on information and good decision making for their success.
Behavioural and organisational theories
Clearly, good information management is crucial to the smooth working of organisations, and although there is no commonly accepted theory of information management per se, behavioural and organisational theories help. 
Following the behavioural science theory of management, mainly developed at Carnegie Mellon University and prominently supported by March and Simon,[7] most of what goes on in modern organizations is actually information handling and decision making.
 And yet, well before there was any general recognition of the importance of information management in organisations, March and Simon [7] argued that organizations have to be considered as cooperative systems, with a high level of information processing and a vast need for decision making at various levels. 
Instead of using the model of the "economic man", as advocated in classical theory [8] they proposed "administrative man" as an alternative, based on their argumentation about the cognitive limits of rationality.
Additionally they proposed the notion of satisficing, which entails searching through the available alternatives until an acceptability threshold is met - another idea that still has currency.[9]
Economic theory
In addition to the organisational factors mentioned by March and Simon, there are other issues that stem from economic and environmental dynamics. 
There is the cost of collecting and evaluating the information needed to take a decision, including the time and effort required.
The transaction cost associated with information processes can be high. 
In particular, established organizational rules and procedures can prevent the taking of the most appropriate decision, leading to sub-optimum outcomes .
 This is an issue that has been presented as a major problem with bureaucratic organizations that lose the economies of strategic change because of entrenched attitudes.
Strategic Information Management
Background
According to the Carnegie Mellon School an organization's ability to process information is at the core of organizational and managerial competency, and an organization's strategies must be designed to improve information processing capability [14] and as information systems that provide that capability became formalised and automated, competencies were severely tested at many levels.
 It was recognised that organisations needed to be able to learn and adapt in ways that were never so evident before [16] and academics began to organise and publish definitive works concerning the strategic management of information, and information systems.
 Concurrently, the ideas of business process management [18] and knowledge management [19] although much of the optimistic early thinking about business process redesign has since been discredited in the information management literature.[20]
Aligning technology and business strategy with information management
Venkatraman has provided a simple view of the requisite capabilities of an organisation that wants to manage information well – the DIKAR model (see above). 
He also worked with others to understand how technology and business strategies could be appropriately aligned in order to identify specific capabilities that are needed.
 This work was paralleled by other writers in the world of consulting,[22] practice [23] and academia.[24]
A contemporary portfolio model for information
Bytheway has collected and organised basic tools and techniques for information management in a single volume.
At the heart of his view of information management is a portfolio model that takes account of the surging interest in external sources of information and the need to organise un-structured information external so as to make it useful (see the figure).
This portfolio model organizes issues of internal and external sourcing and management of information, that may be either structured or unstructured.
Such an information portfolio as this shows how information can be gathered and usefully organised, in four stages:
Stage 1: Taking advantage of public information: recognise and adopt well-structured external schemes of reference data, such as post codes, weather data, GPS positioning data and travel timetables, exemplified in the personal computing press.[25]
Stage 2: Tagging the noise on the world wide web: use existing schemes such as post codes and GPS data or more typically by adding “tags”, or construct a formal ontology that provides structure. 
Shirky provides an overview of these two approaches.
Stage 3: Sifting and analysing: in the wider world the generalised ontologies that are under development extend to hundreds of entities and hundreds of relations between them and provide the means to elicit meaning from large volumes of data. 
Structured data in databases works best when that structure reflects a higher-level information model – an ontology, or an entity-relationship model.
Stage 4: Structuring and archiving: with the large volume of data available from sources such as the social web and from the miniature telemetry systems used in personal health management, new ways to archive and then trawl data for meaningful information.
 Map-reduce methods, originating from functional programming, are a more recent way of eliciting information from large archival datasets that is becoming interesting to regular businesses that have very large data resources to work with, but it requires advanced multi-processor resources.[28]
Competencies to manage information well
The Information Management Body of Knowledge was made available on the world wide web in 2004 [29] and sets out to show that the required management competencies to derive real benefits from an investment in information are complex and multi-layered. 
The framework model that is the basis for understanding competencies comprises six “knowledge” areas and four “process” areas:
This framework is the basis of organising the "Information Management Body of Knowledge" first made available in 2004. 
This version is adapted by the addition of "Business information" in 2014.
The information management knowledge areas:
The IMBOK is based on the argument that there are six areas of required management competency, two of which (“business process management” and “business information management”) are very closely related.[30]
Information technology: The pace of change of technology and the pressure to constantly acquire the newest technological products can undermine the stability of the infrastructure that supports systems, and thereby optimises business processes and delivers benefits. 
It is necessary to manage the “supply side” and recognise that technology is, increasingly, becoming a commodity.
Information system: While historically information systems were developed in-house, over the years it has become possible to acquire most of the software systems that an organisation needs from the software package industry. 
However, there is still the potential for competitive advantage from the implementation of new systems ideas that deliver to the strategic intentions of organisations.
Business processes and Business information: Information systems are applied to business processes in order to improve them, and they bring data to the business that becomes useful as business information. 
Business process management is still seen as a relatively new idea because it is not universally adopted, and it has been difficult in many cases; business information management is even more of a challenge.[20][32]
Business benefit: What are the benefits that we are seeking? It is necessary not only to be brutally honest about what can be achieved, but also to ensure the active management and assessment of benefit delivery.
 Since the emergence and popularisation of the Balanced scorecard [33] there has been huge interest in business performance management but not much serious effort has been made to relate business performance management to the benefits of information technology investments and the introduction of new information systems until the turn of the millennium.[24]
Business strategy: Although a long way from the workaday issues of managing information in organisations, strategy in most organisations simply has to be informed by information technology and information systems opportunities, whether to address poor performance or to improve differentiation and competitiveness. 
Strategic analysis tools such as the value chain and critical success factor analysis are directly dependent on proper attention to the information that is (or could be) managed [4]
The information management processes:
Even with full capability and competency within the six knowledge areas, it is argued that things can still go wrong. The problem lies in the migration of ideas and information management value from one area of competency to another. 
Summarising what Bytheway explains in some detail (and supported by selected secondary references):[34]
Projects: Information technology is without value until it is engineered into information systems that meet the needs of the business by means of good project management.[35]
Business change: The best information systems succeed in delivering benefits through the achievement of change within the business systems, but people do not appreciate change that makes new demands upon their skills in the ways that new information systems often do. 
Contrary to common expectations, there is some evidence that the public sector has succeeded with information technology induced business change.[36]
Business operations: With new systems in place, with business processes and business information improved, and with staff finally ready and able to work with new processes, then the business can get to work, even when new systems extend far beyond the boundaries of a single business.[37]
Performance management: Investments are no longer solely about financial results, financial success must be balanced with internal efficiency, customer satisfaction, and with organisational learning and development.[33]
Summary
There are always many ways to see a business, and the information management viewpoint is only one way.
 It is important to remember that other areas of business activity will also contribute to strategy – it is not only good information management that moves a business forwards. 
Corporate governance, human resource management, product development and marketing will all have an important role to play in strategic ways, and we must not see one domain of activity alone as the sole source of strategic success.
 On the other hand, corporate governance, human resource management, product development and marketing are all dependent on effective information management, and so in the final analysis our competency to manage information well, on the broad basis that is offered here, can be said to be predominant.
Operationalising Information Management
Managing requisite change
Organizations are often confronted with many information management challenges and issues at the operational level, especially when organisational change is engendered. 
The novelty of new systems architectures and a lack of experience with new styles of information management requires a level of organisational change management that is notoriously difficult to deliver. 
The early work of Galbraith
In early work, taking an information processing view of organisation design, Jay Galbraith has identified five tactical areas to increase information processing capacity and reduce the need for information processing.[39]
Developing, implementing, and monitoring all aspects of the “environment” of an organization.
Creation of slack resources so as to decrease the load on the overall hierarchy of resources and to reduce information processing relating to overload.
Creation of self-contained tasks with defined boundaries and that can achieve proper closure, and with all the resources at hand required to perform the task.
Recognition of lateral relations that cut across functional units, so as to move decision power to the process instead of fragmenting it within the hierarchy.
Investment in vertical information systems that route information flows for a specific task (or set of tasks) in accordance to the applied business logic.
The matrix organisation
The lateral relations concept leads to an organizational form that is different from the simple hierarchy, the “matrix organization”. 
This brings together the vertical (hierarchical) view of an organisation and the horizontal (product or project) view of the work that it does visible to the outside world. 
The creation of a matrix organization is one management response to a persistent fluidity of external demand, avoiding multifarious and spurious responses to episodic demands that tend to be dealt with individually.
In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. 
Given a set of training examples, each marked for belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier. 
An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible.
 New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are not labeled, a supervised learning is not possible, and an unsupervised learning is required, that would find natural clustering of the data to groups, and map new data to these formed groups. 
The clustering algorithm which provides an improvement to the support vector machines is called support vector clustering (SVC[2]) and is highly used in industrial applications either when data is not labeled or when only some data is labeled as a preprocessing for a classification pass; the clustering method was published.
More formally, a support vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks.
 Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.
Whereas the original problem may be stated in a finite dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. 
For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. 
To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function k(x,y) selected to suit the problem.
 The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant. 
The vectors defining the hyperplanes can be chosen to be linear combinations with parameters \alpha_i of images of feature vectors x_i that occur in the data base. 
With this choice of a hyperplane, the points x in the feature space that are mapped into the hyperplane are defined by the relation: \textstyle\sum_i \alpha_i k(x_i,x) = \mathrm{constant}.
 Note that if k(x,y) becomes small as y grows further away from x, each term in the sum measures the degree of closeness of the test point x to the corresponding data base point x_i. 
In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in one or the other of the sets to be discriminated. 
Note the fact that the set of points x mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets which are not convex at all in the original space.
The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963. 
In 1992, Bernhard E. Boser, Isabelle M. Guyon and Vladimir N. Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes.
 The current standard incarnation (soft margin) was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.[1]
Motivation
H1 does not separate the classes.
 H2 does, but only with a small margin. 
H3 separates them with the maximum margin.
Classifying data is a common task in machine learning. 
Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. 
In the case of support vector machines, a data point is viewed as a p-dimensional vector (a list of p numbers), and we want to know whether we can separate such points with a (p-1)-dimensional hyperplane. 
This is called a linear classifier. 
There are many hyperplanes that might classify the data. 
One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes.
 So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized.
 If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum margin classifier; or equivalently, the perceptron of optimal stability
In statistical modeling, regression analysis is a statistical process for estimating the relationships among variables. 
It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors'). 
More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed.
 Most commonly, regression analysis estimates the conditional expectation of the dependent variable given the independent variables – that is, the average value of the dependent variable when the independent variables are fixed.
 Less commonly, the focus is on a quantile, or other location parameter of the conditional distribution of the dependent variable given the independent variables. 
In all cases, the estimation target is a function of the independent variables called the regression function. 
In regression analysis, it is also of interest to characterize the variation of the dependent variable around the regression function which can be described by a probability distribution.
Regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. 
Regression analysis is also used to understand which among the independent variables are related to the dependent variable, and to explore the forms of these relationships. 
In restricted circumstances, regression analysis can be used to infer causal relationships between the independent and dependent variables. 
However this can lead to illusions or false relationships, so caution is advisable;[1] for example, correlation does not imply causation.
Many techniques for carrying out regression analysis have been developed. 
Familiar methods such as linear regression and ordinary least squares regression are parametric, in that the regression function is defined in terms of a finite number of unknown parameters that are estimated from the data.
 Nonparametric regression refers to techniques that allow the regression function to lie in a specified set of functions, which may be infinite-dimensional.
The performance of regression analysis methods in practice depends on the form of the data generating process, and how it relates to the regression approach being used. 
Since the true form of the data-generating process is generally not known, regression analysis often depends to some extent on making assumptions about this process. 
these assumptions are sometimes testable if a sufficient quantity of data is available. 
Regression models for prediction are often useful even when the assumptions are moderately violated, although they may not perform optimally. 
However, in many applications, especially with small effects or questions of causality based on observational data, regression methods can give misleading results.[2][3]
In a narrower sense, regression may refer specifically to the estimation of continuous response variables, as opposed to the discrete response variables used in classification.[4] 
The case of a continuous output variable may be more specifically referred to as metric regression to distinguish it from related problems.
The earliest form of regression was the method of least squares, which was published by Legendre in 1805,[6] and by Gauss in 1809.[7]
 Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the Sun (mostly comets, but also later the then newly discovered minor planets). 
Gauss published a further development of the theory of least squares in 1821,[8] including a version of the Gauss–Markov theorem.
The term "regression" was coined by Francis Galton in the nineteenth century to describe a biological phenomenon.
 The phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as regression toward the mean).
 For Galton, regression had only this biological meaning,[11][12] but his work was later extended by Udny Yule and Karl Pearson to a more general statistical context.
 In the work of Yule and Pearson, the joint distribution of the response and explanatory variables is assumed to be Gaussian. 
This assumption was weakened by R.A. Fisher in his works of 1922 and 1925.
 Fisher assumed that the conditional distribution of the response variable is Gaussian, but the joint distribution need not be.
 In this respect, Fisher's assumption is closer to Gauss's formulation of 1821.
In the 1950s and 1960s, economists used electromechanical desk calculators to calculate regressions. 
Before 1970, it sometimes took up to 24 hours to receive the result from one regression.[18]
Regression methods continue to be an area of active research. 
Regression models
Regression models involve the following variables:
The unknown parameters, denoted as β, which may represent a scalar or a vector.
The independent variables, X.
The dependent variable, Y.
In various fields of application, different terminologies are used in place of dependent and independent variables.
A regression model relates Y to a function of X and β.
Y \approx f (\mathbf {X}, \boldsymbol{\beta} )
The approximation is usually formalized as E(Y | X) = f(X, β).
 To carry out regression analysis, the form of the function f must be specified. 
Sometimes the form of this function is based on knowledge about the relationship between Y and X that does not rely on the data. 
If no such knowledge is available, a flexible or convenient form for f is chosen.
Assume now that the vector of unknown parameters β is of length k.
 In order to perform a regression analysis the user must provide information about the dependent variable Y:
If N data points of the form (Y, X) are observed, where N < k, most classical approaches to regression analysis cannot be performed: since the system of equations defining the regression model is underdetermined, there are not enough data to recover β.
If exactly N = k data points are observed, and the function f is linear, the equations Y = f(X, β) can be solved exactly rather than approximately. 
This reduces to solving a set of N equations with N unknowns (the elements of β), which has a unique solution as long as the X are linearly independent. 
If f is nonlinear, a solution may not exist, or many solutions may exist.
The most common situation is where N > k data points are observed.
 In this case, there is enough information in the data to estimate a unique value for β that best fits the data in some sense, and the regression model when applied to the data can be viewed as an overdetermined system in β.
In the last case, the regression analysis provides the tools for:
Finding a solution for unknown parameters β that will, for example, minimize the distance between the measured and predicted values of the dependent variable Y (also known as method of least squares).
Under certain statistical assumptions, the regression analysis uses the surplus of information to provide statistical information about the unknown parameters β and predicted values of the dependent variable Y.
Necessary number of independent measurements
Consider a regression model which has three unknown parameters, β0, β1, and β2. 
Suppose an experimenter performs 10 measurements all at exactly the same value of independent variable vector X (which contains the independent variables X1, X2, and X3). 
In this case, regression analysis fails to give a unique set of estimated values for the three unknown parameters; the experimenter did not provide enough information. 
The best one can do is to estimate the average value and the standard deviation of the dependent variable Y.
Similarly, measuring at two different values of X would give enough data for a regression with two unknowns, but not for three or more unknowns.
If the experimenter had performed measurements at three different values of the independent variable vector X, then regression analysis would provide a unique set of estimates for the three unknown parameters in β.
In the case of general linear regression, the above statement is equivalent to the requirement that the matrix XTX is invertible.
Statistical assumptions
When the number of measurements, N, is larger than the number of unknown parameters, k, and the measurement errors εi are normally distributed then the excess of information contained in (N − k) measurements is used to make statistical predictions about the unknown parameters. 
This excess of information is referred to as the degrees of freedom of the regression.
Underlying assumptions
Classical assumptions for regression analysis include:
The sample is representative of the population for the inference prediction.
The error is a random variable with a mean of zero conditional on the explanatory variables.
The independent variables are measured with no error. (Note: If this is not so, modeling may be done instead using errors-in-variables model techniques).
The independent variables (predictors) are linearly independent, i.e. it is not possible to express any predictor as a linear combination of the others.
The errors are uncorrelated, that is, the variance–covariance matrix of the errors is diagonal and each non-zero element is the variance of the error.
The variance of the error is constant across observations (homoscedasticity). If not, weighted least squares or other methods might instead be used.
These are sufficient conditions for the least-squares estimator to possess desirable properties; in particular, these assumptions imply that the parameter estimates will be unbiased, consistent, and efficient in the class of linear unbiased estimators. 
It is important to note that actual data rarely satisfies the assumptions. 
That is, the method is used even though the assumptions are not true. 
Variation from the assumptions can sometimes be used as a measure of how far the model is from being useful. 
Many of these assumptions may be relaxed in more advanced treatments.
 Reports of statistical analyses usually include analyses of tests on the sample data and methodology for the fit and usefulness of the model.
Assumptions include the geometrical support of the variables.
 Independent and dependent variables often refer to values measured at point locations. 
There may be spatial trends and spatial autocorrelation in the variables that violate statistical assumptions of regression. 
Geographic weighted regression is one technique to deal with such data.
Also, variables may include values aggregated by areas. 
With aggregated data the modifiable areal unit problem can cause extreme variation in regression parameters.[21] When analyzing data aggregated by political boundaries, postal codes or census areas results may be very distinct with a different choice of units.
Linear regression
Main article: Linear regression
See simple linear regression for a derivation of these formulas and a numerical example
In linear regression, the model specification is that the dependent variable,  y_i  is a linear combination of the parameters (but need not be linear in the independent variables). 
For example, in simple linear regression for modeling  n  data points there is one independent variable:  x_i , and two parameters, \beta_0 and \beta_1:
straight line: y_i=\beta_0 +\beta_1 x_i +\varepsilon_i,\quad i=1,\dots,n.\!
In multiple linear regression, there are several independent variables or functions of independent variables.
Adding a term in xi2 to the preceding regression gives:
parabola: y_i=\beta_0 +\beta_1 x_i +\beta_2 x_i^2+\varepsilon_i,\ i=1,\dots,n.\!
This is still linear regression; although the expression on the right hand side is quadratic in the independent variable x_i, it is linear in the parameters \beta_0, \beta_1 and \beta_2.
In both cases, \varepsilon_i is an error term and the subscript i indexes a particular observation.
Returning our attention to the straight line case: Given a random sample from the population, we estimate the population parameters and obtain the sample linear regression model:
 \widehat{y_i} = \widehat{\beta}_0 + \widehat{\beta}_1 x_i. 
The residual,  e_i = y_i - \widehat{y}_i , is the difference between the value of the dependent variable predicted by the model,  \widehat{y_i}, and the true value of the dependent variable, y_i.
 One method of estimation is ordinary least squares. 
This method obtains parameter estimates that minimize the sum of squared residuals, SSE,[22][23] also sometimes denoted RSS:
SSE=\sum_{i=1}^n e_i^2. \, 
Minimization of this function results in a set of normal equations, a set of simultaneous linear equations in the parameters, which are solved to yield the parameter estimators, \widehat{\beta}_0, \widehat{\beta}_1.
Illustration of linear regression on a data set.
In the case of simple regression, the formulas for the least squares estimates are
\widehat{\beta_1}=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}\text{ and }\hat{\beta_0}=\bar{y}-\widehat{\beta_1}\bar{x}
where \bar{x} is the mean (average) of the x values and \bar{y} is the mean of the y values.
Under the assumption that the population error term has a constant variance, the estimate of that variance is given by:
 \hat{\sigma}^2_\varepsilon = \frac{SSE}{n-2}.\,
This is called the mean square error (MSE) of the regression.
 The denominator is the sample size reduced by the number of model parameters estimated from the same data, (n-p) for p regressors or (n-p-1) if an intercept is used.
In this case, p=1 so the denominator is n-2.
The standard errors of the parameter estimates are given by
\hat\sigma_{\beta_0}=\hat\sigma_{\varepsilon} \sqrt{\frac{1}{n} + \frac{\bar{x}^2}{\sum(x_i-\bar x)^2}}
\hat\sigma_{\beta_1}=\hat\sigma_{\varepsilon} \sqrt{\frac{1}{\sum(x_i-\bar x)^2}}.
Under the further assumption that the population error term is normally distributed, the researcher can use these estimated standard errors to create confidence intervals and conduct hypothesis tests about the population parameters.
General linear model
For a derivation, see linear least squares
For a numerical example, see linear regression
In the more general multiple regression model, there are p independent variables:
 y_i = \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \varepsilon_i, \, 
where xij is the ith observation on the jth independent variable, and where the first independent variable takes the value 1 for all i (so \beta_1 is the regression intercept).
The least squares parameter estimates are obtained from p normal equations. The residual can be written as
\varepsilon_i=y_i -  \hat\beta_1 x_{i1} - \cdots - \hat\beta_p x_{ip}.
The normal equations are
\sum_{i=1}^n \sum_{k=1}^p X_{ij}X_{ik}\hat \beta_k=\sum_{i=1}^n X_{ij}y_i,\  j=1,\dots,p.\,
In matrix notation, the normal equations are written as
\mathbf{(X^\top X )\hat{\boldsymbol{\beta}}= {}X^\top Y},\,
where the ij element of X is xij, the i element of the column vector Y is yi, and the j element of \hat \beta is \hat \beta_j. 
Thus X is n×p, Y is n×1, and \hat \beta is p×1. The solution is
\mathbf{\hat{\boldsymbol{\beta}}= {}(X^\top X )^{-1}X^\top Y}.\,
Diagnostics
Main article: Regression diagnostics
See also: Category:Regression diagnostics.
Once a regression model has been constructed, it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters. 
Commonly used checks of goodness of fit include the R-squared, analyses of the pattern of residuals and hypothesis testing.
Statistical significance can be checked by an F-test of the overall fit, followed by t-tests of individual parameters.
Interpretations of these diagnostic tests rest heavily on the model assumptions.
 Although examination of the residuals can be used to invalidate a model, the results of a t-test or F-test are sometimes more difficult to interpret if the model's assumptions are violated.
 For example, if the error term does not have a normal distribution, in small samples the estimated parameters will not follow normal distributions and complicate inference. 
With relatively large samples, however, a central limit theorem can be invoked such that hypothesis testing may proceed using asymptotic approximations.
"Limited dependent" variables
The phrase "limited dependent" is used in econometric statistics for categorical and constrained variables.
The response variable may be non-continuous ("limited" to lie on some subset of the real line). 
For binary (zero or one) variables, if analysis proceeds with least-squares linear regression, the model is called the linear probability model. 
Nonlinear models for binary dependent variables include the probit and logit model. 
The multivariate probit model is a standard method of estimating a joint relationship between several binary dependent variables and some independent variables.
For categorical variables with more than two values there is the multinomial logit.
 For ordinal variables with more than two values, there are the ordered logit and ordered probit models. 
Censored regression models may be used when the dependent variable is only sometimes observed, and Heckman correction type models may be used when the sample is not randomly selected from the population of interest. 
An alternative to such procedures is linear regression based on polychoric correlation (or polyserial correlations) between the categorical variables. 
Such procedures differ in the assumptions made about the distribution of the variables in the population. 
If the variable is positive with low values and represents the repetition of the occurrence of an event, then count models like the Poisson regression or the negative binomial model may be used instead.
Interpolation and extrapolation
Regression models predict a value of the Y variable given known values of the X variables. 
Prediction within the range of values in the dataset used for model-fitting is known informally as interpolation. 
Prediction outside this range of the data is known as extrapolation. 
Performing extrapolation relies strongly on the regression assumptions. 
The further the extrapolation goes outside the data, the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values.
It is generally advised[citation needed] that when performing extrapolation, one should accompany the estimated value of the dependent variable with a prediction interval that represents the uncertainty. 
Such intervals tend to expand rapidly as the values of the independent variable(s) moved outside the range covered by the observed data.
For such reasons and others, some tend to say that it might be unwise to undertake extrapolation.[25]
However, this does not cover the full set of modelling errors that may be being made: in particular, the assumption of a particular form for the relation between Y and X.
 A properly conducted regression analysis will include an assessment of how well the assumed form is matched by the observed data, but it can only do so within the range of values of the independent variables actually available.
 This means that any extrapolation is particularly reliant on the assumptions being made about the structural form of the regression relationship. 
Best-practice advice here[citation needed] is that a linear-in-variables and linear-in-parameters relationship should not be chosen simply for computational convenience, but that all available knowledge should be deployed in constructing a regression model. 
If this knowledge includes the fact that the dependent variable cannot go outside a certain range of values, this can be made use of in selecting the model – even if the observed dataset has no values particularly near such bounds. 
The implications of this step of choosing an appropriate functional form for the regression can be great when extrapolation is considered. 
At a minimum, it can ensure that any extrapolation arising from a fitted model is "realistic" (or in accord with what is known).
Nonlinear regression
Main article: Nonlinear regression
When the model function is not linear in the parameters, the sum of squares must be minimized by an iterative procedure. 
This introduces many complications which are summarized in Differences between linear and non-linear least squares
Power and sample size calculations
There are no generally agreed methods for relating the number of observations versus the number of independent variables in the model.
 One rule of thumb suggested by Good and Hardin is N=m^n, where N is the sample size, n is the number of independent variables and m is the number of observations needed to reach the desired precision if the model had only one independent variable.
For example, a researcher is building a linear regression model using a dataset that contains 1000 patients (N). 
If the researcher decides that five observations are needed to precisely define a straight line (m), then the maximum number of independent variables the model can support is 4, because
\frac{\log{1000}}{\log{5}}=4.29.
Other methods
Although the parameters of a regression model are usually estimated using the method of least squares, other methods which have been used include:
Bayesian methods, e.g. Bayesian linear regression
Percentage regression, for situations where reducing percentage errors is deemed more appropriate.[27]
Least absolute deviations, which is more robust in the presence of outliers, leading to quantile regression
Nonparametric regression, requires a large number of observations and is computationally intensive
Distance metric learning, which is learned by the search of a meaningful distance metric in a given input space.
In computer science and operations research, the Ant Colony Optimization algorithm (ACO) is a probabilistic technique for solving computational problems which can be reduced to finding good paths through graphs.
This algorithm is a member of the Ant Colony Algorithms family, in swarm intelligence methods, and it constitutes some metaheuristic optimizations. 
Initially proposed by Marco Dorigo in 1992 in his PhD thesis,[1][2] the first algorithm was aiming to search for an optimal path in a graph, based on the behavior of ants seeking a path between their colony and a source of food. 
The original idea has since diversified to solve a wider class of numerical problems, and as a result, several problems have emerged, drawing on various aspects of the behavior of ants.
In the natural world, ants (initially) wander randomly, and upon finding food return to their colony while laying down pheromone trails. 
If other ants find such a path, they are likely not to keep travelling at random, but instead to follow the trail, returning and reinforcing it if they eventually find food (see Ant communication).
Over time, however, the pheromone trail starts to evaporate, thus reducing its attractive strength. 
The more time it takes for an ant to travel down the path and back again, the more time the pheromones have to evaporate. 
A short path, by comparison, gets marched over more frequently, and thus the pheromone density becomes higher on shorter paths than longer ones. 
Pheromone evaporation also has the advantage of avoiding the convergence to a locally optimal solution. 
If there were no evaporation at all, the paths chosen by the first ants would tend to be excessively attractive to the following ones. 
In that case, the exploration of the solution space would be constrained.
Thus, when one ant finds a good (i.e., short) path from the colony to a food source, other ants are more likely to follow that path, and positive feedback eventually leads to all the ants following a single path. 
The idea of the ant colony algorithm is to mimic this behavior with "simulated ants" walking around the graph representing the problem to solve.
Common extensions
Here are some of the most popular variations of ACO algorithms.
Elitist ant system
The global best solution deposits pheromone on every iteration along with all the other ants.
Max-min ant system (MMAS)
Added maximum and minimum pheromone amounts [τmax,τmin]. 
Only global best or iteration best tour deposited pheromone <MAZ>. 
All edges are initialized to τmax and reinitialized to τmax when nearing stagnation.[3]
Ant colony system
It has been presented above.[4]
Rank-based ant system (ASrank)
All solutions are ranked according to their length. 
The amount of pheromone deposited is then weighted for each solution, such that solutions with shorter paths deposit more pheromone than the solutions with longer paths.
Continuous orthogonal ant colony (COAC)
The pheromone deposit mechanism of COAC is to enable ants to search for solutions collaboratively and effectively. 
By using an orthogonal design method, ants in the feasible domain can explore their chosen regions rapidly and efficiently, with enhanced global search capability and accuracy.
The orthogonal design method and the adaptive radius adjustment method can also be extended to other optimization algorithms for delivering wider advantages in solving practical problems.[5]
Recursive ant colony optimization
It is a recursive form of ant system which divides the whole search domain into several sub-domains and solves the objective on these subdomains.
The results from all the subdomains are compared and the best few of them are promoted for the next level. 
The subdomains corresponding to the selected results are further subdivided and the process is repeated till an output of desired precision is obtained.
 This method has been tested on ill-posed geophysical inversion problems and works well.
Convergence
For some versions of the algorithm, it is possible to prove that it is convergent (i.e., it is able to find the global optimum in finite time).
 The first evidence of a convergence ant colony algorithm was made in 2000, the graph-based ant system algorithm, and then algorithms for ACS and MMAS. 
Like most metaheuristics, it is very difficult to estimate the theoretical speed of convergence. 
In 2004, Zlochin and his colleagues[8] showed that COA-type algorithms could be assimilated methods of stochastic gradient descent, on the cross-entropy and estimation of distribution algorithm. 
They proposed these metaheuristics as a "research-based model".
 A performance analysis of continuous ant colony algorithm based on its various parameter suggest its sensitivity of convergence on parameter tuning.
It has also been used to produce near-optimal solutions to the travelling salesman problem. 
They have an advantage over simulated annealing and genetic algorithm approaches of similar problems when the graph may change dynamically; the ant colony algorithm can be run continuously and adapt to changes in real time.
 This is of interest in network routing and urban transportation systems.
The first ACO algorithm was called the ant system[10] and it was aimed to solve the travelling salesman problem, in which the goal is to find the shortest round-trip to link a series of cities. 
The general algorithm is relatively simple and based on a set of ants, each making one of the possible round-trips along the cities. 
At each stage, the ant chooses to move from one city to another according to some rules:
It must visit each city exactly once;
A distant city has less chance of being chosen (the visibility);
The more intense the pheromone trail laid out on an edge between two cities, the greater the probability that that edge will be chosen;
Having completed its journey, the ant deposits more pheromones on all edges it traversed, if the journey is short;
After each iteration, trails of pheromones evaporate.
In computer science, particle swarm optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. 
PSO optimizes a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. 
Each particle's movement is influenced by its local best known position but, is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. 
This is expected to move the swarm toward the best solutions.
PSO is originally attributed to Kennedy, Eberhart and Shi[1][2] and was first intended for simulating social behaviour,[3] as a stylized representation of the movement of organisms in a bird flock or fish school. 
The algorithm was simplified and it was observed to be performing optimization. 
The book by Kennedy and Eberhart[4] describes many philosophical aspects of PSO and swarm intelligence An extensive survey of PSO applications is made by Poli.
PSO is a metaheuristic as it makes few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. 
However, metaheuristics such as PSO do not guarantee an optimal solution is ever found. 
More specifically, PSO does not use the gradient of the problem being optimized, which means PSO does not require that the optimization problem be differentiable as is required by classic optimization methods such as gradient descent and quasi-newton methods.
A basic variant of the PSO algorithm works by having a population (called a swarm) of candidate solutions (called particles). 
These particles are moved around in the search-space according to a few simple formulae.
 The movements of the particles are guided by their own best known position in the search-space as well as the entire swarm's best known position.
 When improved positions are being discovered these will then come to guide the movements of the swarm. 
The process is repeated and by doing so it is hoped, but not guaranteed, that a satisfactory solution will eventually be discovered.
Formally, let f: ℝn → ℝ be the cost function which must be minimized. 
The function takes a candidate solution as argument in the form of a vector of real numbers and produces a real number as output which indicates the objective function value of the given candidate solution. 
The gradient of f is not known. 
The goal is to find a solution a for which f(a) ≤ f(b) for all b in the search-space, which would mean a is the global minimum.
 Maximization can be performed by considering the function h = -f instead.
Let S be the number of particles in the swarm, each having a position xi ∈ ℝn in the search-space and a velocity vi ∈ ℝn. 
Let pi be the best known position of particle i and let g be the best known position of the entire swarm. 
A basic PSO algorithm is then:[7]
For each particle i = 1, ..., S do:
Initialize the particle's position with a uniformly distributed random vector: xi ~ U(blo, bup), where blo and bup are the lower and upper boundaries of the search-space.
Initialize the particle's best known position to its initial position: pi ← xi
If (f(pi) < f(g)) update the swarm's best known position: g ← pi
Initialize the particle's velocity: vi ~ U(-|bup-blo|, |bup-blo|)
Until a termination criterion is met (e.g. number of iterations performed, or a solution with adequate objective function value is found), repeat:
For each particle i = 1, ..., S do:
For each dimension d = 1, ..., n do:
Pick random numbers: rp, rg ~ U(0,1)
Update the particle's velocity: vi,d ← ω vi,d + φp rp (pi,d-xi,d) + φg rg (gd-xi,d)
Update the particle's position: xi ← xi + vi
If (f(xi) < f(pi)) do:
Update the particle's best known position: pi ← xi
If (f(pi) < f(g)) update the swarm's best known position: g ← pi
Now g holds the best found solution.
The parameters ω, φp, and φg are selected by the practitioner and control the behaviour and efficacy of the PSO method, see below.
Parameter selection
Performance landscape showing how a simple PSO variant performs in aggregate on several benchmark problems when varying two PSO parameters.
The choice of PSO parameters can have a large impact on optimization performance. 
Selecting PSO parameters that yield good performance has therefore been the subject of much research.
The PSO parameters can also be tuned by using another overlaying optimizer, a concept known as meta-optimization.
 Parameters have also been tuned for various optimization scenarios.
Neighborhoods and Topologies
The basic PSO is easily trapped into a local minimum. 
This premature convergence can be avoided by not using the entire swarm's best known position g but just the best known position l of a sub-swarm "around" the particle that is moved. 
Such a sub-swarm can be a geometrical one - for example "the m nearest particles" - or, more often, a social one, i.e. a set of particles that is not depending on any distance. 
In such a case, the PSO variant is said to be local best (vs global best for the basic PSO).
If we suppose there is an information link between each particle and its neighbours, the set of these links builds a graph, a communication network, that is called the topology of the PSO variant. 
A commonly used social topology is the ring, in which each particle has just two neighbours, but there are many others.
 The topology is not necessarily fixed, and can be adaptive (SPSO,[22] stochastic star,[23] TRIBES,[24] Cyber Swarm,[25] C-PSO[26]).
Inner workings
There are several schools of thought as to why and how the PSO algorithm can perform optimization.
A common belief amongst researchers is that the swarm behaviour varies between exploratory behaviour, that is, searching a broader region of the search-space, and exploitative behaviour, that is, a locally oriented search so as to get closer to a (possibly local) optimum.
This school of thought has been prevalent since the inception of PSO.
This school of thought contends that the PSO algorithm and its parameters must be chosen so as to properly balance between exploration and exploitation to avoid premature convergence to a local optimum yet still ensure a good rate of convergence to the optimum. 
This belief is the precursor of many PSO variants, see below.
Another school of thought is that the behaviour of a PSO swarm is not well understood in terms of how it affects actual optimization performance, especially for higher-dimensional search-spaces and optimization problems that may be discontinuous, noisy, and time-varying. 
This school of thought merely tries to find PSO algorithms and parameters that cause good performance regardless of how the swarm behaviour can be interpreted in relation to e.g. exploration and exploitation. 
Such studies have led to the simplification of the PSO algorithm, see below.
Convergence
In relation to PSO the word convergence typically refers to two different definitions:
Convergence of the sequence of solutions (aka, stability analysis, converging) in which all particles have converged to a point in the search-space, which may or may not be the optimum,
Convergence to a local optimum where all personal bests p or, alternatively, the swarm's best known position g, approaches a local optimum of the problem, regardless of how the swarm behaves.
Convergence of the sequence of solutions has been investigated for PSO.
 These analyses have resulted in guidelines for selecting PSO parameters that are believed to cause convergence to a point and prevent divergence of the swarm's particles (particles do not move unboundedly and will converge to somewhere). 
However, the analyses were criticized by Pedersen[19] for being oversimplified as they assume the swarm has only one particle, that it does not use stochastic variables and that the points of attraction, that is, the particle's best known position p and the swarm's best known position g, remain constant throughout the optimization process. 
However, it was shown [27] that these simplifications do not affect the boundaries found by these studies for parameter where the swarm is convergent.
Convergence to a local optimum has been analyzed for PSO in [28] and.
It has been proven that PSO need some modification to guarantee to find a local optimum.
This means that determining convergence capabilities of different PSO algorithms and parameters therefore still depends on empirical results.
 One attempt at addressing this issue is the development of an "orthogonal learning" strategy for an improved use of the information already existing in the relationship between p and g, so as to form a leading converging exemplar and to be effective with any PSO topology. 
The aims are to improve the performance of PSO overall, including faster global convergence, higher solution quality, and stronger robustness.
 However, such studies do not provide theoretical evidence to actually prove their claims.
Biases
As the basic PSO works dimension by dimension, the solution point is easier found when it lies on an axis of the search space, on a diagonal, and even easier if it is right on the centre.[31][32]
One approach is to modify the algorithm so that it is not any more sensitive to the system of coordinates.
 Note that some of these methods have a higher computational complexity (are in O(n^2) where n is the number of dimensions) that make the algorithm very slow for large scale optimization.[29]
The only currently existing PSO variant that is not sensitive to the rotation of the coordinates while is locally convergent has been proposed at 2014.
 The method has shown a very good performance on many benchmark problems while its rotation invariance and local convergence have been mathematically proven.
Variants
Numerous variants of even a basic PSO algorithm are possible.
 For example, there are different ways to initialize the particles and velocities (e.g. start with zero velocities instead), how to dampen the velocity, only update pi and g after the entire swarm has been updated, etc. 
Some of these choices and their possible performance impact have been discussed in the literature.[11]
A series of standard implementations have been created by leading researchers, "intended for use both as a baseline for performance testing of improvements to the technique, as well as to represent PSO to the wider optimization community. 
Having a well-known, strictly-defined standard algorithm provides a valuable point of comparison which can be used throughout the field of research to better test new advances." 
 The latest is Standard PSO 2011 (SPSO-2011).[38]
Hybridization
New and more sophisticated PSO variants are also continually being introduced in an attempt to improve optimization performance. 
There are certain trends in that research; one is to make a hybrid optimization method using PSO combined with other optimizers,[39][40][41] e.g., combined PSO with biogeography-based optimization,[42] and the incorporation of an effective learning method.[30]
Alleviate Premature
Another research trend is to try and alleviate premature convergence (that is, optimization stagnation), e.g. by reversing or perturbing the movement of the PSO particles,[16][43][44][45] another approach to deal with premature convergence is the use of multiple swarms[46] (multi-swarm optimization).
The multi-swarm approach can also be used to implement multi-objective optimization.
 Finally, there are developments in adapting the behavioural parameters of PSO during optimization.
Simplifications
Another school of thought is that PSO should be simplified as much as possible without impairing its performance; a general concept often referred to as Occam's razor.
 Simplifying PSO was originally suggested by Kennedy[3] and has been studied more extensively,[15][18][19][49] where it appeared that optimization performance was improved, and the parameters were easier to tune and they performed more consistently across different optimization problems.
Another argument in favour of simplifying PSO is that metaheuristics can only have their efficacy demonstrated empirically by doing computational experiments on a finite number of optimization problems. 
This means a metaheuristic such as PSO cannot be proven correct and this increases the risk of making errors in its description and implementation.
 A good example of this[50] presented a promising variant of a genetic algorithm (another popular metaheuristic) but it was later found to be defective as it was strongly biased in its optimization search towards similar values for different dimensions in the search space, which happened to be the optimum of the benchmark problems considered. 
This bias was because of a programming error, and has now been fixed.
Initialization of velocities may require extra inputs. 
A simpler variant is the accelerated particle swarm optimization (APSO),[52] which does not need to use velocity at all and can speed up the convergence in many applications. 
A simple demo code of APSO is available.[53]
Multi-objective optimization
PSO has also been applied to multi-objective problems,[54][55] in which the objective function comparison takes pareto dominance into account when moving the PSO particles and non-dominated solutions are stored so as to approximate the pareto front.
Binary, Discrete, and Combinatorial PSO
As the PSO equations given above work on real numbers, a commonly used method to solve discrete problems is to map the discrete search space to a continuous domain, to apply a classical PSO, and then to demap the result.
 Such a mapping can be very simple (for example by just using rounded values) or more sophisticated.[56]
However, it can be noted that the equations of movement make use of operators that perform four actions:
computing the difference of two positions.
 The result is a velocity (more precisely a displacement)
multiplying a velocity by a numerical coefficient
adding two velocities
applying a velocity to a position
Usually a position and a velocity are represented by n real numbers, and these operators are simply -, *, +, and again +. 
But all these mathematical objects can be defined in a completely different way, in order to cope with binary problems (or more generally discrete ones), or even combinatorial ones.
 One approach is to redefine the operators based on sets.[61]
Applications
Optimization of hydrocarbon field development planning
Many of the optimization problems in science and engineering involve nonlinear objective functions. Particle swarm optimization has been applied for optimization of oil and gas field development planning,[62] and in photovoltaic(PV) systems[63]
Individualized Modeling
Individualized Modeling [64] is a computational technique that models a problem by trying to achieve an optimal solution with regard to a specified input data or query.
Applications of PSO in project scheduling
PSO is not intensively adopted in the scheduling field context, especially if compared to Genetic Algorithms (GA). 
Despite that, the results of its application, especially for Single-mode Resource-Constrained Project Scheduling Problems (SRCPSPs), is highly ranked in general comparisons with all optimization techniques 
Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. 
The concept is employed in work on artificial intelligence.
 The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.[1]
SI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. 
The inspiration often comes from nature, especially biological systems. 
The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of "intelligent" global behavior, unknown to the individual agents.
 Examples in natural systems of SI include ant colonies, bird flocking, animal herding, bacterial growth, fish schooling and microbial intelligence.
The application of swarm principles to robots is called swarm robotics, while 'swarm intelligence' refers to the more general set of algorithms.
 'Swarm prediction' has been used in the context of forecasting problems.
Particle swarm optimization
Particle swarm optimization (PSO) is not a global optimization algorithm for dealing with problems in which a best solution can be represented as a point or surface in an n-dimensional space. 
Hypotheses are plotted in this space and seeded with an initial velocity, as well as a communication channel between the particles.
 Particles then move through the solution space, and are evaluated according to some fitness criterion after each timestep. 
Over time, particles are accelerated towards those particles within their communication grouping which have better fitness values. 
The main advantage of such an approach over other global minimization strategies such as simulated annealing is that the large number of members that make up the particle swarm make the technique impressively resilient to the problem of local minima.
Ant colony optimization
Ant colony optimization (ACO), introduced by Dorigo in his doctoral dissertation, is a class of optimization algorithms modeled on the actions of an ant colony.
 ACO is a probabilistic technique useful in problems that deal with finding better paths through graphs.
 Artificial 'ants'—simulation agents—locate optimal solutions by moving through a parameter space representing all possible solutions. 
Natural ants lay down pheromones directing each other to resources while exploring their environment. The simulated 'ants' similarly record their positions and the quality of their solutions, so that in later simulation iterations more ants locate better solutions.[4]
Artificial bee colony algorithm
Artificial beet colony algorithm (ABC) is a meta-heuristic algorithm introduced by Karaboga in 2005,[5] and simulates the foraging behaviour of honey bees.
 The ABC algorithm has three phases: employed bee, onlooker bee and scout bee.
In the employed bee and the onlooker bee phases, bees exploit the sources by local searches in the neighbourhood of the solutions selected based on deterministic selection in the employed bee phase and the probabilistic selection in the onlooker bee phase. 
In the scout bee phase which is an analogy of abandoning exhausted food sources in the foraging process, solutions that are not beneficial anymore for search progress are abandoned, and new solutions are inserted instead of them to explore new regions in the search space. 
The algorithm has a well-balanced exploration and exploitation ability.
Differential evolution
Differential evolution is similar to genetic algorithm and pattern search.
 It uses multiagents or search vectors to carry out search. 
It has mutation and crossover, but do not have the global best solution in its search equations, in contrast with the particle swarm optimization.
The bees algorithm
The bees algorithm in its basic formulation was created by Pham and his co-workers in 2005,[6] and further refined in the following years.
 Modelled on the foraging behaviour of honey bees, the algorithm combines global explorative search with local exploitative search. 
A small number of artificial bees (scouts) explores randomly the solution space (environment) for solutions of high fitness (highly profitable food sources), whilst the bulk of the population search (harvest) the neighbourhood of the fittest solutions looking for the fitness optimum. 
A deterministics recruitment procedure which simulates the waggle dance of biological bees is used to communicate the scouts' findings to the foragers, and distribute the foragers depending on the fitness of the neighbourhoods selected for local search. 
Once the search in the neighbourhood of a solution stagnates, the local fitness optimum is considered to be found, and the site is abandoned. 
In summary, the Bees Algorithm searches concurrently the most promising regions of the solution space, whilst continuously sampling it in search of new favourable regions.
Artificial immune systems
Artificial immune systems (AIS) concerns the usage of abstract structure and function of the immune system to computational systems, and investigating the application of these systems towards solving computational problems from mathematics, engineering, and information technology. 
AIS is a sub-field of Biologically inspired computing, and natural computation, with interests in Machine Learning and belonging to the broader field of Artificial Intelligence.
Bat algorithm
Bat algorithm (BA) is a swarm-intelligence-based algorithm, inspired by the echolocation behavior of microbats. 
BA automatically balances exploration (long-range jumps around the global search space to avoid getting stuck around one local maxima) with exploitation (searching in more detail around known good solutions to find local maxima) by controlling loudness and pulse emission rates of simulated bats in the multi-dimensional search space.[8]
Glowworm swarm optimization
Glowworm swarm optimization (GSO), introduced by Krishnanand and Ghose in 2005 for simultaneous computation of multiple optima of multimodal functions.
 The algorithm shares a few features with some better known algorithms, such as ant colony optimization and particle swarm optimization, but with several significant differences. 
The agents in GSO are thought of as glowworms that carry a luminescence quantity called luciferin along with them. 
The glowworms encode the fitness of their current locations, evaluated using the objective function, into a luciferin value that they broadcast to their neighbors. 
The glowworm identifies its neighbors and computes its movements by exploiting an adaptive neighborhood, which is bounded above by its sensor range. 
Each glowworm selects, using a probabilistic mechanism, a neighbor that has a luciferin value higher than its own and moves toward it. 
These movements—based only on local information and selective neighbor interactions—enable the swarm of glowworms to partition into disjoint subgroups that converge on multiple optima of a given multimodal function.
Gravitational search algorithm
Agents in Gravitational Search Algorithm (GSA) swarm to find the minimum of the paraboloid {\textstyle \textstyle f(x_1,x_2)=x_1^2+x_2^2}, at (0,0).
 Particles' masses are increased by reaching the origin.
Gravitational search algorithm (GSA) based on the law of gravity and the notion of mass interactions. 
The GSA algorithm uses the theory of Newtonian physics and its searcher agents are the collection of masses. 
In GSA, there is an isolated system of masses.
 Using the gravitational force, every mass in the system can see the situation of other masses. 
The gravitational force is therefore a way of transferring information between different masses (Rashedi, Nezamabadi-pour and Saryazdi 2009).
 In GSA, agents are considered as objects and their performance is measured by their masses.
 All these objects attract each other by a gravity force, and this force causes a movement of all objects globally towards the objects with heavier masses. 
The heavy masses correspond to good solutions of the problem. 
The position of the agent corresponds to a solution of the problem, and its mass is determined using a fitness function. 
By lapse of time, masses are attracted by the heaviest mass, which would ideally present an optimum solution in the search space.
 The GSA could be considered as an isolated system of masses. 
It is like a small artificial world of masses obeying the Newtonian laws of gravitation and motion (Rashedi, Nezamabadi-pour and Saryazdi 2009).
 A multi-objective variant of GSA, called MOGSA, was proposed by Hassanzadeh et al. in 2010.
River Formation Dynamics
River Formation Dynamics (RFD) is based on imitating how water forms rivers by eroding the ground and depositing sediments (the drops act as the swarm). 
After drops transform the landscape by increasing/decreasing the altitude of places, solutions are given in the form of paths of decreasing altitudes. 
Decreasing gradients are constructed, and these gradients are followed by subsequent drops to compose new gradients and reinforce the best ones. 
This heuristic optimization method was first presented in 2007 by Rabanal et al.
 The applicability of RFD to other NP-complete problems was studied in. 
The algorithm has also been applied in other fields as routing,[19][20] or robot navigation.[21]
Self-propelled particles
Self-propelled particles (SPP), also referred to as the Vicsek model, was introduced in 1995 by Vicsek et al.[22] as a special case of the boids model introduced in 1986 by Reynolds.
 A swarm is modelled in SPP by a collection of particles that move with a constant speed but respond to a random perturbation by adopting at each time increment the average direction of motion of the other particles in their local neighbourhood.
SPP models predict that swarming animals share certain properties at the group level, regardless of the type of animals in the swarm.
Swarming systems give rise to emergent behaviours which occur at many different scales, some of which are turning out to be both universal and robust. 
It has become a challenge in theoretical physics to find minimal statistical models that capture these behaviours.
Stochastic diffusion search
Stochastic diffusion search (SDS)[29][30] is an agent-based probabilistic global search and optimization technique best suited to problems where the objective function can be decomposed into multiple independent partial-functions. 
Each agent maintains a hypothesis which is iteratively tested by evaluating a randomly selected partial objective function parameterised by the agent's current hypothesis. 
In the standard version of SDS such partial function evaluations are binary, resulting in each agent becoming active or inactive. 
Information on hypotheses is diffused across the population via inter-agent communication.
 Unlike the stigmergic communication used in ACO, in SDS agents communicate hypotheses via a one-to-one communication strategy analogous to the tandem running procedure observed in Leptothorax acervorum.
A positive feedback mechanism ensures that, over time, a population of agents stabilise around the global-best solution.
 SDS is both an efficient and robust global search and optimisation algorithm, which has been extensively mathematically described.
 Recent work has involved merging the global search properties of SDS with other swarm intelligence algorithms.
Multi-swarm optimization
Multi-swarm optimization is a variant of particle swarm optimization (PSO) based on the use of multiple sub-swarms instead of one (standard) swarm. 
The general approach in multi-swarm optimization is that each sub-swarm focuses on a specific region while a specific diversification method decides where and when to launch the sub-swarms. 
The multi-swarm framework is especially fitted for the optimization on multi-modal problems, where multiple (local) optima exist.
Applications
Swarm Intelligence-based techniques can be used in a number of applications. 
The U.S. military is investigating swarm techniques for controlling unmanned vehicles. 
The European Space Agency is thinking about an orbital swarm for self-assembly and interferometry.
 NASA is investigating the use of swarm technology for planetary mapping. 
A 1992 paper by M. Anthony Lewis and George A. Bekey discusses the possibility of using swarm intelligence to control nanobots within the body for the purpose of killing cancer tumors.
 Conversely al-Rifaie and Aber have used Stochastic Diffusion Search to help locate tumours.
 Swarm intelligence has also been applied for data mining.
Ant-based routing
The use of Swarm Intelligence in telecommunication networks has also been researched, in the form of ant-based routing. 
This was pioneered separately by Dorigo et al. and Hewlett Packard in the mid-1990s, with a number of variations since. 
Basically this uses a probabilistic routing table rewarding/reinforcing the route successfully traversed by each "ant" (a small control packet) which flood the network. 
As the system behaves stochastically and is therefore lacking repeatability, there are large hurdles to commercial deployment. 
Mobile media and new technologies have the potential to change the threshold for collective action due to swarm intelligence (Rheingold: 2002, P175).
The location of transmission infrastructure for wireless communication networks is an important engineering problem involving competing objectives. 
A minimal selection of locations (or sites) are required subject to providing adequate area coverage for users. 
A very different-ant inspired swarm intelligence algorithm, stochastic diffusion search (SDS), has been successfully used to provide a general model for this problem, related to circle packing and set covering. 
It has been shown that the SDS can be applied to identify suitable solutions even for large problem instances.[41]
Airlines have also used ant-based routing in assigning aircraft arrivals to airport gates.
 At Southwest Airlines a software program uses swarm theory, or swarm intelligence—the idea that a colony of ants works better than one alone. 
Each pilot acts like an ant searching for the best airport gate. 
"The pilot learns from his experience what's the best for him, and it turns out that that's the best solution for the airline," Douglas A. Lawson explains.
 As a result, the "colony" of pilots always go to gates they can arrive at and depart from quickly. 
The program can even alert a pilot of plane back-ups before they happen.
 "We can anticipate that it's going to happen, so we'll have a gate available," Lawson says.[42]
Crowd simulation
Artists are using swarm technology as a means of creating complex interactive systems or simulating crowds.
Stanley and Stella in: Breaking the Ice was the first movie to make use of swarm technology for rendering, realistically depicting the movements of groups of fish and birds using the Boids system. 
Tim Burton's Batman Returns also made use of swarm technology for showing the movements of a group of bats. 
The Lord of the Rings film trilogy made use of similar technology, known as Massive, during battle scenes.
 Swarm technology is particularly attractive because it is cheap, robust, and simple.
Airlines have used swarm theory to simulate passengers boarding a plane. 
Southwest Airlines researcher Douglas A. Lawson used an ant-based computer simulation employing only six interaction rules to evaluate boarding times using various boarding methods.
Human swarming
Enabled by mediating software such as the UNU [44] collective intelligence platform, networks of distributed users can be organized into "human swarms" (also referred to as "social swarms") through the implementation of real-time closed-loop control systems. 
As published by Rosenberg (2015), such real-time control systems enable groups of human participants to behave as a unified collective intelligence.
When logged into the UNU platform, for example, groups of distributed users can collectively answer questions, generate ideas, and make predictions as a singular emergent entity.
 Early testing shows that human swarms can out-predict individuals across a variety of real-world projections.
Swarmic art
The resulting hybrid algorithm is used to sketch novel drawings of an input image, exploiting an artistic tension between the local behaviour of the ‘birds ﬂocking’ - as they seek to follow the input sketch - and the global behaviour of the "ants foraging" - as they seek to encourage the ﬂock to explore novel regions of the canvas. 
The "creativity" of this hybrid swarm system has been analysed under the philosophical light of the "rhizome" in the context of Deleuze’s "Orchid and Wasp" metaphor.[50]
In a more recent work of al-Rifaie et al., "Swarmic Sketches and Attention Mechanism",[51] introduces a novel approach deploying the mechanism of 'attention' by adapting SDS to selectively attend to detailed areas of a digital canvas. 
Once the attention of the swarm is drawn to a certain line within the canvas, the capability of PSO is used to produce a 'swarmic sketch' of the attended line. 
The swarms move throughout the digital canvas in an attempt to satisfy their dynamic roles – attention to areas with more details – associated to them via their fitness function. 
Having associated the rendering process with the concepts of attention, the performance of the participating swarms creates a unique, non-identical sketch each time the ‘artist’ swarms embark on interpreting the input line drawings. 
In other works while PSO is responsible for the sketching process, SDS controls the attention of the swarm.
In a similar work, "Swarmic Paintings and Colour Attention",[52] non-photorealistic images are produced using SDS algorithm which, in the context of this work, is responsible for colour attention.
The "computational creativity" of the above-mentioned systems are discussed in[49][53][54][55] through the two prerequisites of creativity (i.e. freedom and constraints) within the swarm intelligence's two infamous phases of exploration and exploitation.
Michael Theodore and Nikolaus Correll use swarm intelligent art installation to explore what it takes to have engineered systems to appear lifelike[56] Notable work include Swarm Wall (2012) and endo-exo (2014).
In popular culture
This section needs additional citations for verification. 
lease help improve this article by adding citations to reliable sources.
 Unsourced material may be challenged and removed. (August 2013)
main article: Group mind (science fiction)
Swarm intelligence-related concepts and references can be found throughout popular culture, frequently as some form of collective intelligence or group mind involving far more agents than used in current applications.
Science fiction writer Olaf Stapledon may have been the first to discuss swarm intelligences equal or superior to humanity.
 In Last and First Men (1931), a swarm intelligence from Mars consists of tiny individual cells that communicate with each other by radio waves; in Star Maker (1937) swarm intelligences founded numerous civilizations.
The Invincible (1964), a science fiction novel by Stanisław Lem where a human spaceship finds intelligent behavior in a flock of small particles that were able to defend themselves against what they found as a menace.
In the dramatic novel and subsequent mini-series The Andromeda Strain (1969) by Michael Crichton, an extraterrestrial virus communicates between individual cells and displays the ability to think and react individually and as a whole, and as such displays a semblance of "swarm intelligence".
Ygramul, the Many – an intelligent being consisting of a swarm of many wasp-like insects, a character in the novel The Neverending Story (1979) written by Michael Ende. 
Ygramul is also mentioned in a scientific paper, "Flocks, Herds, and Schools" written by Knut Hartmann (Computer Graphics and Interactive Systems, Otto-von-Guericke University of Magdeburg).[57]
Swarm (1982), a short story by Bruce Sterling about a mission undertaken by a faction of humans, to understand and exploit a space-faring swarm intelligence.
In the book Ender's Game (1985), the Formics (known popularly as Buggers) are a swarm intelligence with colonies or armadas each directed by a single queen.
The Hacker and the Ants (1994), a book by Rudy Rucker on AI ants within a virtual environment.
Hallucination (1995), a posthumously-published short story by Isaac Asimov about an alien insect-like swarm, capable of organization and provided with a sort of swarm intelligence.
The Zerg (1998) of the Starcraft universe demonstrate such concepts when in groups and enhanced by the psychic control of taskmaster breeds.
Decipher (2001) by Stel Pavlou deals with the swarm intelligence of nanobots that guard against intruders in Atlantis.
In the video game series Halo, the Covenant (2001) species known as the Hunters are made up of thousands of worm-like creatures which are individually non-sentient, but, collectively form a sentient being. 
Also, in the same game series, if there is a Gravemind present among the parasitic organisms known as The Flood, the Gravemind will function as the leader of a hive mind that controls each individual member of the Flood.
Prey (2002), by Michael Crichton deals with the danger of nanobots escaping from human control and developing a swarm intelligence.
In the Legends of Dune series (2002), Omnius becomes a swarm intelligence by taking over almost all of the artificial intelligence that exists in the universe
The science fiction novel The Swarm (2004), by Frank Schätzing, deals with underwater single-celled creatures who act in unison to destroy humanity.
In the video game Mass Effect (2007), a galactic race known as the Quarians created a race of humanoid machines known as the Geth which worked as a swarm intelligence in order to avoid restrictions on true-AI. 
However the Geth obtained a shared sentience through the combined processing power of every geth unit.
In Sandworms of Dune (2007), the Face Dancers are revealed to have developed into a swarm intelligence represented by Khrone
In the video game Penumbra: Black Plague (2008), the Tuurngait is a hivemind that grows by infecting other organisms with a virus.
Kill Decision (2012), a novel by Daniel Suarez features autonomous drones programmed with the aggressive swarming intelligence of Weaver ants
Bioinformatics Listeni/ˌbaɪ.oʊˌɪnfərˈmætɪks/ is an interdisciplinary field that develops methods and software tools for understanding biological data. 
As an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data. 
Bioinformatics has been used for in silico analyses of biological queries using mathematical and statistical techniques.
Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis "pipelines" that are repeatedly used, particularly in the field of genomics. 
Common uses of bioinformatics include the identification of candidate genes and nucleotides (SNPs).
 Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. 
In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences.
Bioinformatics has become an important part of many areas of biology. 
In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data.
 In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations.
 It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. 
It also plays a role in the analysis of gene and protein expression and regulation. 
Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. 
At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. 
In structural biology, it aids in the simulation and modeling of DNA, RNA, and protein structures as well as molecular interactions.
History
Historically, the term bioinformatics did not mean what it means today. 
Paulien Hogeweg and Ben Hesper coined it in 1970 to refer to the study of information processes in biotic systems.
 This definition placed bioinformatics as a field parallel to biophysics (the study of physical processes in biological systems) or biochemistry (the study of chemical processes in biological systems).[1]
Sequences
Sequences of genetic material are frequently used in bioinformatics and are easier to manage using computers than manually.
Computers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. 
Comparing multiple sequences manually turned out to be impractical.
 A pioneer in the field was Margaret Oakley Dayhoff, who has been hailed by David Lipman, director of the National Center for Biotechnology Information, as the "mother and father of bioinformatics."
 Dayhoff compiled one of the first protein sequence databases, initially published as books[5] and pioneered methods of sequence alignment and molecular evolution.
 Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released with Tai Te Wu between 1980 and 1991.[7]
Goals
To study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities.
 Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. 
This includes nucleotide and amino acid sequences, protein domains, and protein structures.
 The actual process of analyzing and interpreting data is referred to as computational biology. 
Important sub-disciplines within bioinformatics and computational biology include:
Development and implementation of computer programs that enable efficient access to, use and management of, various types of information
Development of new algorithms (mathematical formulas) and statistical measures that assess relationships among members of large data sets. 
For example, there are methods to locate a gene within a sequence, to predict protein structure and/or function, and to cluster protein sequences into families of related sequences.
The primary goal of bioinformatics is to increase the understanding of biological processes. 
What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. 
Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. 
Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein–protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.
Bioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.
Over the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. 
Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.
Common activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.
Relation to other fields
Bioinformatics is a science field that is similar to but distinct from biological computation and computational biology. 
Biological computation uses bioengineering and biology to build biological computers, whereas bioinformatics uses computation to better understand biology. 
Bioinformatics and computational biology have similar aims and approaches, but they differ in scale: bioinformatics organizes and analyzes basic biological data, whereas computational biology builds theoretical models of biological systems, just as mathematical biology does with mathematical models.
Analyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence, soft computing, data mining, image processing, and computer simulation.
 The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.
Sequence analysis
Main articles: Sequence alignment and Sequence database
The sequences of different genes or proteins may be aligned side-by-side to measure their similarity. 
This alignment compares protein sequences containing WPP domains.
Since the Phage Φ-X174 was sequenced in 1977,[9] the DNA sequences of thousands of organisms have been decoded and stored in databases.
 This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences.
 A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). 
With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. 
Today, computer programs such as BLAST are used daily to search sequences from more than 260 000 organisms, containing over 190 billion nucleotides.
These programs can compensate for mutations (exchanged, deleted or inserted bases) in the DNA sequence, to identify sequences that are related, but not identical. 
A variant of this sequence alignment is used in the sequencing process itself. 
The so-called shotgun sequencing technique (which was used, for example, by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, Haemophilus influenzae)[11] does not produce entire chromosomes.
 Instead it generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). 
The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. 
Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. 
For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. 
Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.
Following the goals that the Human Genome Project left to achieve after its closure in 2003, a new project developed by the National Human Genome Research Institute in the U.S appeared.
 The so-called ENCODE project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to generate automatically large amounts of data with lower research costs but with the same quality and viability.
Another aspect of bioinformatics in sequence analysis is annotation. 
This involves computational gene finding to search for protein-coding genes, RNA genes, and other functional sequences within a genome. 
Not all of the nucleotides within a genome are part of genes.
 Within the genomes of higher organisms, large parts of the DNA do not serve any obvious purpose.
See also: sequence analysis, sequence mining, sequence profiling tool and sequence motif
Genome annotation
Main article: Gene prediction
In the context of genomics, annotation is the process of marking the genes and other biological features in a DNA sequence. 
This process needs to be automated because most genomes are too large to annotate by hand, not to mention the desire to annotate as many genomes as possible, as the rate of sequencing has ceased to pose a bottleneck. 
Annotation is made possible by the fact that genes have recognisable start and stop regions, although the exact sequence found in these regions can vary between genes.
The first genome annotation software system was designed in 1995 by Owen White, who was part of the team at The Institute for Genomic Research that sequenced and analyzed the first genome of a free-living organism to be decoded, the bacterium Haemophilus influenzae.
 White built a software system to find the genes (fragments of genomic sequence that encode proteins), the transfer RNAs, and to make initial assignments of function to those genes.
 Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA, such as the GeneMark program trained and used to find protein-coding genes in Haemophilus influenzae, are constantly changing and improving.
Computational evolutionary biology
Evolutionary biology is the study of the origin and descent of species, as well as their change over time. 
Informatics has assisted evolutionary biologists by enabling researchers to:
trace the evolution of a large number of organisms by measuring changes in their DNA, rather than through physical taxonomy or physiological observations alone,
more recently, compare entire genomes, which permits the study of more complex evolutionary events, such as gene duplication, horizontal gene transfer, and the prediction of factors important in bacterial speciation,
build complex computational models of populations to predict the outcome of the system over time[12]
track and share information on an increasingly large number of species and organisms
Future work endeavours to reconstruct the now more complex tree of life.
The area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology, but the two areas are not necessarily related.
Comparative genomics
Main article: Comparative genomics
The core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. 
It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. 
A multitude of evolutionary events acting at various organizational levels shape genome evolution. 
At the lowest level, point mutations affect individual nucleotides. 
At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion.
Ultimately, whole genomes are involved in processes of hybridization, polyploidization and endosymbiosis, often leading to rapid speciation. 
Many of these studies are based on the homology detection and protein families computation.[14]
Pan genomics
Main article: Pan-genome
Pan genomics is a concept introduced in 2005 by Tettelin and Medini which eventually took root in bioinformatics.
 Pan genome is the complete gene repertoire of a particular taxonomic group: although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum etc. 
It is divided in two parts- The Core genome: Set of genes common to all the genomes under study (These are often housekeeping genes vital for survival) and The Dispensable/Flexible Genome: Set of genes not present in all but one or some genomes under study.
Genetics of disease
Main article: Genome-wide association studies
With the advent of next-generation sequencing we are obtaining enough sequence data to map the genes of complex diseases such as diabetes,[15] infertility,[16] breast cancer[17] or Alzheimer's Disease.
 Genome-wide association studies are a useful approach to pinpoint the mutations responsible for such complex diseases.
 Through these studies, thousands of DNA variants have been identified that are associated with similar diseases and traits.
 Furthermore, the possibility for genes to be used at prognosis, diagnosis or treatment is one of the most essential applications. 
Many studies are discussing both the promising ways to choose the genes to be used and the problems and pitfalls of using genes to predict disease presence or prognosis.[21]
Analysis of mutations in cancer
Main article: Oncogenomics
In cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. 
Massive sequencing efforts are used to identify previously unknown point mutations in a variety of genes in cancer. 
Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms. 
New physical detection technologies are employed, such as oligonucleotide microarrays to identify chromosomal gains and losses (called comparative genomic hybridization), and single-nucleotide polymorphism arrays to detect known point mutations. 
These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate terabytes of data per experiment. 
Again the massive amounts and new types of data generate new opportunities for bioinformaticians. 
The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.
However, with the breakthroughs that the next-generation sequencing technology is providing to the field of Bioinformatics, cancer genomics may be drastically change. 
This new methods and software allow bioinformaticians to sequence in a rapid and affordable way many cancer genomes. 
This could mean a more flexible process to classify types of cancer by analysis of cancer driven mutations in the genome.
 Furthermore, individual tracking of patients during the progression of the disease may be possible in the future with the sequence of cancer samples.[22]
Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.
Gene and protein expression
Analysis of gene expression
All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies. 
Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.
Analysis of protein expression
Protein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample.
 Analysis of regulation
Regulation is the complex orchestration of events starting with an extracellular signal such as a hormone and leading to an increase or decrease in the activity of one or more proteins.
 Bioinformatics techniques have been applied to explore various steps in this process.
 For example, promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. 
These motifs influence the extent to which that region is transcribed into mRNA.
 Expression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state.
 In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). 
One can then apply clustering algorithms to that expression data to determine which genes are co-expressed. 
For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. 
Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods such as the Bi-CoPaM. 
The later, namely Bi-CoPaM, has been actually proposed to address various issues specific to gene discovery problems such as consistent co-expression of genes over multiple microarray datasets.[23][24]
Structural bioinformatics
Main articles: Structural bioinformatics and Protein structure prediction
See also: Structural motif and Structural domain
3-dimensional protein structures such as this one are common subjects in bioinformatic analyses.
Protein structure prediction is another important application of bioinformatics. 
The amino acid sequence of a protein, the so-called primary structure, can be easily determined from the sequence on the gene that codes for it. 
In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. 
(Of course, there are exceptions, such as the bovine spongiform encephalopathy – a.k.a. Mad Cow Disease – prion.) 
Knowledge of this structure is vital in understanding the function of the protein. 
Structural information is usually classified as one of secondary, tertiary and quaternary structure.
 A viable general solution to such predictions remains an open problem. 
Most efforts have so far been directed towards heuristics that work most of the time.[citation needed]
One of the key ideas in bioinformatics is the notion of homology. 
In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene A, whose function is known, is homologous to the sequence of gene B, whose function is unknown, one could infer that B may share A's function. 
In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. 
In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. 
This currently remains the only way to predict protein structures reliably.
One example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes (leghemoglobin).
 Both serve the same purpose of transporting oxygen in the organism.
 Though both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes.[25]
Other techniques for predicting protein structure include protein threading and de novo (from scratch) physics-based modeling.
Network and systems biology
Main articles: Computational systems biology, Biological network and Interactome
Network analysis seeks to understand the relationships within biological networks such as metabolic or protein-protein interaction networks.
 Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.
Systems biology involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. 
Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.
Molecular interaction networks
Interactions between proteins are frequently visualized and analyzed using networks.
 This network is made up of protein-protein interactions from Treponema pallidum, the causative agent of syphilis and other diseases.
Main articles: Protein–protein interaction prediction and interactome
Other interactions encountered in the field include Protein–ligand (including drug) and protein–peptide. 
Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.
Others
Literature analysis
Main articles: Text mining and Biomedical text mining
The growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed sub-fields of research. 
Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:
Abbreviation recognition – identify the long-form and abbreviation of biological terms
Named entity recognition – recognizing biological terms such as gene names
Protein-protein interaction – identify which proteins interact with which proteins from text
The area of research draws from statistics and computational linguistics.
High-throughput image analysis
Computational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery.
 Modern image analysis systems augment an observer's ability to make measurements from a large or complex set of images, by improving accuracy, objectivity, or speed. 
A fully developed analysis system may completely replace the observer. 
Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both diagnostics and research. 
Some examples are:
high-throughput and high-fidelity quantification and sub-cellular localization (high-content screening, cytohistopathology, Bioimage informatics)
morphometrics
clinical image analysis and visualization
determining the real-time air-flow patterns in breathing lungs of living animals
quantifying occlusion size in real-time imagery from the development of and recovery during arterial injury
making behavioral observations from extended video recordings of laboratory animals
infrared measurements for metabolic activity determination
inferring clone overlaps in DNA mapping, e.g. the Sulston score
High-throughput single cell data analysis
Main article: Flow cytometry bioinformatics
Computational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. 
These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.
Biodiversity informatics
Main article: Biodiversity informatics
Biodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. 
Examples of such analyses include phylogenetics, niche modelling, species richness mapping, or species identification tools.
Databases
Main articles: List of biological databases and Biological database
Databases are essential for bioinformatics research and applications.
 There is a huge number of available databases covering almost everything from DNA and protein sequences, molecular structures, to phenotypes and biodiversity.
 Databases generally fall into one of three types.
 Some contain data resulting directly from empirical methods such as gene knockouts.
 Others consist of predicted data, and most contain data from both sources. 
There are meta-databases that incorporate data compiled from multiple other databases.
 Some others are specialized, such as those specific to an organism. 
These databases vary in their format, way of accession and whether they are public or not. 
Some of the most commonly used databases are listed below. 
For a more comprehensive list, please check the link at the beginning of the subsection.
Used in Motif Finding: GenomeNet MOTIF Search
Used in Gene Ontology: ToppGene FuncAssociate, Enrichr, GATHER
Used in Gene Finding: Hidden Markov Model
Used in finding Protein Structures/Family: PFAM
Used for Next Generation Sequencing: (Not database but data format), FASTQ Format
Used in Gene Expression Analysis: GEO, ArrayExpress
Used in Network Analysis: Interaction Analysis Databases(BioGRID, MINT, HPRD, Curated Human Signaling Network), Functional Networks (STRING, KEGG)
Used in design of synthetic genetic circuits: GenoCAD
Please keep in mind that this is a quick sampling and generally most computation data is supported by wet lab data as well.
Web services in bioinformatics
SOAP- and REST-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. 
The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.
Basic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis).
 Bioinformatics workflow management systems
Main article: Bioinformatics workflow management systems
A Bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application.
 Such systems are designed to
provide an easy-to-use environment for individual application scientists themselves to create their own workflows
provide interactive tools for the scientists enabling them to execute their workflows and view their results in real-time
simplify the process of sharing and reusing workflows between the scientists.
enable scientists to track the provenance of the workflow execution results and the workflow creation steps.
Some of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril.
Education platforms
Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. 
The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license.
Text mining, also referred to as text data mining, roughly equivalent to text analytics, refers to the process of deriving high-quality information from text.
 High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. 
Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 
'High quality' in text mining usually refers to some combination of relevance, novelty, and interestingness.
 Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).
Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics.
 The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods.
A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.
Text mining and text analytics
The term text analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation.
] The term is roughly synonymous with text mining; indeed, Ronen Feldman modified a 2000 description of "text mining"[2] in 2004 to describe "text analytics."
 The latter term is now used more frequently in business settings while "text mining" is used in some of the earliest application areas, dating to the 1980s,[4] notably life-sciences research and government intelligence.
The term text analytics also describes that application of text analytics to respond to business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. 
It is a truism that 80 percent of business-relevant information originates in unstructured form, primarily text.
 These techniques and processes discover and present knowledge – facts, business rules, and relationships – that is otherwise locked in textual form, impenetrable to automated processing.
History
Labor-intensive manual text mining approaches first surfaced in the mid-1980s,[6] but technological advances have enabled the field to advance during the past decade. 
Text mining is an interdisciplinary field that draws on information retrieval, data mining, machine learning, statistics, and computational linguistics. 
As most information (common estimates say over 80%)[5] is currently stored as text, text mining is believed to have a high commercial potential value. 
Increasing interest is being paid to multilingual data mining: the ability to gain information across languages and cluster similar items from different linguistic sources according to their meaning.
The challenge of exploiting the large proportion of enterprise information that originates in "unstructured" form has been recognized for decades.
 It is recognized in the earliest definition of business intelligence (BI), in an October 1958 IBM Journal article by H.P. Luhn, A Business Intelligence System, which describes a system that will:
"...utilize data-processing machines for auto-abstracting and auto-encoding of documents and for creating interest profiles for each of the 'action points' in an organization. 
Both incoming and internally generated documents are automatically abstracted, characterized by a word pattern, and sent automatically to appropriate action points."
Yet as management information systems developed starting in the 1960s, and as BI emerged in the '80s and '90s as a software category and field of practice, the emphasis was on numerical data stored in relational databases.
 This is not surprising: text in "unstructured" documents is hard to process. 
The emergence of text analytics in its current form stems from a refocusing of research in the late 1990s from algorithm development to application, as described by Prof. Marti A. Hearst in the paper Untangling Text Data Mining:[8]
For almost a decade the computational linguistics community has viewed large text collections as a resource to be tapped in order to produce better text analysis algorithms. 
In this paper, I have attempted to suggest a new emphasis: the use of large online text collections to discover new facts and trends about the world itself. 
I suggest that to make progress we do not need fully artificial intelligent text analysis; rather, a mixture of computationally-driven and user-guided analysis may open the door to exciting new results.
Hearst's 1999 statement of need fairly well describes the state of text analytics technology and practice a decade later.
Text analysis processes
Subtasks — components of a larger text-analytics effort — typically include:
Information retrieval or identification of a corpus is a preparatory step: collecting or identifying a set of textual materials, on the Web or held in a file system, database, or content corpus manager, for analysis.
Although some text analytics systems apply exclusively advanced statistical methods, many others apply more extensive natural language processing, such as part of speech tagging, syntactic parsing, and other types of linguistic analysis.[citation needed]
Named entity recognition is the use of gazetteers or statistical techniques to identify named text features: people, organizations, place names, stock ticker symbols, certain abbreviations, and so on. 
Disambiguation — the use of contextual clues — may be required to decide where, for instance, "Ford" can refer to a former U.S. president, a vehicle manufacturer, a movie star, a river crossing, or some other entity.
Recognition of Pattern Identified Entities: Features such as telephone numbers, e-mail addresses, quantities (with units) can be discerned via regular expression or other pattern matches.
Coreference: identification of noun phrases and other terms that refer to the same object.
Relationship, fact, and event Extraction: identification of associations among entities and other information in text
Sentiment analysis involves discerning subjective (as opposed to factual) material and extracting various forms of attitudinal information: sentiment, opinion, mood, and emotion. 
Text analytics techniques are helpful in analyzing, sentiment at the entity, concept, or topic level and in distinguishing opinion holder and opinion object.[9]
Applications
The technology is now broadly applied for a wide variety of government, research, and business needs. 
Applications can be sorted into a number of categories by analysis type or by business function. 
Using this approach to classifying solutions, application categories include:
Enterprise Business Intelligence/Data Mining, Competitive Intelligence
E-Discovery, Records Management
National Security/Intelligence
Scientific discovery, especially Life Sciences
Sentiment Analysis Tools, Listening Platforms
Natural Language/Semantic Toolkit or Service
Publishing
Automated ad placement
Search/Information Access
Social media monitoring
Security applications
Many text mining software packages are marketed for security applications, especially monitoring and analysis of online plain text sources such as Internet news, blogs, etc. for national security purposes.
 It is also involved in the study of text encryption/decryption.
Biomedical applications
Main article: Biomedical text mining
A range of text mining applications in the biomedical literature has been described.[12]
One online text mining application in the biomedical literature is PubGene that combines biomedical text mining with network visualization as an Internet service.
 TPX is a concept-assisted search and navigation tool for biomedical literature analyses[15] - it runs on PubMed/PMC and can be configured, on request, to run on local literature repositories too.
GoPubMed is a knowledge-based search engine for biomedical texts.
Software applications
Text mining methods and software is also being researched and developed by major firms, including IBM and Microsoft, to further automate the mining and analysis processes, and by different firms working in the area of search and indexing in general as a way to improve their results.
 Within public sector much effort has been concentrated on creating software for tracking and monitoring terrorist activities.[16]
Online media applications
Text mining is being used by large media companies, such as the Tribune Company, to clarify information and to provide readers with greater search experiences, which in turn increases site "stickiness" and revenue. 
Additionally, on the back end, editors are benefiting by being able to share, associate and package news across properties, significantly increasing opportunities to monetize content.
Marketing applications
Text mining is starting to be used in marketing as well, more specifically in analytical customer relationship management.
Coussement and Van den Poel (2008)[18][19] apply it to improve predictive analytics models for customer churn (customer attrition).[18]
Sentiment analysis
Sentiment analysis may involve analysis of movie reviews for estimating how favorable a review is for a movie.
 Such an analysis may need a labeled data set or labeling of the affectivity of words.
 Resources for affectivity of words and concepts have been made for WordNet[21] and ConceptNet,[22] respectively.
Text has been used to detect emotions in the related area of affective computing.[23] Text based approaches to affective computing have been used on multiple corpora such as students evaluations, children stories and news stories.
Academic applications
The issue of text mining is of importance to publishers who hold large databases of information needing indexing for retrieval. 
This is especially true in scientific disciplines, in which highly specific information is often contained within written text. 
Academic institutions have also become involved in the text mining initiative:
The National Centre for Text Mining (NaCTeM), is the first publicly funded text mining centre in the world. 
NaCTeM is operated by the University of Manchester[24] in close collaboration with the Tsujii Lab,[25] University of Tokyo.
 NaCTeM provides customised tools, research facilities and offers advice to the academic community. 
They are funded by the Joint Information Systems Committee (JISC) and two of the UK Research Councils (EPSRC & BBSRC).
 With an initial focus on text mining in the biological and biomedical sciences, research has since expanded into the areas of social sciences.
In the United States, the School of Information at University of California, Berkeley is developing a program called BioText to assist biology researchers in text mining and analysis.
Digital Humanities and Computational Sociology
The automatic analysis of vast textual corpora has created the possibility for scholars to analyse millions of documents in multiple languages with very limited manual intervention. 
Key enabling technologies have been Parsing, Machine Translation, Topic categorization, Machine Learning.
Narrative network of US Elections 2012[27]
The automatic parsing of textual corpora has enabled the extraction of actors and their relational networks on a vast scale, turning textual data into network data. 
The resulting networks, which can contain thousands of nodes, are then analysed by using tools from Network theory to identify the key actors, the key communities or parties, and general properties such as robustness or structural stability of the overall network, or centrality of certain nodes.
 This automates the approach introduced by Quantitative Narrative Analysis,[29] whereby subject-verb-object triplets are identified with pairs of actors linked by an action, or pairs formed by actor-object.[27]
Content analysis has been a traditional part of social sciences and media studies for a long time. 
The automation of content analysis has allowed a "big data" revolution to take place in that field, with studies in social media and newspaper content that include millions of news items. 
Gender bias, readability, content similarity, reader preferences, and even mood have been analyzed based on text mining methods over millions of documents. 
The analysis of readability, gender bias and topic bias was demonstrated in [34] showing how different topics have different gender biases and levels of readability; the possibility to detect mood shifts in a vast population by analysing Twitter content was demonstrated as well.[35]
Software
Text mining computer programs are available from many commercial and open source companies and sources. 
See List of text mining software.
Intellectual Property Law and Text Mining
Situation in Europe
Due to a lack of flexibilities in European copyright and database law, the mining of in-copyright works such as web mining without the permission of the copyright owner is not legal.
 In the UK in 2014, on the recommendation of the Hargreaves review the government amended copyright law[36] to allow text mining as a limitation and exception. 
Only the second country in the world to do so after Japan, which introduced a mining specific exception in 2009. 
However, due to the restriction of the Copyright Directive, the UK exception only allows content mining for non-commercial purposes. 
UK copyright law does not allow this provision to be overridden by contractual terms and conditions.
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.
 The focus on the solution to this legal issue being licences and not limitations and exceptions to copyright law led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[38]
Situation in United States
By contrast to Europe, the flexible nature of US copyright law, and in particular fair use means that text mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea is viewed as being legal. 
As text mining is transformative, meaning that is it does not supplant the original work, it is viewed as being lawful under fair use.
 For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitisation project of in-copyright books was lawful, in part because of the transformative uses that the digitisation project displayed - one such use being text and data mining.[39]
Implications
Until recently, websites most often used text-based searches, which only found documents containing specific user-defined words or phrases.
 Now, through use of a semantic web, text mining can find content based on meaning and context (rather than just by a specific word). 
Additionally, text mining software can be used to build large dossiers of information about specific people and events. 
For example, large datasets based on data extracted from news reports can be built to facilitate social networks analysis or counter-intelligence.
 In effect, the text mining software may act in a capacity similar to an intelligence analyst or research librarian, albeit with a more limited scope of analysis.
 Text mining is also used in some email spam filters as a way of determining the characteristics of messages that are likely to be advertisements or other unwanted material.
Text mining and text analytics
The term text analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation.
 The term is roughly synonymous with text mining; indeed, Ronen Feldman modified a 2000 description of "text mining"[2] in 2004 to describe "text analytics."
 The latter term is now used more frequently in business settings while "text mining" is used in some of the earliest application areas, dating to the 1980s,[4] notably life-sciences research and government intelligence.
The term text analytics also describes that application of text analytics to respond to business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. 
It is a truism that 80 percent of business-relevant information originates in unstructured form, primarily text.
 These techniques and processes discover and present knowledge – facts, business rules, and relationships – that is otherwise locked in textual form, impenetrable to automated processing.
History
Labor-intensive manual text mining approaches first surfaced in the mid-1980s,[6] but technological advances have enabled the field to advance during the past decade. 
Text mining is an interdisciplinary field that draws on information retrieval, data mining, machine learning, statistics, and computational linguistics. 
As most information (common estimates say over 80%)[5] is currently stored as text, text mining is believed to have a high commercial potential value.
 Increasing interest is being paid to multilingual data mining: the ability to gain information across languages and cluster similar items from different linguistic sources according to their meaning.
The challenge of exploiting the large proportion of enterprise information that originates in "unstructured" form has been recognized for decades.
 It is recognized in the earliest definition of business intelligence (BI), in an October 1958 IBM Journal article by H.P. Luhn, A Business Intelligence System, which describes a system that will:
"...utilize data-processing machines for auto-abstracting and auto-encoding of documents and for creating interest profiles for each of the 'action points' in an organization. 
Both incoming and internally generated documents are automatically abstracted, characterized by a word pattern, and sent automatically to appropriate action points."
Yet as management information systems developed starting in the 1960s, and as BI emerged in the '80s and '90s as a software category and field of practice, the emphasis was on numerical data stored in relational databases.
 This is not surprising: text in "unstructured" documents is hard to process. 
The emergence of text analytics in its current form stems from a refocusing of research in the late 1990s from algorithm development to application, as described by Prof. Marti A. Hearst in the paper Untangling Text Data Mining:[8]
For almost a decade the computational linguistics community has viewed large text collections as a resource to be tapped in order to produce better text analysis algorithms.
 In this paper, I have attempted to suggest a new emphasis: the use of large online text collections to discover new facts and trends about the world itself. I suggest that to make progress we do not need fully artificial intelligent text analysis; rather, a mixture of computationally-driven and user-guided analysis may open the door to exciting new results.
Hearst's 1999 statement of need fairly well describes the state of text analytics technology and practice a decade later.
Text analysis processes
Subtasks — components of a larger text-analytics effort — typically include:
Information retrieval or identification of a corpus is a preparatory step: collecting or identifying a set of textual materials, on the Web or held in a file system, database, or content corpus manager, for analysis.
Although some text analytics systems apply exclusively advanced statistical methods, many others apply more extensive natural language processing, such as part of speech tagging, syntactic parsing, and other types of linguistic analysis.[citation needed]
Named entity recognition is the use of gazetteers or statistical techniques to identify named text features: people, organizations, place names, stock ticker symbols, certain abbreviations, and so on. 
Disambiguation — the use of contextual clues — may be required to decide where, for instance, "Ford" can refer to a former U.S. president, a vehicle manufacturer, a movie star, a river crossing, or some other entity.
Recognition of Pattern Identified Entities: Features such as telephone numbers, e-mail addresses, quantities (with units) can be discerned via regular expression or other pattern matches.
Coreference: identification of noun phrases and other terms that refer to the same object.
Relationship, fact, and event Extraction: identification of associations among entities and other information in text
Sentiment analysis involves discerning subjective (as opposed to factual) material and extracting various forms of attitudinal information: sentiment, opinion, mood, and emotion.
 Text analytics techniques are helpful in analyzing, sentiment at the entity, concept, or topic level and in distinguishing opinion holder and opinion object.[9]
Quantitative text analysis is a set of techniques stemming from the social sciences where either a human judge or a computer extracts semantic or grammatical relationships between words in order to find out the meaning or stylistic patterns of, usually, a casual personal text for the purpose of psychological profiling etc.[10]
Applications
The technology is now broadly applied for a wide variety of government, research, and business needs. 
Applications can be sorted into a number of categories by analysis type or by business function. 
Using this approach to classifying solutions, application categories include:
Enterprise Business Intelligence/Data Mining, Competitive Intelligence
E-Discovery, Records Management
National Security/Intelligence
Scientific discovery, especially Life Sciences
Sentiment Analysis Tools, Listening Platforms
Natural Language/Semantic Toolkit or Service
Publishing
Automated ad placement
Search/Information Access
Social media monitoring
Security applications
Many text mining software packages are marketed for security applications, especially monitoring and analysis of online plain text sources such as Internet news, blogs, etc. for national security purposes.
 It is also involved in the study of text encryption/decryption.
Biomedical applications
Main article: Biomedical text mining
A range of text mining applications in the biomedical literature has been described.[12]
One online text mining application in the biomedical literature is PubGene that combines biomedical text mining with network visualization as an Internet service.
 TPX is a concept-assisted search and navigation tool for biomedical literature analyses[15] - it runs on PubMed/PMC and can be configured, on request, to run on local literature repositories too.
GoPubMed is a knowledge-based search engine for biomedical texts.
Software applications
Text mining methods and software is also being researched and developed by major firms, including IBM and Microsoft, to further automate the mining and analysis processes, and by different firms working in the area of search and indexing in general as a way to improve their results. 
Within public sector much effort has been concentrated on creating software for tracking and monitoring terrorist activities.[16]
Online media applications
Text mining is being used by large media companies, such as the Tribune Company, to clarify information and to provide readers with greater search experiences, which in turn increases site "stickiness" and revenue. 
Additionally, on the back end, editors are benefiting by being able to share, associate and package news across properties, significantly increasing opportunities to monetize content.
Marketing applications
Text mining is starting to be used in marketing as well, more specifically in analytical customer relationship management.
Coussement and Van den Poel (2008)[18][19] apply it to improve predictive analytics models for customer churn (customer attrition).[18]
Sentiment analysis
Sentiment analysis may involve analysis of movie reviews for estimating how favorable a review is for a movie.
 Such an analysis may need a labeled data set or labeling of the affectivity of words.
 Resources for affectivity of words and concepts have been made for WordNet[21] and ConceptNet,[22] respectively.
Text has been used to detect emotions in the related area of affective computing.[23] Text based approaches to affective computing have been used on multiple corpora such as students evaluations, children stories and news stories.
Academic applications
The issue of text mining is of importance to publishers who hold large databases of information needing indexing for retrieval.
 This is especially true in scientific disciplines, in which highly specific information is often contained within written text.
 Academic institutions have also become involved in the text mining initiative:
The National Centre for Text Mining (NaCTeM), is the first publicly funded text mining centre in the world.
 NaCTeM is operated by the University of Manchester[24] in close collaboration with the Tsujii Lab,[25] University of Tokyo.
 NaCTeM provides customised tools, research facilities and offers advice to the academic community. 
They are funded by the Joint Information Systems Committee (JISC) and two of the UK Research Councils (EPSRC & BBSRC). 
With an initial focus on text mining in the biological and biomedical sciences, research has since expanded into the areas of social sciences.
In the United States, the School of Information at University of California, Berkeley is developing a program called BioText to assist biology researchers in text mining and analysis.
Digital Humanities and Computational Sociology
The automatic analysis of vast textual corpora has created the possibility for scholars to analyse millions of documents in multiple languages with very limited manual intervention. 
Key enabling technologies have been Parsing, Machine Translation, Topic categorization, Machine Learning.
Narrative network of US Elections 2012[27]
The automatic parsing of textual corpora has enabled the extraction of actors and their relational networks on a vast scale, turning textual data into network data. 
The resulting networks, which can contain thousands of nodes, are then analysed by using tools from Network theory to identify the key actors, the key communities or parties, and general properties such as robustness or structural stability of the overall network, or centrality of certain nodes.
 This automates the approach introduced by Quantitative Narrative Analysis,[29] whereby subject-verb-object triplets are identified with pairs of actors linked by an action, or pairs formed by actor-object.[27]
Content analysis has been a traditional part of social sciences and media studies for a long time. 
The automation of content analysis has allowed a "big data" revolution to take place in that field, with studies in social media and newspaper content that include millions of news items. 
Gender bias, readability, content similarity, reader preferences, and even mood have been analyzed based on text mining methods over millions of documents. 
 The analysis of readability, gender bias and topic bias was demonstrated in [34] showing how different topics have different gender biases and levels of readability; the possibility to detect mood shifts in a vast population by analysing Twitter content was demonstrated as well.[35]
Software
Text mining computer programs are available from many commercial and open source companies and sources. 
See List of text mining software.
Intellectual Property Law and Text Mining
Situation in Europe
Due to a lack of flexibilities in European copyright and database law, the mining of in-copyright works such as web mining without the permission of the copyright owner is not legal. 
In the UK in 2014, on the recommendation of the Hargreaves review the government amended copyright law[36] to allow text mining as a limitation and exception. 
Only the second country in the world to do so after Japan, which introduced a mining specific exception in 2009. 
However, due to the restriction of the Copyright Directive, the UK exception only allows content mining for non-commercial purposes. 
UK copyright law does not allow this provision to be overridden by contractual terms and conditions.
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.
 The focus on the solution to this legal issue being licences and not limitations and exceptions to copyright law led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[38]
Situation in United States
By contrast to Europe, the flexible nature of US copyright law, and in particular fair use means that text mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea is viewed as being legal.
 As text mining is transformative, meaning that is it does not supplant the original work, it is viewed as being lawful under fair use. 
For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitisation project of in-copyright books was lawful, in part because of the transformative uses that the digitisation project displayed - one such use being text and data mining.[39]
Implications
Until recently, websites most often used text-based searches, which only found documents containing specific user-defined words or phrases. 
ow, through use of a semantic web, text mining can find content based on meaning and context (rather than just by a specific word). 
Additionally, text mining software can be used to build large dossiers of information about specific people and events. 
For example, large datasets based on data extracted from news reports can be built to facilitate social networks analysis or counter-intelligence. 
In effect, the text mining software may act in a capacity similar to an intelligence analyst or research librarian, albeit with a more limited scope of analysis. 
Text mining is also used in some email spam filters as a way of determining the characteristics of messages that are likely to be advertisements or other unwanted material.
Traditionally, the conversion of words to concepts has been performed using a thesaurus,[2] and for computational techniques the tendency is to do the same.
 The thesauri used are either specially created for the task, or a pre-existing language model, usually related to Princeton's WordNet.
The mappings of words to concepts[3] are often ambiguous. 
Typically each word in a given language will relate to several possible concepts. 
Humans use context to disambiguate the various meanings of a given piece of text, where available machine translation systems cannot easily infer context.
For the purposes of concept mining however, these ambiguities tend to be less important than they are with machine translation, for in large documents the ambiguities tend to even out, much as is the case with text mining.
There are many techniques for disambiguation that may be used. 
Examples are linguistic analysis of the text and the use of word and concept association frequency information that may be inferred from large text corpora.
 Recently, techniques that base on semantic similarity between the possible concepts and the context have appeared and gained interest in the scientific community.
Applications
Detecting and indexing similar documents in large corpora
One of the spin-offs of calculating document statistics in the concept domain, rather than the word domain, is that concepts form natural tree structures based on hypernymy and meronymy. 
These structures can be used to produce simple tree membership statistics, that can be used to locate any document in a Euclidean concept space. 
If the size of a document is also considered as another dimension of this space then an extremely efficient indexing system can be created. 
This technique is currently in commercial use locating similar legal documents in a 2.5 million document corpus.
Clustering documents by topic
Standard numeric clustering techniques may be used in "concept space" as described above to locate and index documents by the inferred topic. 
These are numerically far more efficient than their text mining cousins, and tend to behave more intuitively, in that they map better to the similarity measures a human would generate.
Web mining - is the application of data mining techniques to discover patterns from the World Wide Web. 
Web mining can be divided into three different types – Web usage mining, Web content mining and Web structure mining.
Web usage mining
Web Usage Mining is the application of data mining techniques to discover interesting usage patterns from Web data in order to understand and better serve the needs of Web-based applications. 
Usage data captures the identity or origin of Web users along with their browsing behavior at a Web site.
Web usage mining itself can be classified further depending on the kind of usage data considered:
Web Server Data: The user logs are collected by the Web server.
 Typical data includes IP address, page reference and access time.
Application Server Data: Commercial application servers have significant features to enable e-commerce applications to be built on top of them with little effort.
 A key feature is the ability to track various kinds of business events and log them in application server logs.
Application Level Data: New kinds of events can be defined in an application, and logging can be turned on for them thus generating histories of these specially defined events. 
It must be noted, however, that many end applications require a combination of one or more of the techniques applied in the categories above.
Studies related to work [Weichbroth et al.] are concerned with two areas: constraint-based data mining algorithms applied in Web Usage Mining and developed software tools (systems). 
Costa and Seco demonstrated that web log mining can be used to extract semantic information (hyponymy relationships in particular) about the user and a given community.
Pros
Web usage mining essentially has many advantages which makes this technology attractive to corporations including the government agencies. 
This technology has enabled e-commerce to do personalized marketing, which eventually results in higher trade volumes. 
Government agencies are using this technology to classify threats and fight against terrorism.
 The predicting capability of mining applications can benefit society by identifying criminal activities.
 The companies can establish better customer relationship by giving them exactly what they need. 
Companies can understand the needs of the customer better and they can react to customer needs faster.
 The companies can find, attract and retain customers; they can save on production costs by utilizing the acquired insight of customer requirements.
 They can increase profitability by target pricing based on the profiles created. 
They can even find the customer who might default to a competitor the company will try to retain the customer by providing promotional offers to the specific customer, thus reducing the risk of losing a customer or customers.
Cons
Web usage mining by itself does not create issues, but this technology when used on data of personal nature might cause concerns. 
The most criticized ethical issue involving web usage mining is the invasion of privacy. 
Privacy is considered lost when information concerning an individual is obtained, used, or disseminated, especially if this occurs without their knowledge or consent.
 The obtained data will be analyzed, and clustered to form profiles; the data will be made anonymous before clustering so that there are no personal profiles.
Thus these applications de-individualize the users by judging them by their mouse clicks. 
De-individualization, can be defined as a tendency of judging and treating people on the basis of group characteristics instead of on their own individual characteristics and merits.[1]
Another important concern is that the companies collecting the data for a specific purpose might use the data for a totally different purpose, and this essentially violates the user’s interests.
The growing trend of selling personal data as a commodity encourages website owners to trade personal data obtained from their site. 
This trend has increased the amount of data being captured and traded increasing the likeliness of one’s privacy being invaded. 
The companies which buy the data are obliged make it anonymous and these companies are considered authors of any specific release of mining patterns.
 They are legally responsible for the contents of the release; any inaccuracies in the release will result in serious lawsuits, but there is no law preventing them from trading the data.
Some mining algorithms might use controversial attributes like sex, race, religion, or sexual orientation to categorize individuals.
 These practices might be against the anti-discrimination legislation.
 The applications make it hard to identify the use of such controversial attributes, and there is no strong rule against the usage of such algorithms with such attributes.
 This process could result in denial of service or a privilege to an individual based on his race, religion or sexual orientation. 
Right now this situation can be avoided by the high ethical standards maintained by the data mining company.
 The collected data is being made anonymous so that, the obtained data and the obtained patterns cannot be traced back to an individual. 
It might look as if this poses no threat to one’s privacy, however additional information can be inferred by the application by combining two separate unscrupulous data from the user.
Web structure mining
[icon]	This section requires expansion. (June 2015)
Web structure mining is the process of using graph theory to analyze the node and connection structure of a web site. 
According to the type of web structural data, web structure mining can be divided into two kinds:
Extracting patterns from hyperlinks in the web: a hyperlink is a structural component that connects the web page to a different location.
Mining the document structure: analysis of the tree-like structure of page structures to describe HTML or XML tag usage.
Web structure mining Terminologies:
web graph: directed graph representing web.
node: web page in graph.
edge: hyperlinks.
in degree: number of links pointing to particular node.
out degree: Number of links generated from particular node.
Techniques of web structure mining:
PageRank: this algorithm is used by Google to rank search results. The name of this algorithm is given by Google-founder Larry Page. 
The rank of a page is decided by the number of links pointing to the target node.
Web content mining
Web content mining is the mining, extraction and integration of useful data, information and knowledge from Web page content. 
In recent years these factors have prompted researchers to develop more intelligent tools for information retrieval, such as intelligent web agents, as well as to extend database and data mining techniques to provide a higher level of organization for semi-structured data available on the web. 
The agent-based approach to web mining involves the development of sophisticated AI systems that can act autonomously or semi-autonomously on behalf of a particular user, to discover and organize web-based information.
Web content mining is differentiated from two different points of view:[3] Information Retrieval View and Database View.
 summarized the research works done for unstructured data and semi-structured data from information retrieval view. 
It shows that most of the researches use bag of words, which is based on the statistics about single words in isolation, to represent unstructured text and take single word found in the training corpus as features.
 For the semi-structured data, all the works utilize the HTML structures inside the documents and some utilized the hyperlink structure between the documents for document representation. 
As for the database view, in order to have the better information management and querying on the web, the mining always tries to infer the structure of the web site to transform a web site to become a database.
There are several ways to represent documents; vector space model is typically used.
 The documents constitute the whole vector space.
 This representation does not realize the importance of words in a document.
 To resolve this, tf-idf (Term Frequency Times Inverse Document Frequency) is introduced.
By multi-scanning the document, we can implement feature selection.
 Under the condition that the category result is rarely affected, the extraction of feature subset is needed.
 The general algorithm is to construct an evaluating function to evaluate the features. 
As feature set, Information Gain, Cross Entropy, Mutual Information, and Odds Ratio are usually used. 
The classifier and pattern analysis methods of text data mining are very similar to traditional data mining techniques. 
The usual evaluative merits are Classification Accuracy, Precision, Recall and Information Score.
Web mining is an important component of content pipeline for web portals. 
It is used in data confirmation and validity verification, data integrity and building taxonomies, content management, content generation and opinion mining.[5]
Web mining in foreign languages
It should be noted that the language code of Chinese words is very complicated compared to that of English.
 The GB code, BIG5 code and HZ code are common Chinese word codes in web documents. 
Before text mining, one needs to identify the code standard of the HTML documents and transform it into inner code, then use other data mining techniques to find useful knowledge and useful patterns.
Sequential Pattern mining is a topic of data mining concerned with finding statistically relevant patterns between data examples where the values are delivered in a sequence.
 It is usually presumed that the values are discrete, and thus time series mining is closely related, but usually considered a different activity. 
Sequential pattern mining is a special case of structured data mining.
There are several key traditional computational problems addressed within this field.
 These include building efficient databases and indexes for sequence information, extracting the frequently occurring patterns, comparing sequences for similarity, and recovering missing sequence members.
 In general, sequence mining problems can be classified as string mining which is typically based on string processing algorithms and itemset mining which is typically based on association rule learning.
String Mining
String mining typically deals with a limited alphabet for items that appear in a sequence, but the sequence itself may be typically very long.
 Examples of an alphabet can be those in the ASCII character set used in natural language text, nucleotide bases 'A', 'G', 'C' and 'T' in DNA sequences, or amino acids for protein sequences. 
In biology applications analysis of the arrangement of the alphabet in strings can be used to examine gene and protein sequences to determine their properties. 
Knowing the sequence of letters of a DNA a protein is not an ultimate goal in itself.
 Rather, the major task is to understand the sequence, in terms of its structure and biological function.
 This is typically achieved first by identifying individual regions or structural units within each sequence and then assigning a function to each structural unit. 
In many cases this requires comparing a given sequence with previously studied ones. 
The comparison between the strings becomes complicated when insertions, deletions and mutations occur in a string.
A survey and taxonomy of the key algorithms for sequence comparison for bioinformatics is presented by Abouelhoda & Ghanem (2010), which include:[2]
Repeat-related problems: that deal with operations on single sequences and can be based on exact string matching or approximate string matching methods for finding dispersed fixed length and maximal length repeats, finding tandem repeats, and finding unique subsequences and missing (un-spelled) subsequences.
Alignment problems: that deal with comparison between strings by first aligning one or more sequences; examples of popular methods include BLAST for comparing a single sequence with multiple sequences in a database, and ClustalW for multiple alignments. 
Alignment algorithms can be based on either exact or approximate methods, and can also be classified as global alignments, semi-global alignments and local alignment. 
See sequence alignment.
Itemset Mining
Some problems in sequence mining lend themselves discovering frequent itemsets and the order they appear, for example, one is seeking rules of the form "if a {customer buys a car}, he or she is likely to {buy insurance} within 1 week", or in the context of stock prices, "if {Nokia up and Ericsson Up}, it is likely that {Motorola up and Samsung up} within 2 days". 
Traditionally, itemset mining is used in marketing applications for discovering regularities between frequently co-occurring items in large transactions. 
For example, by analysing transactions of customer shopping baskets in a supermarket, one can produce a rule which reads "if a customer buys onions and potatoes together, he or she is likely to also buy hamburger meat in the same transaction".
A survey and taxonomy of the key algorithms for item set mining is presented by Han et al. (2007).[3]
The two common techniques that are applied to sequence databases for frequent itemset mining are the influential apriori algorithm and the more-recent FP-Growth technique.
Application
With a great variation of products and user buying behaviors, shelf on which products are being displayed is one of the most important resources in retail environment. 
Retailers can not only increase their profit but, also decrease cost by proper management of shelf space allocation and products display.
 To solve this problem, George and Binu (2013) have proposed an approach to mine user buying patterns using PrefixSpan algorithm and place the products on shelves based on the order of mined purchasing patterns
Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify elements in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.
Most research on NER systems has been structured as taking an unannotated block of text, such as this one:
Jim bought 300 shares of Acme Corp. in 2006.
And producing an annotated block of text that highlights the names of entities:
[Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.
In this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified.
State-of-the-art NER systems for English produce near-human performance. 
For example, the best system entering MUC-7 scored 93.39% of F-measure while human annotators scored 97.60% and 96.95%
In the expression named entity, the word named restricts the task to those entities for which one or many rigid designators, as defined by Kripke, stands for the referent. 
For instance, the automotive company created by Henry Ford in 1903 is referred to as Ford or Ford Motor Company.
 Rigid designators include proper names as well as certain natural kind terms like biological species and substances.[3]
Full named-entity recognition is often broken down, conceptually and possibly also in implementations,[4] as two distinct problems: detection of names, and classification of the names by the type of entity they refer to (e.g. person, organization, location and other[5]). 
The first phase is typically simplified to a segmentation problem: names are defined to be contiguous spans of tokens, with no nesting, so that "Bank of America" is a single name, disregarding the fact that inside this name, the substring "America" is itself a name.
 This segmentation problem is formally similar to chunking.
Temporal expressions and some numerical expressions (i.e., money, percentages, etc.) may also be considered as named entities in the context of the NER task. 
While some instances of these types are good examples of rigid designators (e.g., the year 2001) there are also many invalid ones (e.g., I take my vacations in “June”). 
In the first case, the year 2001 refers to the 2001st year of the Gregorian calendar. 
In the second case, the month June may refer to the month of an undefined year (past June, next June, June 2020, etc.). 
It is arguable that the named entity definition is loosened in such cases for practical reasons. 
The definition of the term named entity is therefore not strict and often has to be explained in the context in which it is used.[6]
Certain hierarchies of named entity types have been proposed in the literature. 
BBN categories, proposed in 2002, is used for Question Answering and consists of 29 types and 64 subtypes.
 Sekine's extended hierarchy, proposed in 2002, is made of 200 subtypes.
 More recently, in 2011 Ritter used a hierarchy based on common Freebase entity types in ground-breaking experiments on NER over social media text.[9]
Formal evaluation
To evaluate the quality of a NER system's output, several measures have been defined. 
In academic conferences such as CoNLL, a variant of the F1 score has been defined as follows:[5]
Precision is the number of predicted entity name spans that line up exactly with spans in the gold standard evaluation data. 
I.e. when [Person Hans] [Person Blick] is predicted but [Person Hans Blick] was required, precision for the predicted name is zero. 
Precision is then averaged over all predicted entity names.
Recall is similarly the number of names in the gold standard that appear at exactly the same location in the predictions.
F1 score is the harmonic mean of these two.
It follows from the above definition that any prediction that misses a single token, includes a spurious token, or has the wrong class, is a hard error and does not contribute to either precision or recall.
Evaluation models based on a token-by-token matching have been proposed.
 Such models are able to handle also partially overlapping matches, yet fully rewarding only exact matches. 
They allow a finer grained evaluation and comparison of extraction systems, taking into account also the degree of mismatch in non-exact predictions.
Approaches
NER systems have been created that use linguistic grammar-based techniques as well as statistical models, i.e. machine learning. 
Hand-crafted grammar-based systems typically obtain better precision, but at the cost of lower recall and months of work by experienced computational linguists. 
Statistical NER systems typically require a large amount of manually annotated training data.
 Semisupervised approaches have been suggested to avoid part of the annotation effort.[11][12]
Many different classifier types have been used to perform machine-learned NER, with conditional random fields being a typical choice.[13]
Problem domains
Research indicates that even state-of-the-art NER systems are brittle, meaning that NER systems developed for one domain do not typically perform well on other domains.
Considerable effort is involved in tuning NER systems to perform well in a new domain; this is true for both rule-based and trainable statistical systems.
Early work in NER systems in the 1990s was aimed primarily at extraction from journalistic articles. 
Attention then turned to processing of military dispatches and reports.
 Later stages of the automatic content extraction (ACE) evaluation also included several types of informal text styles, such as weblogs and text transcripts from conversational telephone speech conversations. 
Since about 1998, there has been a great deal of interest in entity identification in the molecular biology, bioinformatics, and medical natural language processing communities. 
The most common entity of interest in that domain has been names of genes and gene products. 
There has been also considerable interest in the recognition of chemical entities and drugs in the context of the CHEMDNER competition, with 27 teams participating in this task.[15]
Current challenges and research
Despite the high F1 numbers reported on the MUC-7 dataset, the problem of Named Entity Recognition is far from being solved.
 The main efforts are directed to reducing the annotation labor by employing semi-supervised learning,[11][16] robust performance across domains[17][18] and scaling up to fine-grained entity types.
 In recent years, many projects have turned to a crowdsourcing, which is a promising solution to obtain high-quality aggregate human judgments for supervised and semi-supervised machine learning approaches to NER.
 Another challenging task is devising models to deal with linguistically complex contexts such as Twitter and search queries.[21]
A recently emerging task of identifying "important expressions" in text and cross-linking them to Wikipedia[22] [23][24] can be seen as an instance of extremely fine-grained named entity recognition, where the types are the actual Wikipedia pages describing the (potentially ambiguous) concepts. 
Below is an example output of a Wikification system:
Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. 
In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). 
Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video could be seen as information extraction.
Due to the difficulty of the problem, current approaches to IE focus on narrowly restricted domains. An example is the extraction from news wire reports of corporate mergers, such as denoted by the formal relation:
MergerBetween(company_1, company_2, date),
from an online news sentence such as:
"Yesterday, New York based Foo Inc. announced their acquisition of Bar Corp."
A broad goal of IE is to allow computation to be done on the previously unstructured data. 
A more specific goal is to allow logical reasoning to draw inferences based on the logical content of the input data. 
Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context.
Information Extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. 
The discipline of information retrieval (IR)[1][2] has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents.
 Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. 
In terms of both difficulty and emphasis, IE deals with tasks in between both IR and NLP.
 In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events in a manner that is similar to those in other documents but differing in the details. 
An example, consider a group of newswire articles on Latin American terrorism with each article is presumed to be based upon one or more terroristic acts. 
We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. 
For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened.
 An IE system for this problem is required to “understand” an attack article only enough to find data corresponding to the slots in this template.
Information extraction dates back to the late 1970s in the early days of NLP.
 An early commercial system from the mid-1980s was JASPER built for Reuters by the Carnegie Group with the aim of providing real-time financial news to financial traders.
Beginning in 1987, IE was spurred by a series of Message Understanding Conferences. 
MUC is a competition-based conference that focused on the following domains:
MUC-1 (1987), MUC-2 (1989): Naval operations messages.
MUC-3 (1991), MUC-4 (1992): Terrorism in Latin American countries.
MUC-5 (1993): Joint ventures and microelectronics domain.
MUC-6 (1995): News articles on management changes.
MUC-7 (1998): Satellite launch reports.
Considerable support came from the U.S. Defense Advanced Research Projects Agency (DARPA), who wished to automate mundane tasks performed by government analysts, such as scanning newspapers for possible links to terrorism.
Present significance
The present significance of IE pertains to the growing amount of information available in unstructured form. 
Tim Berners-Lee, inventor of the world wide web, refers to the existing Internet as the web of documents [5] and advocates that more of the content be made available as a web of data.
 Until this transpires, the web largely consists of unstructured documents lacking semantic metadata. 
Knowledge contained within these documents can be made more accessible for machine processing by means of transformation into relational form, or by marking-up with XML tags. 
An intelligent agent monitoring a news data feed requires IE to transform unstructured data into something that can be reasoned with. 
A typical application of IE is to scan a set of documents written in a natural language and populate a database with the information extracted.[7]
Tasks and subtasks
Applying information extraction on text, is linked to the problem of text simplification in order to create a structured view of the information present in free text. 
The overall goal being to create a more easily machine-readable text to process the sentences. 
Typical subtasks of IE include:
Named entity extraction which could include:
Named entity recognition: recognition of known entity names (for people and organizations), place names, temporal expressions, and certain types of numerical expressions, employing existing knowledge of the domain or information extracted from other sentences. 
Typically the recognition task involves assigning a unique identifier to the extracted entity.
A simpler task is named entity detection, which aims to detect entities without having any existing knowledge about the entity instances. 
For example, in processing the sentence "M. Smith likes fishing", named entity detection would denote detecting that the phrase "M. Smith" does refer to a person, but without necessarily having (or using) any knowledge about a certain M. Smith who is (or, "might be") the specific person whom that sentence is talking about.
Coreference resolution: detection of coreference and anaphoric links between text entities. 
In IE tasks, this is typically restricted to finding links between previously-extracted named entities.
 For example, "International Business Machines" and "IBM" refer to the same real-world entity. 
If we take the two sentences "M. 
Smith likes fishing. 
But he doesn't like biking", it would be beneficial to detect that "he" is referring to the previously detected person "M. Smith".
Relationship extraction: identification of relations between entities, such as:
PERSON works for ORGANIZATION (extracted from the sentence "Bill works for IBM.")
PERSON located in LOCATION (extracted from the sentence "Bill is in France.")
Semi-structured information extraction which may refer to any IE that tries to restore some kind information structure that has been lost through publication such as:
Table extraction: finding and extracting tables from documents.
Comments extraction : extracting comments from actual content of article in order to restore the link between author of each sentence
Language and vocabulary analysis
Terminology extraction: finding the relevant terms for a given corpus
Audio extraction
Template-based music extraction: finding relevant characteristic in an audio signal taken from a given repertoire; for instance [8] time indexes of occurrences of percussive sounds can be extracted in order to represent the essential rhythmic component of a music piece.
Note this list is not exhaustive and that the exact meaning of IE activities is not commonly accepted and that many approaches combine multiple sub-tasks of IE in order to achieve a wider goal. 
Machine learning, statistical analysis and/or natural language processing are often used in IE.
IE on non-text documents is becoming an increasing topic in research and information extracted from multimedia documents can now be expressed in a high level structure as it is done on text. 
This naturally lead to the fusion of extracted information from multiple kind of documents and sources.
World Wide Web applications
IE has been the focus of the MUC conferences.
 The proliferation of the Web, however, intensified the need for developing IE systems that help people to cope with the enormous amount of data that is available online.
 Systems that perform IE from online text should meet the requirements of low cost, flexibility in development and easy adaptation to new domains. 
MUC systems fail to meet those criteria. 
Moreover, linguistic analysis performed for unstructured text does not exploit the HTML/XML tags and layout format that are available in online text. 
As a result, less linguistically intensive approaches have been developed for IE on the Web using wrappers, which are sets of highly accurate rules that extract a particular page's content.
 Manually developing wrappers has proved to be a time-consuming task, requiring a high level of expertise.
 Machine learning techniques, either supervised or unsupervised, have been used to induce such rules automatically.
Wrappers typically handle highly structured collections of web pages, such as product catalogs and telephone directories. 
They fail, however, when the text type is less structured, which is also common on the Web. 
Recent effort on adaptive information extraction motivates the development of IE systems that can handle different types of text, from well-structured to almost free text -where common wrappers fail- including mixed types. 
Such systems can exploit shallow natural language knowledge and thus can be also applied to less structured text.
A recent development is Visual Information Extraction,[9][10] that relies on rendering a webpage in a browser and creating rules based on the proximity of regions in the rendered web page. 
This helps in extracting entities from complex web pages that may exhibit a visual pattern, but lack a discernible pattern in the HTML source code.
Approaches
Three standard approaches are now widely accepted
Hand-written regular expressions (perhaps stacked)
Using classifiers
Generative: naïve Bayes classifier
Discriminative: maximum entropy models
Sequence models
Hidden Markov model
Conditional Markov model (CMM) / Maximum-entropy Markov model (MEMM)
Conditional random fields (CRF) are commonly used in conjunction with IE for tasks as varied as extracting information from research papers[11] to extracting navigation instructions.
Numerous other approaches exist for IE including hybrid approaches that combine some of the standard approaches previously listed.
Steganography (US Listeni/ˌstɛ.ɡəˈnɔː.ɡrə.fi/, UK /ˌstɛɡ.əˈnɒɡ.rə.fi/) is the practice of concealing a file, message, image, or video within another file, message, image, or video. 
The word steganography combines the Greek words steganos (στεγανός), meaning "covered, concealed, or protected", and graphein (γράφειν) meaning "writing".
The first recorded use of the term was in 1499 by Johannes Trithemius in his Steganographia, a treatise on cryptography and steganography, disguised as a book on magic.
 Generally, the hidden messages appear to be (or be part of) something else: images, articles, shopping lists, or some other cover text. 
For example, the hidden message may be in invisible ink between the visible lines of a private letter. 
Some implementations of steganography that lack a shared secret are forms of security through obscurity, whereas key-dependent steganographic schemes adhere to Kerckhoffs's principle.
The advantage of steganography over cryptography alone is that the intended secret message does not attract attention to itself as an object of scrutiny. 
Plainly visible encrypted messages—no matter how unbreakable—arouse interest, and may in themselves be incriminating in countries where encryption is illegal.[2]
 Thus, whereas cryptography is the practice of protecting the contents of a message alone, steganography is concerned with concealing the fact that a secret message is being sent, as well as concealing the contents of the message.
Steganography includes the concealment of information within computer files. 
In digital steganography, electronic communications may include steganographic coding inside of a transport layer, such as a document file, image file, program or protocol. 
Media files are ideal for steganographic transmission because of their large size. 
For example, a sender might start with an innocuous image file and adjust the color of every 100th pixel to correspond to a letter in the alphabet, a change so subtle that someone not specifically looking for it is unlikely to notice it.
The first recorded uses of steganography can be traced back to 440 BC when Herodotus mentions two examples in his Histories.
[3] Demaratus sent a warning about a forthcoming attack to Greece by writing it directly on the wooden backing of a wax tablet before applying its beeswax surface. 
Wax tablets were in common use then as reusable writing surfaces, sometimes used for shorthand.
In his work Polygraphiae Johannes Trithemius developed his so-called "Ave-Maria-Cipher" that can hide information in a Latin praise of God. 
"Auctor Sapientissimus Conseruans Angelica Deferat Nobis Charitas Potentissimi Creatoris" for example contains the concealed word VICIPEDIA.[4]
Techniques[edit]
Deciphering the code. Steganographia
Physical[edit]
Steganography has been widely used, including in recent historical times and the present day. Known examples include:
Hidden messages within wax tablet—in ancient Greece, people wrote messages on wood and covered it with wax that bore an innocent covering message.
Hidden messages on messenger's body—also used in ancient Greece. 
Herodotus tells the story of a message tattooed on the shaved head of a slave of Histiaeus, hidden by the hair that afterwards grew over it, and exposed by shaving the head. 
The message allegedly carried a warning to Greece about Persian invasion plans. 
This method has obvious drawbacks, such as delayed transmission while waiting for the slave's hair to grow, and restrictions on the number and size of messages that can be encoded on one person's scalp.
During World War II, the French Resistance sent some messages written on the backs of couriers in invisible ink.
Hidden messages on paper written in secret inks, under other messages or on the blank parts of other messages
Messages written in Morse code on yarn and then knitted into a piece of clothing worn by a courier.
Messages written on envelopes in the area covered by postage stamps.
In the early days of the printing press, it was common to mix different typefaces on a printed page due to the printer not having enough copies of some letters in one typeface. 
Because of this, a message could be hidden using two (or more) different typefaces, such as normal or italic.
During and after World War II, espionage agents used photographically produced microdots to send information back and forth. 
Microdots were typically minute (less than the size of the period produced by a typewriter). 
World War II microdots were embedded in the paper and covered with an adhesive, such as collodion. 
This was reflective, and thus detectable by viewing against glancing light.
 Alternative techniques included inserting microdots into slits cut into the edge of post cards.
During WWII, Velvalee Dickinson, a spy for Japan in New York City, sent information to accommodation addresses in neutral South America.
 She was a dealer in dolls, and her letters discussed the quantity and type of doll to ship. 
The stegotext was the doll orders, while the concealed "plaintext" was itself encoded and gave information about ship movements, etc. 
Her case became somewhat famous and she became known as the Doll Woman.
Jeremiah Denton repeatedly blinked his eyes in Morse Code during the 1966 televised press conference that he was forced into as an American POW by his North Vietnamese captors, spelling out "T-O-R-T-U-R-E". 
This confirmed for the first time to the U.S. Military (naval intelligence) and Americans that the North Vietnamese were torturing American POWs.
Cold War counter-propaganda. 
In 1968, crew members of the USS Pueblo intelligence ship held as prisoners by North Korea, communicated in sign language during staged photo opportunities, informing the United States they were not defectors, but captives of the North Koreans. 
In other photos presented to the US, crew members gave "the finger" to the unsuspecting North Koreans, in an attempt to discredit photos that showed them smiling and comfortable.
Digital messages[edit]
Image of a tree with a steganographically hidden image. 
The hidden image is revealed by removing all but the two least significant bits of each color component and a subsequent normalization. The hidden image is shown below.
Image of a cat extracted from the tree image above.
modern steganography entered the world in 1985 with the advent of personal computers being applied to classical steganography problems.
[5] Development following that was very slow, but has since taken off, going by the large number of steganography software available:
Concealing messages within the lowest bits of noisy images or sound files.
Concealing data within encrypted data or within random data. 
The message to conceal is encrypted, then used to overwrite part of a much larger block of encrypted data or a block of random data (an unbreakable cipher like the one-time pad generates ciphertexts that look perfectly random without the private key).
Chaffing and winnowing.
Mimic functions convert one file to have the statistical profile of another.
 This can thwart statistical methods that help brute-force attacks identify the right solution in a ciphertext-only attack.
Concealed messages in tampered executable files, exploiting redundancy in the targeted instruction set.
Pictures embedded in video material (optionally played at slower or faster speed).
Injecting imperceptible delays to packets sent over the network from the keyboard. 
Delays in keypresses in some applications (telnet or remote desktop software) can mean a delay in packets, and the delays in the packets can be used to encode data.
Changing the order of elements in a set.
Content-Aware Steganography hides information in the semantics a human user assigns to a datagram.
 These systems offer security against a nonhuman adversary/warden.
Blog-Steganography. Messages are fractionalized and the (encrypted) pieces are added as comments of orphaned web-logs (or pin boards on social network platforms). 
In this case the selection of blogs is the symmetric key that sender and recipient are using; the carrier of the hidden message is the whole blogosphere.
Modifying the echo of a sound file (Echo Steganography).[6]
Steganography for audio signals.[7]
Image bit-plane complexity segmentation steganography
Including data in ignored sections of a file, such as after the logical end of the carrier file.
Digital text[edit]
Making text the same color as the background in word processor documents, e-mails, and forum posts.
Using Unicode characters that look like the standard ASCII character set. On most systems, there is no visual difference from ordinary text. 
Some systems may display the fonts differently, and the extra information would then be easily spotted, of course.
Using hidden (control) characters, and redundant use of markup (e.g., empty bold, underline or italics) to embed information within HTML, which is visible by examining the document source. 
HTML pages can contain code for extra blank spaces and tabs at the end of lines, and colours, fonts and sizes, which are not visible when displayed.
Using non-printing Unicode characters Zero-Width Joiner (ZWJ) and Zero-Width Non-Joiner (ZWNJ).[8] 
These characters are used for joining and disjoining letters in Arabic and Persian, but can be used in Roman alphabets for hiding information because they have no meaning in Roman alphabets: because they are "zero-width" they are not displayed. 
ZWJ and ZWNJ can represent "1" and "0".
Social steganography[edit]
In communities with social or government taboos or censorship, people use cultural steganography—hiding messages in idiom, pop culture references, and other messages they share publicly and assume are monitored. 
This relies on social context to make the underlying messages visible only to certain readers.[9][10] Examples include:
Hiding a message in the title and context of a shared video or image
Misspelling names or words that are popular in the media in a given week, to suggest an alternate meaning
Network[edit]
All information hiding techniques that may be used to exchange steganograms in telecommunication networks can be classified under the general term of network steganography. 
This nomenclature was originally introduced by Krzysztof Szczypiorski in 2003.[11] 
Contrary to typical steganographic methods that use digital media (images, audio and video files) to hide data, network steganography uses communication protocols' control elements and their intrinsic functionality. 
As a result, such methods are harder to detect and eliminate.[12]
Typical network steganography methods involve modification of the properties of a single network protocol. 
Such modification can be applied to the PDU (Protocol Data Unit),[13][14][15] to the time relations between the exchanged PDUs,[16] or both (hybrid methods).[17]
Moreover, it is feasible to utilize the relation between two or more different network protocols to enable secret communication. 
These applications fall under the term inter-protocol steganography.[18]
Network steganography covers a broad spectrum of techniques, which include, among others:
Steganophony — the concealment of messages in Voice-over-IP conversations, e.g. the employment of delayed or corrupted packets that would normally be ignored by the receiver (this method is called LACK — Lost Audio Packets Steganography), or, alternatively, hiding information in unused header fields.[19]
WLAN Steganography – transmission of steganograms in Wireless Local Area Networks. 
A practical example of WLAN Steganography is the HICCUPS system (Hidden Communication System for Corrupted Networks)[20]
Steganography is a kind of cryptography which is the art and science of hiding information by embedding messages within other, seemingly harmless messages. 
Printed[edit]
Digital steganography output may be in the form of printed documents. 
A message, the plaintext, may be first encrypted by traditional means, producing a ciphertext. 
Then, an innocuous covertext is modified in some way so as to contain the ciphertext, resulting in the stegotext.
 For example, the letter size, spacing, typeface, or other characteristics of a covertext can be manipulated to carry the hidden message. 
Only a recipient who knows the technique used can recover the message and then decrypt it. 
Francis Bacon developed Bacon's cipher as such a technique.
The ciphertext produced by most digital steganography methods, however, is not printable. 
Traditional digital methods rely on perturbing noise in the channel file to hide the message, as such, the channel file must be transmitted to the recipient with no additional noise from the transmission. 
Printing introduces much noise in the ciphertext, generally rendering the message unrecoverable. 
There are techniques that address this limitation, one notable example is ASCII Art Steganography.[21]
Using puzzles[edit]
The art of concealing data in a puzzle can take advantage of the degrees of freedom in stating the puzzle, using the starting information to encode a key within the puzzle / puzzle image.
For instance, steganography using sudoku puzzles has as many keys as there are possible solutions of a sudoku puzzle, which is 6.71×1021. 
This is equivalent to around 70 bits, making it much stronger than the DES method, which uses a 56 bit key.[22]
Additional terminology[edit]
Discussions of steganography generally use terminology analogous to (and consistent with) conventional radio and communications technology. 
However, some terms show up in software specifically, and are easily confused. 
These are most relevant to digital steganographic systems.
The payload is the data covertly communicated. 
The carrier is the signal, stream, or data file that hides the payload—which differs from the channel (which typically means the type of input, such as a JPEG image). 
The resulting signal, stream, or data file with the encoded payload is sometimes called the package, stego file, or covert message. 
The percentage of bytes, samples, or other signal elements modified to encode the payload is called the encoding density, and is typically expressed as a number between 0 and 1.
In a set of files, those files considered likely to contain a payload are suspects. 
A suspect identified through some type of statistical analysis might be referred to as a candidate.
Countermeasures and detection[edit]
Detecting physical steganography requires careful physical examination—including the use of magnification, developer chemicals and ultraviolet light. 
It is a time-consuming process with obvious resource implications, even in countries that employ large numbers of people to spy on their fellow nationals. 
However, it is feasible to screen mail of certain suspected individuals or institutions, such as prisons or prisoner-of-war (POW) camps.
During World War II, prisoner of war camps gave prisoners specially treated paper that would reveal invisible ink. 
An article in the 24 June 1948 issue of Paper Trade Journal by the Technical Director of the United States Government Printing Office, Morris S. 
Kantrowitz, describes, in general terms, the development of this paper. 
They used three prototype papers named Sensicoat, Anilith, and Coatalith.
 These were for the manufacture of post cards and stationery provided to German prisoners of war in the US and Canada. 
If POWs tried to write a hidden message, the special paper rendered it visible. 
The U.S. granted at least two patents related to this technology—one to Kantrowitz, U.S. 
Patent 2,515,232, "Water-Detecting paper and Water-Detecting Coating Composition Therefor," patented 18 July 1950, and an earlier one, "Moisture-Sensitive Paper and the Manufacture Thereof", U.S. 
Patent 2,445,586, patented 20 July 1948. 
A similar strategy is to issue prisoners with writing paper ruled with a water-soluble ink that runs in contact with water-based invisible ink.
In computing, steganographically encoded package detection is called steganalysis. 
The simplest method to detect modified files, however, is to compare them to known originals. 
For example, to detect information being moved through the graphics on a website, an analyst can maintain known clean-copies of these materials and compare them against the current contents of the site. 
The differences, assuming the carrier is the same, comprise the payload. 
In general, using extremely high compression rates makes steganography difficult, but not impossible.
 Compression errors provide a hiding place for data, but high compression reduces the amount of data available to hold the payload, raising the encoding density, which facilitates easier detection (in extreme cases, even by casual observation).
Applications[edit]
Use in modern printers[edit]
Main article: Printer steganography
Some modern computer printers use steganography, including HP and Xerox brand color laser printers. 
These printers add tiny yellow dots to each page. 
The barely-visible dots contain encoded printer serial numbers and date and time stamps.[23]
Example from modern practice[edit]
The larger the cover message (in binary data, the number of bits) relative to the hidden message, the easier it is to hide the latter. 
For this reason, digital pictures (which contain large amounts of data) are used to hide messages on the Internet and on other communication media. 
It is not clear how common this actually is. 
For example: a 24-bit bitmap uses 8 bits to represent each of the three color values (red, green, and blue) at each pixel.
 The blue alone has 28 different levels of blue intensity. 
The difference between 11111111 and 11111110 in the value for blue intensity is likely to be undetectable by the human eye.
 Therefore, the least significant bit can be used more or less undetectably for something else other than color information.
 If this is repeated for the green and the red elements of each pixel as well, it is possible to encode one letter of ASCII text for every three pixels.
Any medium can be a carrier, but media with a large amount of redundant or compressible information are better suited.
From an information theoretical point of view, this means that the channel must have more capacity than the "surface" signal requires; that is, there must be redundancy. 
For a digital image, this may be noise from the imaging element; for digital audio, it may be noise from recording techniques or amplification equipment. 
In general, electronics that digitize an analog signal suffer from several noise sources such as thermal noise, flicker noise, and shot noise. 
This noise provides enough variation in the captured digital information that it can be exploited as a noise cover for hidden data. 
In addition, lossy compression schemes (such as JPEG) always introduce some error into the decompressed data; it is possible to exploit this for steganographic use as well.
Steganography can be used for digital watermarking, where a message (being simply an identifier) is hidden in an image so that its source can be tracked or verified (for example, Coded Anti-Piracy), or even just to identify an image (as in the EURion constellation).
Alleged use by intelligence services[edit]
In 2010, the Federal Bureau of Investigation alleged that the Russian foreign intelligence service uses customized steganography software for embedding encrypted text messages inside image files for certain communications with "illegal agents" (agents under non-diplomatic cover) stationed abroad.[24]
Distributed steganography[edit]
There are distributed steganography methods,[25] including methodologies that distribute the payload through multiple carrier files in diverse locations to make detection more difficult. 
For example, U.S. Patent 8,527,779 by cryptographer William Easttom (Chuck Easttom).
Online challenge[edit]
The puzzles presented by Cicada 3301 incorporates steganography with cryptography and other solving techniques since 2012.
Sentiment analysis (also known as opinion mining) refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials. 
Sentiment analysis is widely applied to reviews and social media for a variety of applications, ranging from marketing to customer service.
Generally speaking, sentiment analysis aims to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document. 
The attitude may be his or her judgment or evaluation (see appraisal theory), affective state (that is to say, the emotional state of the author when writing), or the intended emotional communication (that is to say, the emotional effect the author wishes to have on the reader).
A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level — whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral. 
Advanced, "beyond polarity" sentiment classification looks, for instance, at emotional states such as "angry," "sad," and "happy."
Early work in that area includes Turney[1] and Pang[2] who applied different methods for detecting the polarity of product reviews and movie reviews respectively. 
This work is at the document level. 
One can also classify a document's polarity on a multi-way scale, which was attempted by Pang[3] and Snyder[4] among others: Pang and Lee[3] expanded the basic task of classifying a movie review as either positive or negative to predicting star ratings on either a 3 or a 4 star scale, while Snyder[4] performed an in-depth analysis of restaurant reviews, predicting ratings for various aspects of the given restaurant, such as the food and atmosphere (on a five-star scale). 
Even though in most statistical classification methods, the neutral class is ignored under the assumption that neutral texts lie near the boundary of the binary classifier, several researchers suggest that, as in every polarity problem, three categories must be identified. 
Moreover it can be proven that specific classifiers such as the Max Entropy[5] and the SVMs[6] can benefit from the introduction of neutral class and improve the overall accuracy of the classification.
A different method for determining sentiment is the use of a scaling system whereby words commonly associated with having a negative, neutral or positive sentiment with them are given an associated number on a -10 to +10 scale (most negative up to most positive) and when a piece of unstructured text is analyzed using natural language processing, the subsequent concepts are analyzed for an understanding of these words and how they relate to the concept.[citation needed] 
Each concept is then given a score based on the way sentiment words relate to the concept, and their associated score. 
This allows movement to a more sophisticated understanding of sentiment based on an 11 point scale.
 Alternatively, texts can be given a positive and negative sentiment strength score if the goal is to determine the sentiment in a text rather than the overall polarity and strength of the text.[7]
Subjectivity/objectivity identification
This task is commonly defined as classifying a given text (usually a sentence) into one of two classes: objective or subjective.[8]
 This problem can sometimes be more difficult than polarity classification.[9] 
The subjectivity of words and phrases may depend on their context and an objective document may contain subjective sentences (e.g., a news article quoting people's opinions). 
Moreover, as mentioned by Su,[10] results are largely dependent on the definition of subjectivity used when annotating texts. 
However, Pang[11] showed that removing objective sentences from a document before classifying its polarity helped improve performance.
Feature/aspect-based sentiment analysis
It refers to determining the opinions or sentiments expressed on different features or aspects of entities, e.g., of a cell phone, a digital camera, or a bank.[12]
 A feature or aspect is an attribute or component of an entity, e.g., the screen of a cell phone, the service for a restaurant, or the picture quality of a camera.
 The advantage of feature-based sentiment analysis is the possibility to capture nuances about objects of interest.
 Different features can generate different sentiment responses, for example a hotel can have a convenient location, but mediocre food.[13]
 This problem involves several sub-problems, e.g., identifying relevant entities, extracting their features/aspects, and determining whether an opinion expressed on each feature/aspect is positive, negative or neutral.[14] 
The automatic identification of features can be performed with syntactic methods or with topic modeling.
 More detailed discussions about this level of sentiment analysis can be found in Liu's work.[17]
Methods and features
Existing approaches to sentiment analysis can be grouped into four main categories: keyword spotting, lexical affinity, statistical methods, and concept-level techniques.[18] 
Keyword spotting classifies text by affect categories based on the presence of unambiguous affect words such as happy, sad, afraid, and bored.[19]
 Lexical affinity not only detects obvious affect words, it also assigns arbitrary words a probable “affinity” to particular emotions.[20] 
Statistical methods leverage on elements from machine learning such as latent semantic analysis, support vector machines, "bag of words" and Semantic Orientation — Pointwise Mutual Information (See Peter Turney's[1] work in this area). 
More sophisticated methods try to detect the holder of a sentiment (i.e. the person who maintains that affective state) and the target (i.e. the entity about which the affect is felt).[21]
 To mine the opinion in context and get the feature which has been opinionated, the grammatical relationships of words are used. 
Grammatical dependency relations are obtained by deep parsing of the text.[22] 
Unlike purely syntactical techniques, concept-level approaches leverage on elements from knowledge representation such as ontologies and semantic networks and, hence, are also able to detect semantics that are expressed in a subtle manner, e.g., through the analysis of concepts that do not explicitly convey relevant information, but which are implicitly linked to other concepts that do so.[23]
Open source software tools deploy machine learning, statistics, and natural language processing techniques to automate sentiment analysis on large collections of texts, including web pages, online news, internet discussion groups, online reviews, web blogs, and social media.[24] 
Knowledge-based systems, instead, make use of publicly available resources, e.g., WordNet-Affect,[25] SentiWordNet,[26] SenticNet,[27][28] and Emoji Sentiment Ranking[29][30] to extract the semantic and affective information associated with natural language concepts. 
Sentiment Analysis can also be performed on visual content i.e. images and videos. 
One of the first approach in this direction is SentiBank[31] utilizing an adjective noun pair representation of visual content.
A human analysis component is required in sentiment analysis, as automated systems are not able to analyze historical tendencies of the individual commenter, or the platform and are often classified incorrectly in their expressed sentiment. 
Automation impacts approximately 23% of comments that are correctly classified by humans.[32]
Sometimes, the structure of sentiments and topics is fairly complex.
 Also, the problem of sentiment analysis is non-monotonic in respect to sentence extension and stop-word substitution (compare THEY would not let my dog stay in this hotel vs I would not let my dog stay in this hotel).
 To address this issue a number of rule-based and reasoning-based approaches have been applied to sentiment analysis, including Defeasible Logic Programming.[33]
 Also, there is a number of tree traversal rules applied to syntactic parse tree to extract the topicality of sentiment in open domain setting.[34][35]
Evaluation
The accuracy of a sentiment analysis system is, in principle, how well it agrees with human judgments.
 This is usually measured by precision and recall. 
However, according to research human raters typically agree 79%[36] of the time (see Inter-rater reliability).
Thus, a 70% accurate program is doing nearly as well as humans, even though such accuracy may not sound impressive. 
If a program were "right" 100% of the time, humans would still disagree with it about 20% of the time, since they disagree that much about any answer .[37]
 More sophisticated measures can be applied, but evaluation of sentiment analysis systems remains a complex matter. 
For sentiment analysis tasks returning a scale rather than a binary judgement, correlation is a better measure than precision because it takes into account how close the predicted value is to the target value.
Sentiment analysis and Web 2.0
The rise of social media such as blogs and social networks has fueled interest in sentiment analysis. 
With the proliferation of reviews, ratings, recommendations and other forms of online expression, online opinion has turned into a kind of virtual currency for businesses looking to market their products, identify new opportunities and manage their reputations. 
As businesses look to automate the process of filtering out the noise, understanding the conversations, identifying the relevant content and actioning it appropriately, many are now looking to the field of sentiment analysis.[38] 
Further complicating the matter, is the rise of anonymous social media platforms such as 4chan and Reddit.[39] 
If web 2.0 was all about democratizing publishing, then the next stage of the web may well be based on democratizing data mining of all the content that is getting published.[40]
One step towards this aim is accomplished in research.
 Several research teams in universities around the world currently focus on understanding the dynamics of sentiment in e-communities through sentiment analysis.[41] 
The CyberEmotions project, for instance, recently identified the role of negative emotions in driving social networks discussions.[42]
The problem is that most sentiment analysis algorithms use simple terms to express sentiment about a product or service. 
However, cultural factors, linguistic nuances and differing contexts make it extremely difficult to turn a string of written text into a simple pro or con sentiment.[38] 
The fact that humans often disagree on the sentiment of text illustrates how big a task it is for computers to get this right. 
The shorter the string of text, the harder it becomes.
Even though short text strings might be a problem, sentiment analysis within microblogging has shown that Twitter can be seen as a valid online indicator of political sentiment. 
Tweets’ political sentiment demonstrates close correspondence to parties’ and politicians’ political positions, indicating that the content of Twitter messages plausibly reflects the offline political landscape.
In bioinformatics, sequence analysis is the process of subjecting a DNA, RNA or peptide sequence to any of a wide range of analytical methods to understand its features, function, structure, or evolution. 
Methodologies used include sequence alignment, searches against biological databases, and others.[1]
 Since the development of methods of high-throughput production of gene and protein sequences, the rate of addition of new sequences to the databases increased exponentially. 
Such a collection of sequences does not, by itself, increase the scientist's understanding of the biology of organisms. 
However, comparing these new sequences to those with known functions is a key way of understanding the biology of an organism from which the new sequence comes. 
Thus, sequence analysis can be used to assign function to genes and proteins by the study of the similarities between the compared sequences. 
Nowadays, there are many tools and techniques that provide the sequence comparisons (sequence alignment) and analyze the alignment product to understand its biology.
Sequence analysis in molecular biology includes a very wide range of relevant topics:
The comparison of sequences in order to find similarity, often to infer if they are related (homologous)
Identification of intrinsic features of the sequence such as active sites, post translational modification sites, gene-structures, reading frames, distributions of introns and exons and regulatory elements
Identification of sequence differences and variations such as point mutations and single nucleotide polymorphism (SNP) in order to get the genetic marker.
Revealing the evolution and genetic diversity of sequences and organisms
Identification of molecular structure from sequence alone
In chemistry, sequence analysis comprises techniques used to determine the sequence of a polymer formed of several monomers. 
In molecular biology and genetics, the same process is called simply "sequencing".
In marketing, sequence analysis is often used in analytical customer relationship management applications, such as NPTB models (Next Product to Buy).
In sociology, sequence methods are increasingly used to study life-course and career trajectories, patterns of organizational and national development, conversation and interaction structure, and the problem of work/family synchrony. 
This body of research has given rise to the emerging subfield of social sequence analysis.
Since the very first sequences of the insulin protein was characterised by Fred Sanger in 1951 biologists have been trying to use this knowledge to understand the function of molecules.
 According to Michael Levitt, sequence analysis was born in the period from 1969-1977.
 In 1969 the analysis of sequences of transfer RNAs were used to infer residue interactions from correlated changes in the nucleotide sequences giving rise to a model of the tRNA secondary structure.
 In 1970, Saul B. Needleman and Christian D. Wunsch published the first computer algorithm for aligning two sequences.
Over this time developments in obtaining nucleotide sequence greatly improved leading to the publication of the first complete genome of a bacteriophage in 1977.[7]
Sequence Alignment
Main article: Sequence_alignment § Pairwise_alignment
Main article: Multiple sequence alignment
Example multiple sequence alignment
There are millions of protein and nucleotide sequences known. 
These sequences fall into many groups of related sequences known as protein families or gene families. 
Relationships between these sequences are usually discovered by aligning them together and assigning this alignment a score. 
There are two main types of sequence alignment.
 Pair-wise sequence alignment only compares two sequences at a time and multiple sequence alignment compares many sequences in one go. 
Two important algorithms for aligning pairs of sequences are the Needleman-Wunsch algorithm and the Smith-Waterman algorithm. 
Popular tools for sequence alignment include:
Pair-wise alignment - BLAST
Multiple alignment - ClustalW, PROBCONS, MUSCLE, MAFFT, and T-Coffee.
A common use for pairwise sequence alignment is to take a sequence of interest and compare it to all known sequences in a database to identify homologous sequences.
In general the matches in the database are ordered to show the most closely related sequences first followed by sequences with diminishing similarity. 
These matches are usually reported with a measure of statistical significance such as an Expectation value.
Profile comparison
In 1987, Michael Gribskov, Andrew McLachlan, and David Eisenberg introduced the method of profile comparison for identifying distant similarities between proteins.[8] 
Rather than using a single sequence, profile methods use a multiple sequence alignment to encode a profile which contains information about the conservation level of each residue. 
These profiles can then be used to search collections of sequences to find sequences that are related. 
Profiles are also known as Position Specific Scoring Matrices (PSSMs). 
In 1993, a probabilistic interpretation of profiles was introduced by David Haussler and colleagues using hidden Markov models.
These models have become known as profile-HMMs.
In recent years,[when?] methods have been developed that allow the comparison of profiles directly to each other. 
These are known as profile-profile comparison methods.[11]
Sequence assembly
Main article: Sequence assembly
Sequence assembly refers to the reconstruction of a DNA sequence by aligning and merging small DNA fragments.
 It is an integral part of modern DNA sequencing. 
Since presently-available DNA sequencing technologies are ill-suited for reading long sequences, large pieces of DNA (such as genomes) are often sequenced by (1) cutting the DNA into small pieces, (2) reading the small fragments, and (3) reconstituting the original DNA by merging the information on various fragment.
Gene prediction
Main article: Gene prediction
Gene prediction or gene finding refers to the process of identifying the regions of genomic DNA that encode genes. 
This includes protein-coding genes as well as RNA genes, but may also include prediction of other functional elements such as regulatory regions. 
Gene finding is one of the first and most important steps in understanding the genome of a species once it has been sequenced. 
In general the prediction of bacterial genes is significantly simpler and more accurate than the prediction of genes in eukaryotic species that usually have complex intron/exon patterns.
protein Structure Prediction
Main article: Threading (protein sequence)
Target protein structure (3dsm, shown in ribbons), with Calpha backbones (in gray) of 354 predicted models for it submitted in the CASP8 structure-prediction experiment.
The 3D structures of molecules are of great importance to their functions in nature.
 Since structural prediction of large molecules at an atomic level is largely intractable problem, some biologists introduced ways to predict 3D structure at a primary sequence level. 
This includes biochemical or statistical analysis of amino acid residues in local regions and structural inference from homologs (or other potentially related proteins) with known 3D structures.
There have been a large number of diverse approaches to solve the structure prediction problem. 
In order to determine which methods were most effective a structure prediction competition was founded called CASP (Critical Assessment of Structure Prediction).[12]
Methodology
The tasks that lie in the space of sequence analysis are often non-trivial to resolve and require the use of relatively complex approaches.
 Of the many types of methods used in practice, the most popular include:
The Semantic Web is an extension of the Web through standards by the World Wide Web Consortium (W3C).[1] 
The standards promote common data formats and exchange protocols on the Web, most fundamentally the Resource Description Framework (RDF).
According to the W3C, "The Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries".[2]
 The term was coined by Tim Berners-Lee for a web of data that can be processed by machines.[3] 
While its critics have questioned its feasibility, proponents argue that applications in industry, biology and human sciences research have already proven the validity of the original concept.[4]
The 2001 Scientific American article by Berners-Lee, Hendler, and Lassila described an expected evolution of the existing Web to a Semantic Web.[5] 
In 2006, Berners-Lee and colleagues stated that: "This simple idea…remains largely unrealized".[6] 
In 2013, more than four million Web domains contained Semantic Web markup
The concept of the Semantic Network Model was formed in the early 1960s by the cognitive scientist Allan M. Collins, linguist M. Ross Quillian and psychologist Elizabeth F. Loftus as a form to represent semantically structured knowledge. 
When applied in the context of the modern internet, it extends the network of hyperlinked human-readable web pages by inserting machine-readable metadata about pages and how they are related to each other. 
This enables automated agents to access the Web more intelligently and perform more tasks on behalf of users. 
The term "Semantic Web" was coined by Tim Berners-Lee,[3] the inventor of the World Wide Web and director of the World Wide Web Consortium ("W3C"), which oversees the development of proposed Semantic Web standards. 
He defines the Semantic Web as "a web of data that can be processed directly and indirectly by machines".
Many of the technologies proposed by the W3C already existed before they were positioned under the W3C umbrella. 
These are used in various contexts, particularly those dealing with information that encompasses a limited and defined domain, and where sharing data is a common necessity, such as scientific research or data exchange among businesses. 
In addition, other technologies with similar goals have emerged, such as microformats.
Tim Berners-Lee originally expressed the vision of the Semantic Web as follows:
I have a dream for the Web [in which computers] become capable of analyzing all the data on the Web – the content, links, and transactions between people and computers.
 A "Semantic Web", which makes this possible, has yet to emerge, but when it does, the day-to-day mechanisms of trade, bureaucracy and our daily lives will be handled by machines talking to machines. 
The "intelligent agents" people have touted for ages will finally materialize.[8]
The Semantic Web is regarded as an integrator across different content, information applications and systems. 
It has applications in publishing, blogging, and many other areas.
Limitations of HTML
Many files on a typical computer can also be loosely divided into human readable documents and machine readable data. 
Documents like mail messages, reports, and brochures are read by humans. 
Data, like calendars, addressbooks, playlists, and spreadsheets are presented using an application program that lets them be viewed, searched and combined.
Currently, the World Wide Web is based mainly on documents written in Hypertext Markup Language (HTML), a markup convention that is used for coding a body of text interspersed with multimedia objects such as images and interactive forms. 
Metadata tags provide a method by which computers can categorise the content of web pages, for example:
<meta name="keywords" content="computing, computer studies, computer" />
<meta name="description" content="Cheap widgets for sale" />
<meta name="author" content="John Doe" />
With HTML and a tool to render it (perhaps web browser software, perhaps another user agent), one can create and present a page that lists items for sale. 
The HTML of this catalog page can make simple, document-level assertions such as "this document's title is 'Widget Superstore'", but there is no capability within the HTML itself to assert unambiguously that, for example, item number X586172 is an Acme Gizmo with a retail price of €199, or that it is a consumer product. 
Rather, HTML can only say that the span of text "X586172" is something that should be positioned near "Acme Gizmo" and "€199", etc. 
There is no way to say "this is a catalog" or even to establish that "Acme Gizmo" is a kind of title or that "€199" is a price.
 There is also no way to express that these pieces of information are bound together in describing a discrete item, distinct from other items perhaps listed on the page.
Semantic HTML refers to the traditional HTML practice of markup following intention, rather than specifying layout details directly. 
For example, the use of <em> denoting "emphasis" rather than <i>, which specifies italics. 
Layout details are left up to the browser, in combination with Cascading Style Sheets. 
But this practice falls short of specifying the semantics of objects such as items for sale or prices.
Microformats extend HTML syntax to create machine-readable semantic markup about objects including people, organisations, events and products.[9] 
Similar initiatives include RDFa, Microdata and Schema.org.
Semantic Web solutions
The Semantic Web takes the solution further. 
It involves publishing in languages specifically designed for data: Resource Description Framework (RDF), Web Ontology Language (OWL), and Extensible Markup Language (XML). 
HTML describes documents and the links between them. 
RDF, OWL, and XML, by contrast, can describe arbitrary things such as people, meetings, or airplane parts.
These technologies are combined in order to provide descriptions that supplement or replace the content of Web documents.
 Thus, content may manifest itself as descriptive data stored in Web-accessible databases,[10] or as markup within documents (particularly, in Extensible HTML (XHTML) interspersed with XML, or, more often, purely in XML, with layout or rendering cues stored separately). 
The machine-readable descriptions enable content managers to add meaning to the content, i.e., to describe the structure of the knowledge we have about that content. 
In this way, a machine can process knowledge itself, instead of text, using processes similar to human deductive reasoning and inference, thereby obtaining more meaningful results and helping computers to perform automated information gathering and research.
An example of a tag that would be used in a non-semantic web page:
<item>blog</item>
Encoding similar information in a semantic web page might look like this:
<item rdf:about="http://example.org/semantic-web/">Semantic Web</item>
Tim Berners-Lee calls the resulting network of Linked Data the Giant Global Graph, in contrast to the HTML-based World Wide Web. 
Berners-Lee posits that if the past was document sharing, the future is data sharing. 
His answer to the question of "how" provides three points of instruction. One, a URL should point to the data. 
Two, anyone accessing the URL should get data back. 
Three, relationships in the data should point to additional URLs with data.
Web 3.0
Tim Berners-Lee has described the semantic web as a component of "Web 3.0".[11]
People keep asking what Web 3.0 is. I think maybe when you've got an overlay of scalable vector graphics – everything rippling and folding and looking misty – on Web 2.0 and access to a semantic Web integrated across a huge space of data, you'll have access to an unbelievable data resource …
—?Tim Berners-Lee, 2006
"Semantic Web" is sometimes used as a synonym for "Web 3.0",[12] though each term's definition varies.
Challenges
Some of the challenges for the Semantic Web include vastness, vagueness, uncertainty, inconsistency, and deceit. 
Automated reasoning systems will have to deal with all of these issues in order to deliver on the promise of the Semantic Web.
Vastness: The World Wide Web contains many billions of pages. 
The SNOMED CT medical terminology ontology alone contains 370,000 class names, and existing technology has not yet been able to eliminate all semantically duplicated terms. 
Any automated reasoning system will have to deal with truly huge inputs.
Vagueness: These are imprecise concepts like "young" or "tall". 
This arises from the vagueness of user queries, of concepts represented by content providers, of matching query terms to provider terms and of trying to combine different knowledge bases with overlapping but subtly different concepts. 
Fuzzy logic is the most common technique for dealing with vagueness.
Uncertainty: These are precise concepts with uncertain values.
For example, a patient might present a set of symptoms that correspond to a number of different distinct diagnoses each with a different probability. 
Probabilistic reasoning techniques are generally employed to address uncertainty.
Inconsistency: These are logical contradictions that will inevitably arise during the development of large ontologies, and when ontologies from separate sources are combined. 
Deductive reasoning fails catastrophically when faced with inconsistency, because "anything follows from a contradiction". 
Defeasible reasoning and paraconsistent reasoning are two techniques that can be employed to deal with inconsistency.
Deceit: This is when the producer of the information is intentionally misleading the consumer of the information. 
Cryptography techniques are currently utilized to alleviate this threat.
This list of challenges is illustrative rather than exhaustive, and it focuses on the challenges to the "unifying logic" and "proof" layers of the Semantic Web. 
The World Wide Web Consortium (W3C) Incubator Group for Uncertainty Reasoning for the World Wide Web (URW3-XG) final report lumps these problems together under the single heading of "uncertainty". 
Many of the techniques mentioned here will require extensions to the Web Ontology Language (OWL) for example to annotate conditional probabilities. 
This is an area of active research.[13]
Standards
Standardization for Semantic Web in the context of Web 3.0 is under the care of W3C.[14]
Components
The term "Semantic Web" is often used more specifically to refer to the formats and technologies that enable it.[2] 
The collection, structuring and recovery of linked data are enabled by technologies that provide a formal description of concepts, terms, and relationships within a given knowledge domain. 
These technologies are specified as W3C standards and include:
Resource Description Framework (RDF), a general method for describing information
RDF Schema (RDFS)
Simple Knowledge Organization System (SKOS)
SPARQL, an RDF query language
Notation3 (N3), designed with human-readability in mind
N-Triples, a format for storing and transmitting data
Turtle (Terse RDF Triple Language)
Web Ontology Language (OWL), a family of knowledge representation languages
Rule Interchange Format (RIF), a framework of web rule language dialects supporting rule interchange on the Web
The Semantic Web Stack.
The Semantic Web Stack illustrates the architecture of the Semantic Web. 
The functions and relationships of the components can be summarized as follows:[15]
XML provides an elemental syntax for content structure within documents, yet associates no semantics with the meaning of the content contained within. 
XML is not at present a necessary component of Semantic Web technologies in most cases, as alternative syntaxes exists, such as Turtle. 
Turtle is a de facto standard, but has not been through a formal standardization process.
XML Schema is a language for providing and restricting the structure and content of elements contained within XML documents.
RDF is a simple language for expressing data models, which refer to objects ("web resources") and their relationships. 
An RDF-based model can be represented in a variety of syntaxes, e.g., RDF/XML, N3, Turtle, and RDFa. 
RDF is a fundamental standard of the Semantic Web.[16][17]
RDF Schema extends RDF and is a vocabulary for describing properties and classes of RDF-based resources, with semantics for generalized-hierarchies of such properties and classes.
OWL adds more vocabulary for describing properties and classes: among others, relations between classes (e.g. disjointness), cardinality (e.g. "exactly one"), equality, richer typing of properties, characteristics of properties (e.g. symmetry), and enumerated classes.
SPARQL is a protocol and query language for semantic web data sources.
RIF is the W3C Rule Interchange Format. 
It's an XML language for expressing Web rules that computers can execute. RIF provides multiple versions, called dialects. 
It includes a RIF Basic Logic Dialect (RIF-BLD) and RIF Production Rules Dialect (RIF PRD).
The Resource Description Framework (RDF) is a family of World Wide Web Consortium (W3C) specifications[1] originally designed as a metadata data model. 
It has come to be used as a general method for conceptual description or modeling of information that is implemented in web resources, using a variety of syntax notations and data serialization formats. It is also used in knowledge management applications.
RDF was adopted as a W3C recommendation in 1999. 
The RDF 1.0 specification was published in 2004, the RDF 1.1 specification in 2014.
The RDF data model[2] is similar to classical conceptual modeling approaches such as entity–relationship or class diagrams, as it is based upon the idea of making statements about resources (in particular web resources) in the form of subject–predicate–object expressions.
 These expressions are known as triples in RDF terminology.
 The subject denotes the resource, and the predicate denotes traits or aspects of the resource and expresses a relationship between the subject and the object. 
For example, one way to represent the notion "The sky has the color blue" in RDF is as the triple: a subject denoting "the sky", a predicate denoting "has", and an object denoting "the color blue".
 Therefore, RDF swaps object for subject that would be used in the classical notation of an entity–attribute–value model within object-oriented design; Entity (sky), attribute (color) and value (blue).
 RDF is an abstract model with several serialization formats (i.e., file formats), and so the particular way in which a resource or triple is encoded varies from format to format.
This mechanism for describing resources is a major component in the W3C's Semantic Web activity: an evolutionary stage of the World Wide Web in which automated software can store, exchange, and use machine-readable information distributed throughout the Web, in turn enabling users to deal with the information with greater efficiency and certainty. 
RDF's simple data model and ability to model disparate, abstract concepts has also led to its increasing use in knowledge management applications unrelated to Semantic Web activity.
A collection of RDF statements intrinsically represents a labeled, directed multi-graph.
 As such, an RDF-based data model is more naturally suited to certain kinds of knowledge representation than the relational model and other ontological models. 
However, in practice, RDF data is often persisted in relational database or native representations also called Triplestores, or Quad stores if context (i.e. the named graph) is also persisted for each RDF triple.[3] 
ShEX, or Shape Expressions,[4] is a language for expressing constraints on RDF graphs.
 It includes the cardinality constraints from OSLC Resource Shapes and Dublin Core Description Set Profiles as well as logical connectives for disjunction and polymorphism. 
As RDFS and OWL demonstrate, one can build additional ontology languages upon RDF.
History
The initial RDF design, intended to "build a vendor-neutral and operating system-independent system of metadata,"[5] derived from the W3C's Platform for Internet Content Selection (PICS), an early web content labelling system,[6] but the project was also shaped by ideas from Dublin Core, and from the Meta Content Framework (MCF),[5] which had been developed during 1995-1997 by Ramanathan V. Guha at Apple and Tim Bray at Netscape.[7]
A first public draft of RDF appeared in October 1997,[8][9] issued by a W3C working group that included representatives from IBM, Microsoft, Netscape, Nokia, Reuters, SoftQuad, and the University of Michigan.[6]
The W3C published a specification of RDF's data model and an XML serialization as a recommendation in February 1999.[10]
Two persistent misunderstandings developed around RDF at this time: firstly, from the MCF influence and the RDF "Resource Description" acronym, the idea that RDF was specifically for use in representing metadata. 
Secondly that RDF was an XML format, rather than RDF being a data model and only the RDF/XML serialisation being XML-based. 
RDF saw little take-up in this period, but there was significant work carried out in Bristol, around ILRT at Bristol University and HP Labs, and also in Boston at MIT. 
RSS 1.0 and FOAF became exemplar applications for RDF in this period.
The recommendation of 1999 was replaced in 2004 by a set of six specifications: "The RDF Primer",[11] "RDF Concepts and Abstract",[12] "RDF/XML Syntax Specification (revised)",[13] "RDF Semantics",[14] "RDF Vocabulary Description Language 1.0",[15] and "The RDF Test Cases".[16]
This series was superseded in 2014 by the following six "RDF 1.1" documents: "RDF 1.1 Primer,"[17] "RDF 1.1 Concepts and Abstract Syntax,"[18] "RDF 1.1 XML Syntax,"[19] "RDF 1.1 Semantics,"[20] "RDF Schema 1.1,"[21] and "RDF 1.1 Test Cases"
The subject of an RDF statement is either a uniform resource identifier (URI) or a blank node, both of which denote resources. 
Resources indicated by blank nodes are called anonymous resources. 
They are not directly identifiable from the RDF statement. 
The predicate is a URI which also indicates a resource, representing a relationship. The object is a URI, blank node or a Unicode string literal.
In Semantic Web applications, and in relatively popular applications of RDF like RSS and FOAF (Friend of a Friend), resources tend to be represented by URIs that intentionally denote, and can be used to access, actual data on the World Wide Web. 
But RDF, in general, is not limited to the description of Internet-based resources.
 In fact, the URI that names a resource does not have to be dereferenceable at all. 
For example, a URI that begins with "http:" and is used as the subject of an RDF statement does not necessarily have to represent a resource that is accessible via HTTP, nor does it need to represent a tangible, network-accessible resource — such a URI could represent absolutely anything. 
However, there is broad agreement that a bare URI (without a # symbol) which returns a 300-level coded response when used in an HTTP GET request should be treated as denoting the internet resource that it succeeds in accessing.
Therefore, producers and consumers of RDF statements must agree on the semantics of resource identifiers. 
Such agreement is not inherent to RDF itself, although there are some controlled vocabularies in common use, such as Dublin Core Metadata, which is partially mapped to a URI space for use in RDF. 
The intent of publishing RDF-based ontologies on the Web is often to establish, or circumscribe, the intended meanings of the resource identifiers used to express data in RDF. 
For example, the URI:
http://www.w3.org/TR/2004/REC-owl-guide-20040210/wine#Merlot
is intended by its owners to refer to the class of all Merlot red wines by vintner (i.e., instances of the above URI each represent the class of all wine produced by a single vintner), a definition which is expressed by the OWL ontology — itself an RDF document — in which it occurs.
 Without careful analysis of the definition, one might erroneously conclude that an instance of the above URI was something physical, instead of a type of wine.
Note that this is not a 'bare' resource identifier, but is rather a URI reference, containing the '#' character and ending with a fragment identifier.
Statement reification and context
The body of knowledge modeled by a collection of statements may be subjected to reification, in which each statement (that is each triple subject-predicate-object altogether) is assigned a URI and treated as a resource about which additional statements can be made, as in "Jane says that John is the author of document X". 
Reification is sometimes important in order to deduce a level of confidence or degree of usefulness for each statement.
In a reified RDF database, each original statement, being a resource, itself, most likely has at least three additional statements made about it: one to assert that its subject is some resource, one to assert that its predicate is some resource, and one to assert that its object is some resource or literal. 
More statements about the original statement may also exist, depending on the application's needs.
Borrowing from concepts available in logic (and as illustrated in graphical notations such as conceptual graphs and topic maps), some RDF model implementations acknowledge that it is sometimes useful to group statements according to different criteria, called situations, contexts, or scopes, as discussed in articles by RDF specification co-editor Graham Klyne.
For example, a statement can be associated with a context, named by a URI, in order to assert an "is true in" relationship. 
As another example, it is sometimes convenient to group statements by their source, which can be identified by a URI, such as the URI of a particular RDF/XML document. 
Then, when updates are made to the source, corresponding statements can be changed in the model, as well.
Implementation of scopes does not necessarily require fully reified statements. 
Some implementations allow a single scope identifier to be associated with a statement that has not been assigned a URI, itself.
 Likewise named graphs in which a set of triples is named by a URI can represent context without the need to reify the triples.[37]
Query and inference languages
The predominant query language for RDF graphs is SPARQL. 
SPARQL is an SQL-like language, and a recommendation of the W3C as of January 15, 2008.
An example of a SPARQL query to show country capitals in Africa, using a fictional ontology.
Machine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation (MAHT) or interactive translation) is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.
On a basic level, MT performs simple substitution of words in one language for words in another, but that alone usually cannot produce a good translation of a text because recognition of whole phrases and their closest counterparts in the target language is needed. 
Solving this problem with corpus and statistical techniques is a rapidly growing field that is leading to better translations, handling differences in linguistic typology, translation of idioms, and the isolation of anomalies.[1]
Current machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions.
 This technique is particularly effective in domains where formal or formulaic language is used. 
It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.
Improved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. 
With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).
The progress and potential of machine translation have been debated much through its history. 
Since the 1950s, a number of scholars have questioned the possibility of achieving fully automatic machine translation of high quality.[2] 
Some critics claim that there are in-principle obstacles to automating the translation process.[3]
Main article: History of machine translation
The idea of machine translation may be traced back to the 17th century. 
In 1629, René Descartes proposed a universal language, with equivalent ideas in different tongues sharing one symbol. 
The field of "machine translation" appeared in Warren Weaver's Memorandum on Translation (1949). 
The first researcher in the field, Yehosha Bar-Hillel, began his research at MIT (1951). 
A Georgetown University MT research team followed (1951) with a public demonstration of its Georgetown-IBM experiment system in 1954. 
MT research programs popped up in Japan and Russia (1955), and the first MT conference was held in London (1956). 
Researchers continued to join the field as the Association for Machine Translation and Computational Linguistics was formed in the U.S. (1962) and the National Academy of Sciences formed the Automatic Language Processing Advisory Committee (ALPAC) to study MT (1964). 
Real progress was much slower, however, and after the ALPAC report (1966), which found that the ten-year-long research had failed to fulfill expectations, funding was greatly reduced. 
According to a 1972 report by the Director of Defense Research and Engineering (DDR&E), the feasibility of large-scale MT was reestablished by the success of the Logos MT system in translating military manuals into Vietnamese during that conflict.
The French Textile Institute also used MT to translate abstracts from and into French, English, German and Spanish (1970); Brigham Young University started a project to translate Mormon texts by automated translation (1971); and Xerox used SYSTRAN to translate technical manuals (1978). 
Beginning in the late 1980s, as computational power increased and became less expensive, more interest was shown in statistical models for machine translation. 
Various MT companies were launched, including Trados (1984), which was the first to develop and market translation memory technology (1989). 
The first commercial MT system for Russian / English / German-Ukrainian was developed at Kharkov State University (1991).
MT on the web started with SYSTRAN Offering free translation of small texts (1996), followed by AltaVista Babelfish, which racked up 500,000 requests a day (1997). 
Franz-Josef Och (the future head of Translation Development AT Google) won DARPA's speed MT competition (2003).
 More innovations during this time included MOSES, the open-source statistical MT engine (2007), a text/SMS translation service for mobiles in Japan (2008), and a mobile phone with built-in speech-to-speech translation functionality for English, Japanese and Chinese (2009). 
Recently, Google announced that Google Translate translates roughly enough text to fill 1 million books in one day (2012).
The idea of using digital computers for translation of natural languages was proposed as early as 1946 by A. D. Booth and possibly others. 
Warren Weaver wrote an important memorandum "Translation" in 1949. 
The Georgetown experiment was by no means the first such application, and a demonstration was made in 1954 on the APEXC machine at Birkbeck College (University of London) of a rudimentary translation of English into French. 
Several papers on the topic were published at the time, and even articles in popular journals (see for example Wireless World, Sept. 1955, Cleave and Zacharov). 
A similar application, also pioneered at Birkbeck College at the time, was reading and composing Braille texts by computer.
Translation process
Main article: Translation process
The human translation process may be described as:
Decoding the meaning of the source text; and
Re-encoding this meaning in the target language.
Behind this ostensibly simple procedure lies a complex cognitive operation.
 To decode the meaning of the source text in its entirety, the translator must interpret and analyse all the features of the text, a process that requires in-depth knowledge of the grammar, semantics, syntax, idioms, etc., of the source language, as well as the culture of its speakers. 
The translator needs the same in-depth knowledge to re-encode the meaning in the target language.
Therein lies the challenge in machine translation: how to program a computer that will "understand" a text as a person does, and that will "create" a new text in the target language that "sounds" as if it has been written by a person.
In its most general application, this is beyond current technology.
 Though it works much faster, no automated translation program or procedure, with no human participation, can produce output even close to the quality a human translator can produce. 
What it can do, however, is provide a general, though imperfect, approximation of the original text, getting the "gist" of it (a process called "gisting"). 
This is sufficient for many purposes, including making best use of the finite and expensive time of a human translator, reserved for those cases in which total accuracy is indispensable.
This problem may be approached in a number of ways, through the evolution of which accuracy has improved.
Approaches
Bernard Vauquois' pyramid showing comparative depths of intermediary representation, interlingual machine translation at the peak, followed by transfer-based, then direct translation.
Machine translation can use a method based on linguistic rules, which means that words will be translated in a linguistic way – the most suitable (orally speaking) words of the target language will replace the ones in the source language.
It is often argued that the success of machine translation requires the problem of natural language understanding to be solved first.
Generally, rule-based methods parse a text, usually creating an intermediary, symbolic representation, from which the text in the target language is generated. 
According to the nature of the intermediary representation, an approach is described as interlingual machine translation or transfer-based machine translation. 
These methods require extensive lexicons with morphological, syntactic, and semantic information, and large sets of rules.
Given enough data, machine translation programs often work well enough for a native speaker of one language to get the approximate meaning of what is written by the other native speaker. The difficulty is getting enough data of the right kind to support the particular method. 
For example, the large multilingual corpus of data needed for statistical methods to work is not necessary for the grammar-based methods. 
But then, the grammar methods need a skilled linguist to carefully design the grammar that they use.
To translate between closely related languages, the technique referred to as rule-based machine translation may be used.
Rule-based
Main article: Rule-based machine translation
The rule-based machine translation paradigm includes transfer-based machine translation, interlingual machine translation and dictionary-based machine translation paradigms.
 This type of translation is used mostly in the creation of dictionaries and grammar programs. 
Unlike other methods, RBMT involves more information about the linguistics of the source and target languages, using the morphological and syntactic rules and semantic analysis of both languages. 
The basic approach involves linking the structure of the input sentence with the structure of the output sentence using a parser and an analyzer for the source language, a generator for the target language, and a transfer lexicon for the actual translation. 
RBMT's biggest downfall is that everything must be made explicit: orthographical variation and erroneous input must be made part of the source language analyser in order to cope with it, and lexical selection rules must be written for all instances of ambiguity.
 Adapting to new domains in itself is not that hard, as the core grammar is the same across domains, and the domain-specific adjustment is limited to lexical selection adjustment.
Transfer-based machine translation
Main article: Transfer-based machine translation
Transfer-based machine translation is similar to interlingual machine translation in that it creates a translation from an intermediate representation that simulates the meaning of the original sentence. 
Unlike interlingual MT, it depends partially on the language pair involved in the translation.
Interlingual
Main article: Interlingual machine translation
Interlingual machine translation is one instance of rule-based machine-translation approaches. 
In this approach, the source language, i.e. the text to be translated, is transformed into an interlingual language, i.e. a "language neutral" representation that is independent of any language. 
The target language is then generated out of the interlingua.
 One of the major advantages of this system is that the interlingua becomes more valuable as the number of target languages it can be turned into increases. 
However, the only interlingual machine translation system that has been made operational at the commercial level is the KANT system (Nyberg and Mitamura, 1992), which is designed to translate Caterpillar Technical English (CTE) into other languages.
Dictionary-based
Main article: Dictionary-based machine translation
Machine translation can use a method based on dictionary entries, which means that the words will be translated as they are by a dictionary.
Statistical
Main article: Statistical machine translation
Statistical machine translation tries to generate translations using statistical methods based on bilingual text corpora, such as the Canadian Hansard corpus, the English-French record of the Canadian parliament and EUROPARL, the record of the European Parliament. 
Where such corpora are available, good results can be achieved translating similar texts, but such corpora are still rare for many language pairs. 
The first statistical machine translation software was CANDIDE from IBM. 
Google used SYSTRAN for several years, but switched to a statistical translation method in October 2007.[4]
 In 2005, Google improved its internal translation capabilities by using approximately 200 billion words from United Nations materials to train their system; translation accuracy improved.[5]
 Google Translate and similar statistical translation programs work by detecting patterns in hundreds of millions of documents that have previously been translated by humans and making intelligent guesses based on the findings. 
Generally, the more human-translated documents available in a given language, the more likely it is that the translation will be of good quality.[6]
 Newer approaches into Statistical Machine translation such as METIS II and PRESEMT use minimal corpus size and instead focus on derivation of syntactic structure through pattern recognition.
 With further development, this may allow statistical machine translation to operate off of a monolingual text corpus.[7]
 SMT's biggest downfall includes it being dependent upon huge amounts of parallel texts, its problems with morphology-rich languages (especially with translating into such languages), and its inability to correct singleton errors.
Example-based
Main article: Example-based machine translation
Example-based machine translation (EBMT) approach was proposed by Makoto Nagao in 1984.
 Example-based machine translation is based on the idea of analogy. 
In this approach, the corpus that is used is one that contains texts that have already been translated. 
Given a sentence that is to be translated, sentences from this corpus are selected that contain similar sub-sentential components.[10]
 The similar sentences are then used to translate the sub-sentential components of the original sentence into the target language, and these phrases are put together to form a complete translation.
Hybrid MT
Main article: Hybrid machine translation
Hybrid machine translation (HMT) leverages the strengths of statistical and rule-based translation methodologies.[11]
 Several MT organizations (such as Asia Online, LinguaSys, Systran, and Polytechnic University of Valencia) claim a hybrid approach that uses both rules and statistics. 
The approaches differ in a number of ways:
Rules post-processed by statistics: Translations are performed using a rules based engine. 
Statistics are then used in an attempt to adjust/correct the output from the rules engine.
Statistics guided by rules: Rules are used to pre-process data in an attempt to better guide the statistical engine. 
Rules are also used to post-process the statistical output to perform functions such as normalization.
 This approach has a lot more power, flexibility and control when translating.
Major issues
Disambiguation
Main article: Word sense disambiguation
Word-sense disambiguation concerns finding a suitable translation when a word can have more than one meaning.
 The problem was first raised in the 1950s by Yehoshua Bar-Hillel.[12] 
He pointed out that without a "universal encyclopedia", a machine would never be able to distinguish between the two meanings of a word.[13] 
Today there are numerous approaches designed to overcome this problem. They can be approximately divided into "shallow" approaches and "deep" approaches.
Shallow approaches assume no knowledge of the text.
 They simply apply statistical methods to the words surrounding the ambiguous word.
 Deep approaches presume a comprehensive knowledge of the word. 
So far, shallow approaches have been more successful.[citation needed]
Claude Piron, a long-time translator for the United Nations and the World Health Organization, wrote that machine translation, at its best, automates the easier part of a translator's job; 
the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved:
Why does a translator need a whole workday to translate five pages, and not an hour or two? ..... About 90% of an average text corresponds to these simple conditions. 
But unfortunately, there's the other 10%. 
It's that part that requires six [more] hours of work.
 There are ambiguities one has to resolve. 
For instance, the author of the source text, an Australian physician, cited the example of an epidemic which was declared during World War II in a "Japanese prisoner of war camp". 
Was he talking about an American camp with Japanese prisoners or a Japanese camp with American prisoners? The English has two senses.
It's necessary therefore to do research, maybe to the extent of a phone call to Australia.[14]
The ideal deep approach would require the translation software to do all the research necessary for this kind of disambiguation on its own; but this would require a higher degree of AI than has yet been attained.
 A shallow approach which simply guessed at the sense of the ambiguous English phrase that Piron mentions (based, perhaps, on which kind of prisoner-of-war camp is more often mentioned in a given corpus) would have a reasonable chance of guessing wrong fairly often. 
A shallow approach that involves "ask the user about each ambiguity" would, by Piron's estimate, only automate about 25% of a professional translator's job, leaving the harder 75% still to be done by a human.
Non-standard speech
One of the major pitfalls of MT is its inability to translate non-standard language with the same accuracy as standard language. 
Heuristic or statistical based MT takes input from various sources in standard form of a language. 
Rule-based translation, by nature, does not include common non-standard usages. 
This causes errors in translation from a vernacular source or into colloquial language. 
Limitations on translation from casual speech present issues in the use of machine translation in mobile devices.
Named entities
[icon]	This section requires expansion. (January 2010)
Related to named entity recognition in information extraction.
Name entities, in narrow sense, refer to concrete or abstract entities in the real world including people, organizations, companies, places etc. 
It also refers to expressing of time, space, quantity such as 1 July 2011, $79.99 and so on.[15]
Named entities occur in the text being analyzed in statistical machine translation. 
The initial difficulty that arises in dealing with named entities is simply identifying them in the text. 
Consider the list of names common in a particular language to illustrate this – the most common names are different for each language and also are constantly changing. 
If named entities cannot be recognized by the machine translator, they may be erroneously translated as common nouns, which would most likely not affect the BLEU rating of the translation but would change the text's human readability.[16] 
It is also possible that, when not identified, named entities will be omitted from the output translation, which would also have implications for the text's readability and message.
Another way to deal with named entities is to use transliteration instead of translation, meaning that you find the letters in the target language that most closely correspond to the name in the source language. 
There have been attempts to incorporate this into machine translation by adding a transliteration step into the translation procedure. 
However, these attempts still have their problems and have even been cited as worsening the quality of translation.[17] 
Named entities were still identified incorrectly, with words not being transliterated when they should or being transliterated when they shouldn't. 
For example, for "Southern California" the first word should be translated directly, while the second word should be transliterated. 
However, machines would often transliterate both because they treated them as one entity.
 Words like these are hard for machine translators, even those with a transliteration component, to process.
The lack of attention to the issue of named entity translation has been recognized as potentially stemming from a lack of resources to devote to the task in addition to the complexity of creating a good system for named entity translation. 
One approach to named entity translation has been to transliterate, and not translate, those words. 
A second is to create a "do-not-translate" list, which has the same end goal – transliteration as opposed to translation.[18]
 Both of these approaches still rely on the correct identification of named entities, however.
A third approach to successful named entity translation is a class-based model. 
In this method, named entities are replaced with a token to represent the class they belong to.
 For example, "Ted" and "Erica" would both be replaced with "person" class token. 
In this way the statistical distribution and use of person names in general can be analyzed instead of looking at the distributions of "Ted" and "Erica" individually. 
A problem that the class based model solves is that the probability of a given name in a specific language will not affect the assigned probability of a translation. 
A study by Stanford on improving this area of translation gives the examples that different probabilities will be assigned to "David is going for a walk" and "Ankit is going for a walk" for English as a target language due to the different number of occurrences for each name in the training data.
 A frustrating outcome of the same study by Stanford (and other attempts to improve named recognition translation) is that many times, a decrease in the BLEU scores for translation will result from the inclusion of methods for named entity translation.[18]
Translation from multiparallel sources
Some work has been done in the utilization of multiparallel corpora, that is, a body of text which has been translated into 3 or more languages. 
Using these methods, a text which has been translated into 2 or more languages may be utilized in combination to provide a more accurate translation into a third language compared to if just one of those source languages were used alone.[19][20][21]
Ontologies in MT
An ontology is a formal representation of knowledge which includes the concepts (such as objects, processes etc.) in a domain and some relations between them. 
If the stored information is of linguistic nature, one can speak of a lexicon.[22]
 In NLP, ontologies can be used as a source of knowledge for machine translation systems. 
With access to a large knowledge base, systems can be enabled to resolve many (especially lexical) ambiguities on their own. 
In the following classic examples, as humans, we are able to interpret the prepositional phrase according to the context because we use our world knowledge, stored in our lexicons:
"I saw a man/star/molecule with a microscope/telescope/binoculars."[22]
A machine translation system initially would not be able to differentiate between the meanings because syntax does not change. 
With a large enough ontology as a source of knowledge however, the possible interpretations of ambiguous words in a specific context can be reduced. 
Other areas of usage for ontologies within NLP include information retrieval, information extraction and text summarization.[22]
Building ontologies
The ontology generated for the PANGLOSS knowledge-based machine translation system in 1993 may serve as an example of how an ontology for NLP purposes can be compiled:[23]
A large-scale ontology is necessary to help parsing in the active modules of the machine translation system.
In the PANGLOSS example, about 50.000 nodes were intended to be subsumed under the smaller, manually-built upper (abstract) region of the ontology. 
Because of its size, it had to be created automatically.
The goal was to merge the two resources LDOCE online and WordNet to combine the benefits of both: concise definitions from Longman, and semantic relations allowing for semi-automatic taxonomization to the ontology from WordNet.
A definition match algorithm was created to automatically merge the correct meanings of ambiguous words between the two online resources, based on the words that the definitions of those meanings have in common in LDOCE and WordNet. 
Using a similarity matrix, the algorithm delivered matches between meanings including a confidence factor.
 This algorithm alone, however, did not match all meanings correctly on its own.
A second hierarchy match algorithm was therefore created which uses the taxonomic hierarchies found in WordNet (deep hierarchies) and partially in LDOCE (flat hierarchies). 
This works by first matching unambiguous meanings, then limiting the search space to only the respective ancestors and descendants of those matched meanings. 
Thus, the algorithm matched locally unambiguous meanings (for instance, while the word seal as such is ambiguous, there is only one meaning of "seal" in the animal subhierarchy).
Both algorithms complemented each other and helped constructing a large-scale ontology for the machine translation system. 
The WordNet hierarchies, coupled with the matching definitions of LDOCE, were subordinated to the ontology's upper region. 
As a result, the PANGLOSS MT system was able to make use of this knowledge base, mainly in its generation element.
Applications
While no system provides the holy grail of fully automatic high-quality machine translation of unrestricted text, many fully automated systems produce reasonable output.
 The quality of machine translation is substantially improved if the domain is restricted and controlled.[27]
Despite their inherent limitations, MT programs are used around the world. 
Probably the largest institutional user is the European Commission. 
The MOLTO project, for example, coordinated by the University of Gothenburg, received more than 2.375 million euros project support from the EU to create a reliable translation tool that covers a majority of the EU languages.
 The further development of MT systems comes at a time when budget cuts in human translation may increase the EU's dependency on reliable MT programs.
 The European Commission contributed 3.072 million euros (via its ISA programme) for the creation of MT@EC, a statistical machine translation program tailored to the administrative needs of the EU, to replace a previous rule-based machine translation system.[30]
Google has claimed that promising results were obtained using a proprietary statistical machine translation engine.
 The statistical translation engine used in the Google language tools for Arabic <-> English and Chinese <-> English had an overall score of 0.4281 over the runner-up IBM's BLEU-4 score of 0.3954 (Summer 2006) in tests conducted by the National Institute for Standards and Technology.[32][33][34]
With the recent focus on terrorism, the military sources in the United States have been investing significant amounts of money in natural language engineering. 
In-Q-Tel[35] (a venture capital fund, largely funded by the US Intelligence Community, to stimulate new technologies through private sector entrepreneurs) brought up companies like Language Weaver. 
Currently the military community is interested in translation and processing of languages like Arabic, Pashto, and Dari.
 Within these languages, the focus is on key phrases and quick communication between military members and civilians through the use of mobile phone apps.
 The Information Processing Technology Office in DARPA hosts programs like TIDES and Babylon translator.
 US Air Force has awarded a $1 million contract to develop a language translation technology.[37]
The notable rise of social networking on the web in recent years has created yet another niche for the application of machine translation software – in utilities such as Facebook, or instant messaging clients such as Skype, GoogleTalk, MSN Messenger, etc. – allowing users speaking different languages to communicate with each other. 
Machine translation applications have also been released for most mobile devices, including mobile telephones, pocket PCs, PDAs, etc. 
Due to their portability, such instruments have come to be designated as mobile translation tools enabling mobile business networking between partners speaking different languages, or facilitating both foreign language learning and unaccompanied traveling to foreign countries without the need of the intermediation of a human translator.
Despite being labelled as an unworthy competitor to human translation in 1966 by the Automated Language Processing Advisory Committee put together by the United States government,[38] the quality of machine translation has now been improved to such levels that its application in online collaboration and in the medical field are being investigated. 
In the Ishida and Matsubara lab of Kyoto University, methods of improving the accuracy of machine translation as a support tool for inter-cultural collaboration in today's globalized society are being studied.[39] 
The application of this technology in medical settings where human translators are absent is another topic of research however difficulties arise due to the importance of accurate translations in medical diagnoses.[40]
Evaluation
Main article: Evaluation of machine translation
There are many factors that affect how machine translation systems are evaluated. 
These factors include the intended use of the translation, the nature of the machine translation software, and the nature of the translation process.
Different programs may work well for different purposes. 
For example, statistical machine translation (SMT) typically outperforms example-based machine translation (EBMT), but researchers found that when evaluating English to French translation, EBMT performs better.
 The same concept applies for technical documents, which can be more easily translated by SMT because of their formal language.
In certain applications, however, e.g., product descriptions written in a controlled language, a dictionary-based machine-translation system has produced satisfactory translations that require no human intervention save for quality inspection.[42]
There are various means for evaluating the output quality of machine translation systems. 
The oldest is the use of human judges[43] to assess a translation's quality. 
Even though human evaluation is time-consuming, it is still the most reliable method to compare different systems such as rule-based and statistical systems.
 Automated means of evaluation include BLEU, NIST, METEOR, and LEPOR.[45]
Relying exclusively on unedited machine translation ignores the fact that communication in human language is context-embedded and that it takes a person to comprehend the context of the original text with a reasonable degree of probability. 
It is certainly true that even purely human-generated translations are prone to error. 
Therefore, to ensure that a machine-generated translation will be useful to a human being and that publishable-quality translation is achieved, such translations must be reviewed and edited by a human.
 The late Claude Piron wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved. 
Such research is a necessary prelude to the pre-editing necessary in order to provide input for machine-translation software such that the output will not be meaningless.[47]
In addition to disambiguation problems, decreased accuracy can occur due to varying levels of training data for machine translating programs. 
Both example-based and statistical machine translation rely on a vast array of real example sentences as a base for translation, and when too many or too few sentences are analyzed accuracy is jeopardized. 
Researchers found that when a program is trained on 203,529 sentence pairings, accuracy actually decreases.
The optimal level of training data seems to be just over 100,000 sentences, possibly because as training data increasing, the number of possible sentences increases, making it harder to find an exact translation match.
Using machine translation as a teaching tool
Although there have been concerns about machine translation's accuracy, Dr. Ana Nino of the University of Manchester has researched some of the advantages in utilizing machine translation in the classroom. 
One such pedagogical method is called using "MT as a Bad Model."[48]
 MT as a Bad Model forces the language learner to identify inconsistencies or incorrect aspects of a translation; in turn, the individual will (hopefully) possess a better grasp of the language. 
Dr. Nino cites that this teaching tool was implemented in the late 1980s. 
At the end of various semesters, Dr. Nino was able to obtain survey results from students who had used MT as a Bad Model (as well as other models.) 
Overwhelmingly, students felt that they had observed improved comprehension, lexical retrieval, and increased confidence in their target language.[48]
Machine translation and signed languages
In the early 2000s, options for machine translation between spoken and signed languages were severely limited. 
It was a common belief that deaf individuals could use traditional translators. 
However, stress, intonation, pitch, and timing are conveyed much differently in spoken languages compared to signed languages. 
Therefore, a deaf individual may misinterpret or become confused about the meaning of written text that is based on a spoken language.[49]
Researchers Zhao, et al. (2000), developed a prototype called TEAM (translation from English to ASL by machine) that completed English to American Sign Language (ASL) translations.
 The program would first analyze the syntactic, grammatical, and morphological aspects of the English text. 
Following this step, the program accessed a sign synthesizer, which acted as a dictionary for ASL. 
This synthesizer housed the process one must follow to complete ASL signs, as well as the meanings of these signs. 
Once the entire text is analyzed and the signs necessary to complete the translation are located in the synthesizer, a computer generated human appeared and would use ASL to sign the English text to the user.[49]
Copyright
Only works that are original are subject to copyright protection, so some scholars claim that machine translation results are not entitled to copyright protection because MT does not involve creativity.[50] 
The copyright at issue is for a derivative work; the author of the original work in the original language does not lose his rights when a work is translated: a translator must have permission to publish a translation.
Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document. 
Technologies that can make a coherent summary take into account variables such as length, writing style and syntax. 
Automatic data summarization is part of machine learning and data mining. 
The main idea of summarization is to find a representative subset of the data, which contains the information of the entire set. 
Summarization technologies are used in a large number of sectors in industry today. 
An example of the use of summarization technology is search engines such as Google. 
Other examples include document summarization, image collection summarization and video summarization. 
Document summarization, tries to automatically create a representative summary or abstract of the entire document, by finding the most informative sentences. 
Similarly, in image summarization the system finds the most representative and important (or salient) images. 
Similarly, in consumer videos one would want to remove the boring or repetitive scenes, and extract out a much shorter and concise version of the video. 
This is also important, say for surveillance videos, where one might want to extract only important events in the recorded video, since most part of the video may be uninteresting with nothing going on. 
As the problem of information overload grows, and as the amount of data increases, the interest in automatic summarization is also increasing.
Generally, there are two approaches to automatic summarization: extraction and abstraction.
 Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. 
In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might generate.
 Such a summary might contain words not explicitly present in the original. 
Research into abstractive methods is an increasingly important and active research area, however due to complexity constraints, research to date has focused primarily on extractive methods.
 In some application domains, extractive summarization makes more sense. 
Examples of these include image collection summarization and video summarization.
Contents  [hide] 
1	Types of Summarization
1.1	Extraction-based summarization
1.2	Abstraction-based summarization
1.3	Aided summarization
2	Applications and Systems for Summarization
2.1	Keyphrase extraction
2.1.1	Supervised learning approaches
2.1.2	Unsupervised approach: TextRank
2.2	Document summarization
2.2.1	Supervised learning approaches
2.2.2	Maximum entropy-based summarization
2.2.3	TextRank and LexRank
2.2.4	Multi-document summarization
2.2.4.1	Incorporating diversity
2.3	Submodular Functions as generic tools for summarization
3	Evaluation techniques
3.1	Intrinsic and extrinsic evaluation
3.2	Inter-textual and intra-textual
3.3	Current challenges in evaluating summaries automatically
3.4	Domain specific versus domain independent summarization techniques
3.5	Evaluating summaries qualitatively
4	See also
5	References
5.1	Further reading
Types of Summarization
The different types of automatic summarization include extraction-based, abstraction-based, maximum entropy-based, and aided summarization.
Extraction-based summarization
In this summarization task, the automatic system extracts objects from the entire collection, without modifying the objects themselves. 
Examples of this include keyphrase extraction, where the goal is to select individual words or phrases to "tag" a document, and document summarization, where the goal is to select whole sentences (without modifying them) to create a short paragraph summary. 
Similarly, in image collection summarization, the system extracts images from the collection without modifying the images themselves.
Abstraction-based summarization
Extraction techniques merely copy the information deemed most important by the system to the summary (for example, key clauses, sentences or paragraphs), while abstraction involves paraphrasing sections of the source document. 
In general, abstraction can condense a text more strongly than extraction, but the programs that can do this are harder to develop as they require use of natural language generation technology, which itself is a growing field.
While some work has been done in abstractive summarization (creating an abstract synopsis like that of a human), the majority of summarization systems are extractive (selecting a subset of sentences to place in a summary).
Aided summarization
Machine learning techniques from closely related fields such as information retrieval or text mining have been successfully adapted to help automatic summarization.
Apart from Fully Automated Summarizers (FAS), there are systems that aid users with the task of summarization (MAHS = Machine Aided Human Summarization), for example by highlighting candidate passages to be included in the summary, and there are systems that depend on post-processing by a human (HAMS = Human Aided Machine Summarization).
Applications and Systems for Summarization
There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. 
The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). 
The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.
An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. 
Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). 
This problem is called multi-document summarization.
 A related application is summarizing news articles. 
Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.
Image collection summarization is other application example of automatic summarization. 
It consists in selecting a representative set of images from a larger set of images.[1] 
A summary in this context is useful to show the most representative images of results in an image collection exploration system. 
Video summarization is a related domain, where the system automatically creates a trailer of a long video. 
This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. 
Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.
At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set.
 This is also called the core-set.
 These algorithms model notions like diversity, coverage, information and representativeness of the summary. 
Query based summarization techniques, additionally model for relevance of the summary with the query. 
Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.
Keyphrase extraction
The task is the following.
 You are given a piece of text, such as a journal article, and you must produce a list of keywords or keyphrases that capture the primary topics discussed in the text. 
In the case of research articles, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases. 
For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below. 
Consider the example text from a news article:
"The Army Corps of Engineers, rushing to meet President Bush's promise to protect New Orleans by the start of the 2006 hurricane season, installed defective flood-control pumps last year despite warnings from its own expert that the equipment would fail during a storm, according to documents obtained by The Associated Press".
An keyphrase extractor might select "Army Corps of Engineers", "President Bush", "New Orleans", and "defective flood-control pumps" as keyphrases. 
These are pulled directly from the text. 
In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as "political negligence" or "inadequate protection from floods". 
Abstraction requires a deep understanding of the text, which makes it difficult for a computer system. 
Keyphrases have many applications. 
They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.
Depending on the different literature and the definition of key terms, words or phrases, highly related theme is certainly the Keyword extraction.
Supervised learning approaches
Beginning with the work of Turney,[2] many researchers have approached keyphrase extraction as a supervised machine learning problem. 
Given a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below). 
We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). 
We assume there are known keyphrases available for a set of training documents. 
Using the known keyphrases, we can assign positive or negative labels to the examples. 
Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. 
Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase. 
For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases. 
After training a learner, we can select keyphrases for test documents in the following manner. 
We apply the same example-generation strategy to the test documents, then run each example through the learner. 
We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. 
If probabilities are given, a threshold is used to select the keyphrases. 
Keyphrase extractors are generally evaluated using precision and recall. 
Precision measures how many of the proposed keyphrases are actually correct.
 Recall measures how many of the true keyphrases your system proposed. The two measures can be combined in an F-score, which is the harmonic mean of the two (F = 2PR/(P + R) ). 
Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.
Designing a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too).
 The first choice is exactly how to generate examples.
 Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. 
Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. 
Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. 
For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words. Thus, recall may suffer. 
However, generating too many examples can also lead to low precision.
We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases. 
Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various boolean syntactic features (e.g., contains all caps), etc. 
The Turney paper used about 12 such features. 
Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney’s seminal paper.
In the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number. 
Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases.
 This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number.
Once examples and features are created, we need a way to learn to predict keyphrases. 
Virtually any supervised learning algorithm could be used, such as decision trees, Naive Bayes, and rule induction. 
In the case of Turney's GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm. 
The extractor follows a series of heuristics to identify keyphrases. 
The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases.
Unsupervised approach: TextRank
Another keyphrase extraction algorithm is TextRank. 
While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data. 
Many documents with known keyphrases are needed. 
Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney's results demonstrate. 
Unsupervised keyphrase extraction removes the need for training data.
 It approaches the problem from a different angle. 
Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[3] exploits the structure of the text itself to determine keyphrases that appear "central" to the text in the same way that PageRank selects important Web pages. 
Recall this is based on the notion of "prestige" or "recommendation" from social networks. 
In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties.
 Thus the algorithm is easily portable to new domains and languages.
TextRank is a general purpose graph-based ranking algorithm for NLP.
 Essentially, it runs PageRank on a graph specially designed for a particular NLP task. 
For keyphrase extraction, it builds a graph using some set of text units as vertices. 
Edges are based on some measure of semantic or lexical similarity between the text unit vertices. 
Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. 
Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the "random surfer model"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).
The vertices should correspond to what we want to rank.
 Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. 
However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases.
 This has a nice side effect of allowing us to produce keyphrases of arbitrary length. 
For example, if we rank unigrams and find that "advanced", "natural", "language", and "processing" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.
 Note that the unigrams placed in the graph can be filtered by part of speech. 
The authors found that adjectives and nouns were the best to include.
 Thus, some linguistic knowledge comes into play in this step.
Edges are created based on word co-occurrence in this application of TextRank. 
Two vertices are connected by an edge if the unigrams appear within a window of size N in the original text.
 N is typically around 2–10. 
Thus, "natural" and "language" might be linked in a text about NLP.
 "Natural" and "processing" would also be linked because they would both appear in the same string of N words. 
These edges build on the notion of "text cohesion" and the idea that words that appear near each other are likely related in a meaningful way and "recommend" each other to the reader.
Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. 
The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. 
Then the top T vertices/unigrams are selected based on their stationary probabilities.
 A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.
It is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. 
One way to think about it is the following.
 A word that appears multiple times throughout a text may have many different co-occurring neighbors. 
For example, in a text about machine learning, the unigram "learning" might co-occur with "machine", "supervised", "un-supervised", and "semi-supervised" in four different sentences. 
Thus, the "learning" vertex would be a central "hub" that connects to these other modifying words. 
Running PageRank/TextRank on the graph is likely to rank "learning" highly. 
Similarly, if the text contains the phrase "supervised classification", then there would be an edge between "supervised" and "classification". 
If "classification" appears several other places and thus has many neighbors, its importance would contribute to the importance of "supervised". 
If it ends up with a high rank, it will be selected as one of the top T unigrams, along with "learning" and probably "classification".
 In the final post-processing step, we would then end up with keyphrases "supervised learning" and "supervised classification".
In short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. 
A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. 
This is similar to densely connected Web pages getting ranked highly by PageRank. This approach has also been used in document summarization, considered below.
Document summarization
Like keyphrase extraction, document summarization aims to identify the essence of a text. 
The only real difference is that now we are dealing with larger text units—whole sentences instead of words and phrases.
Before getting into the details of some summarization methods, we will mention how summarization systems are typically evaluated.
 The most common way is using the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure. 
This is a recall-based measure that determines how well a system-generated summary covers the content present in one or more human-generated model summaries known as references. 
It is recall-based to encourage systems to include all the important topics in the text. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. 
For example, ROUGE-1 is computed as division of count of unigrams in reference that appear in system and count of unigrams in reference summary.
If there are multiple references, the ROUGE-1 scores are averaged.
 Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner.
 High-order n-gram ROUGE measures try to judge fluency to some degree. 
Note that ROUGE is similar to the BLEU measure for machine translation, but BLEU is precision- based, because translation systems favor accuracy.
A promising line in document summarization is adaptive document/text summarization.[4]
 The idea of adaptive summarization involves preliminary recognition of document/text genre and subsequent application of summarization algorithms optimized for this genre.
 First summarizes that perform adaptive summarization have been created.[5]
Supervised learning approaches
Supervised text summarization is very much like supervised keyphrase extraction. 
Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary.
 Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc. 
The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as "in summary" or "not in summary". 
This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient.
 The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training. 
Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 only cares about unigrams.
Maximum entropy-based summarization
During the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain. 
The system was based on a hybrid system using a naive Bayes classifier and statistical language models for modeling salience. 
Although the system exhibited good results, the researchers wanted to explore the effectiveness of a maximum entropy (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies. 
Maximum entropy has also been applied successfully for summarization in the broadcast news domain.
TextRank and LexRank
The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data. 
Some unsupervised summarization approaches are based on finding a "centroid" sentence, which is the mean word vector of all the sentences in the document. 
Then the sentences can be ranked with regard to their similarity to this centroid sentence.
A more principled way to estimate sentence importance is using random walks and eigenvector centrality.
 LexRank[6] is an algorithm essentially identical to TextRank, and both use this approach for document summarization. 
The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.
In both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document.
The edges between sentences are based on some form of semantic similarity or content overlap. 
While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (normalized by the sentences' lengths). 
The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score. 
TextRank uses continuous similarity scores as weights.
In both algorithms, the sentences are ranked by applying PageRank to the resulting graph.
 A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.
It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights. 
In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary.
Another important distinction is that TextRank was used for single document summarization, while LexRank has been applied to multi-document summarization. 
The task remains the same in both cases—only the number of sentences to choose from has grown. 
However, when summarizing multiple documents, there is a greater risk of selecting duplicate or highly redundant sentences to place in the same summary. 
Imagine you have a cluster of news articles on a particular event, and you want to produce one summary. 
Each article is likely to have many similar sentences, and you would only want to include distinct ideas in the summary. 
To address this issue, LexRank applies a heuristic post-processing step that builds up a summary by adding sentences in rank order, but discards any sentences that are too similar to ones already placed in the summary.
 The method used is called Cross-Sentence Information Subsumption (CSIS).
These methods work based on the idea that sentences "recommend" other similar sentences to the reader. 
Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. 
The importance of this sentence also stems from the importance of the sentences "recommending" it.
 Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences. 
This makes intuitive sense and allows the algorithms to be applied to any arbitrary new text.
 The methods are domain-independent and easily portable. 
One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain.
 However, the unsupervised "recommendation"-based approach applies to any domain.
Multi-document summarization
Main article: Multi-document summarization
Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic.
 Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents.
 In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload. 
Multi-document summarization may also be done in response to a question.[7]
Multi-document summarization creates information reports that are both concise and comprehensive.
 With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document.
 While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.
 Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.
Incorporating diversity
Multi-document extractive summarization faces a problem of potential redundancy.
 Ideally, we would like to extract sentences that are both "central" (i.e., contain the main ideas) and "diverse" (i.e., they differ from one another).
 LexRank deals with diversity as a heuristic final stage using CSIS, and other systems have used similar methods, such as Maximal Marginal Relevance (MMR),[8] in trying to eliminate redundancy in information retrieval results.
 There is a general purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both "centrality" and "diversity" in a unified mathematical framework based on absorbing Markov chain random walks. 
(An absorbing random walk is like a standard random walk, except some states are now absorbing states that act as "black holes" that cause the walk to end abruptly at that state.) 
The algorithm is called GRASSHOPPER.[9] In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).
The state of the art results for multi-document summarization, however, are obtained using mixtures of submodular functions. 
These methods have achieved the state of the art results for Documment Summarization Corpora, DUC 04 - 07.[10]
 Similar results were also achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.[11]
Submodular Functions as generic tools for summarization
The idea of a Submodular set function has recently emerged as a powerful modeling tool for various summarization problems.
 Submodular functions naturally model notions of coverage, information, representation and diversity. 
Moreover, several important combinatorial optimization problems occur as special instances of submodular optimization. 
For example, the set cover problem is a special case of submodular optimization, since the set cover function is submodular.
 The set cover function attempts to find a subset of objects which cover a given set of concepts. 
For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document. 
This is an instance of set cover. 
Similarly, the facility location problem is a special case of submodular functions. 
The Facility Location function also naturally models coverage and diversity. 
Another example of a submodular optimization problem is using a Determinantal point process to model diversity. 
Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. 
All these important models encouraging coverage, diversity and information are all submodular. 
Moreover, submodular functions can be efficiently combined together, and the resulting function is still submodular. 
Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.
While submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization. 
For example, a simple greedy algorithm admits a constant factor guarantee.[12]
 Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems.
Submodular functions have achieved state-of-the-art for almost all summarization problems. 
For example, work by Lin and Bilmes, 2012[13] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization.
 Similarly, work by Lin and Bilmes, 2011,[14] shows that many existing systems for automatic summarization are instances of submodular functions. 
This was a break through result establishing submodular functions as the right models for summarization problems.
Submodular Functions have also been used for other summarization tasks. 
Tschiatschek et al., 2014 show[15] that mixtures of submodular functions achieve state-of-the-art results for image collection summarization. 
Similarly, Bairi et al., 2015[16] show the utility of submodular functions for summarizing multi-document topic hierarchies. 
Submodular Functions have also successfully been used for summarizing machine learning datasets.[17]
Evaluation techniques
The most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.
Evaluation techniques fall into intrinsic and extrinsic,[18] inter-texual and intra-texual.[19]
Intrinsic and extrinsic evaluation
An intrinsic evaluation tests the summarization system in of itself while an extrinsic evaluation tests the summarization based on how it affects the completion of some other task.
 Intrinsic evaluations have assessed mainly the coherence and informativeness of summaries.
 Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc.
Inter-textual and intra-textual
Intra-textual methods assess the output of a specific summarization system, and the inter-texual ones focus on contrastive analysis of outputs of several summarization systems.
Human judgement often has wide variance on what is considered a "good" summary, which means that making the evaluation process automatic is particularly difficult.
 Manual evaluation can be used, but this is both time and labor-intensive as it requires humans to read not only the summaries but also the source documents. 
Other issues are those concerning coherence and coverage[disambiguation needed].
One of the metrics used in NIST's annual Document Understanding Conferences, in which research groups submit their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation [3]). 
It essentially calculates n-gram overlaps between automatically generated summaries and previously-written human summaries. 
A high level of overlap should indicate a high level of shared concepts between the two summaries. 
Note that overlap metrics like this are unable to provide any feedback on a summary's coherence. 
Anaphor resolution remains another problem yet to be fully solved. 
Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.[20]
Current challenges in evaluating summaries automatically
Evaluating summaries, either manually or automatically, is a hard task.
 The main difficulty in evaluation comes from the impossibility of building a fair gold-standard against which the results of the systems can be compared.
 Furthermore, it is also very hard to determine what a correct summary is, because there is always the possibility of a system to generate a good summary that is quite different from any human summary used as an approximation to the correct output.
Content selection is not a deterministic problem.
 People are subjective, and different authors would choose different sentences. 
And individuals may not be consistent. 
A particular person may chose different sentences at different times. 
Two distinct sentences expressed in different words can express the same meaning. 
This phenomenon is known as paraphrasing. 
We can find an approach to automatically evaluating summaries using paraphrases (ParaEval).
Most summarization systems perform an extractive approach, selecting and copying important sentences from the source documents. 
Although humans can also cut and paste relevant information of a text, most of the times they rephrase sentences when necessary, or they join different related information into one sentence.
Domain specific versus domain independent summarization techniques
Domain independent summarization techniques generally apply sets of general features which can be used to identify information-rich text segments. 
Recent research focus has drifted to domain-specific summarization techniques that utilize the available knowledge specific to the domain of text.
 For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.[21]
Evaluating summaries qualitatively
The main drawback of the evaluation systems existing so far is that we need at least one reference summary, and for some methods more than one, to be able to compare automatic summaries with models.
 This is a hard and expensive task.
 Much effort has to be done in order to have corpus of texts and their corresponding summaries. 
Furthermore, for some methods, not only do we need to have human-made summaries available for comparison, but also manual annotation has to be performed in some of them (e.g. SCU in the Pyramid Method).
 In any case, what the evaluation methods need as an input, is a set of summaries to serve as gold standards and a set of automatic summaries. 
Moreover, they all perform a quantitative evaluation with regard to different similarity metrics. 
To overcome these problems, we think that the quantitative evaluation might not be the only way to evaluate summaries, and a qualitative automatic evaluation would be also important.
The Semantic Web Stack illustrates the architecture of the Semantic Web. 
The functions and relationships of the components can be summarized as follows:[15]
XML provides an elemental syntax for content structure within documents, yet associates no semantics with the meaning of the content contained within. 
XML is not at present a necessary component of Semantic Web technologies in most cases, as alternative syntaxes exists, such as Turtle. 
Turtle is a de facto standard, but has not been through a formal standardization process.
XML Schema is a language for providing and restricting the structure and content of elements contained within XML documents.
RDF is a simple language for expressing data models, which refer to objects ("web resources") and their relationships. 
An RDF-based model can be represented in a variety of syntaxes, e.g., RDF/XML, N3, Turtle, and RDFa. 
RDF is a fundamental standard of the Semantic Web.[16][17]
RDF Schema extends RDF and is a vocabulary for describing properties and classes of RDF-based resources, with semantics for generalized-hierarchies of such properties and classes.
OWL adds more vocabulary for describing properties and classes: among others, relations between classes (e.g. disjointness), cardinality (e.g. "exactly one"), equality, richer typing of properties, characteristics of properties (e.g. symmetry), and enumerated classes.
SPARQL is a protocol and query language for semantic web data sources.
RIF is the W3C Rule Interchange Format. 
It's an XML language for expressing Web rules that computers can execute. RIF provides multiple versions, called dialects. 
It includes a RIF Basic Logic Dialect (RIF-BLD) and RIF Production Rules Dialect (RIF PRD).
The Resource Description Framework (RDF) is a family of World Wide Web Consortium (W3C) specifications[1] originally designed as a metadata data model. 
It has come to be used as a general method for conceptual description or modeling of information that is implemented in web resources, using a variety of syntax notations and data serialization formats. It is also used in knowledge management applications.
RDF was adopted as a W3C recommendation in 1999. 
The RDF 1.0 specification was published in 2004, the RDF 1.1 specification in 2014.
The RDF data model[2] is similar to classical conceptual modeling approaches such as entity–relationship or class diagrams, as it is based upon the idea of making statements about resources (in particular web resources) in the form of subject–predicate–object expressions.
 These expressions are known as triples in RDF terminology.
 The subject denotes the resource, and the predicate denotes traits or aspects of the resource and expresses a relationship between the subject and the object. 
For example, one way to represent the notion "The sky has the color blue" in RDF is as the triple: a subject denoting "the sky", a predicate denoting "has", and an object denoting "the color blue".
 Therefore, RDF swaps object for subject that would be used in the classical notation of an entity–attribute–value model within object-oriented design; Entity (sky), attribute (color) and value (blue).
 RDF is an abstract model with several serialization formats (i.e., file formats), and so the particular way in which a resource or triple is encoded varies from format to format.
This mechanism for describing resources is a major component in the W3C's Semantic Web activity: an evolutionary stage of the World Wide Web in which automated software can store, exchange, and use machine-readable information distributed throughout the Web, in turn enabling users to deal with the information with greater efficiency and certainty. 
RDF's simple data model and ability to model disparate, abstract concepts has also led to its increasing use in knowledge management applications unrelated to Semantic Web activity.
A Java virtual machine (JVM) is an abstract computing machine that enables a computer to run a Java program.
 There are three notions of the JVM: specification, implementation, and instance. 
The specification is a document that formally describes what is required of a JVM implementation. 
Having a single specification ensures all implementations are interoperable. 
A JVM implementation is a computer program that meets the requirements of the JVM specification. 
An instance of a JVM is an implementation running in a process that executes a computer program compiled into Java bytecode.
Java Runtime Environment (JRE) is a software package that contains what is required to run a Java program. 
It includes a Java Virtual Machine implementation together with an implementation of the Java Class Library. 
The Oracle Corporation, which owns the Java trademark, distributes a Java Runtime environment with their Java Virtual Machine called HotSpot.
Java Development Kit (JDK) is a superset of a JRE and contains tools for Java programmers, e.g. a javac compiler. 
Java Development Kit is provided free of charge either by Oracle Corporation directly, or by the OpenJDK open source project, which is governed by Oracle.
The Java virtual machine is an abstract (virtual) computer defined by a specification. 
This specification omits implementation details that are not essential to ensure interoperability.
 For example, the memory layout of run-time data areas, the garbage-collection algorithm used, and any internal optimization of the Java virtual machine instructions (their translation into machine code). 
The main reason for this omission is to not unnecessarily constrain implementers. 
Any Java application can be run only inside some concrete implementation of the abstract specification of the Java virtual machine.
Starting with Java Platform, Standard Edition (J2SE) 5.0, changes to the JVM specification have been developed under the Java Community Process as JSR 924.
 As of 2006, changes to specification to support changes proposed to the class file format (JSR 202)[3] are being done as a maintenance release of JSR 924. 
The specification for the JVM was published as the blue book,[4] The preface states:
We intend that this specification should sufficiently document the Java Virtual Machine to make possible compatible clean-room implementations. 
Oracle provides tests that verify the proper operation of implementations of the Java Virtual Machine.
One of Oracle's JVMs is named HotSpot, the other, inherited from BEA Systems is JRockit. Clean-room Java implementations include Kaffe and IBM J9.
 Oracle owns the Java trademark, and may allow its use to certify implementation suites as fully compatible with Oracle's specification..
One of the organizational units of JVM byte code is a class. 
A class loader implementation must be able to recognize and load anything that conforms to the Java class file format. 
Any implementation is free to recognize other binary forms besides class files, but it must recognize class files.
The class loader performs three basic activities in this strict order:
Loading: finds and imports the binary data for a type
Linking: performs verification, preparation, and (optionally) resolution
Verification: ensures the correctness of the imported type
Preparation: allocates memory for class variables and initializing the memory to default values
Resolution: transforms symbolic references from the type into direct references.
Initialization: invokes Java code that initializes class variables to their proper starting values.
In general, there are two types of class loader: bootstrap class loader and user defined class loader.
Every Java virtual machine implementation must have a bootstrap class loader, capable of loading trusted classes. 
The Java virtual machine specification doesn't specify how a class loader should locate classes.
Bytecode instructions
Main article: Java bytecode
The JVM has instructions for the following groups of tasks:
Load and store Arithmetic Type conversion Object creation and manipulation Operand stack management (push / pop) Control transfer (branching) Method invocation and return Throwing exceptions Monitor-based concurrency
The aim is binary compatibility. 
Each particular host operating system needs its own implementation of the JVM and runtime. 
These JVMs interpret the bytecode semantically the same way, but the actual implementation may be different. 
More complex than just emulating bytecode is compatibly and efficiently implementing the Java core API that must be mapped to each host operating system.
JVM languages
Main article: List of JVM languages
A JVM language is any language with functionality that can be expressed in terms of a valid class file which can be hosted by the Java Virtual Machine.
 A class file contains Java Virtual Machine instructions (Java byte code) and a symbol table, as well as other ancillary information. 
The class file format is the hardware- and operating system-independent binary format used to represent compiled classes and interfaces.[5]
There are several JVM languages, both old languages ported to JVM and completely new languages. 
JRuby and Jython are perhaps the most well-known ports of existing languages, i.e. Ruby and Python respectively. 
Of the new languages that have been created from scratch to compile to Java bytecode, Clojure, Groovy and Scala may be the most popular ones.
 A notable feature with the JVM languages is that they are compatible with each other, so that, for example, Scala libraries can be used with Java programs and vice versa.[6]
Java 7 JVM implements JSR 292: Supporting Dynamically Typed Languages[7] on the Java Platform, a new feature which supports dynamically typed languages in the JVM. 
This feature is developed within the Da Vinci Machine project whose mission is to extend the JVM so that it supports languages other than Java.[8][9]
Bytecode verifier
Furthermore, common programmer errors that often led to data corruption or unpredictable behavior such as accessing off the end of an array or using an uninitialized pointer are not allowed to occur.
 Several features of Java combine to provide this safety, including the class model, the garbage-collected heap, and the verifier.
The JVM verifies all bytecode before it is executed.
 This verification consists primarily of three types of checks:
Branches are always to valid locations
Data is always initialized and references are always type-safe
Access to private or package private data and methods is rigidly controlled
The first two of these checks take place primarily during the verification step that occurs when a class is loaded and made eligible for use. 
The third is primarily performed dynamically, when data items or methods of a class are first accessed by another class.
The verifier permits only some bytecode sequences in valid programs, e.g. a jump (branch) instruction can only target an instruction within the same method. 
Furthermore, the verifier ensures that any given instruction operates on a fixed stack location,[10] allowing the JIT compiler to transform stack accesses into fixed register accesses. 
Because of this, that the JVM is a stack architecture does not imply a speed penalty for emulation on register-based architectures when using a JIT compiler. 
In the face of the code-verified JVM architecture, it makes no difference to a JIT compiler whether it gets named imaginary registers or imaginary stack positions that must be allocated to the target architecture's registers. 
In fact, code verification makes the JVM different from a classic stack architecture, of which efficient emulation with a JIT compiler is more complicated and typically carried out by a slower interpreter.
The original specification for the bytecode verifier used natural language that was incomplete or incorrect in some respects. 
A number of attempts have been made to specify the JVM as a formal system.
 By doing this, the security of current JVM implementations can more thoroughly be analyzed, and potential security exploits prevented. 
It will also be possible to optimize the JVM by skipping unnecessary safety checks, if the application being run is proven to be safe.[11]
Secure execution of remote code
A virtual machine architecture allows very fine-grained control over the actions that code within the machine is permitted to take. 
This is designed to allow safe execution of untrusted code from remote sources, a model used by Java applets. 
Applets run within a VM incorporated into a user's browser, executing code downloaded from a remote HTTP server.
 The remote code runs in a restricted sandbox, which is designed to protect the user from misbehaving or malicious code. 
Publishers can purchase a certificate with which to digitally sign applets as safe, giving them permission to ask the user to break out of the sandbox and access the local file system, clipboard, execute external pieces of software, or network.
Bytecode interpreter and just-in-time compiler
For each hardware architecture a different Java bytecode interpreter is needed. 
When a computer has a Java bytecode interpreter, it can run any Java bytecode program, and the same program can be run on any computer that has such an interpreter.
When Java bytecode is executed by an interpreter, the execution will always be slower than the execution of the same program compiled into native machine language. 
This problem is mitigated by just-in-time (JIT) compilers for executing Java bytecode.
A JIT compiler may translate Java bytecode into native machine language while executing the program. 
The translated parts of the program can then be executed much more quickly than they could be interpreted. 
This technique gets applied to those parts of a program frequently executed. This way a JIT compiler can significantly speed up the overall execution time.
There is no necessary connection between Java and Java bytecode.
 A program written in Java can be compiled directly into the machine language of a real computer and programs written in other languages than Java can be compiled into Java bytecode.
Java bytecode is intended to be platform-independent and secure.
Some JVM implementations do not include an interpreter, but consist only of a just-in-time compiler.[13]
JVM in the web browser
Since very early stages of the design process, Java (and JVM) has been marketed as a web technology for creating Rich Internet Applications.
Java applets
Main article: Java applet
On the client side, web browsers may be extended with a NPAPI Java plugin which executes so called Java applets embedded into HTML pages.
 The applet is allowed to draw into a rectangular region on the page assigned to it and use a restricted set of APIs that allow for example access to user's microphone or 3D acceleration. 
Java applets were superior to JavaScript both in performance and features until approximately 2011, when JavaScript engines in browsers were made significantly faster and the HTML 5 suite of web technologies started enhancing JavaScript with new APIs. 
Java applets are not able to modify the page outside its rectangular region which is not true about JavaScript. 
Adobe Flash Player, the main competing technology, works in the same way in this respect. 
Java applets are not restricted to Java and in general can be created in any JVM language.
As of April 2014, Google Chrome does not allow the use of any NPAPI plugins.
 Mozilla Firefox will also ban NPAPI plugins by the end of 2016. 
This means that Java applets can no longer be used in either browser.
As of June 2015 according to W3Techs, Java applet use had fallen to 0.1% of all web sites. Flash had fallen to 10.8% and Silverlight to 0.1% of web sites.[16]
JavaScript JVMs and interpreters
JVM implementations in JavaScript do exist, but are mostly limited to hobby projects unsuitable for production deployment or development tools to avoid having to recompile every time the developer wants to preview the changes just made.
Compilation to JavaScript
With the continuing improvements in JavaScript execution speed, combined with the increased use of mobile devices whose web browsers do not implement support for plugins, there are efforts to target those users through compilation to JavaScript.
 It is possible to either compile the source code or JVM bytecode to JavaScript. 
Compiling the JVM bytecode which is universal across JVM languages allows building upon the existing compiler to bytecode.
Main JVM bytecode to JavaScript compilers are TeaVM,[17] the compiler contained in Dragome Web SDK,[18] Bck2Brwsr,[19] and j2js-compiler.
Leading compilers from JVM languages to JavaScript include the Java to JavaScript compiler contained in Google Web Toolkit, Clojure script (Clojure), GrooScript (Groovy), Scala.js (Scala) and others.[21]
Java Runtime Environment from Oracle
Main article: HotSpot
The Java Runtime Environment (JRE) released by Oracle is a software distribution containing a stand-alone Java VM (HotSpot), browser plugin, Java standard libraries and a configuration tool. 
It is the most common Java environment installed on Windows computers. 
It is freely available for download at the website java.com.
Performance
Main article: Java performance
The JVM specification gives a lot of leeway to implementors regarding the implementation details. 
Since Java 1.3, JRE from Oracle contains a JVM called HotSpot. 
It has been designed to be a high-performance JVM.
To speed-up code execution, HotSpot relies on just-in-time compilation.
 To speed-up object allocation and garbage collection, HotSpot uses generational heap.
The Java virtual machine heap is the area of memory used by the JVM for dynamic memory allocation.
In HotSpot the heap is divided into generations:
The young generation stores short-lived objects that are created and immediately garbage collected.
Objects that persist longer are moved to the old generation (also called the tenured generation). This memory is subdivided into (two) Survivors spaces where the objects that survived the first and next garbage collections are stored.
The permanent generation (or permgen) was used for class definitions and associated metadata prior to Java 8. 
Permanent generation was not part of the heap.[23][24] The permanent generation was removed from Java 8.
Originally there was no permanent generation, and objects and classes were stored together in the same area. 
But as class unloading occurs much more rarely than objects are collected, moving class structures to a specific area allowed significant performance improvements.[23]
Security
Oracle's JRE is installed on a large number of computers. 
Since any web page the user visits may run Java applets, Java provides an easily accessible attack surface to malicious web sites that the user visits. 
Kaspersky Labs reports that the Java web browser plugin is the method of choice for computer criminals. 
Java exploits are included in many exploit packs that hackers deploy onto hacked web sites.
In the past, end users were often using an out-of-date version of JRE which was vulnerable to many known attacks.
 This led to the widely shared belief between users that Java is inherently insecure.
Since Java 1.7, Oracle's JRE for Windows includes automatic update functionality.
Toolbar controversy
Beginning in 2005, Sun's (now Oracle's) JRE included unrelated software which was installed by default. 
In the beginning it was Google Toolbar, later MSN Toolbar, Yahoo Toolbar and finally the Ask Toolbar. 
The Ask Toolbar proved to be especially controversial. 
There has been a petition asking Oracle to remove it.
 The signers voiced their belief that Oracle was "violating the trust of the hundreds of millions of users who run Java on their machines.
 They are tarnishing the reputation of a once proud platform".
 Zdnet called their conduct deceptive, since the installer continued to offer the toolbar during every update, even after the user had previously refused to install it, increasing the chances of the toolbar being installed when the user was too busy or distracted.
In June 2015, Oracle announced that it had ended its partnership with Ask.com in favor of one with Yahoo!, in which users will be, by default, asked to change their home page and default search engine to that of Yahoo.
Cloud computing, also known as on-demand computing, is a kind of Internet-based computing that provides shared processing resources and data to computers and other devices on-demand. 
It is a model for enabling ubiquitous, on-demand access to a shared pool of configurable computing resources.
 Cloud computing and storage solutions provide users and enterprises with various capabilities to store and process their data in third-party data centers.
 It relies on sharing of resources to achieve coherence and economies of scale, similar to a utility (like the electricity grid) over a network. 
At the foundation of cloud computing is the broader concept of converged infrastructure and shared services.
Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications and services) that can be rapidly provisioned and released with minimal management effort.
Proponents claim that cloud computing allows companies to avoid upfront infrastructure costs, and focus on projects that differentiate their businesses instead of on infrastructure.
 Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and enables IT to more rapidly adjust resources to meet fluctuating and unpredictable business demand.
 Cloud providers typically use a "pay as you go" model.
 This can lead to unexpectedly high charges if administrators do not adapt to the cloud pricing model.
The present availability of high-capacity networks, low-cost computers and storage devices as well as the widespread adoption of hardware virtualization, service-oriented architecture, and autonomic and utility computing have led to a growth in cloud computing.
 Companies can scale up as computing needs increase and then scale down again as demands decrease.
Cloud computing has become a highly demanded service or utility due to the advantages of high computing power, cheap cost of services, high performance, scalability, accessibility as well as availability. 
Some cloud vendors are experiencing growth rates of 50% per annum.
 But due to being in a stage of infancy, it still has pitfalls that need proper attention to make cloud computing services more reliable and user friendly
The origin of the term cloud computing is unclear. 
The word "cloud" is commonly used in science to describe a large agglomeration of objects that visually appear from a distance as a cloud and describes any set of things whose details are not inspected further in a given context.
Another explanation is that the old programs that drew network schematics surrounded the icons for servers with a circle, and a cluster of servers in a network diagram had several overlapping circles, which resembled a cloud.[15]
In analogy to above usage the word cloud was used as a metaphor for the Internet and a standardized cloud-like shape was used to denote a network on telephony schematics and later to depict the Internet in computer network diagrams. 
With this simplification, the implication is that the specifics of how the end points of a network are connected are not relevant for the purposes of understanding the diagram. 
The cloud symbol was used to represent networks of computing equipment in the original ARPANET by as early as 1977,[16] and the CSNET by 1981[17]—both predecessors to the Internet itself.
The term cloud has been used to refer to platforms for distributed computing.
 In Wired's April 1994 feature "Bill and Andy's Excellent Adventure II" on the Apple spin-off General Magic, Andy Hertzfeld comments on General Magic's distributed programming language Telescript that:
"The beauty of Telescript ... is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create sort of a virtual service. 
No one had conceived that before. 
The example Jim White [the designer of Telescript, X.400 and ASN.1] uses now is a date-arranging service where a software agent goes to the flower store and orders flowers and then goes to the ticket shop and gets the tickets for the show, and everything is communicated to both parties."
References to "cloud computing" in its modern sense appeared as early as 1996, with the earliest known mention in a Compaq internal document.
The popularization of the term can be traced to 2006 when Amazon.com introduced the Elastic Compute Cloud.
During the mid-1970s, Time-sharing was popularly known as RJE (Remote Job Entry);[citation needed] this terminology was mostly associated with large vendors such as IBM and DEC.
IBM developed the VM Operating System (first released in 1972) to provide time-sharing services[citation needed] via virtual machines.
The 1990s
In the 1990s, telecommunications companies, who previously offered primarily dedicated point-to-point data circuits, began offering virtual private network (VPN) services with comparable quality of service, but at a lower cost. 
By switching traffic as they saw fit to balance server use, they could use overall network bandwidth more effectively.
 They began to use the cloud symbol to denote the demarcation point between what the provider was responsible for and what users were responsible for.
 Cloud computing extends this boundary to cover all servers as well as the network infrastructure.[21]
As computers became more prevalent, scientists and technologists explored ways to make large-scale computing power available to more users through time-sharing.
 They experimented with algorithms to optimize the infrastructure, platform, and applications to prioritize CPUs and increase efficiency for end users.
The New Millennium: 2000s
Since 2000, cloud computing has come into existence. 
In early 2008, NASA's OpenNebula, enhanced in the RESERVOIR European Commission-funded project, became the first open-source software for deploying private and hybrid clouds, and for the federation of clouds.
 In the same year, efforts were focused on providing quality of service guarantees (as required by real-time interactive applications) to cloud-based infrastructures, in the framework of the IRMOS European Commission-funded project, resulting in a real-time cloud environment.
 Microsoft Azure was announced as "Azure" in October 2008 and released on 1 February 2010 as Windows Azure, before being renamed to Microsoft Azure on 25 March 2014.
For a time, Azure was on the TOP500 supercomputer list, before it dropped off it.
In July 2010, Rackspace Hosting and NASA jointly launched an open-source cloud-software initiative known as OpenStack. 
The OpenStack project intended to help organizations offer cloud-computing services running on standard hardware.
 The early code came from NASA's Nebula platform as well as from Rackspace's Cloud Files platform.
On March 1, 2011, IBM announced the IBM SmartCloud framework to support Smarter Planet.
 Among the various components of the Smarter Computing foundation, cloud computing is a critical piece.
On June 7, 2012, Oracle announced the Oracle Cloud.
 While aspects of the Oracle Cloud are still in development, this cloud offering is poised to be the first to provide users with access to an integrated set of IT solutions, including the Applications (SaaS), Platform (PaaS), and Infrastructure (IaaS) layers.
Similar concepts
Cloud computing is the result of the evolution and adoption of existing technologies and paradigms. 
The goal of cloud computing is to allow users to take beneﬁt from all of these technologies, without the need for deep knowledge about or expertise with each one of them. 
The cloud aims to cut costs, and helps the users focus on their core business instead of being impeded by IT obstacles.
The main enabling technology for cloud computing is virtualization. 
Virtualization software separates a physical computing device into one or more "virtual" devices, each of which can be easily used and managed to perform computing tasks.
 With operating system–level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. 
Virtualization provides the agility required to speed up IT operations, and reduces cost by increasing infrastructure utilization. 
Autonomic computing automates the process through which the user can provision resources on-demand.
 By minimizing user involvement, automation speeds up the process, reduces labor costs and reduces the possibility of human errors.
Users routinely face difficult business problems. 
Cloud computing adopts concepts from Service-oriented Architecture (SOA) that can help the user break these problems into services that can be integrated to provide a solution.
 Cloud computing provides all of its resources as services, and makes use of the well-established standards and best practices gained in the domain of SOA to allow global and easy access to cloud services in a standardized way.
Cloud computing also leverages concepts from utility computing to provide metrics for the services used. 
Such metrics are at the core of the public cloud pay-per-use models.
 In addition, measured services are an essential part of the feedback loop in autonomic computing, allowing services to scale on-demand and to perform automatic failure recovery.
Cloud computing is a kind of grid computing; it has evolved by addressing the QoS (quality of service) and reliability problems. 
Cloud computing provides the tools and technologies to build data/compute intensive parallel applications with much more affordable prices compared to traditional parallel computing techniques.
Cloud computing shares characteristics with:
Client–server model—Client–server computing refers broadly to any distributed application that distinguishes between service providers (servers) and service requestors (clients).
Grid computing—"A form of distributed and parallel computing, whereby a 'super and virtual computer' is composed of a cluster of networked, loosely coupled computers acting in concert to perform very large tasks."
Fog computing—Distributed computing paradigm that provides data, compute, storage and application services closer to client or near-user edge devices, such as network routers.
 Furthermore, fog computing handles data at the network level, on smart devices and on the end-user client side (e.g. mobile devices), instead of sending data to a remote location for processing.
Dew computing—In the existing computing hierarchy, the Dew computing is positioned as the ground level for the cloud and fog computing paradigms. 
Compared to fog computing, which supports emerging IoT applications that demand real-time and predictable latency and the dynamic network reconfigurability, Dew computing pushes the frontiers to computing applications, data, and low level services away from centralized virtual nodes to the end users.
Mainframe computer—Powerful computers used mainly by large organizations for critical applications, typically bulk data processing such as: census; industry and consumer statistics; police and secret intelligence services; enterprise resource planning; and financial transaction processing.
Utility computing—The "packaging of computing resources, such as computation and storage, as a metered service similar to a traditional public utility, such as electricity."
Peer-to-peer—A distributed architecture without the need for central coordination. 
Participants are both suppliers and consumers of resources (in contrast to the traditional client–server model).
Characteristics
Cloud computing exhibits the following key characteristics:

Agility improves with users' ability to re-provision technological infrastructure resources.[wtf?]
Cost reductions claimed by cloud providers.
A public-cloud delivery model converts capital expenditure to operational expenditure.
 This purportedly lowers barriers to entry, as infrastructure is typically provided by a third party and need not be purchased for one-time or infrequent intensive computing tasks. 
Pricing on a utility computing basis is fine-grained, with usage-based options and fewer IT skills are required for implementation (in-house).
The e-FISCAL project's state-of-the-art repository[42] contains several articles looking into cost aspects in more detail, most of them concluding that costs savings depend on the type of activities supported and the type of infrastructure available in-house.
Device and location independence[43] enable users to access systems using a web browser regardless of their location or what device they use (e.g., PC, mobile phone). 
As infrastructure is off-site (typically provided by a third-party) and accessed via the Internet, users can connect from anywhere.[41]
Maintenance of cloud computing applications is easier, because they do not need to be installed on each user's computer and can be accessed from different places.
Multitenancy enables sharing of resources and costs across a large pool of users thus allowing for:
centralization of infrastructure in locations with lower costs (such as real estate, electricity, etc.)
peak-load capacity increases (users need not engineer for highest possible load-levels)
utilisation and efficiency improvements for systems that are often only 10–20% utilised.[44][45]
Performance is monitored, and consistent and loosely coupled architectures are constructed using web services as the system interface.[41][46][47]
Productivity may be increased when multiple users can work on the same data simultaneously, rather than waiting for it to be saved and emailed. Time may be saved as information does not need to be re-entered when fields are matched, nor do users need to install application software upgrades to their computer.
Reliability improves with the use of multiple redundant sites, which makes well-designed cloud computing suitable for business continuity and disaster recovery.
Scalability and elasticity via dynamic ("on-demand") provisioning of resources on a fine-grained, self-service basis in near real-time[50][51] (Note, the VM startup time varies by VM type, location, OS and cloud providers[50]), without users having to engineer for peak loads.
 This gives the ability to scale up when the usage need increases or down if resources are not being used.
Security can improve due to centralization of data, increased security-focused resources, etc., but concerns can persist about loss of control over certain sensitive data, and the lack of security for stored kernels. 
In addition, user access to security audit logs may be difficult or impossible. 
Private cloud installations are in part motivated by users' desire to retain control over the infrastructure and avoid losing control of information security.
The National Institute of Standards and Technology's definition of cloud computing identifies "five essential characteristics":
On-demand self-service. 
A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider.
Broad network access.
Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations).
Resource pooling. 
The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand. 
Rapid elasticity.
 Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. 
To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time.
Measured service. 
Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). 
Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service.
— National Institute of Standards and Technology[57]
Service models
Though service-oriented architecture advocates "everything as a service" (with the acronyms EaaS or XaaS or simply aas),[58] cloud-computing providers offer their "services" according to different models,need quotation to verify] which happen to form a stack: infrastructure-, platform- and software-as-a-service.
Cloud-computing layers accessible within a stack
Infrastructure as a service (IaaS)
See also: Category:Cloud infrastructure
In the most basic cloud-service model—and according to the IETF (Internet Engineering Task Force)—providers of IaaS offer computers—physical or (more often) virtual machines—and other resources. 
IaaS refers to online services that abstract user from the detail of infrastructure like physical computing resources, location, data partitioning, scaling, security, backup etc.
 A hypervisor, such as Xen, Oracle VirtualBox, KVM, VMware ESX/ESXi, or Hyper-V runs the virtual machines as guests.
 Pools of hypervisors within the cloud operational system can support large numbers of virtual machines and the ability to scale services up and down according to customers' varying requirements. 
IaaS clouds often offer additional resources such as a virtual-machine disk-image library, raw block storage, file or object storage, firewalls, load balancers, IP addresses, virtual local area networks (VLANs), and software bundles.
 IaaS-cloud providers supply these resources on-demand from their large pools of equipment installed in data centers. 
For wide-area connectivity, customers can use either the Internet or carrier clouds (dedicated virtual private networks).
To deploy their applications, cloud users install operating-system images and their application software on the cloud infrastructure.
[unreliable source?] In this model, the cloud user patches and maintains the operating systems and the application software. 
Cloud providers typically bill IaaS services on a utility computing basis: cost reflects the amount of resources allocated and consumed.
Platform as a service (PaaS)
Main article: Platform as a service
See also: Category:Cloud platforms
PaaS vendors offer a development environment to application developers.
 The provider typically develops toolkit and standards for development and channels for distribution and payment. 
In the PaaS models, cloud providers deliver a computing platform, typically including operating system, programming-language execution environment, database, and web server. 
Application developers can develop and run their software solutions on a cloud platform without the cost and complexity of buying and managing the underlying hardware and software layers. 
With some PaaS offers like Microsoft Azure and Google App Engine, the underlying computer and storage resources scale automatically to match application demand so that the cloud user does not have to allocate resources manually. 
The latter has also been proposed by an architecture aiming to facilitate real-time in cloud environments.
need quotation to verify] Even more specific application types can be provided via PaaS, such as media encoding as provided by services like bitcodin.com or media.io.
Some integration and data management providers have also embraced specialized applications of PaaS as delivery models for data solutions. 
Examples include iPaaS and dPaaS. iPaaS (Integration Platform as a Service) enables customers to develop, execute and govern integration flows.
 Under the iPaaS integration model, customers drive the development and deployment of integrations without installing or managing any hardware or middleware.
 dPaaS (Data Platform as a Service) delivers integration—and data-management—products as a fully managed service.
Under the dPaaS model, the PaaS provider, not the customer, manages the development and execution of data solutions by building tailored data applications for the customer.
 dPaaS users retain transparency and control over data through data-visualization tools.
Software as a service (SaaS)
Main article: Software as a service
In the software as a service (SaaS) model, users gain access to application software and databases.
 Cloud providers manage the infrastructure and platforms that run the applications. 
SaaS is sometimes referred to as "on-demand software" and is usually priced on a pay-per-use basis or using a subscription fee.
In the SaaS model, cloud providers install and operate application software in the cloud and cloud users access the software from cloud clients.
 Cloud users do not manage the cloud infrastructure and platform where the application runs. 
This eliminates the need to install and run the application on the cloud user's own computers, which simplifies maintenance and support. 
Cloud applications differ from other applications in their scalability—which can be achieved by cloning tasks onto multiple virtual machines at run-time to meet changing work demand.
 Load balancers distribute the work over the set of virtual machines.
 This process is transparent to the cloud user, who sees only a single access-point. 
To accommodate a large number of cloud users, cloud applications can be multitenant, meaning that any machine may serve more than one cloud-user organization.
The pricing model for SaaS applications is typically a monthly or yearly flat fee per user,[75] so prices become scalable and adjustable if users are added or removed at any point.
Proponents claim that SaaS gives a business the potential to reduce IT operational costs by outsourcing hardware and software maintenance and support to the cloud provider. 
This enables the business to reallocate IT operations costs away from hardware/software spending and from personnel expenses, towards meeting other goals. 
In addition, with applications hosted centrally, updates can be released without the need for users to install new software. 
One drawback of SaaS comes with storing the users' data on the cloud provider's server. 
As a result,[citation needed] there could be unauthorized access to the data. 
For this reason, users are increasingly[quantify] adopting intelligent third-party key-management systems to help secure their data.
Cloud clients
See also: Category:Cloud clients and Cloud API
Users access cloud computing using networked client devices, such as desktop computers, laptops, tablets and smartphones and any Ethernet enabled device such as Home Automation Gadgets.
 Some of these devices—cloud clients—rely on cloud computing for all or a majority of their applications so as to be essentially useless without it. 
Examples are thin clients and the browser-based Chromebook.
 Many cloud applications do not require specific software on the client and instead use a web browser to interact with the cloud application.
 With Ajax and HTML5 these Web user interfaces can achieve a similar, or even better, look and feel to native applications. 
Some cloud applications, however, support specific client software dedicated to these applications (e.g., virtual desktop clients and most email clients).
 Some legacy applications (line of business applications that until now have been prevalent in thin client computing) are delivered via a screen-sharing technology.
Private cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third-party, and hosted either internally or externally.
Undertaking a private cloud project requires a significant level and degree of engagement to virtualize the business environment, and requires the organization to reevaluate decisions about existing resources. 
When done right, it can improve business, but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities. 
Self-run data centers[77] are generally capital intensive. 
They have a significant physical footprint, requiring allocations of space, hardware, and environmental controls. 
These assets have to be refreshed periodically, resulting in additional capital expenditures. 
They have attracted criticism because users "still have to buy, build, and manage them" and thus do not benefit from less hands-on management,[78] essentially "[lacking] the economic model that makes cloud computing such an intriguing concept".
Public cloud
A cloud is called a "public cloud" when the services are rendered over a network that is open for public use. 
Public cloud services may be free.
 Generally, public cloud service providers like Amazon AWS, Microsoft and Google own and operate the infrastructure at their data center and access is generally via the Internet. 
AWS and Microsoft also offer direct connect services called "AWS Direct Connect" and "Azure ExpressRoute" respectively, such connections require customers to purchase or lease a private connection to a peering point offered by the cloud provider.
Hybrid cloud
Hybrid cloud is a composition of two or more clouds (private, community or public) that remain distinct entities but are bound together, offering the benefits of multiple deployment models.
 Hybrid cloud can also mean the ability to connect collocation, managed and/or dedicated services with cloud resources.
Gartner, Inc. defines a hybrid cloud service as a cloud computing service that is composed of some combination of private, public and community cloud services, from different service providers.
 A hybrid cloud service crosses isolation and provider boundaries so that it can't be simply put in one category of private, public, or community cloud service. 
It allows one to extend either the capacity or the capability of a cloud service, by aggregation, integration or customization with another cloud service.
Varied use cases for hybrid cloud composition exist. 
For example, an organization may store sensitive client data in house on a private cloud application, but interconnect that application to a business intelligence application provided on a public cloud as a software service.
 This example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services. 
Hybrid cloud adoption depends on a number of factors such as data security and compliance requirements, level of control needed over data, and the applications an organization uses.
Another example of hybrid cloud is one where IT organizations use public cloud computing resources to meet temporary capacity needs that can not be met by the private cloud.
This capability enables hybrid clouds to employ cloud bursting for scaling across clouds.
 Cloud bursting is an application deployment model in which an application runs in a private cloud or data center and "bursts" to a public cloud when the demand for computing capacity increases.
 A primary advantage of cloud bursting and a hybrid cloud model is that an organization only pays for extra compute resources when they are needed.
Cloud bursting enables data centers to create an in-house IT infrastructure that supports average workloads, and use cloud resources from public or private clouds, during spikes in processing demands.
The specialized model of hybrid cloud, which is built atop heterogeneous hardware, is called "Cross-platform Hybrid Cloud".
 A cross-platform hybrid cloud is usually powered by different CPU architectures, for example, x86-64 and ARM, underneath. 
Users can transparently deploy and scale applications without knowledge of the cloud's hardware diversity.
 This kind of cloud emerges from the raise of ARM-based system-on-chip for server-class computing.
Others
Community cloud
Community cloud shares infrastructure between several organizations from a specific community with common concerns (security, compliance, jurisdiction, etc.), whether managed internally or by a third-party, and either hosted internally or externally. 
The costs are spread over fewer users than a public cloud (but more than a private cloud), so only some of the cost savings potential of cloud computing are realized.
Distributed cloud
A cloud computing platform can be assembled from a distributed set of machines in different locations, connected to a single network or hub service. 
It is possible to distinguish between two types of distributed clouds: public-resource computing and volunteer cloud.
Public-resource computing—This type of distributed cloud results from an expansive definition of cloud computing, because they are more akin to distributed computing than cloud computing. 
Nonetheless, it is considered a sub-class of cloud computing, and some examples include distributed computing platforms such as BOINC and Folding@Home.
Volunteer cloud—Volunteer cloud computing is characterized as the intersection of public-resource computing and cloud computing, where a cloud computing infrastructure is built using volunteered resources. 
Many challenges arise from this type of infrastructure, because of the volatility of the resources used to built it and the dynamic environment it operates in. 
It can also be called peer-to-peer clouds, or ad-hoc clouds. 
An interesting effort in such direction is Cloud@Home, it aims to implement a cloud computing infrastructure using volunteered resources providing a business-model to incentivize contributions through financial restitution.
Intercloud
Main article: Intercloud
The Intercloud[90] is an interconnected global "cloud of clouds"[91][92] and an extension of the Internet "network of networks" on which it is based. 
The focus is on direct interoperability between public cloud service providers, more so than between providers and consumers (as is the case for hybrid- and multi-cloud).
Multicloud
Main article: Multicloud
Multicloud is the use of multiple cloud computing services in a single heterogeneous architecture to reduce reliance on single vendors, increase flexibility through choice, mitigate against disasters, etc. 
It differs from hybrid cloud in that it refers to multiple cloud services, rather than multiple deployment modes (public, private, legacy).
Architecture
Cloud computing sample architecture
Cloud architecture,[99] the systems architecture of the software systems involved in the delivery of cloud computing, typically involves multiple cloud components communicating with each other over a loose coupling mechanism such as a messaging queue.
 Elastic provision implies intelligence in the use of tight or loose coupling as applied to mechanisms such as these and others.
Cloud engineering
Cloud engineering is the application of engineering disciplines to cloud computing. 
It brings a systematic approach to the high-level concerns of commercialization, standardization, and governance in conceiving, developing, operating and maintaining cloud computing systems.
 It is a multidisciplinary method encompassing contributions from diverse areas such as systems, software, web, performance, information, security, platform, risk, and quality engineering.
Security and privacy
Main article: Cloud computing issues
Cloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time. 
It could accidentally or deliberately alter or even delete information.
Many cloud providers can share information with third parties if necessary for purposes of law and order even without a warrant.
That is permitted in their privacy policies, which users must agree to before they start using cloud services.
 Solutions to privacy include policy and legislation as well as end users' choices for how data is stored.
 Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access.

According to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and API's, Data Loss & Leakage, and Hardware Failure—which accounted for 29%, 25% and 10% of all cloud security outages respectively. 
Together, these form shared technology vulnerabilities. 
In a cloud provider platform being shared by different users there may be a possibility that information belonging to different customers resides on same data server. 
Therefore, Information leakage may arise by mistake when information for one customer is given to other.
Additionally, Eugene Schultz, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. 
"There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into". 
Because data from hundreds or thousands of companies can be stored on large cloud servers, hackers can theoretically gain control of huge stores of information through a single attack—a process he called "hyperjacking". 
Some examples of this include the Dropbox security breach, and iCloud 2014 leak.
 Dropbox had been breached in October 2014, having over 7 million of its users passwords stolen by hackers in an effort to get monetary value from it by Bitcoins (BTC). 
By having these passwords, they are able to read private data as well as have this data be indexed by search engines (making the information public).[102]
There is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?).
 Many Terms of Service agreements are silent on the question of ownership.
Physical control of the computer equipment (private cloud) is more secure than having the equipment off site and under someone else's control (public cloud).
 This delivers great incentive to public cloud computing service providers to prioritize building and maintaining strong management of secure services.
Some small businesses that don't have expertise in IT security could find that it's more secure for them to use a public cloud.
There is the risk that end users don't understand the issues involved when signing on to a cloud service (persons sometimes don't read the many pages of the terms of service agreement, and just click "Accept" without reading).
 This is important now that cloud computing is becoming popular and required for some services to work, for example for an intelligent personal assistant (Apple's Siri or Google Now).
Fundamentally private cloud is seen as more secure with higher levels of control for the owner, however public cloud is seen to be more flexible and requires less time and money investment from the user.[105]
The future
Cloud computing is therefore still as much a research topic, as it is a market offering.
 What is clear through the evolution of cloud computing services is that the chief technical officer (CTO) is a major driving force behind cloud adoption.
 The major cloud technology developers continue to invest billions a year in cloud R&D; for example: in 2011 Microsoft committed 90% of its US$9.6bn R&D budget to its cloud.
 Centaur Partners also predict that SaaS revenue will grow from US$13.5B in 2011 to $32.8B in 2016.
This expansion also includes Finance and Accounting SaaS.
Additionally, more industries are turning to cloud technology as an efficient way to improve quality services due to its capabilities to reduce overhead costs, downtime, and automate infrastructure deployment.
A service-oriented architecture (SOA) is an architectural pattern in computer software design in which application components provide services to other components via a communications protocol, typically over a network. 
The principles of service-orientation are independent of any vendor, product or technology.
A service is a self-contained unit of functionality, such as retrieving an online bank statement.
 By that definition, a service is an operation that may be discretely invoked. 
However, in the Web Services Description Language (WSDL), a service is an interface definition that may list several discrete services/operations. 
And elsewhere, the term service is used for a component that is encapsulated behind an interface. This widespread ambiguity is reflected in what follows.
Services can be combined to provide the functionality of a large software application.
 SOA makes it easier for software components on computers connected over a network to cooperate. 
Every computer can run any number of services, and each service is built in a way that ensures that the service can exchange information with any other service in the network without human interaction and without the need to make changes to the underlying program itself.
The OASIS group[4] and the Open Group[5] have both created formal definitions. 
OASIS defines SOA as:
A paradigm for organizing and utilizing distributed capabilities that may be under the control of different ownership domains.
 It provides a uniform means to offer, discover, interact with and use capabilities to produce desired effects consistent with measurable preconditions and expectations.
The Open Group's definition is:
Service-Oriented Architecture (SOA) is an architectural style that supports service-orientation. 
Service-orientation is a way of thinking in terms of services and service-based development and the outcomes of services.
A service:
Is a logical representation of a repeatable business activity that has a specified outcome (e.g., check customer credit, provide weather data, consolidate drilling reports)
Is self-contained
May be composed of other services
Is a "black box" to consumers of the service
Overview
Services are unassociated, loosely coupled units of functionality that are self-contained. 
Each service implements at least one action, such as submitting an online application for an account, retrieving an online bank statement or modifying an online booking or airline ticket order. 
Within a SOA, services use defined protocols that describe how services pass and parse messages using description metadata, which in sufficient details describes not only the characteristics of these services, but also the data that drives them.
Programmers have made extensive use of XML in SOA to structure data that they wrap in a nearly exhaustive description-container.
 Analogously, the Web Services Description Language (WSDL) typically describes the services themselves, while SOAP (originally Simple Object Access Protocol) describes the communications protocols. 
SOA depends on data and services that are described by metadata that should meet the following two criteria:
The metadata should be provided in a form that software systems can use to configure dynamically by discovery and incorporation of defined services, and also to maintain coherence and integrity. 
For example, metadata could be used by other applications, like a catalogue, to perform auto discovery of services without modifying the functional contract of a service.
The metadata should be provided in a form that system designers can understand and manage with a reasonable expenditure of cost and effort.
The purpose of SOA is to allow users to combine fairly large chunks of functionality to form ad hoc applications built almost entirely from existing software services.
 The larger the chunks, the fewer the interfaces required to implement any given set of functionality; however, very large chunks of functionality may not prove sufficiently granular for easy reuse. 
Each interface brings with it some amount of processing overhead, so there is a performance consideration in choosing the granularity of services.
SOA as an architecture relies on service-orientation as its fundamental design principle.
 If a service presents a simple interface that abstracts away its underlying complexity, then users can access independent services without knowledge of the service's platform implementation.[6]
SOA framework
SOA-based solutions endeavour to enable business objectives while building an enterprise-quality system. 
SOA architecture is viewed as five horizontal layers:[7]
Consumer Interface Layer – These are GUI for end users or apps accessing apps/service interfaces.
Business Process Layer – These are choreographed services representing business use-cases in terms of applications.
Services – Services are consolidated together for whole-enterprise in-service inventory.
Service Components – The components used to build the services, such as functional and technical libraries, technological interfaces etc.
Operational Systems – This layer contains the data models, enterprise data repository, technological platforms etc.
There are four cross-cutting vertical layers, each of which are applied to and supported by each of the following horizontal layers:
Integration Layer – starts with platform integration (protocols support), data integration, service integration, application integration, leading to enterprise application integration supporting B2B and B2C.
Quality of Service – Security, availability, performance etc. 
constitute the quality of service parameters which are configured based on required SLAs, OLAs.
Informational – provide business information.
Governance – IT strategy is governed to each horizontal layer to achieve required operating and capability model.
Design concept
SOA is based on the concept of a service. 
Depending on the service design approach taken, each SOA service is designed to perform one or more activities by implementing one or more service operations.
 As a result, each service is built as a discrete piece of code.
 This makes it possible to reuse the code in different ways throughout the application by changing only the way an individual service interoperates with other services that make up the application, versus making code changes to the service itself. 
SOA design principles are used during software development and integration.
SOA generally provides a way for consumers of services, such as web-based applications, to be aware of available SOA-based services. 
For example, several disparate departments within a company may develop and deploy SOA services in different implementation languages; their respective clients will benefit from a well-defined interface to access them.
SOA defines how to integrate widely disparate applications for a Web-based environment and uses multiple implementation platforms.
 Rather than defining an API, SOA defines the interface in terms of protocols and functionality. 
An endpoint is the entry point for such a SOA implementation.
Service-orientation requires loose coupling of services with operating systems and other technologies that underlie applications. 
SOA separates functions into distinct units, or services,[8] which developers make accessible over a network in order to allow users to combine and reuse them in the production of applications. 
These services and their corresponding consumers communicate with each other by passing data in a well-defined, shared format, or by coordinating an activity between two or more services.[9]
For some, SOA can be seen as part of the continuum which ranges from the older concept of distributed computing[8][10] and modular programming, through SOA, and on to current practices of mashups, SaaS, and cloud computing (which some see as the offspring of SOA).
There are no industry standards relating to the exact composition of a service-oriented architecture, although many industry sources have published their own principles. 
Some of these include the following:
Standardized service contract: Services adhere to a communications agreement, as defined collectively by one or more service-description documents.
Service loose coupling: Services maintain a relationship that minimizes dependencies and only requires that they maintain an awareness of each other.
Service abstraction: Beyond descriptions in the service contract, services hide logic from the outside world.
Service reusability: Logic is divided into services with the intention of promoting reuse.
Service autonomy: Services have control over the logic they encapsulate, from a Design-time and a Run-time perspective.
Service statelessness: Services minimize resource consumption by deferring the management of state information when necessary[16]
Service discoverability: Services are supplemented with communicative meta data by which they can be effectively discovered and interpreted.
Service composability: Services are effective composition participants, regardless of the size and complexity of the composition.
Service granularity: A design consideration to provide optimal scope and right granular level of the business functionality in a service operation.
Service normalization: Services are decomposed or consolidated to a level of normal form to minimize redundancy. 
In some cases, services are denormalized for specific purposes, such as performance optimization, access, and aggregation.[17]
Service optimization: All else being equal, high-quality services are generally preferable to low-quality ones.
Service relevance: Functionality is presented at a granularity recognized by the user as a meaningful service.
Service encapsulation: Many services are consolidated for use under the SOA. 
Often such services were not planned to be under SOA.
Service location transparency: This refers to the ability of a service consumer to invoke a service regardless of its actual location in the network. 
This also recognizes the discoverability property (one of the core principle of SOA) and the right of a consumer to access the service. 
Often, the idea of service virtualization also relates to location transparency. 
This is where the consumer simply calls a logical service while a suitable SOA-enabling runtime infrastructure component, commonly a service bus, maps this logical service call to a physical service.
Service architecture
This is the physical design of an individual service that encompasses all the resources used by a service.
 This would normally include databases, software components, legacy systems, identity stores, XML schemas and any backing stores, e.g. shared directories. 
It is also beneficial to include any service agents employed by the service, as any change in these service agents would affect the message processing capabilities of the service.
The (standardized service contract) design principle keeps service contracts independent from their implementation. 
The service contract needs to be documented to formalize the required processing resources by the individual service capabilities. 
Although it is beneficial to document details about the service architecture, the service abstraction design principle dictates that any internal details about the service are invisible to its consumers so that they do not develop any unstated couplings.
 The service architecture serves as a point of reference for evolving the service or gauging the impact of any change in the service.
Service composition architecture
One of the core characteristics of services developed using the service-orientation design paradigm is that they are composition-centric. 
Services with this characteristic can potentially address novel requirements by recomposing the same services in different configurations. 
Service composition architecture is itself a composition of the individual architectures of the participating services. 
In the light of the Service Abstraction principle, this type of architecture only documents the service contract and any published service-level agreement (SLA); internal details of each service are not included.
If a service composition is a part of another (parent) composition, the parent composition can also be referenced in the child service composition. 
The design of service composition also includes any alternate paths, such as error conditions, which may introduce new services into the current service composition.
Service composition is also a key technique in software integration, including enterprise software integration, business process composition and workflow composition.
Service inventory architecture
A service inventory is composed of services that automate business processes. 
It is important to account for the combined processing requirements of all services within the service inventory.
 Documenting the requirements of services, independently from the business processes that they automate, helps identify processing bottlenecks.
 The service inventory architecture is documented from the service inventory blueprint, so that service candidates[18] can be redesigned before their implementation.
Service-oriented enterprise architecture